[
    {
        "title": "CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in City-scale Scenarios",
        "summary": "Semantic segmentation of city-scale point clouds is a critical technology for\nUnmanned Aerial Vehicle (UAV) perception systems, enabling the classification\nof 3D points without relying on any visual information to achieve comprehensive\n3D understanding. However, existing models are frequently constrained by the\nlimited scale of 3D data and the domain gap between datasets, which lead to\nreduced generalization capability. To address these challenges, we propose\nCitySeg, a foundation model for city-scale point cloud semantic segmentation\nthat incorporates text modality to achieve open vocabulary segmentation and\nzero-shot inference. Specifically, in order to mitigate the issue of\nnon-uniform data distribution across multiple domains, we customize the data\npreprocessing rules, and propose a local-global cross-attention network to\nenhance the perception capabilities of point networks in UAV scenarios. To\nresolve semantic label discrepancies across datasets, we introduce a\nhierarchical classification strategy. A hierarchical graph established\naccording to the data annotation rules consolidates the data labels, and the\ngraph encoder is used to model the hierarchical relationships between\ncategories. In addition, we propose a two-stage training strategy and employ\nhinge loss to increase the feature separability of subcategories. Experimental\nresults demonstrate that the proposed CitySeg achieves state-of-the-art (SOTA)\nperformance on nine closed-set benchmarks, significantly outperforming existing\napproaches. Moreover, for the first time, CitySeg enables zero-shot\ngeneralization in city-scale point cloud scenarios without relying on visual\ninformation.",
        "url": "http://arxiv.org/abs/2508.09470v1",
        "published_date": "2025-08-13T03:55:56+00:00",
        "updated_date": "2025-08-13T03:55:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jialei Xu",
            "Zizhuang Wei",
            "Weikang You",
            "Linyun Li",
            "Weijian Sun"
        ],
        "tldr": "CitySeg, a foundation model for city-scale point cloud semantic segmentation, utilizes text modality for open vocabulary segmentation and zero-shot inference, achieving SOTA performance on multiple benchmarks.",
        "tldr_zh": "CitySeg是一个用于城市规模点云语义分割的基础模型，它利用文本模态实现开放词汇分割和零样本推理，并在多个基准测试中实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving",
        "summary": "Large-scale high-quality 3D motion datasets with multi-person interactions\nare crucial for data-driven models in autonomous driving to achieve\nfine-grained pedestrian interaction understanding in dynamic urban\nenvironments. However, existing datasets mostly rely on estimating 3D poses\nfrom monocular RGB video frames, which suffer from occlusion and lack of\ntemporal continuity, thus resulting in unrealistic and low-quality human\nmotion. In this paper, we introduce Waymo-3DSkelMo, the first large-scale\ndataset providing high-quality, temporally coherent 3D skeletal motions with\nexplicit interaction semantics, derived from the Waymo Perception dataset. Our\nkey insight is to utilize 3D human body shape and motion priors to enhance the\nquality of the 3D pose sequences extracted from the raw LiDRA point clouds. The\ndataset covers over 14,000 seconds across more than 800 real driving scenarios,\nincluding rich interactions among an average of 27 agents per scene (with up to\n250 agents in the largest scene). Furthermore, we establish 3D pose forecasting\nbenchmarks under varying pedestrian densities, and the results demonstrate its\nvalue as a foundational resource for future research on fine-grained human\nbehavior understanding in complex urban environments. The dataset and code will\nbe available at https://github.com/GuangxunZhu/Waymo-3DSkelMo",
        "url": "http://arxiv.org/abs/2508.09404v1",
        "published_date": "2025-08-13T00:39:56+00:00",
        "updated_date": "2025-08-13T00:39:56+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Guangxun Zhu",
            "Shiyu Fan",
            "Hang Dai",
            "Edmond S. L. Ho"
        ],
        "tldr": "The paper introduces Waymo-3DSkelMo, a large-scale, high-quality 3D skeletal motion dataset derived from the Waymo Perception dataset, designed for pedestrian interaction modeling in autonomous driving. It offers temporally coherent 3D skeletal motions with explicit interaction semantics.",
        "tldr_zh": "本文介绍了 Waymo-3DSkelMo，这是一个大规模、高质量的 3D 骨骼运动数据集，来源于 Waymo 感知数据集，专为自动驾驶中的行人交互建模而设计。它提供了时间上连贯的 3D 骨骼运动以及明确的交互语义。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Physical Autoregressive Model for Robotic Manipulation without Action Pretraining",
        "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining.",
        "url": "http://arxiv.org/abs/2508.09822v1",
        "published_date": "2025-08-13T13:54:51+00:00",
        "updated_date": "2025-08-13T13:54:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zijian Song",
            "Sihan Qin",
            "Tianshui Chen",
            "Liang Lin",
            "Guangrun Wang"
        ],
        "tldr": "This paper introduces a Physical Autoregressive Model (PAR) for robotic manipulation that leverages video pretraining to understand physical dynamics without requiring action pretraining, achieving state-of-the-art performance on manipulation tasks.",
        "tldr_zh": "该论文提出了一种用于机器人操作的物理自回归模型（PAR），该模型利用视频预训练来理解物理动力学，而无需动作预训练，并在操作任务上实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Plane Detection and Ranking via Model Information Optimization",
        "summary": "Plane detection from depth images is a crucial subtask with broad robotic\napplications, often accomplished by iterative methods such as Random Sample\nConsensus (RANSAC). While RANSAC is a robust strategy with strong probabilistic\nguarantees, the ambiguity of its inlier threshold criterion makes it\nsusceptible to false positive plane detections. This issue is particularly\nprevalent in complex real-world scenes, where the true number of planes is\nunknown and multiple planes coexist. In this paper, we aim to address this\nlimitation by proposing a generalised framework for plane detection based on\nmodel information optimization. Building on previous works, we treat the\nobserved depth readings as discrete random variables, with their probability\ndistributions constrained by the ground truth planes. Various models containing\ndifferent candidate plane constraints are then generated through repeated\nrandom sub-sampling to explain our observations. By incorporating the physics\nand noise model of the depth sensor, we can calculate the information for each\nmodel, and the model with the least information is accepted as the most likely\nground truth. This information optimization process serves as an objective\nmechanism for determining the true number of planes and preventing false\npositive detections. Additionally, the quality of each detected plane can be\nranked by summing the information reduction of inlier points for each plane. We\nvalidate these properties through experiments with synthetic data and find that\nour algorithm estimates plane parameters more accurately compared to the\ndefault Open3D RANSAC plane segmentation. Furthermore, we accelerate our\nalgorithm by partitioning the depth map using neural network segmentation,\nwhich enhances its ability to generate more realistic plane parameters in\nreal-world data.",
        "url": "http://arxiv.org/abs/2508.09625v1",
        "published_date": "2025-08-13T08:56:05+00:00",
        "updated_date": "2025-08-13T08:56:05+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Daoxin Zhong",
            "Jun Li",
            "Meng Yee Michael Chuah"
        ],
        "tldr": "This paper introduces a novel plane detection method based on model information optimization to improve robustness and accuracy in complex 3D scenes, validated with synthetic and real-world data.",
        "tldr_zh": "本文提出了一种基于模型信息优化的新型平面检测方法，以提高复杂3D场景中的鲁棒性和准确性，并通过合成数据和真实世界数据进行了验证。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "BridgeTA: Bridging the Representation Gap in Knowledge Distillation via Teacher Assistant for Bird's Eye View Map Segmentation",
        "summary": "Bird's-Eye-View (BEV) map segmentation is one of the most important and\nchallenging tasks in autonomous driving. Camera-only approaches have drawn\nattention as cost-effective alternatives to LiDAR, but they still fall behind\nLiDAR-Camera (LC) fusion-based methods. Knowledge Distillation (KD) has been\nexplored to narrow this gap, but existing methods mainly enlarge the student\nmodel by mimicking the teacher's architecture, leading to higher inference\ncost. To address this issue, we introduce BridgeTA, a cost-effective\ndistillation framework to bridge the representation gap between LC fusion and\nCamera-only models through a Teacher Assistant (TA) network while keeping the\nstudent's architecture and inference cost unchanged. A lightweight TA network\ncombines the BEV representations of the teacher and student, creating a shared\nlatent space that serves as an intermediate representation. To ground the\nframework theoretically, we derive a distillation loss using Young's\nInequality, which decomposes the direct teacher-student distillation path into\nteacher-TA and TA-student dual paths, stabilizing optimization and\nstrengthening knowledge transfer. Extensive experiments on the challenging\nnuScenes dataset demonstrate the effectiveness of our method, achieving an\nimprovement of 4.2% mIoU over the Camera-only baseline, up to 45% higher than\nthe improvement of other state-of-the-art KD methods.",
        "url": "http://arxiv.org/abs/2508.09599v1",
        "published_date": "2025-08-13T08:28:21+00:00",
        "updated_date": "2025-08-13T08:28:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Beomjun Kim",
            "Suhan Woo",
            "Sejong Heo",
            "Euntai Kim"
        ],
        "tldr": "The paper introduces BridgeTA, a knowledge distillation framework using a Teacher Assistant network to bridge the representation gap between LiDAR-Camera fusion and Camera-only models for BEV map segmentation in autonomous driving, achieving significant mIoU improvement on nuScenes.",
        "tldr_zh": "该论文介绍了 BridgeTA，一种知识蒸馏框架，使用教师助手网络来弥合 LiDAR-Camera 融合模型和纯相机模型在自动驾驶中鸟瞰图分割方面的表示差距，并在 nuScenes 数据集上取得了显著的 mIoU 提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Offline Auto Labeling: BAAS",
        "summary": "This paper introduces BAAS, a new Extended Object Tracking (EOT) and\nfusion-based label annotation framework for radar detections in autonomous\ndriving. Our framework utilizes Bayesian-based tracking, smoothing and\neventually fusion methods to provide veritable and precise object trajectories\nalong with shape estimation to provide annotation labels on the detection level\nunder various supervision levels. Simultaneously, the framework provides\nevaluation of tracking performance and label annotation. If manually labeled\ndata is available, each processing module can be analyzed independently or\ncombined with other modules to enable closed-loop continuous improvements. The\nframework performance is evaluated in a challenging urban real-world scenario\nin terms of tracking performance and the label annotation errors. We\ndemonstrate the functionality of the proposed approach for varying dynamic\nobjects and class types",
        "url": "http://arxiv.org/abs/2508.09585v1",
        "published_date": "2025-08-13T07:58:59+00:00",
        "updated_date": "2025-08-13T07:58:59+00:00",
        "categories": [
            "cs.CV",
            "cs.SY",
            "eess.SY"
        ],
        "authors": [
            "Stefan Haag",
            "Bharanidhar Duraisamy",
            "Felix Govaers",
            "Wolfgang Koch",
            "Martin Fritzsche",
            "Juergen Dickmann"
        ],
        "tldr": "The paper introduces BAAS, a Bayesian-based framework for automatically labeling radar detections in autonomous driving using tracking, smoothing, and fusion techniques, with independent module evaluation and closed-loop improvement capabilities.",
        "tldr_zh": "该论文介绍了BAAS，一个基于贝叶斯的框架，用于自动标注自动驾驶中雷达探测数据，使用跟踪、平滑和融合技术，并具备独立模块评估和闭环改进能力。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GoViG: Goal-Conditioned Visual Navigation Instruction Generation",
        "summary": "We introduce Goal-Conditioned Visual Navigation Instruction Generation\n(GoViG), a new task that aims to autonomously generate precise and contextually\ncoherent navigation instructions solely from egocentric visual observations of\ninitial and goal states. Unlike conventional approaches that rely on structured\ninputs such as semantic annotations or environmental maps, GoViG exclusively\nleverages raw egocentric visual data, substantially improving its adaptability\nto unseen and unstructured environments. Our method addresses this task by\ndecomposing it into two interconnected subtasks: (1) visual forecasting, which\npredicts intermediate visual states bridging the initial and goal views; and\n(2) instruction generation, which synthesizes linguistically coherent\ninstructions grounded in both observed and anticipated visuals. These subtasks\nare integrated within an autoregressive multimodal large language model trained\nwith tailored objectives to ensure spatial accuracy and linguistic clarity.\nFurthermore, we introduce two complementary multimodal reasoning strategies,\none-pass and interleaved reasoning, to mimic incremental human cognitive\nprocesses during navigation. To evaluate our method, we propose the R2R-Goal\ndataset, combining diverse synthetic and real-world trajectories. Empirical\nresults demonstrate significant improvements over state-of-the-art methods,\nachieving superior BLEU-4 and CIDEr scores along with robust cross-domain\ngeneralization.",
        "url": "http://arxiv.org/abs/2508.09547v1",
        "published_date": "2025-08-13T07:05:17+00:00",
        "updated_date": "2025-08-13T07:05:17+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Fengyi Wu",
            "Yifei Dong",
            "Zhi-Qi Cheng",
            "Yilong Dai",
            "Guangyu Chen",
            "Hang Wang",
            "Qi Dai",
            "Alexander G. Hauptmann"
        ],
        "tldr": "The paper introduces GoViG, a new task and method for generating navigation instructions from egocentric visual observations of initial and goal states using a multimodal LLM, and proposes the R2R-Goal dataset for evaluation.",
        "tldr_zh": "该论文介绍了GoViG，一项新的任务和方法，旨在仅通过初始和目标状态的自我中心视觉观察，使用多模态LLM生成导航指令，并提出了用于评估的R2R-Goal数据集。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DAgger Diffusion Navigation: DAgger Boosted Diffusion Policy for Vision-Language Navigation",
        "summary": "Vision-Language Navigation in Continuous Environments (VLN-CE) requires\nagents to follow natural language instructions through free-form 3D spaces.\nExisting VLN-CE approaches typically use a two-stage waypoint planning\nframework, where a high-level waypoint predictor generates the navigable\nwaypoints, and then a navigation planner suggests the intermediate goals in the\nhigh-level action space. However, this two-stage decomposition framework\nsuffers from: (1) global sub-optimization due to the proxy objective in each\nstage, and (2) a performance bottleneck caused by the strong reliance on the\nquality of the first-stage predicted waypoints. To address these limitations,\nwe propose DAgger Diffusion Navigation (DifNav), an end-to-end optimized VLN-CE\npolicy that unifies the traditional two stages, i.e. waypoint generation and\nplanning, into a single diffusion policy. Notably, DifNav employs a conditional\ndiffusion policy to directly model multi-modal action distributions over future\nactions in continuous navigation space, eliminating the need for a waypoint\npredictor while enabling the agent to capture multiple possible\ninstruction-following behaviors. To address the issues of compounding error in\nimitation learning and enhance spatial reasoning in long-horizon navigation\ntasks, we employ DAgger for online policy training and expert trajectory\naugmentation, and use the aggregated data to further fine-tune the policy. This\napproach significantly improves the policy's robustness and its ability to\nrecover from error states. Extensive experiments on benchmark datasets\ndemonstrate that, even without a waypoint predictor, the proposed method\nsubstantially outperforms previous state-of-the-art two-stage waypoint-based\nmodels in terms of navigation performance. Our code is available at:\nhttps://github.com/Tokishx/DifNav.",
        "url": "http://arxiv.org/abs/2508.09444v1",
        "published_date": "2025-08-13T02:51:43+00:00",
        "updated_date": "2025-08-13T02:51:43+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Haoxiang Shi",
            "Xiang Deng",
            "Zaijing Li",
            "Gongwei Chen",
            "Yaowei Wang",
            "Liqiang Nie"
        ],
        "tldr": "The paper proposes DAgger Diffusion Navigation (DifNav), an end-to-end diffusion policy for Vision-Language Navigation in Continuous Environments, which unifies waypoint generation and planning into a single policy and outperforms existing two-stage methods.",
        "tldr_zh": "该论文提出了DAgger扩散导航（DifNav），一种用于连续环境中视觉语言导航的端到端扩散策略，它将航点生成和规划统一到一个策略中，并且优于现有的两阶段方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SegDAC: Segmentation-Driven Actor-Critic for Visual Reinforcement Learning",
        "summary": "Visual reinforcement learning (RL) is challenging due to the need to learn\nboth perception and actions from high-dimensional inputs and noisy rewards.\nAlthough large perception models exist, integrating them effectively into RL\nfor visual generalization and improved sample efficiency remains unclear. We\npropose SegDAC, a Segmentation-Driven Actor-Critic method. SegDAC uses Segment\nAnything (SAM) for object-centric decomposition and YOLO-World to ground\nsegments semantically via text prompts. It includes a novel transformer-based\narchitecture that supports a dynamic number of segments at each time step and\neffectively learns which segments to focus on using online RL, without using\nhuman labels. By evaluating SegDAC over a challenging visual generalization\nbenchmark using Maniskill3, which covers diverse manipulation tasks under\nstrong visual perturbations, we demonstrate that SegDAC achieves significantly\nbetter visual generalization, doubling prior performance on the hardest setting\nand matching or surpassing prior methods in sample efficiency across all\nevaluated tasks.",
        "url": "http://arxiv.org/abs/2508.09325v1",
        "published_date": "2025-08-12T20:16:54+00:00",
        "updated_date": "2025-08-12T20:16:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Alexandre Brown",
            "Glen Berseth"
        ],
        "tldr": "The paper introduces SegDAC, a visual reinforcement learning method that leverages Segment Anything (SAM) and YOLO-World for object-centric decomposition and semantic grounding, demonstrating improved visual generalization and sample efficiency in manipulation tasks.",
        "tldr_zh": "该论文介绍了SegDAC，一种视觉强化学习方法，利用Segment Anything (SAM)和YOLO-World进行以物体为中心的分解和语义对齐，在操作任务中展示了更好的视觉泛化能力和样本效率。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Distilling LLM Prior to Flow Model for Generalizable Agent's Imagination in Object Goal Navigation",
        "summary": "The Object Goal Navigation (ObjectNav) task challenges agents to locate a\nspecified object in an unseen environment by imagining unobserved regions of\nthe scene. Prior approaches rely on deterministic and discriminative models to\ncomplete semantic maps, overlooking the inherent uncertainty in indoor layouts\nand limiting their ability to generalize to unseen environments. In this work,\nwe propose GOAL, a generative flow-based framework that models the semantic\ndistribution of indoor environments by bridging observed regions with\nLLM-enriched full-scene semantic maps. During training, spatial priors inferred\nfrom large language models (LLMs) are encoded as two-dimensional Gaussian\nfields and injected into target maps, distilling rich contextual knowledge into\nthe flow model and enabling more generalizable completions. Extensive\nexperiments demonstrate that GOAL achieves state-of-the-art performance on MP3D\nand Gibson, and shows strong generalization in transfer settings to HM3D. Codes\nand pretrained models are available at https://github.com/Badi-Li/GOAL.",
        "url": "http://arxiv.org/abs/2508.09423v1",
        "published_date": "2025-08-13T01:57:48+00:00",
        "updated_date": "2025-08-13T01:57:48+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Badi Li",
            "Ren-jie Lu",
            "Yu Zhou",
            "Jingke Meng",
            "Wei-shi Zheng"
        ],
        "tldr": "This paper introduces GOAL, a generative flow-based framework that leverages LLM priors to complete semantic maps for ObjectNav tasks, achieving state-of-the-art performance and generalization.",
        "tldr_zh": "该论文介绍了GOAL，一个生成式基于流的框架，利用LLM先验知识来完成用于ObjectNav任务的语义地图，实现了最先进的性能和泛化能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]