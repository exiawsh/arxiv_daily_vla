[
    {
        "title": "iFlyBot-VLA Technical Report",
        "summary": "We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model\ntrained under a novel framework. The main contributions are listed as follows:\n(1) a latent action model thoroughly trained on large-scale human and robotic\nmanipulation videos; (2) a dual-level action representation framework that\njointly supervises both the Vision-Language Model (VLM) and the action expert\nduring training; (3) a mixed training strategy that combines robot trajectory\ndata with general QA and spatial QA datasets, effectively enhancing the 3D\nperceptual and reasoning capabilities of the VLM backbone. Specifically, the\nVLM is trained to predict two complementary forms of actions: latent actions,\nderived from our latent action model pretrained on cross-embodiment\nmanipulation data, which capture implicit high-level intentions; and structured\ndiscrete action tokens, obtained through frequency-domain transformations of\ncontinuous control signals, which encode explicit low-level dynamics. This dual\nsupervision aligns the representation spaces of language, vision, and action,\nenabling the VLM to directly contribute to action generation. Experimental\nresults on the LIBERO Franka benchmark demonstrate the superiority of our\nframe-work, while real-world evaluations further show that iFlyBot-VLA achieves\ncompetitive success rates across diverse and challenging manipulation tasks.\nFurthermore, we plan to open-source a portion of our self-constructed dataset\nto support future research in the community",
        "url": "http://arxiv.org/abs/2511.01914v1",
        "published_date": "2025-11-01T06:24:56+00:00",
        "updated_date": "2025-11-01T06:24:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Yuan Zhang",
            "Chenyu Xue",
            "Wenjie Xu",
            "Chao Ji",
            "Jiajia wu",
            "Jia Pan"
        ],
        "tldr": "iFlyBot-VLA introduces a new Vision-Language-Action model with a novel training framework that uses dual-level action representation and a mixed training strategy to improve 3D perception and robotic manipulation capabilities, demonstrating strong performance on benchmarks and real-world tasks.",
        "tldr_zh": "iFlyBot-VLA 介绍了一个新的视觉-语言-动作模型，该模型采用新颖的训练框架，利用双层动作表示和混合训练策略来提高 3D 感知和机器人操作能力，并在基准测试和实际任务中表现出强大的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]