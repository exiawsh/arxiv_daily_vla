[
    {
        "title": "EVolSplat4D: Efficient Volume-based Gaussian Splatting for 4D Urban Scene Synthesis",
        "summary": "Novel view synthesis (NVS) of static and dynamic urban scenes is essential for autonomous driving simulation, yet existing methods often struggle to balance reconstruction time with quality. While state-of-the-art neural radiance fields and 3D Gaussian Splatting approaches achieve photorealism, they often rely on time-consuming per-scene optimization. Conversely, emerging feed-forward methods frequently adopt per-pixel Gaussian representations, which lead to 3D inconsistencies when aggregating multi-view predictions in complex, dynamic environments. We propose EvolSplat4D, a feed-forward framework that moves beyond existing per-pixel paradigms by unifying volume-based and pixel-based Gaussian prediction across three specialized branches. For close-range static regions, we predict consistent geometry of 3D Gaussians over multiple frames directly from a 3D feature volume, complemented by a semantically-enhanced image-based rendering module for predicting their appearance. For dynamic actors, we utilize object-centric canonical spaces and a motion-adjusted rendering module to aggregate temporal features, ensuring stable 4D reconstruction despite noisy motion priors. Far-Field scenery is handled by an efficient per-pixel Gaussian branch to ensure full-scene coverage. Experimental results on the KITTI-360, KITTI, Waymo, and PandaSet datasets show that EvolSplat4D reconstructs both static and dynamic environments with superior accuracy and consistency, outperforming both per-scene optimization and state-of-the-art feed-forward baselines.",
        "url": "http://arxiv.org/abs/2601.15951v1",
        "published_date": "2026-01-22T13:39:29+00:00",
        "updated_date": "2026-01-22T13:39:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sheng Miao",
            "Sijin Li",
            "Pan Wang",
            "Dongfeng Bai",
            "Bingbing Liu",
            "Yue Wang",
            "Andreas Geiger",
            "Yiyi Liao"
        ],
        "tldr": "EVolSplat4D presents a feed-forward approach for novel view synthesis of urban scenes, using a multi-branch Gaussian Splatting method that combines volume-based and pixel-based predictions to achieve superior accuracy and consistency in both static and dynamic environments.",
        "tldr_zh": "EVolSplat4D 提出了一种用于城市场景新视角合成的前馈方法，该方法采用多分支高斯溅射方法，结合了基于体积和基于像素的预测，从而在静态和动态环境中实现了卓越的准确性和一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Rethinking Video Generation Model for the Embodied World",
        "summary": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.",
        "url": "http://arxiv.org/abs/2601.15282v1",
        "published_date": "2026-01-21T18:59:18+00:00",
        "updated_date": "2026-01-21T18:59:18+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Yufan Deng",
            "Zilin Pan",
            "Hongyu Zhang",
            "Xiaojie Li",
            "Ruoqing Hu",
            "Yufei Ding",
            "Yiming Zou",
            "Yan Zeng",
            "Daquan Zhou"
        ],
        "tldr": "This paper introduces RBench, a robotics benchmark for evaluating video generation models, and RoVid-X, a large-scale robotic video dataset, to address the lack of standardized evaluation and high-quality training data in embodied AI.",
        "tldr_zh": "本文介绍了一个机器人视频生成模型的基准测试 RBench，以及一个大规模机器人视频数据集 RoVid-X，旨在解决具身智能中缺乏标准化评估和高质量训练数据的问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "DrivIng: A Large-Scale Multimodal Driving Dataset with Full Digital Twin Integration",
        "summary": "Perception is a cornerstone of autonomous driving, enabling vehicles to understand their surroundings and make safe, reliable decisions. Developing robust perception algorithms requires large-scale, high-quality datasets that cover diverse driving conditions and support thorough evaluation. Existing datasets often lack a high-fidelity digital twin, limiting systematic testing, edge-case simulation, sensor modification, and sim-to-real evaluations. To address this gap, we present DrivIng, a large-scale multimodal dataset with a complete geo-referenced digital twin of a ~18 km route spanning urban, suburban, and highway segments. Our dataset provides continuous recordings from six RGB cameras, one LiDAR, and high-precision ADMA-based localization, captured across day, dusk, and night. All sequences are annotated at 10 Hz with 3D bounding boxes and track IDs across 12 classes, yielding ~1.2 million annotated instances. Alongside the benefits of a digital twin, DrivIng enables a 1-to-1 transfer of real traffic into simulation, preserving agent interactions while enabling realistic and flexible scenario testing. To support reproducible research and robust validation, we benchmark DrivIng with state-of-the-art perception models and publicly release the dataset, digital twin, HD map, and codebase.",
        "url": "http://arxiv.org/abs/2601.15260v1",
        "published_date": "2026-01-21T18:41:05+00:00",
        "updated_date": "2026-01-21T18:41:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dominik Rößle",
            "Xujun Xie",
            "Adithya Mohan",
            "Venkatesh Thirugnana Sambandham",
            "Daniel Cremers",
            "Torsten Schön"
        ],
        "tldr": "The paper introduces DrivIng, a large-scale multimodal autonomous driving dataset with a full digital twin integration, providing high-quality data for perception algorithm development and evaluation, especially regarding sim-to-real transfer and realistic scenario testing.",
        "tldr_zh": "该论文介绍了DrivIng，一个大规模多模态自动驾驶数据集，具有完整的数字孪生集成，为感知算法的开发和评估提供高质量的数据，尤其是在模拟到现实的转换和真实的场景测试方面。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "FlowSSC: Universal Generative Monocular Semantic Scene Completion via One-Step Latent Diffusion",
        "summary": "Semantic Scene Completion (SSC) from monocular RGB images is a fundamental yet challenging task due to the inherent ambiguity of inferring occluded 3D geometry from a single view. While feed-forward methods have made progress, they often struggle to generate plausible details in occluded regions and preserve the fundamental spatial relationships of objects. Such accurate generative reasoning capability for the entire 3D space is critical in real-world applications. In this paper, we present FlowSSC, the first generative framework applied directly to monocular semantic scene completion. FlowSSC treats the SSC task as a conditional generation problem and can seamlessly integrate with existing feed-forward SSC methods to significantly boost their performance. To achieve real-time inference without compromising quality, we introduce Shortcut Flow-matching that operates in a compact triplane latent space. Unlike standard diffusion models that require hundreds of steps, our method utilizes a shortcut mechanism to achieve high-fidelity generation in a single step, enabling practical deployment in autonomous systems. Extensive experiments on SemanticKITTI demonstrate that FlowSSC achieves state-of-the-art performance, significantly outperforming existing baselines.",
        "url": "http://arxiv.org/abs/2601.15250v1",
        "published_date": "2026-01-21T18:32:27+00:00",
        "updated_date": "2026-01-21T18:32:27+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Zichen Xi",
            "Hao-Xiang Chen",
            "Nan Xue",
            "Hongyu Yan",
            "Qi-Yuan Feng",
            "Levent Burak Kara",
            "Joaquim Jorge",
            "Qun-Ce Xu"
        ],
        "tldr": "The paper introduces FlowSSC, a novel one-step latent diffusion framework for monocular semantic scene completion, achieving state-of-the-art performance on SemanticKITTI by integrating with existing feed-forward methods and utilizing a shortcut mechanism for real-time inference.",
        "tldr_zh": "该论文介绍了FlowSSC，一种新颖的单步潜在扩散框架，用于单目语义场景补全。通过与现有的前馈方法集成，并利用快捷机制实现实时推理，FlowSSC在SemanticKITTI上实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Keyframe-Based Feed-Forward Visual Odometry",
        "summary": "The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.",
        "url": "http://arxiv.org/abs/2601.16020v1",
        "published_date": "2026-01-22T14:45:42+00:00",
        "updated_date": "2026-01-22T14:45:42+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Weichen Dai",
            "Wenhan Su",
            "Da Kong",
            "Yuhang Ming",
            "Wanzeng Kong"
        ],
        "tldr": "This paper introduces a reinforcement learning based keyframe selection policy for feed-forward visual odometry, improving performance over existing foundation model based methods by addressing computational redundancy and low inter-frame parallax issues.",
        "tldr_zh": "本文提出了一种基于强化学习的关键帧选择策略，用于前馈视觉里程计，通过解决计算冗余和低帧间视差问题，提高了现有基于基础模型的方法的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SuperOcc: Toward Cohesive Temporal Modeling for Superquadric-based Occupancy Prediction",
        "summary": "3D occupancy prediction plays a pivotal role in the realm of autonomous driving, as it provides a comprehensive understanding of the driving environment. Most existing methods construct dense scene representations for occupancy prediction, overlooking the inherent sparsity of real-world driving scenes. Recently, 3D superquadric representation has emerged as a promising sparse alternative to dense scene representations due to the strong geometric expressiveness of superquadrics. However, existing superquadric frameworks still suffer from insufficient temporal modeling, a challenging trade-off between query sparsity and geometric expressiveness, and inefficient superquadric-to-voxel splatting. To address these issues, we propose SuperOcc, a novel framework for superquadric-based 3D occupancy prediction. SuperOcc incorporates three key designs: (1) a cohesive temporal modeling mechanism to simultaneously exploit view-centric and object-centric temporal cues; (2) a multi-superquadric decoding strategy to enhance geometric expressiveness without sacrificing query sparsity; and (3) an efficient superquadric-to-voxel splatting scheme to improve computational efficiency. Extensive experiments on the SurroundOcc and Occ3D benchmarks demonstrate that SuperOcc achieves state-of-the-art performance while maintaining superior efficiency. The code is available at https://github.com/Yzichen/SuperOcc.",
        "url": "http://arxiv.org/abs/2601.15644v1",
        "published_date": "2026-01-22T04:50:29+00:00",
        "updated_date": "2026-01-22T04:50:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zichen Yu",
            "Quanli Liu",
            "Wei Wang",
            "Liyong Zhang",
            "Xiaoguang Zhao"
        ],
        "tldr": "The paper introduces SuperOcc, a new framework for 3D occupancy prediction using superquadrics, addressing limitations in temporal modeling, geometric expressiveness, and computational efficiency compared to existing methods and achieving state-of-the-art performance on relevant benchmarks.",
        "tldr_zh": "该论文介绍了一种新的基于超二次曲面的3D occupancy预测框架SuperOcc，解决了现有方法在时间建模、几何表达能力和计算效率方面的局限性，并在相关基准测试中取得了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Walk through Paintings: Egocentric World Models from Internet Priors",
        "summary": "What if a video generation model could not only imagine a plausible future, but the correct one, accurately reflecting how the world changes with each action? We address this question by presenting the Egocentric World Model (EgoWM), a simple, architecture-agnostic method that transforms any pretrained video diffusion model into an action-conditioned world model, enabling controllable future prediction. Rather than training from scratch, we repurpose the rich world priors of Internet-scale video models and inject motor commands through lightweight conditioning layers. This allows the model to follow actions faithfully while preserving realism and strong generalization. Our approach scales naturally across embodiments and action spaces, ranging from 3-DoF mobile robots to 25-DoF humanoids, where predicting egocentric joint-angle-driven dynamics is substantially more challenging. The model produces coherent rollouts for both navigation and manipulation tasks, requiring only modest fine-tuning. To evaluate physical correctness independently of visual appearance, we introduce the Structural Consistency Score (SCS), which measures whether stable scene elements evolve consistently with the provided actions. EgoWM improves SCS by up to 80 percent over prior state-of-the-art navigation world models, while achieving up to six times lower inference latency and robust generalization to unseen environments, including navigation inside paintings.",
        "url": "http://arxiv.org/abs/2601.15284v1",
        "published_date": "2026-01-21T18:59:32+00:00",
        "updated_date": "2026-01-21T18:59:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Anurag Bagchi",
            "Zhipeng Bao",
            "Homanga Bharadhwaj",
            "Yu-Xiong Wang",
            "Pavel Tokmakov",
            "Martial Hebert"
        ],
        "tldr": "The paper introduces EgoWM, a method to transform pretrained video diffusion models into action-conditioned world models for controllable future prediction, demonstrating improved structural consistency and generalization across different robotic embodiments.",
        "tldr_zh": "该论文介绍了EgoWM，一种将预训练的视频扩散模型转化为动作条件世界模型的方法，用于可控的未来预测，展示了在不同机器人形态上改进的结构一致性和泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SplatBus: A Gaussian Splatting Viewer Framework via GPU Interprocess Communication",
        "summary": "Radiance field-based rendering methods have attracted significant interest from the computer vision and computer graphics communities. They enable high-fidelity rendering with complex real-world lighting effects, but at the cost of high rendering time. 3D Gaussian Splatting solves this issue with a rasterisation-based approach for real-time rendering, enabling applications such as autonomous driving, robotics, virtual reality, and extended reality. However, current 3DGS implementations are difficult to integrate into traditional mesh-based rendering pipelines, which is a common use case for interactive applications and artistic exploration. To address this limitation, this software solution uses Nvidia's interprocess communication (IPC) APIs to easily integrate into implementations and allow the results to be viewed in external clients such as Unity, Blender, Unreal Engine, and OpenGL viewers. The code is available at https://github.com/RockyXu66/splatbus.",
        "url": "http://arxiv.org/abs/2601.15431v1",
        "published_date": "2026-01-21T19:56:22+00:00",
        "updated_date": "2026-01-21T19:56:22+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Yinghan Xu",
            "Théo Morales",
            "John Dingliana"
        ],
        "tldr": "This paper presents SplatBus, a software solution that uses Nvidia's IPC to integrate 3D Gaussian Splatting (3DGS) into traditional mesh-based rendering pipelines, enabling real-time rendering in applications like Unity, Blender, and Unreal Engine.",
        "tldr_zh": "该论文介绍了SplatBus，一个利用英伟达IPC将3D高斯溅射(3DGS)集成到传统网格渲染管线的软件解决方案，从而可以在Unity、Blender和Unreal Engine等应用中实现实时渲染。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]