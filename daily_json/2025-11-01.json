[
    {
        "title": "MoRE: 3D Visual Geometry Reconstruction Meets Mixture-of-Experts",
        "summary": "Recent advances in language and vision have demonstrated that scaling up\nmodel capacity consistently improves performance across diverse tasks. In 3D\nvisual geometry reconstruction, large-scale training has likewise proven\neffective for learning versatile representations. However, further scaling of\n3D models is challenging due to the complexity of geometric supervision and the\ndiversity of 3D data. To overcome these limitations, we propose MoRE, a dense\n3D visual foundation model based on a Mixture-of-Experts (MoE) architecture\nthat dynamically routes features to task-specific experts, allowing them to\nspecialize in complementary data aspects and enhance both scalability and\nadaptability. Aiming to improve robustness under real-world conditions, MoRE\nincorporates a confidence-based depth refinement module that stabilizes and\nrefines geometric estimation. In addition, it integrates dense semantic\nfeatures with globally aligned 3D backbone representations for high-fidelity\nsurface normal prediction. MoRE is further optimized with tailored loss\nfunctions to ensure robust learning across diverse inputs and multiple\ngeometric tasks. Extensive experiments demonstrate that MoRE achieves\nstate-of-the-art performance across multiple benchmarks and supports effective\ndownstream applications without extra computation.",
        "url": "http://arxiv.org/abs/2510.27234v1",
        "published_date": "2025-10-31T06:54:27+00:00",
        "updated_date": "2025-10-31T06:54:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingnan Gao",
            "Zhe Wang",
            "Xianze Fang",
            "Xingyu Ren",
            "Zhuo Chen",
            "Shengqi Liu",
            "Yuhao Cheng",
            "Jiangjing Lyu",
            "Xiaokang Yang",
            "Yichao Yan"
        ],
        "tldr": "This paper introduces MoRE, a Mixture-of-Experts based 3D visual foundation model for improved scalability, adaptability, and robustness in 3D visual geometry reconstruction tasks, achieving state-of-the-art performance across multiple benchmarks.",
        "tldr_zh": "本文介绍了MoRE，一个基于混合专家模型的3D视觉基础模型，旨在提高3D视觉几何重建任务的可扩展性、适应性和鲁棒性，并在多个基准测试中实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model",
        "summary": "Recently, augmenting Vision-Language-Action models (VLAs) with world modeling\nhas shown promise in improving robotic policy learning. However, it remains\nchallenging to jointly predict next-state observations and action sequences\nbecause of the inherent difference between the two modalities. To address this,\nwe propose DUal-STream diffusion (DUST), a world-model augmented VLA framework\nthat handles the modality conflict and enhances the performance of VLAs across\ndiverse tasks. Specifically, we propose a multimodal diffusion transformer\narchitecture that explicitly maintains separate modality streams while still\nenabling cross-modal knowledge sharing. In addition, we introduce independent\nnoise perturbations for each modality and a decoupled flow-matching loss. This\ndesign enables the model to learn the joint distribution in a bidirectional\nmanner while avoiding the need for a unified latent space. Based on the\ndecoupling of modalities during training, we also introduce a joint sampling\nmethod that supports test-time scaling, where action and vision tokens evolve\nasynchronously at different rates. Through experiments on simulated benchmarks\nsuch as RoboCasa and GR-1, DUST achieves up to 6% gains over baseline methods,\nwhile our test-time scaling approach provides an additional 2-5% boost. On\nreal-world tasks with the Franka Research 3, DUST improves success rates by\n13%, confirming its effectiveness beyond simulation. Furthermore, pre-training\non action-free videos from BridgeV2 yields significant transfer gains on\nRoboCasa, underscoring DUST's potential for large-scale VLA pretraining.",
        "url": "http://arxiv.org/abs/2510.27607v1",
        "published_date": "2025-10-31T16:32:12+00:00",
        "updated_date": "2025-10-31T16:32:12+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "John Won",
            "Kyungmin Lee",
            "Huiwon Jang",
            "Dongyoung Kim",
            "Jinwoo Shin"
        ],
        "tldr": "The paper introduces DUST, a dual-stream diffusion model for Vision-Language-Action tasks that improves robotic policy learning by handling modality differences and enabling test-time scaling, demonstrating gains in simulated and real-world robotic tasks.",
        "tldr_zh": "本文介绍了DUST，一种用于视觉-语言-动作任务的双流扩散模型，通过处理模态差异和实现测试时缩放来改进机器人策略学习，并在模拟和真实机器人任务中展示了效果提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning",
        "summary": "Spatial understanding remains a weakness of Large Vision-Language Models\n(LVLMs). Existing supervised fine-tuning (SFT) and recent reinforcement\nlearning with verifiable rewards (RLVR) pipelines depend on costly supervision,\nspecialized tools, or constrained environments that limit scale. We introduce\nSpatial-SSRL, a self-supervised RL paradigm that derives verifiable signals\ndirectly from ordinary RGB or RGB-D images. Spatial-SSRL automatically\nformulates five pretext tasks that capture 2D and 3D spatial structure:\nshuffled patch reordering, flipped patch recognition, cropped patch inpainting,\nregional depth ordering, and relative 3D position prediction. These tasks\nprovide ground-truth answers that are easy to verify and require no human or\nLVLM annotation. Training on our tasks substantially improves spatial reasoning\nwhile preserving general visual capabilities. On seven spatial understanding\nbenchmarks in both image and video settings, Spatial-SSRL delivers average\naccuracy gains of 4.63% (3B) and 3.89% (7B) over the Qwen2.5-VL baselines. Our\nresults show that simple, intrinsic supervision enables RLVR at scale and\nprovides a practical route to stronger spatial intelligence in LVLMs.",
        "url": "http://arxiv.org/abs/2510.27606v1",
        "published_date": "2025-10-31T16:30:08+00:00",
        "updated_date": "2025-10-31T16:30:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yuhong Liu",
            "Beichen Zhang",
            "Yuhang Zang",
            "Yuhang Cao",
            "Long Xing",
            "Xiaoyi Dong",
            "Haodong Duan",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "tldr": "Spatial-SSRL uses self-supervised reinforcement learning with verifiable signals from RGB/RGB-D images to improve spatial understanding in LVLMs, achieving performance gains on spatial reasoning benchmarks.",
        "tldr_zh": "Spatial-SSRL 采用自监督强化学习，利用 RGB/RGB-D 图像中的可验证信号来提高 LVLM 的空间理解能力，并在空间推理基准测试中实现了性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "M^3Detection: Multi-Frame Multi-Level Feature Fusion for Multi-Modal 3D Object Detection with Camera and 4D Imaging Radar",
        "summary": "Recent advances in 4D imaging radar have enabled robust perception in adverse\nweather, while camera sensors provide dense semantic information. Fusing the\nthese complementary modalities has great potential for cost-effective 3D\nperception. However, most existing camera-radar fusion methods are limited to\nsingle-frame inputs, capturing only a partial view of the scene. The incomplete\nscene information, compounded by image degradation and 4D radar sparsity,\nhinders overall detection performance. In contrast, multi-frame fusion offers\nricher spatiotemporal information but faces two challenges: achieving robust\nand effective object feature fusion across frames and modalities, and\nmitigating the computational cost of redundant feature extraction.\nConsequently, we propose M^3Detection, a unified multi-frame 3D object\ndetection framework that performs multi-level feature fusion on multi-modal\ndata from camera and 4D imaging radar. Our framework leverages intermediate\nfeatures from the baseline detector and employs the tracker to produce\nreference trajectories, improving computational efficiency and providing richer\ninformation for second-stage. In the second stage, we design a global-level\ninter-object feature aggregation module guided by radar information to align\nglobal features across candidate proposals and a local-level inter-grid feature\naggregation module that expands local features along the reference trajectories\nto enhance fine-grained object representation. The aggregated features are then\nprocessed by a trajectory-level multi-frame spatiotemporal reasoning module to\nencode cross-frame interactions and enhance temporal representation. Extensive\nexperiments on the VoD and TJ4DRadSet datasets demonstrate that M^3Detection\nachieves state-of-the-art 3D detection performance, validating its\neffectiveness in multi-frame detection with camera-4D imaging radar fusion.",
        "url": "http://arxiv.org/abs/2510.27166v1",
        "published_date": "2025-10-31T04:34:15+00:00",
        "updated_date": "2025-10-31T04:34:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaozhi Li",
            "Huijun Di",
            "Jian Li",
            "Feng Liu",
            "Wei Liang"
        ],
        "tldr": "The paper introduces M^3Detection, a multi-frame 3D object detection framework fusing camera and 4D imaging radar data using multi-level feature fusion and spatiotemporal reasoning, achieving state-of-the-art performance on VoD and TJ4DRadSet datasets.",
        "tldr_zh": "该论文介绍了M^3Detection，一个多帧3D目标检测框架，它融合了相机和4D成像雷达数据，利用多层次特征融合和时空推理，在VoD和TJ4DRadSet数据集上实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AD-SAM: Fine-Tuning the Segment Anything Vision Foundation Model for Autonomous Driving Perception",
        "summary": "This paper presents the Autonomous Driving Segment Anything Model (AD-SAM), a\nfine-tuned vision foundation model for semantic segmentation in autonomous\ndriving (AD). AD-SAM extends the Segment Anything Model (SAM) with a\ndual-encoder and deformable decoder tailored to spatial and geometric\ncomplexity of road scenes. The dual-encoder produces multi-scale fused\nrepresentations by combining global semantic context from SAM's pretrained\nVision Transformer (ViT-H) with local spatial detail from a trainable\nconvolutional deep learning backbone (i.e., ResNet-50). A deformable fusion\nmodule aligns heterogeneous features across scales and object geometries. The\ndecoder performs progressive multi-stage refinement using deformable attention.\nTraining is guided by a hybrid loss that integrates Focal, Dice,\nLovasz-Softmax, and Surface losses, improving semantic class balance, boundary\nprecision, and optimization stability. Experiments on the Cityscapes and\nBerkeley DeepDrive 100K (BDD100K) benchmarks show that AD-SAM surpasses SAM,\nGeneralized SAM (G-SAM), and a deep learning baseline (DeepLabV3) in\nsegmentation accuracy. It achieves 68.1 mean Intersection over Union (mIoU) on\nCityscapes and 59.5 mIoU on BDD100K, outperforming SAM, G-SAM, and DeepLabV3 by\nmargins of up to +22.9 and +19.2 mIoU in structured and diverse road scenes,\nrespectively. AD-SAM demonstrates strong cross-domain generalization with a\n0.87 retention score (vs. 0.76 for SAM), and faster, more stable learning\ndynamics, converging within 30-40 epochs, enjoying double the learning speed of\nbenchmark models. It maintains 0.607 mIoU with only 1000 samples, suggesting\ndata efficiency critical for reducing annotation costs. These results confirm\nthat targeted architectural and optimization enhancements to foundation models\nenable reliable and scalable AD perception.",
        "url": "http://arxiv.org/abs/2510.27047v1",
        "published_date": "2025-10-30T23:30:33+00:00",
        "updated_date": "2025-10-30T23:30:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mario Camarena",
            "Het Patel",
            "Fatemeh Nazari",
            "Evangelos Papalexakis",
            "Mohamadhossein Noruzoliaee",
            "Jia Chen"
        ],
        "tldr": "AD-SAM is a fine-tuned SAM model for autonomous driving semantic segmentation, using a dual-encoder and deformable decoder to achieve significant mIoU improvements on Cityscapes and BDD100K datasets compared to SAM, G-SAM, and DeepLabV3.",
        "tldr_zh": "AD-SAM 是一个针对自动驾驶语义分割进行微调的 SAM 模型，它使用双编码器和可变形解码器，在 Cityscapes 和 BDD100K 数据集上实现了显著的 mIoU 改进，优于 SAM、G-SAM 和 DeepLabV3。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Multi-Modal Neuro-Symbolic Approach for Spatial Reasoning-Based Visual Grounding in Robotics",
        "summary": "Visual reasoning, particularly spatial reasoning, is a challenging cognitive\ntask that requires understanding object relationships and their interactions\nwithin complex environments, especially in robotics domain. Existing\nvision_language models (VLMs) excel at perception tasks but struggle with\nfine-grained spatial reasoning due to their implicit, correlation-driven\nreasoning and reliance solely on images. We propose a novel neuro_symbolic\nframework that integrates both panoramic-image and 3D point cloud information,\ncombining neural perception with symbolic reasoning to explicitly model spatial\nand logical relationships. Our framework consists of a perception module for\ndetecting entities and extracting attributes, and a reasoning module that\nconstructs a structured scene graph to support precise, interpretable queries.\nEvaluated on the JRDB-Reasoning dataset, our approach demonstrates superior\nperformance and reliability in crowded, human_built environments while\nmaintaining a lightweight design suitable for robotics and embodied AI\napplications.",
        "url": "http://arxiv.org/abs/2510.27033v1",
        "published_date": "2025-10-30T22:40:23+00:00",
        "updated_date": "2025-10-30T22:40:23+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Simindokht Jahangard",
            "Mehrzad Mohammadi",
            "Abhinav Dhall",
            "Hamid Rezatofighi"
        ],
        "tldr": "This paper introduces a neuro-symbolic framework that combines panoramic images and 3D point clouds for spatial reasoning in robotics, demonstrating improved performance on the JRDB-Reasoning dataset.",
        "tldr_zh": "本文介绍了一种神经符号框架，该框架结合了全景图像和3D点云，用于机器人领域的空间推理，并在JRDB-Reasoning数据集上表现出更好的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]