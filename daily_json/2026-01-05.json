[
    {
        "title": "DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving",
        "summary": "Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making.",
        "url": "http://arxiv.org/abs/2601.01528v1",
        "published_date": "2026-01-04T13:36:21+00:00",
        "updated_date": "2026-01-04T13:36:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Yang Zhou",
            "Hao Shao",
            "Letian Wang",
            "Zhuofan Zong",
            "Hongsheng Li",
            "Steven L. Waslander"
        ],
        "tldr": "The paper introduces DrivingGen, a new comprehensive benchmark for generative driving world models, addressing limitations in existing evaluations by providing a diverse dataset and a suite of metrics for visual realism, trajectory plausibility, temporal coherence, and controllability.",
        "tldr_zh": "该论文介绍了DrivingGen，一种用于生成式驾驶世界模型的全新综合基准，通过提供多样化的数据集和一套用于视觉真实感、轨迹合理性、时间连贯性和可控性的指标，解决了现有评估的局限性。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Rethinking Multimodal Few-Shot 3D Point Cloud Segmentation: From Fused Refinement to Decoupled Arbitration",
        "summary": "In this paper, we revisit multimodal few-shot 3D point cloud semantic segmentation (FS-PCS), identifying a conflict in \"Fuse-then-Refine\" paradigms: the \"Plasticity-Stability Dilemma.\" In addition, CLIP's inter-class confusion can result in semantic blindness. To address these issues, we present the Decoupled-experts Arbitration Few-Shot SegNet (DA-FSS), a model that effectively distinguishes between semantic and geometric paths and mutually regularizes their gradients to achieve better generalization. DA-FSS employs the same backbone and pre-trained text encoder as MM-FSS to generate text embeddings, which can increase free modalities' utilization rate and better leverage each modality's information space. To achieve this, we propose a Parallel Expert Refinement module to generate each modal correlation. We also propose a Stacked Arbitration Module (SAM) to perform convolutional fusion and arbitrate correlations for each modality pathway. The Parallel Experts decouple two paths: a Geometric Expert maintains plasticity, and a Semantic Expert ensures stability. They are coordinated via a Decoupled Alignment Module (DAM) that transfers knowledge without propagating confusion. Experiments on popular datasets (S3DIS, ScanNet) demonstrate the superiority of DA-FSS over MM-FSS. Meanwhile, geometric boundaries, completeness, and texture differentiation are all superior to the baseline. The code is available at: https://github.com/MoWenQAQ/DA-FSS.",
        "url": "http://arxiv.org/abs/2601.01456v1",
        "published_date": "2026-01-04T09:53:49+00:00",
        "updated_date": "2026-01-04T09:53:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Wentao Bian",
            "Fenglei Xu"
        ],
        "tldr": "The paper introduces DA-FSS, a novel architecture for multimodal few-shot 3D point cloud segmentation that addresses the plasticity-stability dilemma and inter-class confusion found in existing methods, achieving state-of-the-art performance.",
        "tldr_zh": "本文介绍了一种新的多模态少样本3D点云分割架构DA-FSS，该架构解决了现有方法中存在的塑性-稳定性困境和类间混淆问题，并实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ParkGaussian: Surround-view 3D Gaussian Splatting for Autonomous Parking",
        "summary": "Parking is a critical task for autonomous driving systems (ADS), with unique challenges in crowded parking slots and GPS-denied environments. However, existing works focus on 2D parking slot perception, mapping, and localization, 3D reconstruction remains underexplored, which is crucial for capturing complex spatial geometry in parking scenarios. Naively improving the visual quality of reconstructed parking scenes does not directly benefit autonomous parking, as the key entry point for parking is the slots perception module. To address these limitations, we curate the first benchmark named ParkRecon3D, specifically designed for parking scene reconstruction. It includes sensor data from four surround-view fisheye cameras with calibrated extrinsics and dense parking slot annotations. We then propose ParkGaussian, the first framework that integrates 3D Gaussian Splatting (3DGS) for parking scene reconstruction. To further improve the alignment between reconstruction and downstream parking slot detection, we introduce a slot-aware reconstruction strategy that leverages existing parking perception methods to enhance the synthesis quality of slot regions. Experiments on ParkRecon3D demonstrate that ParkGaussian achieves state-of-the-art reconstruction quality and better preserves perception consistency for downstream tasks. The code and dataset will be released at: https://github.com/wm-research/ParkGaussian",
        "url": "http://arxiv.org/abs/2601.01386v1",
        "published_date": "2026-01-04T05:54:13+00:00",
        "updated_date": "2026-01-04T05:54:13+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xiaobao Wei",
            "Zhangjie Ye",
            "Yuxiang Gu",
            "Zunjie Zhu",
            "Yunfei Guo",
            "Yingying Shen",
            "Shan Zhao",
            "Ming Lu",
            "Haiyang Sun",
            "Bing Wang",
            "Guang Chen",
            "Rongfeng Lu",
            "Hangjun Ye"
        ],
        "tldr": "The paper introduces ParkGaussian, a 3D Gaussian Splatting framework for parking scene reconstruction, along with a new benchmark dataset, ParkRecon3D, to improve autonomous parking slot detection by incorporating slot-aware reconstruction.",
        "tldr_zh": "该论文介绍了ParkGaussian，一个用于停车场景重建的3D高斯溅射框架，以及一个新的基准数据集ParkRecon3D，通过结合槽位感知的重建来改进自动停车槽检测。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]