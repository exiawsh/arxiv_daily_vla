[
    {
        "title": "GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning",
        "summary": "Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is that the models exhibit limited zero-shot capability, which hampers their ability to generalize effectively to unseen scenarios. In this work, we propose GeneralVLA (Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning), a hierarchical vision-language-action (VLA) model that can be more effective in utilizing the generalization of foundation models, enabling zero-shot manipulation and automatically generating data for robotics. In particular, we study a class of hierarchical VLA model where the high-level ASM (Affordance Segmentation Module) is finetuned to perceive image keypoint affordances of the scene; the mid-level 3DAgent carries out task understanding, skill knowledge, and trajectory planning to produce a 3D path indicating the desired robot end-effector trajectory. The intermediate 3D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Compared to alternative approaches, our method requires no real-world robotic data collection or human demonstration, making it much more scalable to diverse tasks and viewpoints. Empirically, GeneralVLA successfully generates trajectories for 14 tasks, significantly outperforming state-of-the-art methods such as VoxPoser. The generated demonstrations can train more robust behavior cloning policies than training with human demonstrations or from data generated by VoxPoser, Scaling-up, and Code-As-Policies. We believe GeneralVLA can be the scalable method for both generating data for robotics and solving novel tasks in a zero-shot setting. Code: https://github.com/AIGeeksGroup/GeneralVLA. Website: https://aigeeksgroup.github.io/GeneralVLA.",
        "url": "http://arxiv.org/abs/2602.04315v1",
        "published_date": "2026-02-04T08:30:27+00:00",
        "updated_date": "2026-02-04T08:30:27+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Guoqing Ma",
            "Siheng Wang",
            "Zeyu Zhang",
            "Shan Yu",
            "Hao Tang"
        ],
        "tldr": "The paper presents GeneralVLA, a hierarchical Vision-Language-Action model that utilizes knowledge-guided trajectory planning for zero-shot robotic manipulation and data generation, outperforming existing methods without requiring real-world data or human demonstrations.",
        "tldr_zh": "该论文提出了 GeneralVLA，一种分层视觉-语言-动作模型，利用知识引导的轨迹规划进行零样本机器人操作和数据生成，优于现有方法，且无需真实世界数据或人类演示。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "SPOT-Occ: Sparse Prototype-guided Transformer for Camera-based 3D Occupancy Prediction",
        "summary": "Achieving highly accurate and real-time 3D occupancy prediction from cameras is a critical requirement for the safe and practical deployment of autonomous vehicles. While this shift to sparse 3D representations solves the encoding bottleneck, it creates a new challenge for the decoder: how to efficiently aggregate information from a sparse, non-uniformly distributed set of voxel features without resorting to computationally prohibitive dense attention.\n  In this paper, we propose a novel Prototype-based Sparse Transformer Decoder that replaces this costly interaction with an efficient, two-stage process of guided feature selection and focused aggregation. Our core idea is to make the decoder's attention prototype-guided. We achieve this through a sparse prototype selection mechanism, where each query adaptively identifies a compact set of the most salient voxel features, termed prototypes, for focused feature aggregation.\n  To ensure this dynamic selection is stable and effective, we introduce a complementary denoising paradigm. This approach leverages ground-truth masks to provide explicit guidance, guaranteeing a consistent query-prototype association across decoder layers. Our model, dubbed SPOT-Occ, outperforms previous methods with a significant margin in speed while also improving accuracy. Source code is released at https://github.com/chensuzeyu/SpotOcc.",
        "url": "http://arxiv.org/abs/2602.04240v1",
        "published_date": "2026-02-04T05:59:24+00:00",
        "updated_date": "2026-02-04T05:59:24+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Suzeyu Chen",
            "Leheng Li",
            "Ying-Cong Chen"
        ],
        "tldr": "The paper introduces SPOT-Occ, a novel sparse prototype-guided transformer decoder for camera-based 3D occupancy prediction, achieving improved speed and accuracy by using a two-stage process of guided feature selection and focused aggregation with a denoising paradigm.",
        "tldr_zh": "该论文提出了一种新的基于相机的三维占用预测的稀疏原型引导的 Transformer 解码器 SPOT-Occ，通过使用引导特征选择和聚焦聚合的两阶段过程以及去噪范例，实现了速度和准确性的提高。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Natural Language Instructions for Scene-Responsive Human-in-the-Loop Motion Planning in Autonomous Driving using Vision-Language-Action Models",
        "summary": "Instruction-grounded driving, where passenger language guides trajectory planning, requires vehicles to understand intent before motion. However, most prior instruction-following planners rely on simulation or fixed command vocabularies, limiting real-world generalization. doScenes, the first real-world dataset linking free-form instructions (with referentiality) to nuScenes ground-truth motion, enables instruction-conditioned planning. In this work, we adapt OpenEMMA, an open-source MLLM-based end-to-end driving framework that ingests front-camera views and ego-state and outputs 10-step speed-curvature trajectories, to this setting, presenting a reproducible instruction-conditioned baseline on doScenes and investigate the effects of human instruction prompts on predicted driving behavior. We integrate doScenes directives as passenger-style prompts within OpenEMMA's vision-language interface, enabling linguistic conditioning before trajectory generation. Evaluated on 849 annotated scenes using ADE, we observe that instruction conditioning substantially improves robustness by preventing extreme baseline failures, yielding a 98.7% reduction in mean ADE. When such outliers are removed, instructions still influence trajectory alignment, with well-phrased prompts improving ADE by up to 5.1%. We use this analysis to discuss what makes a \"good\" instruction for the OpenEMMA framework. We release the evaluation prompts and scripts to establish a reproducible baseline for instruction-aware planning. GitHub: https://github.com/Mi3-Lab/doScenes-VLM-Planning",
        "url": "http://arxiv.org/abs/2602.04184v1",
        "published_date": "2026-02-04T03:44:56+00:00",
        "updated_date": "2026-02-04T03:44:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Angel Martinez-Sanchez",
            "Parthib Roy",
            "Ross Greer"
        ],
        "tldr": "The paper introduces doScenes, a real-world dataset for instruction-grounded autonomous driving, and adapts OpenEMMA to utilize natural language instructions for improved trajectory planning, showing substantial improvements in robustness and trajectory alignment.",
        "tldr_zh": "该论文介绍了doScenes，一个用于指令引导自动驾驶的真实世界数据集，并调整了OpenEMMA以利用自然语言指令来改进轨迹规划，在鲁棒性和轨迹对齐方面取得了显著改善。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "DRMOT: A Dataset and Framework for RGBD Referring Multi-Object Tracking",
        "summary": "Referring Multi-Object Tracking (RMOT) aims to track specific targets based on language descriptions and is vital for interactive AI systems such as robotics and autonomous driving. However, existing RMOT models rely solely on 2D RGB data, making it challenging to accurately detect and associate targets characterized by complex spatial semantics (e.g., ``the person closest to the camera'') and to maintain reliable identities under severe occlusion, due to the absence of explicit 3D spatial information. In this work, we propose a novel task, RGBD Referring Multi-Object Tracking (DRMOT), which explicitly requires models to fuse RGB, Depth (D), and Language (L) modalities to achieve 3D-aware tracking. To advance research on the DRMOT task, we construct a tailored RGBD referring multi-object tracking dataset, named DRSet, designed to evaluate models' spatial-semantic grounding and tracking capabilities. Specifically, DRSet contains RGB images and depth maps from 187 scenes, along with 240 language descriptions, among which 56 descriptions incorporate depth-related information. Furthermore, we propose DRTrack, a MLLM-guided depth-referring tracking framework. DRTrack performs depth-aware target grounding from joint RGB-D-L inputs and enforces robust trajectory association by incorporating depth cues. Extensive experiments on the DRSet dataset demonstrate the effectiveness of our framework.",
        "url": "http://arxiv.org/abs/2602.04692v1",
        "published_date": "2026-02-04T15:56:16+00:00",
        "updated_date": "2026-02-04T15:56:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sijia Chen",
            "Lijuan Ma",
            "Yanqiu Yu",
            "En Yu",
            "Liman Liu",
            "Wenbing Tao"
        ],
        "tldr": "The paper introduces DRMOT, a new task for RGBD referring multi-object tracking, along with a dataset (DRSet) and a baseline framework (DRTrack) to address the limitations of existing 2D-based RMOT methods in handling complex spatial semantics and occlusions.",
        "tldr_zh": "本文提出了DRMOT，一个新的RGBD指代多目标跟踪任务，以及一个数据集（DRSet）和一个基线框架（DRTrack），旨在解决现有基于2D的RMOT方法在处理复杂空间语义和遮挡方面的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "S-MUSt3R: Sliding Multi-view 3D Reconstruction",
        "summary": "The recent paradigm shift in 3D vision led to the rise of foundation models with remarkable capabilities in 3D perception from uncalibrated images. However, extending these models to large-scale RGB stream 3D reconstruction remains challenging due to memory limitations. This work proposes S-MUSt3R, a simple and efficient pipeline that extends the limits of foundation models for monocular 3D reconstruction. Our approach addresses the scalability bottleneck of foundation models through a simple strategy of sequence segmentation followed by segment alignment and lightweight loop closure optimization. Without model retraining, we benefit from remarkable 3D reconstruction capacities of MUSt3R model and achieve trajectory and reconstruction performance comparable to traditional methods with more complex architecture. We evaluate S-MUSt3R on TUM, 7-Scenes and proprietary robot navigation datasets and show that S-MUSt3R runs successfully on long RGB sequences and produces accurate and consistent 3D reconstruction. Our results highlight the potential of leveraging the MUSt3R model for scalable monocular 3D scene in real-world settings, with an important advantage of making predictions directly in the metric space.",
        "url": "http://arxiv.org/abs/2602.04517v1",
        "published_date": "2026-02-04T13:07:14+00:00",
        "updated_date": "2026-02-04T13:07:14+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Leonid Antsfeld",
            "Boris Chidlovskii",
            "Yohann Cabon",
            "Vincent Leroy",
            "Jerome Revaud"
        ],
        "tldr": "The paper introduces S-MUSt3R, a scalable pipeline for monocular 3D reconstruction using foundation models, addressing the memory limitations when applied to long RGB video sequences. It achieves comparable performance to traditional methods without retraining the foundation model.",
        "tldr_zh": "该论文介绍了S-MUSt3R，一个可扩展的单目3D重建流水线，它使用基础模型解决了应用于长RGB视频序列时的内存限制问题。它在不重新训练基础模型的情况下，实现了与传统方法相当的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models",
        "summary": "Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments.",
        "url": "http://arxiv.org/abs/2602.04515v1",
        "published_date": "2026-02-04T13:04:56+00:00",
        "updated_date": "2026-02-04T13:04:56+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Yu Bai",
            "MingMing Yu",
            "Chaojie Li",
            "Ziyi Bai",
            "Xinlong Wang",
            "Börje F. Karlsson"
        ],
        "tldr": "The paper introduces EgoActor, a vision-language model (VLM) for humanoid robots that grounds high-level instructions into spatially aware actions, enabling real-time perception and execution across diverse tasks in both simulated and real-world environments.",
        "tldr_zh": "该论文介绍了EgoActor，一种用于人形机器人的视觉-语言模型（VLM），可以将高层指令转化为具有空间意识的动作，从而在模拟和真实环境中实现跨多种任务的实时感知和执行。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Finding NeMO: A Geometry-Aware Representation of Template Views for Few-Shot Perception",
        "summary": "We present Neural Memory Object (NeMO), a novel object-centric representation that can be used to detect, segment and estimate the 6DoF pose of objects unseen during training using RGB images. Our method consists of an encoder that requires only a few RGB template views depicting an object to generate a sparse object-like point cloud using a learned UDF containing semantic and geometric information. Next, a decoder takes the object encoding together with a query image to generate a variety of dense predictions. Through extensive experiments, we show that our method can be used for few-shot object perception without requiring any camera-specific parameters or retraining on target data. Our proposed concept of outsourcing object information in a NeMO and using a single network for multiple perception tasks enhances interaction with novel objects, improving scalability and efficiency by enabling quick object onboarding without retraining or extensive pre-processing. We report competitive and state-of-the-art results on various datasets and perception tasks of the BOP benchmark, demonstrating the versatility of our approach. https://github.com/DLR-RM/nemo",
        "url": "http://arxiv.org/abs/2602.04343v1",
        "published_date": "2026-02-04T09:12:05+00:00",
        "updated_date": "2026-02-04T09:12:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sebastian Jung",
            "Leonard Klüpfel",
            "Rudolph Triebel",
            "Maximilian Durner"
        ],
        "tldr": "The paper introduces Neural Memory Object (NeMO), a novel object-centric representation for few-shot object detection, segmentation, and 6DoF pose estimation of unseen objects using RGB images, achieving competitive results on BOP benchmark.",
        "tldr_zh": "该论文介绍了一种新颖的以对象为中心的表示方法，即神经记忆对象 (NeMO)，用于使用 RGB 图像对未见对象进行少样本对象检测、分割和 6DoF 姿态估计，并在 BOP 基准测试中取得了具有竞争力的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Next-Generation SLAM: A Survey on 3DGS-SLAM Focusing on Performance, Robustness, and Future Directions",
        "summary": "Traditional Simultaneous Localization and Mapping (SLAM) systems often face limitations including coarse rendering quality, insufficient recovery of scene details, and poor robustness in dynamic environments. 3D Gaussian Splatting (3DGS), with its efficient explicit representation and high-quality rendering capabilities, offers a new reconstruction paradigm for SLAM. This survey comprehensively reviews key technical approaches for integrating 3DGS with SLAM. We analyze performance optimization of representative methods across four critical dimensions: rendering quality, tracking accuracy, reconstruction speed, and memory consumption, delving into their design principles and breakthroughs. Furthermore, we examine methods for enhancing the robustness of 3DGS-SLAM in complex environments such as motion blur and dynamic environments. Finally, we discuss future challenges and development trends in this area. This survey aims to provide a technical reference for researchers and foster the development of next-generation SLAM systems characterized by high fidelity, efficiency, and robustness.",
        "url": "http://arxiv.org/abs/2602.04251v1",
        "published_date": "2026-02-04T06:20:22+00:00",
        "updated_date": "2026-02-04T06:20:22+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Li Wang",
            "Ruixuan Gong",
            "Yumo Han",
            "Lei Yang",
            "Lu Yang",
            "Ying Li",
            "Bin Xu",
            "Huaping Liu",
            "Rong Fu"
        ],
        "tldr": "This survey paper reviews the integration of 3D Gaussian Splatting (3DGS) into SLAM systems, focusing on performance optimization, robustness in complex environments, and future research directions, aiming to enable high-fidelity, efficient, and robust next-generation SLAM.",
        "tldr_zh": "该综述论文回顾了将3D高斯溅射（3DGS）集成到SLAM系统中的方法，重点关注性能优化、复杂环境中的鲁棒性以及未来的研究方向，旨在实现高保真、高效和鲁棒的下一代SLAM。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Seeing Through Clutter: Structured 3D Scene Reconstruction via Iterative Object Removal",
        "summary": "We present SeeingThroughClutter, a method for reconstructing structured 3D representations from single images by segmenting and modeling objects individually. Prior approaches rely on intermediate tasks such as semantic segmentation and depth estimation, which often underperform in complex scenes, particularly in the presence of occlusion and clutter. We address this by introducing an iterative object removal and reconstruction pipeline that decomposes complex scenes into a sequence of simpler subtasks. Using VLMs as orchestrators, foreground objects are removed one at a time via detection, segmentation, object removal, and 3D fitting. We show that removing objects allows for cleaner segmentations of subsequent objects, even in highly occluded scenes. Our method requires no task-specific training and benefits directly from ongoing advances in foundation models. We demonstrate stateof-the-art robustness on 3D-Front and ADE20K datasets. Project Page: https://rioak.github.io/seeingthroughclutter/",
        "url": "http://arxiv.org/abs/2602.04053v1",
        "published_date": "2026-02-03T22:37:43+00:00",
        "updated_date": "2026-02-03T22:37:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rio Aguina-Kang",
            "Kevin James Blackburn-Matzen",
            "Thibault Groueix",
            "Vladimir Kim",
            "Matheus Gadelha"
        ],
        "tldr": "The paper introduces SeeingThroughClutter, a novel approach for structured 3D scene reconstruction from single images that iteratively removes foreground objects, leveraging VLMs for improved segmentation and 3D fitting in cluttered scenes without task-specific training.",
        "tldr_zh": "该论文介绍了一种名为 SeeingThroughClutter 的新方法，用于从单张图像重建结构化3D场景。该方法通过迭代移除前景对象，并利用VLMs来提高杂乱场景中的分割和3D拟合效果，且无需特定任务的训练。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Long-Horizon Vision-Language-Action Models via Static-Dynamic Disentanglement",
        "summary": "Vision-Language-Action (VLA) models have recently emerged as a promising paradigm for generalist robotic control. Built upon vision-language model (VLM) architectures, VLAs predict actions conditioned on visual observations and language instructions, achieving strong performance and generalization across tasks. However, VLAs face two major challenges: limited long-horizon context and inefficient inference due to the quadratic attention complexity and large parameter counts. Our work is motivated by the observation that much of the visual information in a trajectory remains static across timesteps (e.g., the background). Leveraging this property, we propose SD-VLA, a framework that disentangles visual inputs into multi-level static and dynamic tokens, which enables (1) retaining a single copy of static tokens across frames to significantly reduce context length, and (2) reusing the key-value (KV) cache of static tokens through a lightweight recache gate that updates only when necessary. This design enables efficient multi-frame integration and efficient inference. In addition, we introduce a new benchmark that more effectively evaluates the long-horizon temporal dependency modeling ability of VLAs. Experimental results show that our approach outperforms baselines on this benchmark by 39.8% absolute improvement in success rate, and achieves a 3.9% gain on the SimplerEnv benchmark. Moreover, SD-VLA delivers a 2.26x inference speedup over the base VLA model on the same benchmark, enabling faster and more practical real-world deployment.",
        "url": "http://arxiv.org/abs/2602.03983v1",
        "published_date": "2026-02-03T20:17:47+00:00",
        "updated_date": "2026-02-03T20:17:47+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Weikang Qiu",
            "Tinglin Huang",
            "Aosong Feng",
            "Rex Ying"
        ],
        "tldr": "The paper introduces SD-VLA, a novel VLA framework that disentangles static and dynamic visual information to improve long-horizon context handling and inference efficiency, validated through a new long-horizon benchmark and showing performance gains and speedups.",
        "tldr_zh": "该论文介绍了SD-VLA，一种新的VLA框架，它解耦静态和动态视觉信息，以提高长程上下文处理和推理效率，并通过新的长程基准进行验证，显示出性能提升和加速。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VLS: Steering Pretrained Robot Policies via Vision-Language Models",
        "summary": "Why do pretrained diffusion or flow-matching policies fail when the same task is performed near an obstacle, on a shifted support surface, or amid mild clutter? Such failures rarely reflect missing motor skills; instead, they expose a limitation of imitation learning under train-test shifts, where action generation is tightly coupled to training-specific spatial configurations and task specifications. Retraining or fine-tuning to address these failures is costly and conceptually misaligned, as the required behaviors already exist but cannot be selectively adapted at test time. We propose Vision-Language Steering (VLS), a training-free framework for inference-time adaptation of frozen generative robot policies. VLS treats adaptation as an inference-time control problem, steering the sampling process of a pretrained diffusion or flow-matching policy in response to out-of-distribution observation-language inputs without modifying policy parameters. By leveraging vision-language models to synthesize trajectory-differentiable reward functions, VLS guides denoising toward action trajectories that satisfy test-time spatial and task requirements. Across simulation and real-world evaluations, VLS consistently outperforms prior steering methods, achieving a 31% improvement on CALVIN and a 13% gain on LIBERO-PRO. Real-world deployment on a Franka robot further demonstrates robust inference-time adaptation under test-time spatial and semantic shifts. Project page: https://vision-language-steering.github.io/webpage/",
        "url": "http://arxiv.org/abs/2602.03973v1",
        "published_date": "2026-02-03T19:50:16+00:00",
        "updated_date": "2026-02-03T19:50:16+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Shuo Liu",
            "Ishneet Sukhvinder Singh",
            "Yiqing Xu",
            "Jiafei Duan",
            "Ranjay Krishna"
        ],
        "tldr": "The paper introduces Vision-Language Steering (VLS), a training-free method that uses vision-language models to adapt pre-trained robot policies to novel environments by steering the sampling process of diffusion or flow-matching policies at inference time.",
        "tldr_zh": "该论文介绍了一种名为视觉-语言引导（VLS）的免训练方法，该方法利用视觉-语言模型，通过在推理时引导扩散或流匹配策略的采样过程，使预训练的机器人策略适应新环境。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]