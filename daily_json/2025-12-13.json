[
    {
        "title": "WorldLens: Full-Spectrum Evaluations of Driving World Models in Real World",
        "summary": "Generative world models are reshaping embodied AI, enabling agents to synthesize realistic 4D driving environments that look convincing but often fail physically or behaviorally. Despite rapid progress, the field still lacks a unified way to assess whether generated worlds preserve geometry, obey physics, or support reliable control. We introduce WorldLens, a full-spectrum benchmark evaluating how well a model builds, understands, and behaves within its generated world. It spans five aspects -- Generation, Reconstruction, Action-Following, Downstream Task, and Human Preference -- jointly covering visual realism, geometric consistency, physical plausibility, and functional reliability. Across these dimensions, no existing world model excels universally: those with strong textures often violate physics, while geometry-stable ones lack behavioral fidelity. To align objective metrics with human judgment, we further construct WorldLens-26K, a large-scale dataset of human-annotated videos with numerical scores and textual rationales, and develop WorldLens-Agent, an evaluation model distilled from these annotations to enable scalable, explainable scoring. Together, the benchmark, dataset, and agent form a unified ecosystem for measuring world fidelity -- standardizing how future models are judged not only by how real they look, but by how real they behave.",
        "url": "http://arxiv.org/abs/2512.10958v1",
        "published_date": "2025-12-11T18:59:58+00:00",
        "updated_date": "2025-12-11T18:59:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ao Liang",
            "Lingdong Kong",
            "Tianyi Yan",
            "Hongsi Liu",
            "Wesley Yang",
            "Ziqi Huang",
            "Wei Yin",
            "Jialong Zuo",
            "Yixuan Hu",
            "Dekai Zhu",
            "Dongyue Lu",
            "Youquan Liu",
            "Guangfeng Jiang",
            "Linfeng Li",
            "Xiangtai Li",
            "Long Zhuo",
            "Lai Xing Ng",
            "Benoit R. Cottereau",
            "Changxin Gao",
            "Liang Pan",
            "Wei Tsang Ooi",
            "Ziwei Liu"
        ],
        "tldr": "The paper introduces WorldLens, a comprehensive benchmark for evaluating driving world models across visual realism, geometric consistency, physical plausibility, and functional reliability, along with a large-scale human-annotated dataset and an evaluation agent for scalable scoring.",
        "tldr_zh": "该论文介绍了WorldLens，一个全面的基准，用于评估驾驶世界模型在视觉真实感、几何一致性、物理合理性和功能可靠性方面的表现，同时还提供了一个大规模的人工标注数据集和一个用于可扩展评分的评估代理。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 10,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Empowering Dynamic Urban Navigation with Stereo and Mid-Level Vision",
        "summary": "The success of foundation models in language and vision motivated research in fully end-to-end robot navigation foundation models (NFMs). NFMs directly map monocular visual input to control actions and ignore mid-level vision modules (tracking, depth estimation, etc) entirely. While the assumption that vision capabilities will emerge implicitly is compelling, it requires large amounts of pixel-to-action supervision that are difficult to obtain. The challenge is especially pronounced in dynamic and unstructured settings, where robust navigation requires precise geometric and dynamic understanding, while the depth-scale ambiguity in monocular views further limits accurate spatial reasoning. In this paper, we show that relying on monocular vision and ignoring mid-level vision priors is inefficient.\n  We present StereoWalker, which augments NFMs with stereo inputs and explicit mid-level vision such as depth estimation and dense pixel tracking. Our intuition is straightforward: stereo inputs resolve the depth-scale ambiguity, and modern mid-level vision models provide reliable geometric and motion structure in dynamic scenes. We also curate a large stereo navigation dataset with automatic action annotation from Internet stereo videos to support training of StereoWalker and to facilitate future research. Through our experiments, we find that mid-level vision enables StereoWalker to achieve a comparable performance as the state-of-the-art using only 1.5% of the training data, and surpasses the state-of-the-art using the full data. We also observe that stereo vision yields higher navigation performance than monocular input.",
        "url": "http://arxiv.org/abs/2512.10956v1",
        "published_date": "2025-12-11T18:59:56+00:00",
        "updated_date": "2025-12-11T18:59:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wentao Zhou",
            "Xuweiyi Chen",
            "Vignesh Rajagopal",
            "Jeffrey Chen",
            "Rohan Chandra",
            "Zezhou Cheng"
        ],
        "tldr": "The paper introduces StereoWalker, a navigation foundation model that incorporates stereo vision and mid-level vision components like depth estimation and pixel tracking, demonstrating improved performance and data efficiency compared to monocular NFMs in dynamic urban environments.",
        "tldr_zh": "该论文介绍了StereoWalker，一种融合了立体视觉和中间层视觉组件（如深度估计和像素跟踪）的导航基础模型，实验证明与单目NFMs相比，在动态城市环境中，其性能更高且数据效率更高。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training",
        "summary": "Self-supervised pre-training has revolutionized foundation models for languages, individual 2D images and videos, but remains largely unexplored for learning 3D-aware representations from multi-view images. In this paper, we present E-RayZer, a self-supervised large 3D Vision model that learns truly 3D-aware representations directly from unlabeled images. Unlike prior self-supervised methods such as RayZer that infer 3D indirectly through latent-space view synthesis, E-RayZer operates directly in 3D space, performing self-supervised 3D reconstruction with Explicit geometry. This formulation eliminates shortcut solutions and yields representations that are geometrically grounded. To ensure convergence and scalability, we introduce a novel fine-grained learning curriculum that organizes training from easy to hard samples and harmonizes heterogeneous data sources in an entirely unsupervised manner. Experiments demonstrate that E-RayZer significantly outperforms RayZer on pose estimation, matches or sometimes surpasses fully supervised reconstruction models such as VGGT. Furthermore, its learned representations outperform leading visual pre-training models (e.g., DINOv3, CroCo v2, VideoMAE V2, and RayZer) when transferring to 3D downstream tasks, establishing E-RayZer as a new paradigm for 3D-aware visual pre-training.",
        "url": "http://arxiv.org/abs/2512.10950v1",
        "published_date": "2025-12-11T18:59:53+00:00",
        "updated_date": "2025-12-11T18:59:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qitao Zhao",
            "Hao Tan",
            "Qianqian Wang",
            "Sai Bi",
            "Kai Zhang",
            "Kalyan Sunkavalli",
            "Shubham Tulsiani",
            "Hanwen Jiang"
        ],
        "tldr": "E-RayZer is a self-supervised 3D vision model that learns 3D-aware representations directly from unlabeled multi-view images using explicit 3D reconstruction, outperforming previous methods on pose estimation and transfer learning to 3D downstream tasks.",
        "tldr_zh": "E-RayZer 是一种自监督 3D 视觉模型，它直接从无标签的多视角图像中学习 3D 感知表示，使用显式的 3D 重建，在姿态估计和迁移学习到 3D 下游任务方面优于以往的方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Towards Efficient and Effective Multi-Camera Encoding for End-to-End Driving",
        "summary": "We present Flex, an efficient and effective scene encoder that addresses the computational bottleneck of processing high-volume multi-camera data in end-to-end autonomous driving. Flex employs a small set of learnable scene tokens to jointly encode information from all image tokens across different cameras and timesteps. By design, our approach is geometry-agnostic, learning a compact scene representation directly from data without relying on the explicit 3D inductive biases, such as Bird-Eye-View (BEV), occupancy or tri-plane representations, which are common in prior work. This holistic encoding strategy aggressively compresses the visual input for the downstream Large Language Model (LLM) based policy model. Evaluated on a large-scale proprietary dataset of 20,000 driving hours, our Flex achieves 2.2x greater inference throughput while improving driving performance by a large margin compared to state-of-the-art methods. Furthermore, we show that these compact scene tokens develop an emergent capability for scene decomposition without any explicit supervision. Our findings challenge the prevailing assumption that 3D priors are necessary, demonstrating that a data-driven, joint encoding strategy offers a more scalable, efficient and effective path for future autonomous driving systems.",
        "url": "http://arxiv.org/abs/2512.10947v1",
        "published_date": "2025-12-11T18:59:46+00:00",
        "updated_date": "2025-12-11T18:59:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiawei Yang",
            "Ziyu Chen",
            "Yurong You",
            "Yan Wang",
            "Yiming Li",
            "Yuxiao Chen",
            "Boyi Li",
            "Boris Ivanovic",
            "Marco Pavone",
            "Yue Wang"
        ],
        "tldr": "The paper introduces Flex, a geometry-agnostic, data-driven scene encoder using learnable tokens for efficient multi-camera data processing in autonomous driving, achieving state-of-the-art performance with improved throughput and driving performance without relying on explicit 3D inductive biases.",
        "tldr_zh": "该论文介绍了一种名为Flex的场景编码器，它采用可学习的tokens，无需显式3D先验，通过数据驱动的方式高效处理自动驾驶中的多摄像头数据，并在吞吐量和驾驶性能方面均达到领先水平。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model",
        "summary": "We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.",
        "url": "http://arxiv.org/abs/2512.10957v1",
        "published_date": "2025-12-11T18:59:56+00:00",
        "updated_date": "2025-12-11T18:59:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yukai Shi",
            "Weiyu Li",
            "Zihao Wang",
            "Hongyang Li",
            "Xingyu Chen",
            "Ping Tan",
            "Lei Zhang"
        ],
        "tldr": "SceneMaker is a decoupled 3D scene generation framework addressing open-set de-occlusion and pose estimation challenges, using enhanced de-occlusion and pose estimation models along with a new open-set dataset.",
        "tldr_zh": "SceneMaker是一个解耦的3D场景生成框架，旨在解决开放场景下的去遮挡和姿态估计难题。它通过增强的去遮挡和姿态估计模型，以及一个新的开放场景数据集来实现。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]