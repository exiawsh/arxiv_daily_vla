[
    {
        "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation",
        "summary": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/.",
        "url": "http://arxiv.org/abs/2509.18592v1",
        "published_date": "2025-09-23T03:23:03+00:00",
        "updated_date": "2025-09-23T03:23:03+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.SY",
            "eess.SY"
        ],
        "authors": [
            "Neel P. Bhatt",
            "Yunhao Yang",
            "Rohan Siva",
            "Pranay Samineni",
            "Daniel Milan",
            "Zhangyang Wang",
            "Ufuk Topcu"
        ],
        "tldr": "The paper introduces VLN-Zero, a novel vision-language navigation framework that uses rapid exploration and a cache-enabled neurosymbolic planner for zero-shot transfer in robot navigation, achieving superior performance compared to existing methods.",
        "tldr_zh": "该论文介绍了VLN-Zero，一种新型的视觉语言导航框架，它利用快速探索和缓存式的神经符号规划器，在机器人导航中实现零样本迁移，并且比现有方法表现更优。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Latent Action Pretraining Through World Modeling",
        "summary": "Vision-Language-Action (VLA) models have gained popularity for learning\nrobotic manipulation tasks that follow language instructions. State-of-the-art\nVLAs, such as OpenVLA and $\\pi_{0}$, were trained on large-scale, manually\nlabeled action datasets collected through teleoperation. More recent\napproaches, including LAPA and villa-X, introduce latent action representations\nthat enable unsupervised pretraining on unlabeled datasets by modeling abstract\nvisual changes between frames. Although these methods have shown strong\nresults, their large model sizes make deployment in real-world settings\nchallenging. In this work, we propose LAWM, a model-agnostic framework to\npretrain imitation learning models in a self-supervised way, by learning latent\naction representations from unlabeled video data through world modeling. These\nvideos can be sourced from robot recordings or videos of humans performing\nactions with everyday objects. Our framework is designed to be effective for\ntransferring across tasks, environments, and embodiments. It outperforms models\ntrained with ground-truth robotics actions and similar pretraining methods on\nthe LIBERO benchmark and real-world setup, while being significantly more\nefficient and practical for real-world settings.",
        "url": "http://arxiv.org/abs/2509.18428v1",
        "published_date": "2025-09-22T21:19:10+00:00",
        "updated_date": "2025-09-22T21:19:10+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Bahey Tharwat",
            "Yara Nasser",
            "Ali Abouzeid",
            "Ian Reid"
        ],
        "tldr": "The paper introduces LAWM, a model-agnostic framework for self-supervised pretraining of imitation learning models using latent action representations learned from unlabeled video data, achieving superior performance and efficiency on robotic tasks.",
        "tldr_zh": "该论文介绍了LAWM，一个模型无关的框架，用于使用从无标签视频数据中学习到的潜在动作表示，对模仿学习模型进行自监督预训练，在机器人任务上实现了卓越的性能和效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation",
        "summary": "The ability to generate virtual environments is crucial for applications\nranging from gaming to physical AI domains such as robotics, autonomous\ndriving, and industrial AI. Current learning-based 3D reconstruction methods\nrely on the availability of captured real-world multi-view data, which is not\nalways readily available. Recent advancements in video diffusion models have\nshown remarkable imagination capabilities, yet their 2D nature limits the\napplications to simulation where a robot needs to navigate and interact with\nthe environment. In this paper, we propose a self-distillation framework that\naims to distill the implicit 3D knowledge in the video diffusion models into an\nexplicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for\nmulti-view training data. Specifically, we augment the typical RGB decoder with\na 3DGS decoder, which is supervised by the output of the RGB decoder. In this\napproach, the 3DGS decoder can be purely trained with synthetic data generated\nby video diffusion models. At inference time, our model can synthesize 3D\nscenes from either a text prompt or a single image for real-time rendering. Our\nframework further extends to dynamic 3D scene generation from a monocular input\nvideo. Experimental results show that our framework achieves state-of-the-art\nperformance in static and dynamic 3D scene generation.",
        "url": "http://arxiv.org/abs/2509.19296v1",
        "published_date": "2025-09-23T17:58:01+00:00",
        "updated_date": "2025-09-23T17:58:01+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Sherwin Bahmani",
            "Tianchang Shen",
            "Jiawei Ren",
            "Jiahui Huang",
            "Yifeng Jiang",
            "Haithem Turki",
            "Andrea Tagliasacchi",
            "David B. Lindell",
            "Zan Gojcic",
            "Sanja Fidler",
            "Huan Ling",
            "Jun Gao",
            "Xuanchi Ren"
        ],
        "tldr": "The paper introduces Lyra, a self-distillation framework that leverages video diffusion models to generate 3D scenes from text prompts, single images, or monocular videos by distilling knowledge into a 3D Gaussian Splatting representation, achieving state-of-the-art results.",
        "tldr_zh": "该论文介绍了 Lyra，一个自蒸馏框架，利用视频扩散模型从文本提示、单张图像或单目视频生成 3D 场景，通过将知识提炼成 3D Gaussian Splatting 表示来实现，并取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Category-Level Object Shape and Pose Estimation in Less Than a Millisecond",
        "summary": "Object shape and pose estimation is a foundational robotics problem,\nsupporting tasks from manipulation to scene understanding and navigation. We\npresent a fast local solver for shape and pose estimation which requires only\ncategory-level object priors and admits an efficient certificate of global\noptimality. Given an RGB-D image of an object, we use a learned front-end to\ndetect sparse, category-level semantic keypoints on the target object. We\nrepresent the target object's unknown shape using a linear active shape model\nand pose a maximum a posteriori optimization problem to solve for position,\norientation, and shape simultaneously. Expressed in unit quaternions, this\nproblem admits first-order optimality conditions in the form of an eigenvalue\nproblem with eigenvector nonlinearities. Our primary contribution is to solve\nthis problem efficiently with self-consistent field iteration, which only\nrequires computing a 4-by-4 matrix and finding its minimum eigenvalue-vector\npair at each iterate. Solving a linear system for the corresponding Lagrange\nmultipliers gives a simple global optimality certificate. One iteration of our\nsolver runs in about 100 microseconds, enabling fast outlier rejection. We test\nour method on synthetic data and a variety of real-world settings, including\ntwo public datasets and a drone tracking scenario. Code is released at\nhttps://github.com/MIT-SPARK/Fast-ShapeAndPose.",
        "url": "http://arxiv.org/abs/2509.18979v1",
        "published_date": "2025-09-23T13:29:32+00:00",
        "updated_date": "2025-09-23T13:29:32+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Lorenzo Shaikewitz",
            "Tim Nguyen",
            "Luca Carlone"
        ],
        "tldr": "This paper presents a fast, category-level object shape and pose estimation method using a local solver with a global optimality certificate, achieving sub-millisecond performance.",
        "tldr_zh": "本文提出了一种快速的类别级物体形状和姿态估计方法，该方法使用具有全局最优性证书的局部求解器，实现了亚毫秒级的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation",
        "summary": "LiDAR-based localization and SLAM often rely on iterative matching\nalgorithms, particularly the Iterative Closest Point (ICP) algorithm, to align\nsensor data with pre-existing maps or previous scans. However, ICP is prone to\nerrors in featureless environments and dynamic scenes, leading to inaccurate\npose estimation. Accurately predicting the uncertainty associated with ICP is\ncrucial for robust state estimation but remains challenging, as existing\napproaches often rely on handcrafted models or simplified assumptions.\nMoreover, a few deep learning-based methods for localizability estimation\neither depend on a pre-built map, which may not always be available, or provide\na binary classification of localizable versus non-localizable, which fails to\nproperly model uncertainty. In this work, we propose a data-driven framework\nthat leverages deep learning to estimate the registration error covariance of\nICP before matching, even in the absence of a reference map. By associating\neach LiDAR scan with a reliable 6-DoF error covariance estimate, our method\nenables seamless integration of ICP within Kalman filtering, enhancing\nlocalization accuracy and robustness. Extensive experiments on the KITTI\ndataset demonstrate the effectiveness of our approach, showing that it\naccurately predicts covariance and, when applied to localization using a\npre-built map or SLAM, reduces localization errors and improves robustness.",
        "url": "http://arxiv.org/abs/2509.18954v1",
        "published_date": "2025-09-23T13:02:44+00:00",
        "updated_date": "2025-09-23T13:02:44+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Minoo Dolatabadi",
            "Fardin Ayar",
            "Ehsan Javanmardi",
            "Manabu Tsukada",
            "Mahdi Javanmardi"
        ],
        "tldr": "This paper presents a deep learning-based method for estimating the error covariance of ICP registration, improving LiDAR localization robustness, especially in challenging environments, without requiring a pre-built map.",
        "tldr_zh": "该论文提出了一种基于深度学习的方法，用于估计ICP配准的误差协方差，从而提高激光雷达定位的鲁棒性，尤其是在具有挑战性的环境中，且无需预先构建的地图。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models",
        "summary": "Autonomous vehicles (AVs) are expected to revolutionize transportation by\nimproving efficiency and safety. Their success relies on 3D vision systems that\neffectively sense the environment and detect traffic agents. Among sensors AVs\nuse to create a comprehensive view of surroundings, LiDAR provides\nhigh-resolution depth data enabling accurate object detection, safe navigation,\nand collision avoidance. However, collecting real-world LiDAR data is\ntime-consuming and often affected by noise and sparsity due to adverse weather\nor sensor limitations. This work applies a denoising diffusion probabilistic\nmodel (DDPM), enhanced with novel noise scheduling and time-step embedding\ntechniques to generate high-quality synthetic data for augmentation, thereby\nimproving performance across a range of computer vision tasks, particularly in\nAV perception. These modifications impact the denoising process and the model's\ntemporal awareness, allowing it to produce more realistic point clouds based on\nthe projection. The proposed method was extensively evaluated under various\nconfigurations using the IAMCV and KITTI-360 datasets, with four performance\nmetrics compared against state-of-the-art (SOTA) methods. The results\ndemonstrate the model's superior performance over most existing baselines and\nits effectiveness in mitigating the effects of noisy and sparse LiDAR data,\nproducing diverse point clouds with rich spatial relationships and structural\ndetail.",
        "url": "http://arxiv.org/abs/2509.18917v1",
        "published_date": "2025-09-23T12:35:07+00:00",
        "updated_date": "2025-09-23T12:35:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Amirhesam Aghanouri",
            "Cristina Olaverri-Monreal"
        ],
        "tldr": "This paper introduces a novel denoising diffusion probabilistic model (DDPM) with noise scheduling and time-step embedding techniques to generate high-quality synthetic LiDAR point cloud data for autonomous vehicle perception, showing superior performance compared to SOTA methods on IAMCV and KITTI-360 datasets.",
        "tldr_zh": "本文提出了一种新颖的去噪扩散概率模型（DDPM），该模型具有噪声调度和时间步长嵌入技术，用于生成高质量的合成激光雷达点云数据，用于自动驾驶汽车感知，并在IAMCV和KITTI-360数据集上显示出优于SOTA方法的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing",
        "summary": "LiDAR-based perception is central to autonomous driving and robotics, yet raw\npoint clouds remain highly vulnerable to noise, occlusion, and adversarial\ncorruptions. Autoencoders offer a natural framework for denoising and\nreconstruction, but their performance degrades under challenging real-world\nconditions. In this work, we propose TriFusion-AE, a multimodal cross-attention\nautoencoder that integrates textual priors, monocular depth maps from\nmulti-view images, and LiDAR point clouds to improve robustness. By aligning\nsemantic cues from text, geometric (depth) features from images, and spatial\nstructure from LiDAR, TriFusion-AE learns representations that are resilient to\nstochastic noise and adversarial perturbations. Interestingly, while showing\nlimited gains under mild perturbations, our model achieves significantly more\nrobust reconstruction under strong adversarial attacks and heavy noise, where\nCNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to\nreflect realistic low-data deployment scenarios. Our multimodal fusion\nframework is designed to be model-agnostic, enabling seamless integration with\nany CNN-based point cloud autoencoder for joint representation learning.",
        "url": "http://arxiv.org/abs/2509.18743v1",
        "published_date": "2025-09-23T07:37:28+00:00",
        "updated_date": "2025-09-23T07:37:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Susmit Neogi"
        ],
        "tldr": "This paper introduces TriFusion-AE, a multimodal autoencoder fusing LiDAR, depth maps, and textual priors to improve the robustness of point cloud processing in autonomous driving, particularly under noise and adversarial attacks.",
        "tldr_zh": "本文介绍了TriFusion-AE，一种多模态自编码器，融合了激光雷达、深度图和文本先验，以提高自动驾驶中点云处理的鲁棒性，尤其是在噪声和对抗性攻击下。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving",
        "summary": "The emerging 4D millimeter-wave radar, measuring the range, azimuth,\nelevation, and Doppler velocity of objects, is recognized for its\ncost-effectiveness and robustness in autonomous driving. Nevertheless, its\npoint clouds exhibit significant sparsity and noise, restricting its standalone\napplication in 3D object detection. Recent 4D radar-camera fusion methods have\nprovided effective perception. Most existing approaches, however, adopt\nexplicit Bird's-Eye-View fusion paradigms originally designed for LiDAR-camera\nfusion, neglecting radar's inherent drawbacks. Specifically, they overlook the\nsparse and incomplete geometry of radar point clouds and restrict fusion to\ncoarse scene-level integration. To address these problems, we propose\nMLF-4DRCNet, a novel two-stage framework for 3D object detection via\nmulti-level fusion of 4D radar and camera images. Our model incorporates the\npoint-, scene-, and proposal-level multi-modal information, enabling\ncomprehensive feature representation. It comprises three crucial components:\nthe Enhanced Radar Point Encoder (ERPE) module, the Hierarchical Scene Fusion\nPooling (HSFP) module, and the Proposal-Level Fusion Enhancement (PLFE) module.\nOperating at the point-level, ERPE densities radar point clouds with 2D image\ninstances and encodes them into voxels via the proposed Triple-Attention Voxel\nFeature Encoder. HSFP dynamically integrates multi-scale voxel features with 2D\nimage features using deformable attention to capture scene context and adopts\npooling to the fused features. PLFE refines region proposals by fusing image\nfeatures, and further integrates with the pooled features from HSFP.\nExperimental results on the View-of-Delft (VoD) and TJ4DRadSet datasets\ndemonstrate that MLF-4DRCNet achieves the state-of-the-art performance.\nNotably, it attains performance comparable to LiDAR-based models on the VoD\ndataset.",
        "url": "http://arxiv.org/abs/2509.18613v1",
        "published_date": "2025-09-23T04:02:28+00:00",
        "updated_date": "2025-09-23T04:02:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuzhi Wu",
            "Li Xiao",
            "Jun Liu",
            "Guangfeng Jiang",
            "XiangGen Xia"
        ],
        "tldr": "The paper introduces MLF-4DRCNet, a novel two-stage framework for 3D object detection using multi-level fusion of 4D radar and camera data, achieving state-of-the-art performance and comparable results to LiDAR-based models.",
        "tldr_zh": "该论文介绍了一种新颖的 MLF-4DRCNet 框架，用于通过 4D 雷达和相机数据的多层次融合进行 3D 对象检测，实现了最先进的性能，并取得了与基于 LiDAR 的模型相当的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task Bird's Eye View Perception and Planning",
        "summary": "We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework\nthat distills the full-stack capabilities of a large planning-oriented teacher\n(UniAD [19]) into a compact, real-time student model. Unlike prior efficient\ncamera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the\ncomplete autonomy stack 3D detection, HD-map segmentation, motion forecasting,\noccupancy prediction, and goal-directed planning within a streamlined\n28M-parameter backbone, achieving a 78% reduction in parameters over UniAD\n[19]. Our model-agnostic, multi-stage distillation strategy combines\nfeature-level, output-level, and adaptive region-aware supervision to\neffectively transfer high-capacity multi-modal knowledge to a lightweight BEV\nrepresentation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08\nminADE for motion forecasting, and a 0.32 collision rate, while running 5x\nfaster (11 FPS) and requiring only camera input. These results demonstrate that\nfull-stack driving intelligence can be retained in resource-constrained\nsettings, bridging the gap between large-scale, multi-modal perception-planning\nmodels and deployment-ready real-time autonomy.",
        "url": "http://arxiv.org/abs/2509.18372v1",
        "published_date": "2025-09-22T19:54:02+00:00",
        "updated_date": "2025-09-22T19:54:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Reeshad Khan",
            "John Gauch"
        ],
        "tldr": "TinyBEV presents a camera-only BEV framework that distills the capabilities of a large planning model into a compact, real-time student model, achieving significant parameter reduction and speedup while retaining full-stack driving intelligence.",
        "tldr_zh": "TinyBEV 提出了一个纯视觉的鸟瞰图（BEV）框架，将大型规划模型的能力提炼成一个紧凑的实时学生模型，在保留全栈驾驶智能的同时，显著减少了参数和提高了速度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation",
        "summary": "Accurate localisation is critical for mobile robots in structured outdoor\nenvironments, yet LiDAR-based methods often fail in vineyards due to repetitive\nrow geometry and perceptual aliasing. We propose a semantic particle filter\nthat incorporates stable object-level detections, specifically vine trunks and\nsupport poles into the likelihood estimation process. Detected landmarks are\nprojected into a birds eye view and fused with LiDAR scans to generate semantic\nobservations. A key innovation is the use of semantic walls, which connect\nadjacent landmarks into pseudo-structural constraints that mitigate row\naliasing. To maintain global consistency in headland regions where semantics\nare sparse, we introduce a noisy GPS prior that adaptively supports the filter.\nExperiments in a real vineyard demonstrate that our approach maintains\nlocalisation within the correct row, recovers from deviations where AMCL fails,\nand outperforms vision-based SLAM methods such as RTAB-Map.",
        "url": "http://arxiv.org/abs/2509.18342v1",
        "published_date": "2025-09-22T19:04:31+00:00",
        "updated_date": "2025-09-22T19:04:31+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Rajitha de Silva",
            "Jonathan Cox",
            "James R. Heselden",
            "Marija Popovic",
            "Cesar Cadena",
            "Riccardo Polvara"
        ],
        "tldr": "This paper presents a semantic particle filter for vineyard robot localization, leveraging LiDAR and object detection to overcome challenges of repetitive row structures and perceptual aliasing. It uses semantic walls and GPS prior to improve robustness and outperforms existing methods.",
        "tldr_zh": "该论文提出了一种语义粒子滤波器，用于葡萄园机器人定位，利用激光雷达和物体检测来克服重复行结构和感知混淆的挑战。它使用语义墙和 GPS 先验来提高鲁棒性，并且优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction",
        "summary": "3D Gaussian Splatting (3DGS) has become a powerful representation for\nimage-based object reconstruction, yet its performance drops sharply in\nsparse-view settings. Prior works address this limitation by employing\ndiffusion models to repair corrupted renders, subsequently using them as pseudo\nground truths for later optimization. While effective, such approaches incur\nheavy computation from the diffusion fine-tuning and repair steps. We present\nWaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object\nreconstruction. Our key idea is to shift diffusion into the wavelet domain:\ndiffusion is applied only to the low-resolution LL subband, while\nhigh-frequency subbands are refined with a lightweight network. We further\npropose an efficient online random masking strategy to curate training pairs\nfor diffusion fine-tuning, replacing the commonly used, but inefficient,\nleave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360\nand OmniObject3D, show WaveletGaussian achieves competitive rendering quality\nwhile substantially reducing training time.",
        "url": "http://arxiv.org/abs/2509.19073v1",
        "published_date": "2025-09-23T14:34:10+00:00",
        "updated_date": "2025-09-23T14:34:10+00:00",
        "categories": [
            "cs.CV",
            "eess.IV",
            "eess.SP"
        ],
        "authors": [
            "Hung Nguyen",
            "Runfa Li",
            "An Le",
            "Truong Nguyen"
        ],
        "tldr": "The paper introduces WaveletGaussian, a novel framework for efficient sparse-view 3D Gaussian object reconstruction by performing diffusion in the wavelet domain and using an efficient online masking strategy, achieving competitive rendering quality with reduced training time.",
        "tldr_zh": "该论文介绍了 WaveletGaussian，一种通过在小波域中进行扩散和使用高效的在线掩码策略来实现高效稀疏视图 3D 高斯物体重建的新框架，它在降低训练时间的同时实现了具有竞争力的渲染质量。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]