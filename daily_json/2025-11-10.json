[
    {
        "title": "VLDrive: Vision-Augmented Lightweight MLLMs for Efficient Language-grounded Autonomous Driving",
        "summary": "Recent advancements in language-grounded autonomous driving have been\nsignificantly promoted by the sophisticated cognition and reasoning\ncapabilities of large language models (LLMs). However, current LLM-based\napproaches encounter critical challenges: (1) Failure analysis reveals that\nfrequent collisions and obstructions, stemming from limitations in visual\nrepresentations, remain primary obstacles to robust driving performance. (2)\nThe substantial parameters of LLMs pose considerable deployment hurdles. To\naddress these limitations, we introduce VLDrive, a novel approach featuring a\nlightweight MLLM architecture with enhanced vision components. VLDrive achieves\ncompact visual tokens through innovative strategies, including cycle-consistent\ndynamic visual pruning and memory-enhanced feature aggregation. Furthermore, we\npropose a distance-decoupled instruction attention mechanism to improve joint\nvisual-linguistic feature learning, particularly for long-range visual tokens.\nExtensive experiments conducted in the CARLA simulator demonstrate VLDrive`s\neffectiveness. Notably, VLDrive achieves state-of-the-art driving performance\nwhile reducing parameters by 81% (from 7B to 1.3B), yielding substantial\ndriving score improvements of 15.4%, 16.8%, and 7.6% at tiny, short, and long\ndistances, respectively, in closed-loop evaluations. Code is available at\nhttps://github.com/ReaFly/VLDrive.",
        "url": "http://arxiv.org/abs/2511.06256v1",
        "published_date": "2025-11-09T07:14:53+00:00",
        "updated_date": "2025-11-09T07:14:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruifei Zhang",
            "Wei Zhang",
            "Xiao Tan",
            "Sibei Yang",
            "Xiang Wan",
            "Xiaonan Luo",
            "Guanbin Li"
        ],
        "tldr": "VLDrive, a lightweight MLLM for autonomous driving, addresses limitations in visual representations and model size by using visual token compression and distance-aware attention, achieving SOTA performance with significantly fewer parameters.",
        "tldr_zh": "VLDrive是一种用于自动驾驶的轻量级MLLM，它通过视觉token压缩和距离感知注意力机制解决了视觉表示和模型大小的限制，以更少的参数实现了SOTA性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "VDNeRF: Vision-only Dynamic Neural Radiance Field for Urban Scenes",
        "summary": "Neural Radiance Fields (NeRFs) implicitly model continuous three-dimensional\nscenes using a set of images with known camera poses, enabling the rendering of\nphotorealistic novel views. However, existing NeRF-based methods encounter\nchallenges in applications such as autonomous driving and robotic perception,\nprimarily due to the difficulty of capturing accurate camera poses and\nlimitations in handling large-scale dynamic environments. To address these\nissues, we propose Vision-only Dynamic NeRF (VDNeRF), a method that accurately\nrecovers camera trajectories and learns spatiotemporal representations for\ndynamic urban scenes without requiring additional camera pose information or\nexpensive sensor data. VDNeRF employs two separate NeRF models to jointly\nreconstruct the scene. The static NeRF model optimizes camera poses and static\nbackground, while the dynamic NeRF model incorporates the 3D scene flow to\nensure accurate and consistent reconstruction of dynamic objects. To address\nthe ambiguity between camera motion and independent object motion, we design an\neffective and powerful training framework to achieve robust camera pose\nestimation and self-supervised decomposition of static and dynamic elements in\na scene. Extensive evaluations on mainstream urban driving datasets demonstrate\nthat VDNeRF surpasses state-of-the-art NeRF-based pose-free methods in both\ncamera pose estimation and dynamic novel view synthesis.",
        "url": "http://arxiv.org/abs/2511.06408v1",
        "published_date": "2025-11-09T14:45:08+00:00",
        "updated_date": "2025-11-09T14:45:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhengyu Zou",
            "Jingfeng Li",
            "Hao Li",
            "Xiaolei Hou",
            "Jinwen Hu",
            "Jingkun Chen",
            "Lechao Cheng",
            "Dingwen Zhang"
        ],
        "tldr": "VDNeRF addresses the challenge of NeRFs in dynamic urban environments by jointly optimizing camera poses and spatiotemporal representations without requiring additional sensor data, achieving state-of-the-art performance in camera pose estimation and dynamic novel view synthesis.",
        "tldr_zh": "VDNeRF通过联合优化相机姿态和时空表示，解决了NeRF在动态城市环境中的挑战，无需额外的传感器数据，并在相机姿态估计和动态新视角合成方面实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ArtReg: Visuo-Tactile based Pose Tracking and Manipulation of Unseen Articulated Objects",
        "summary": "Robots operating in real-world environments frequently encounter unknown\nobjects with complex structures and articulated components, such as doors,\ndrawers, cabinets, and tools. The ability to perceive, track, and manipulate\nthese objects without prior knowledge of their geometry or kinematic properties\nremains a fundamental challenge in robotics. In this work, we present a novel\nmethod for visuo-tactile-based tracking of unseen objects (single, multiple, or\narticulated) during robotic interaction without assuming any prior knowledge\nregarding object shape or dynamics. Our novel pose tracking approach termed\nArtReg (stands for Articulated Registration) integrates visuo-tactile point\nclouds in an unscented Kalman Filter formulation in the SE(3) Lie Group for\npoint cloud registration. ArtReg is used to detect possible articulated joints\nin objects using purposeful manipulation maneuvers such as pushing or\nhold-pulling with a two-robot team. Furthermore, we leverage ArtReg to develop\na closed-loop controller for goal-driven manipulation of articulated objects to\nmove the object into the desired pose configuration. We have extensively\nevaluated our approach on various types of unknown objects through real robot\nexperiments. We also demonstrate the robustness of our method by evaluating\nobjects with varying center of mass, low-light conditions, and with challenging\nvisual backgrounds. Furthermore, we benchmarked our approach on a standard\ndataset of articulated objects and demonstrated improved performance in terms\nof pose accuracy compared to state-of-the-art methods. Our experiments indicate\nthat robust and accurate pose tracking leveraging visuo-tactile information\nenables robots to perceive and interact with unseen complex articulated objects\n(with revolute or prismatic joints).",
        "url": "http://arxiv.org/abs/2511.06378v1",
        "published_date": "2025-11-09T13:30:51+00:00",
        "updated_date": "2025-11-09T13:30:51+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Prajval Kumar Murali",
            "Mohsen Kaboli"
        ],
        "tldr": "This paper presents ArtReg, a visuo-tactile approach for tracking and manipulating unseen articulated objects using an unscented Kalman Filter within the SE(3) Lie Group, validated through real-robot experiments and benchmarked against state-of-the-art methods.",
        "tldr_zh": "本文提出了ArtReg，一种基于视觉触觉的方法，使用SE(3)李群中的无迹卡尔曼滤波器来跟踪和操纵未见过的铰接物体。通过真实的机器人实验验证，并与最先进的方法进行了基准测试。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "LaneDiffusion: Improving Centerline Graph Learning via Prior Injected BEV Feature Generation",
        "summary": "Centerline graphs, crucial for path planning in autonomous driving, are\ntraditionally learned using deterministic methods. However, these methods often\nlack spatial reasoning and struggle with occluded or invisible centerlines.\nGenerative approaches, despite their potential, remain underexplored in this\ndomain. We introduce LaneDiffusion, a novel generative paradigm for centerline\ngraph learning. LaneDiffusion innovatively employs diffusion models to generate\nlane centerline priors at the Bird's Eye View (BEV) feature level, instead of\ndirectly predicting vectorized centerlines. Our method integrates a Lane Prior\nInjection Module (LPIM) and a Lane Prior Diffusion Module (LPDM) to effectively\nconstruct diffusion targets and manage the diffusion process. Furthermore,\nvectorized centerlines and topologies are then decoded from these\nprior-injected BEV features. Extensive evaluations on the nuScenes and\nArgoverse2 datasets demonstrate that LaneDiffusion significantly outperforms\nexisting methods, achieving improvements of 4.2%, 4.6%, 4.7%, 6.4% and 1.8% on\nfine-grained point-level metrics (GEO F1, TOPO F1, JTOPO F1, APLS and SDA) and\n2.3%, 6.4%, 6.8% and 2.1% on segment-level metrics (IoU, mAP_cf, DET_l and\nTOP_ll). These results establish state-of-the-art performance in centerline\ngraph learning, offering new insights into generative models for this task.",
        "url": "http://arxiv.org/abs/2511.06272v1",
        "published_date": "2025-11-09T08:15:58+00:00",
        "updated_date": "2025-11-09T08:15:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zijie Wang",
            "Weiming Zhang",
            "Wei Zhang",
            "Xiao Tan",
            "Hongxing Liu",
            "Yaowei Wang",
            "Guanbin Li"
        ],
        "tldr": "LaneDiffusion introduces a novel generative approach using diffusion models at the BEV feature level for centerline graph learning in autonomous driving, achieving state-of-the-art performance on nuScenes and Argoverse2 datasets.",
        "tldr_zh": "LaneDiffusion 提出了一种新颖的生成式方法，利用扩散模型在鸟瞰图（BEV）特征级别进行自动驾驶中的中心线图学习，在nuScenes和Argoverse2数据集上实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AdaDrive: Self-Adaptive Slow-Fast System for Language-Grounded Autonomous Driving",
        "summary": "Effectively integrating Large Language Models (LLMs) into autonomous driving\nrequires a balance between leveraging high-level reasoning and maintaining\nreal-time efficiency. Existing approaches either activate LLMs too frequently,\ncausing excessive computational overhead, or use fixed schedules, failing to\nadapt to dynamic driving conditions. To address these challenges, we propose\nAdaDrive, an adaptively collaborative slow-fast framework that optimally\ndetermines when and how LLMs contribute to decision-making. (1) When to\nactivate the LLM: AdaDrive employs a novel adaptive activation loss that\ndynamically determines LLM invocation based on a comparative learning\nmechanism, ensuring activation only in complex or critical scenarios. (2) How\nto integrate LLM assistance: Instead of rigid binary activation, AdaDrive\nintroduces an adaptive fusion strategy that modulates a continuous, scaled LLM\ninfluence based on scene complexity and prediction confidence, ensuring\nseamless collaboration with conventional planners. Through these strategies,\nAdaDrive provides a flexible, context-aware framework that maximizes decision\naccuracy without compromising real-time performance. Extensive experiments on\nlanguage-grounded autonomous driving benchmarks demonstrate that AdaDrive\nstate-of-the-art performance in terms of both driving accuracy and\ncomputational efficiency. Code is available at\nhttps://github.com/ReaFly/AdaDrive.",
        "url": "http://arxiv.org/abs/2511.06253v1",
        "published_date": "2025-11-09T07:05:03+00:00",
        "updated_date": "2025-11-09T07:05:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruifei Zhang",
            "Junlin Xie",
            "Wei Zhang",
            "Weikai Chen",
            "Xiao Tan",
            "Xiang Wan",
            "Guanbin Li"
        ],
        "tldr": "AdaDrive is a novel autonomous driving framework that adaptively integrates LLMs by dynamically determining when and how to leverage them, improving both accuracy and efficiency.",
        "tldr_zh": "AdaDrive是一个新颖的自动驾驶框架，它通过动态地决定何时以及如何利用大型语言模型来适应性地集成它们，从而提高准确性和效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DiffusionUavLoc: Visually Prompted Diffusion for Cross-View UAV Localization",
        "summary": "With the rapid growth of the low-altitude economy, unmanned aerial vehicles\n(UAVs) have become key platforms for measurement and tracking in intelligent\npatrol systems. However, in GNSS-denied environments, localization schemes that\nrely solely on satellite signals are prone to failure. Cross-view image\nretrieval-based localization is a promising alternative, yet substantial\ngeometric and appearance domain gaps exist between oblique UAV views and nadir\nsatellite orthophotos. Moreover, conventional approaches often depend on\ncomplex network architectures, text prompts, or large amounts of annotation,\nwhich hinders generalization. To address these issues, we propose\nDiffusionUavLoc, a cross-view localization framework that is image-prompted,\ntext-free, diffusion-centric, and employs a VAE for unified representation. We\nfirst use training-free geometric rendering to synthesize pseudo-satellite\nimages from UAV imagery as structural prompts. We then design a text-free\nconditional diffusion model that fuses multimodal structural cues to learn\nfeatures robust to viewpoint changes. At inference, descriptors are computed at\na fixed time step t and compared using cosine similarity. On University-1652\nand SUES-200, the method performs competitively for cross-view localization,\nespecially for satellite-to-drone in University-1652.Our data and code will be\npublished at the following URL:\nhttps://github.com/liutao23/DiffusionUavLoc.git.",
        "url": "http://arxiv.org/abs/2511.06422v1",
        "published_date": "2025-11-09T15:27:17+00:00",
        "updated_date": "2025-11-09T15:27:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tao Liu",
            "Kan Ren",
            "Qian Chen"
        ],
        "tldr": "The paper proposes DiffusionUavLoc, a novel cross-view localization framework for UAVs in GNSS-denied environments, utilizing image-prompted diffusion models and VAE for robust feature learning. It achieves competitive performance on cross-view localization tasks.",
        "tldr_zh": "本文提出了DiffusionUavLoc，一种用于GNSS受限环境下无人机的新型跨视角定位框架，利用图像提示扩散模型和VAE进行鲁棒的特征学习。该方法在跨视角定位任务上取得了具有竞争力的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Learning-Based Vision Systems for Semi-Autonomous Forklift Operation in Industrial Warehouse Environments",
        "summary": "The automation of material handling in warehouses increasingly relies on\nrobust, low cost perception systems for forklifts and Automated Guided Vehicles\n(AGVs). This work presents a vision based framework for pallet and pallet hole\ndetection and mapping using a single standard camera. We utilized YOLOv8 and\nYOLOv11 architectures, enhanced through Optuna driven hyperparameter\noptimization and spatial post processing. An innovative pallet hole mapping\nmodule converts the detections into actionable spatial representations,\nenabling accurate pallet and pallet hole association for forklift operation.\nExperiments on a custom dataset augmented with real warehouse imagery show that\nYOLOv8 achieves high pallet and pallet hole detection accuracy, while YOLOv11,\nparticularly under optimized configurations, offers superior precision and\nstable convergence. The results demonstrate the feasibility of a cost\neffective, retrofittable visual perception module for forklifts. This study\nproposes a scalable approach to advancing warehouse automation, promoting\nsafer, economical, and intelligent logistics operations.",
        "url": "http://arxiv.org/abs/2511.06295v1",
        "published_date": "2025-11-09T09:13:22+00:00",
        "updated_date": "2025-11-09T09:13:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Vamshika Sutar",
            "Mahek Maheshwari",
            "Archak Mittal"
        ],
        "tldr": "This paper explores a vision-based system using YOLOv8 and YOLOv11, optimized with Optuna, for pallet and pallet hole detection in warehouse environments to enable semi-autonomous forklift operation, demonstrating the feasibility of a cost-effective and retrofittable perception module.",
        "tldr_zh": "该论文探讨了一种基于视觉的系统，使用YOLOv8和YOLOv11，并通过Optuna进行优化，用于仓库环境中托盘和托盘孔的检测，以实现半自动叉车操作，证明了经济高效且可改装的感知模块的可行性。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]