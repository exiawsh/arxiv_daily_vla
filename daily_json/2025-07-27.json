[
    {
        "title": "FROSS: Faster-than-Real-Time Online 3D Semantic Scene Graph Generation from RGB-D Images",
        "summary": "The ability to abstract complex 3D environments into simplified and\nstructured representations is crucial across various domains. 3D semantic scene\ngraphs (SSGs) achieve this by representing objects as nodes and their\ninterrelationships as edges, facilitating high-level scene understanding.\nExisting methods for 3D SSG generation, however, face significant challenges,\nincluding high computational demands and non-incremental processing that hinder\ntheir suitability for real-time open-world applications. To address this issue,\nwe propose FROSS (Faster-than-Real-Time Online 3D Semantic Scene Graph\nGeneration), an innovative approach for online and faster-than-real-time 3D SSG\ngeneration that leverages the direct lifting of 2D scene graphs to 3D space and\nrepresents objects as 3D Gaussian distributions. This framework eliminates the\ndependency on precise and computationally-intensive point cloud processing.\nFurthermore, we extend the Replica dataset with inter-object relationship\nannotations, creating the ReplicaSSG dataset for comprehensive evaluation of\nFROSS. The experimental results from evaluations on ReplicaSSG and 3DSSG\ndatasets show that FROSS can achieve superior performance while operating\nsignificantly faster than prior 3D SSG generation methods. Our implementation\nand dataset are publicly available at https://github.com/Howardkhh/FROSS.",
        "url": "http://arxiv.org/abs/2507.19993v1",
        "published_date": "2025-07-26T16:16:52+00:00",
        "updated_date": "2025-07-26T16:16:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao-Yu Hou",
            "Chun-Yi Lee",
            "Motoharu Sonogashira",
            "Yasutomo Kawanishi"
        ],
        "tldr": "The paper introduces FROSS, a faster-than-real-time method for online 3D Semantic Scene Graph generation from RGB-D images by lifting 2D scene graphs to 3D space using 3D Gaussian distributions, and presents a new dataset, ReplicaSSG, for evaluation.",
        "tldr_zh": "本文介绍了一种名为FROSS的实时性更快的在线3D语义场景图生成方法，该方法通过使用3D高斯分布将2D场景图提升到3D空间来实现，并提出了一个新的数据集ReplicaSSG用于评估。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniCT Depth: Event-Image Fusion Based Monocular Depth Estimation with Convolution-Compensated ViT Dual SA Block",
        "summary": "Depth estimation plays a crucial role in 3D scene understanding and is\nextensively used in a wide range of vision tasks. Image-based methods struggle\nin challenging scenarios, while event cameras offer high dynamic range and\ntemporal resolution but face difficulties with sparse data. Combining event and\nimage data provides significant advantages, yet effective integration remains\nchallenging. Existing CNN-based fusion methods struggle with occlusions and\ndepth disparities due to limited receptive fields, while Transformer-based\nfusion methods often lack deep modality interaction. To address these issues,\nwe propose UniCT Depth, an event-image fusion method that unifies CNNs and\nTransformers to model local and global features. We propose the\nConvolution-compensated ViT Dual SA (CcViT-DA) Block, designed for the encoder,\nwhich integrates Context Modeling Self-Attention (CMSA) to capture spatial\ndependencies and Modal Fusion Self-Attention (MFSA) for effective cross-modal\nfusion. Furthermore, we design the tailored Detail Compensation Convolution\n(DCC) Block to improve texture details and enhances edge representations.\nExperiments show that UniCT Depth outperforms existing image, event, and\nfusion-based monocular depth estimation methods across key metrics.",
        "url": "http://arxiv.org/abs/2507.19948v1",
        "published_date": "2025-07-26T13:29:48+00:00",
        "updated_date": "2025-07-26T13:29:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Luoxi Jing",
            "Dianxi Shi",
            "Zhe Liu",
            "Songchang Jin",
            "Chunping Qiu",
            "Ziteng Qiao",
            "Yuxian Li",
            "Jianqiang Xia"
        ],
        "tldr": "The paper introduces UniCT Depth, a novel event-image fusion method for monocular depth estimation using a hybrid CNN-Transformer architecture with attention mechanisms and detail compensation, achieving state-of-the-art performance.",
        "tldr_zh": "本文提出了一种名为UniCT Depth的新型事件-图像融合方法，用于单目深度估计，它采用混合CNN-Transformer架构，结合注意力机制和细节补偿，实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DriveIndia: An Object Detection Dataset for Diverse Indian Traffic Scenes",
        "summary": "We introduce DriveIndia, a large-scale object detection dataset purpose-built\nto capture the complexity and unpredictability of Indian traffic environments.\nThe dataset contains 66,986 high-resolution images annotated in YOLO format\nacross 24 traffic-relevant object categories, encompassing diverse conditions\nsuch as varied weather (fog, rain), illumination changes, heterogeneous road\ninfrastructure, and dense, mixed traffic patterns and collected over 120+ hours\nand covering 3,400+ kilometers across urban, rural, and highway routes.\nDriveIndia offers a comprehensive benchmark for real-world autonomous driving\nchallenges. We provide baseline results using state-of-the-art YOLO family\nmodels, with the top-performing variant achieving a mAP50 of 78.7\\%. Designed\nto support research in robust, generalizable object detection under uncertain\nroad conditions, DriveIndia will be publicly available via the TiHAN-IIT\nHyderabad dataset repository (https://tihan.iith.ac.in/tiand-datasets/).",
        "url": "http://arxiv.org/abs/2507.19912v2",
        "published_date": "2025-07-26T10:52:03+00:00",
        "updated_date": "2025-07-29T18:59:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rishav Kumar",
            "D. Santhosh Reddy",
            "P. Rajalakshmi"
        ],
        "tldr": "The paper introduces DriveIndia, a large-scale object detection dataset with diverse Indian traffic scenes, offering a benchmark for autonomous driving and featuring baseline YOLO results.",
        "tldr_zh": "该论文介绍了 DriveIndia，一个大规模的目标检测数据集，包含多样化的印度交通场景，为自动驾驶提供基准，并提供 YOLO 基线结果。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TrackAny3D: Transferring Pretrained 3D Models for Category-unified 3D Point Cloud Tracking",
        "summary": "3D LiDAR-based single object tracking (SOT) relies on sparse and irregular\npoint clouds, posing challenges from geometric variations in scale, motion\npatterns, and structural complexity across object categories. Current\ncategory-specific approaches achieve good accuracy but are impractical for\nreal-world use, requiring separate models for each category and showing limited\ngeneralization. To tackle these issues, we propose TrackAny3D, the first\nframework to transfer large-scale pretrained 3D models for category-agnostic 3D\nSOT. We first integrate parameter-efficient adapters to bridge the gap between\npretraining and tracking tasks while preserving geometric priors. Then, we\nintroduce a Mixture-of-Geometry-Experts (MoGE) architecture that adaptively\nactivates specialized subnetworks based on distinct geometric characteristics.\nAdditionally, we design a temporal context optimization strategy that\nincorporates learnable temporal tokens and a dynamic mask weighting module to\npropagate historical information and mitigate temporal drift. Experiments on\nthree commonly-used benchmarks show that TrackAny3D establishes new\nstate-of-the-art performance on category-agnostic 3D SOT, demonstrating strong\ngeneralization and competitiveness. We hope this work will enlighten the\ncommunity on the importance of unified models and further expand the use of\nlarge-scale pretrained models in this field.",
        "url": "http://arxiv.org/abs/2507.19908v1",
        "published_date": "2025-07-26T10:41:55+00:00",
        "updated_date": "2025-07-26T10:41:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mengmeng Wang",
            "Haonan Wang",
            "Yulong Li",
            "Xiangjie Kong",
            "Jiaxin Du",
            "Guojiang Shen",
            "Feng Xia"
        ],
        "tldr": "The paper introduces TrackAny3D, a novel framework for category-agnostic 3D single object tracking using transferred pretrained 3D models and a Mixture-of-Geometry-Experts architecture, achieving state-of-the-art performance on standard benchmarks.",
        "tldr_zh": "该论文介绍了TrackAny3D，一个新颖的类别无关的3D单物体跟踪框架，它利用迁移学习的预训练3D模型和一个几何专家混合（MoGE）架构，在标准基准测试上实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FedS2R: One-Shot Federated Domain Generalization for Synthetic-to-Real Semantic Segmentation in Autonomous Driving",
        "summary": "Federated domain generalization has shown promising progress in image\nclassification by enabling collaborative training across multiple clients\nwithout sharing raw data. However, its potential in the semantic segmentation\nof autonomous driving remains underexplored. In this paper, we propose FedS2R,\nthe first one-shot federated domain generalization framework for\nsynthetic-to-real semantic segmentation in autonomous driving. FedS2R comprises\ntwo components: an inconsistency-driven data augmentation strategy that\ngenerates images for unstable classes, and a multi-client knowledge\ndistillation scheme with feature fusion that distills a global model from\nmultiple client models. Experiments on five real-world datasets, Cityscapes,\nBDD100K, Mapillary, IDD, and ACDC, show that the global model significantly\noutperforms individual client models and is only 2 mIoU points behind the model\ntrained with simultaneous access to all client data. These results demonstrate\nthe effectiveness of FedS2R in synthetic-to-real semantic segmentation for\nautonomous driving under federated learning",
        "url": "http://arxiv.org/abs/2507.19881v1",
        "published_date": "2025-07-26T09:24:00+00:00",
        "updated_date": "2025-07-26T09:24:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Tao Lian",
            "Jose L. Gómez",
            "Antonio M. López"
        ],
        "tldr": "The paper introduces FedS2R, a federated domain generalization framework for synthetic-to-real semantic segmentation in autonomous driving, utilizing inconsistency-driven data augmentation and multi-client knowledge distillation. It achieves performance close to centralized training without sharing raw data.",
        "tldr_zh": "该论文介绍了FedS2R，一个用于自动驾驶中合成到真实语义分割的联邦域泛化框架。它利用不一致驱动的数据增强和多客户端知识蒸馏，在不共享原始数据的情况下，实现了接近集中式训练的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Self-Supervised Neuro-Analytic Visual Servoing for Real-time Quadrotor Control",
        "summary": "This work introduces a self-supervised neuro-analytical, cost efficient,\nmodel for visual-based quadrotor control in which a small 1.7M parameters\nstudent ConvNet learns automatically from an analytical teacher, an improved\nimage-based visual servoing (IBVS) controller. Our IBVS system solves numerical\ninstabilities by reducing the classical visual servoing equations and enabling\nefficient stable image feature detection. Through knowledge distillation, the\nstudent model achieves 11x faster inference compared to the teacher IBVS\npipeline, while demonstrating similar control accuracy at a significantly lower\ncomputational and memory cost. Our vision-only self-supervised neuro-analytic\ncontrol, enables quadrotor orientation and movement without requiring explicit\ngeometric models or fiducial markers. The proposed methodology leverages\nsimulation-to-reality transfer learning and is validated on a small drone\nplatform in GPS-denied indoor environments. Our key contributions include: (1)\nan analytical IBVS teacher that solves numerical instabilities inherent in\nclassical approaches, (2) a two-stage segmentation pipeline combining YOLOv11\nwith a U-Net-based mask splitter for robust anterior-posterior vehicle\nsegmentation to correctly estimate the orientation of the target, and (3) an\nefficient knowledge distillation dual-path system, which transfers geometric\nvisual servoing capabilities from the analytical IBVS teacher to a compact and\nsmall student neural network that outperforms the teacher, while being suitable\nfor real-time onboard deployment.",
        "url": "http://arxiv.org/abs/2507.19878v1",
        "published_date": "2025-07-26T09:17:38+00:00",
        "updated_date": "2025-07-26T09:17:38+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Sebastian Mocanu",
            "Sebastian-Ion Nae",
            "Mihai-Eugen Barbu",
            "Marius Leordeanu"
        ],
        "tldr": "This paper presents a self-supervised neuro-analytic approach for visual servoing of quadrotors, using knowledge distillation to transfer control capabilities from an analytical IBVS teacher to a compact student network for real-time onboard deployment in GPS-denied environments.",
        "tldr_zh": "本文提出了一种自监督神经分析方法，用于四旋翼飞行器的视觉伺服，通过知识蒸馏将控制能力从分析 IBVS 教师转移到紧凑的学生网络，用于在无 GPS 环境中进行实时机载部署。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "RaGS: Unleashing 3D Gaussian Splatting from 4D Radar and Monocular Cues for 3D Object Detection",
        "summary": "4D millimeter-wave radar has emerged as a promising sensor for autonomous\ndriving, but effective 3D object detection from both 4D radar and monocular\nimages remains a challenge. Existing fusion approaches typically rely on either\ninstance-based proposals or dense BEV grids, which either lack holistic scene\nunderstanding or are limited by rigid grid structures. To address these, we\npropose RaGS, the first framework to leverage 3D Gaussian Splatting (GS) as\nrepresentation for fusing 4D radar and monocular cues in 3D object detection.\n3D GS naturally suits 3D object detection by modeling the scene as a field of\nGaussians, dynamically allocating resources on foreground objects and providing\na flexible, resource-efficient solution. RaGS uses a cascaded pipeline to\nconstruct and refine the Gaussian field. It starts with the Frustum-based\nLocalization Initiation (FLI), which unprojects foreground pixels to initialize\ncoarse 3D Gaussians positions. Then, the Iterative Multimodal Aggregation (IMA)\nfuses semantics and geometry, refining the limited Gaussians to the regions of\ninterest. Finally, the Multi-level Gaussian Fusion (MGF) renders the Gaussians\ninto multi-level BEV features for 3D object detection. By dynamically focusing\non sparse objects within scenes, RaGS enable object concentrating while\noffering comprehensive scene perception. Extensive experiments on\nView-of-Delft, TJ4DRadSet, and OmniHD-Scenes benchmarks demonstrate its\nstate-of-the-art performance. Code will be released.",
        "url": "http://arxiv.org/abs/2507.19856v2",
        "published_date": "2025-07-26T08:17:12+00:00",
        "updated_date": "2025-07-30T05:32:55+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xiaokai Bai",
            "Chenxu Zhou",
            "Lianqing Zheng",
            "Si-Yuan Cao",
            "Jianan Liu",
            "Xiaohan Zhang",
            "Zhengzhuang Zhang",
            "Hui-liang Shen"
        ],
        "tldr": "The paper introduces RaGS, a novel 3D object detection framework that leverages 3D Gaussian Splatting to fuse 4D radar and monocular image data, achieving state-of-the-art results on several benchmarks.",
        "tldr_zh": "该论文介绍了 RaGS，一种新颖的 3D 对象检测框架，它利用 3D 高斯溅射融合 4D 雷达和单目图像数据，并在多个基准测试中取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HydraMamba: Multi-Head State Space Model for Global Point Cloud Learning",
        "summary": "The attention mechanism has become a dominant operator in point cloud\nlearning, but its quadratic complexity leads to limited inter-point\ninteractions, hindering long-range dependency modeling between objects. Due to\nexcellent long-range modeling capability with linear complexity, the selective\nstate space model (S6), as the core of Mamba, has been exploited in point cloud\nlearning for long-range dependency interactions over the entire point cloud.\nDespite some significant progress, related works still suffer from imperfect\npoint cloud serialization and lack of locality learning. To this end, we\nexplore a state space model-based point cloud network termed HydraMamba to\naddress the above challenges. Specifically, we design a shuffle serialization\nstrategy, making unordered point sets better adapted to the causal nature of\nS6. Meanwhile, to overcome the deficiency of existing techniques in locality\nlearning, we propose a ConvBiS6 layer, which is capable of capturing local\ngeometries and global context dependencies synergistically. Besides, we propose\nMHS6 by extending the multi-head design to S6, further enhancing its modeling\ncapability. HydraMamba achieves state-of-the-art results on various tasks at\nboth object-level and scene-level. The code is available at\nhttps://github.com/Point-Cloud-Learning/HydraMamba.",
        "url": "http://arxiv.org/abs/2507.19778v1",
        "published_date": "2025-07-26T04:04:22+00:00",
        "updated_date": "2025-07-26T04:04:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kanglin Qu",
            "Pan Gao",
            "Qun Dai",
            "Yuanhao Sun"
        ],
        "tldr": "HydraMamba introduces a novel state space model-based point cloud network utilizing shuffle serialization, ConvBiS6 layer, and MHS6 to enhance both local and global point cloud learning, achieving state-of-the-art results.",
        "tldr_zh": "HydraMamba 提出了一种新的基于状态空间模型的点云网络，它利用 shuffle 序列化、ConvBiS6 层和 MHS6 来增强局部和全局点云学习，并取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Leveraging Sparse LiDAR for RAFT-Stereo: A Depth Pre-Fill Perspective",
        "summary": "We investigate LiDAR guidance within the RAFT-Stereo framework, aiming to\nimprove stereo matching accuracy by injecting precise LiDAR depth into the\ninitial disparity map. We find that the effectiveness of LiDAR guidance\ndrastically degrades when the LiDAR points become sparse (e.g., a few hundred\npoints per frame), and we offer a novel explanation from a signal processing\nperspective. This insight leads to a surprisingly simple solution that enables\nLiDAR-guided RAFT-Stereo to thrive: pre-filling the sparse initial disparity\nmap with interpolation. Interestingly, we find that pre-filling is also\neffective when injecting LiDAR depth into image features via early fusion, but\nfor a fundamentally different reason, necessitating a distinct pre-filling\napproach. By combining both solutions, the proposed Guided RAFT-Stereo\n(GRAFT-Stereo) significantly outperforms existing LiDAR-guided methods under\nsparse LiDAR conditions across various datasets. We hope this study inspires\nmore effective LiDAR-guided stereo methods.",
        "url": "http://arxiv.org/abs/2507.19738v1",
        "published_date": "2025-07-26T02:03:02+00:00",
        "updated_date": "2025-07-26T02:03:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinsu Yoo",
            "Sooyoung Jeon",
            "Zanming Huang",
            "Tai-Yu Pan",
            "Wei-Lun Chao"
        ],
        "tldr": "This paper introduces GRAFT-Stereo, a method that improves RAFT-Stereo by effectively incorporating sparse LiDAR data through a novel pre-filling approach for both initial disparity maps and image features, leading to significant performance gains.",
        "tldr_zh": "本文介绍了GRAFT-Stereo，该方法通过一种新颖的预填充方法，有效地将稀疏激光雷达数据整合到初始视差图和图像特征中，从而改进了RAFT-Stereo，并带来了显著的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Co-Win: Joint Object Detection and Instance Segmentation in LiDAR Point Clouds via Collaborative Window Processing",
        "summary": "Accurate perception and scene understanding in complex urban environments is\na critical challenge for ensuring safe and efficient autonomous navigation. In\nthis paper, we present Co-Win, a novel bird's eye view (BEV) perception\nframework that integrates point cloud encoding with efficient parallel\nwindow-based feature extraction to address the multi-modality inherent in\nenvironmental understanding. Our method employs a hierarchical architecture\ncomprising a specialized encoder, a window-based backbone, and a query-based\ndecoder head to effectively capture diverse spatial features and object\nrelationships. Unlike prior approaches that treat perception as a simple\nregression task, our framework incorporates a variational approach with\nmask-based instance segmentation, enabling fine-grained scene decomposition and\nunderstanding. The Co-Win architecture processes point cloud data through\nprogressive feature extraction stages, ensuring that predicted masks are both\ndata-consistent and contextually relevant. Furthermore, our method produces\ninterpretable and diverse instance predictions, enabling enhanced downstream\ndecision-making and planning in autonomous driving systems.",
        "url": "http://arxiv.org/abs/2507.19691v1",
        "published_date": "2025-07-25T22:14:23+00:00",
        "updated_date": "2025-07-25T22:14:23+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Haichuan Li",
            "Tomi Westerlund"
        ],
        "tldr": "The paper introduces Co-Win, a novel BEV perception framework for LiDAR point clouds using window-based feature extraction and variational instance segmentation for improved scene understanding in autonomous driving.",
        "tldr_zh": "该论文介绍了Co-Win，一种新颖的BEV感知框架，用于LiDAR点云，它使用基于窗口的特征提取和变分实例分割，以提高自动驾驶中的场景理解能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]