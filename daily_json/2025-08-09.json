[
    {
        "title": "Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation",
        "summary": "Generalist robot policies trained on large-scale datasets such as Open\nX-Embodiment (OXE) demonstrate strong performance across a wide range of tasks.\nHowever, they often struggle to generalize beyond the distribution of their\ntraining data. In this paper, we investigate the underlying cause of this\nlimited generalization capability. We identify shortcut learning -- the\nreliance on task-irrelevant features -- as a key impediment to generalization.\nThrough comprehensive theoretical and empirical analysis, we uncover two\nprimary contributors to shortcut learning: (1) limited diversity within\nindividual sub-datasets, and (2) significant distributional disparities across\nsub-datasets, leading to dataset fragmentation. These issues arise from the\ninherent structure of large-scale datasets like OXE, which are typically\ncomposed of multiple sub-datasets collected independently across varied\nenvironments and embodiments. Our findings provide critical insights into\ndataset collection strategies that can reduce shortcut learning and enhance the\ngeneralization ability of generalist robot policies. Moreover, in scenarios\nwhere acquiring new large-scale data is impractical, we demonstrate that\ncarefully selected robotic data augmentation strategies can effectively reduce\nshortcut learning in existing offline datasets, thereby improving\ngeneralization capabilities of generalist robot policies, e.g., $\\pi_0$, in\nboth simulation and real-world environments. More information at\nhttps://lucky-light-sun.github.io/proj/shortcut-learning-in-grps/.",
        "url": "http://arxiv.org/abs/2508.06426v1",
        "published_date": "2025-08-08T16:14:01+00:00",
        "updated_date": "2025-08-08T16:14:01+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Youguang Xing",
            "Xu Luo",
            "Junlin Xie",
            "Lianli Gao",
            "Hengtao Shen",
            "Jingkuan Song"
        ],
        "tldr": "This paper identifies dataset diversity and fragmentation as key contributors to shortcut learning in generalist robot policies, hindering their generalization ability, and proposes data augmentation strategies to mitigate this issue.",
        "tldr_zh": "该论文指出数据集多样性和碎片化是通用机器人策略中捷径学习的关键因素，阻碍了它们的泛化能力，并提出了数据增强策略来缓解这个问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Depth Jitter: Seeing through the Depth",
        "summary": "Depth information is essential in computer vision, particularly in underwater\nimaging, robotics, and autonomous navigation. However, conventional\naugmentation techniques overlook depth aware transformations, limiting model\nrobustness in real world depth variations. In this paper, we introduce\nDepth-Jitter, a novel depth-based augmentation technique that simulates natural\ndepth variations to improve generalization. Our approach applies adaptive depth\noffsetting, guided by depth variance thresholds, to generate synthetic depth\nperturbations while preserving structural integrity. We evaluate Depth-Jitter\non two benchmark datasets, FathomNet and UTDAC2020 demonstrating its impact on\nmodel stability under diverse depth conditions. Extensive experiments compare\nDepth-Jitter against traditional augmentation strategies such as ColorJitter,\nanalyzing performance across varying learning rates, encoders, and loss\nfunctions. While Depth-Jitter does not always outperform conventional methods\nin absolute performance, it consistently enhances model stability and\ngeneralization in depth-sensitive environments. These findings highlight the\npotential of depth-aware augmentation for real-world applications and provide a\nfoundation for further research into depth-based learning strategies. The\nproposed technique is publicly available to support advancements in depth-aware\naugmentation. The code is publicly available on\n\\href{https://github.com/mim-team/Depth-Jitter}{github}.",
        "url": "http://arxiv.org/abs/2508.06227v1",
        "published_date": "2025-08-08T11:14:57+00:00",
        "updated_date": "2025-08-08T11:14:57+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Md Sazidur Rahman",
            "David Cabecinhas",
            "Ricard Marxer"
        ],
        "tldr": "The paper introduces Depth-Jitter, a depth-aware data augmentation technique that improves model stability and generalization in depth-sensitive computer vision tasks by simulating natural depth variations.",
        "tldr_zh": "该论文介绍了 Depth-Jitter，一种深度感知的数据增强技术，通过模拟自然的深度变化来提高模型在深度敏感的计算机视觉任务中的稳定性和泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor",
        "summary": "Accurate localization represents a fundamental challenge in\n  robotic navigation. Traditional methodologies, such as Lidar or QR-code based\nsystems, suffer from inherent scalability and adaptability con straints,\nparticularly in complex environments. In this work, we propose\n  an innovative localization framework that harnesses flooring characteris tics\nby employing graph-based representations and Graph Convolutional\n  Networks (GCNs). Our method uses graphs to represent floor features,\n  which helps localize the robot more accurately (0.64cm error) and more\n  efficiently than comparing individual image features. Additionally, this\n  approach successfully addresses the kidnapped robot problem in every\n  frame without requiring complex filtering processes. These advancements\n  open up new possibilities for robotic navigation in diverse environments.",
        "url": "http://arxiv.org/abs/2508.06177v1",
        "published_date": "2025-08-08T09:46:28+00:00",
        "updated_date": "2025-08-08T09:46:28+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Dominik Brämer",
            "Diana Kleingarn",
            "Oliver Urbann"
        ],
        "tldr": "This paper introduces a graph-based robot localization method using a floor camera and Graph Convolutional Networks (GCNs) to represent floor features, achieving high accuracy and addressing the kidnapped robot problem.",
        "tldr_zh": "该论文提出了一种基于图的机器人定位方法，使用地面摄像头和图卷积网络（GCN）来表示地面特征，实现了高精度并解决了机器人被绑架问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving",
        "summary": "Diffusion-based models are redefining the state-of-the-art in end-to-end\nautonomous driving, yet their performance is increasingly hampered by a\nreliance on transformer-based fusion. These architectures face fundamental\nlimitations: quadratic computational complexity restricts the use of\nhigh-resolution features, and a lack of spatial priors prevents them from\neffectively modeling the inherent structure of Bird's Eye View (BEV)\nrepresentations. This paper introduces GMF-Drive (Gated Mamba Fusion for\nDriving), an end-to-end framework that overcomes these challenges through two\nprincipled innovations. First, we supersede the information-limited\nhistogram-based LiDAR representation with a geometrically-augmented pillar\nformat encoding shape descriptors and statistical features, preserving critical\n3D geometric details. Second, we propose a novel hierarchical gated mamba\nfusion (GM-Fusion) architecture that substitutes an expensive transformer with\na highly efficient, spatially-aware state-space model (SSM). Our core BEV-SSM\nleverages directional sequencing and adaptive fusion mechanisms to capture\nlong-range dependencies with linear complexity, while explicitly respecting the\nunique spatial properties of the driving scene. Extensive experiments on the\nchallenging NAVSIM benchmark demonstrate that GMF-Drive achieves a new\nstate-of-the-art performance, significantly outperforming DiffusionDrive.\nComprehensive ablation studies validate the efficacy of each component,\ndemonstrating that task-specific SSMs can surpass a general-purpose transformer\nin both performance and efficiency for autonomous driving.",
        "url": "http://arxiv.org/abs/2508.06113v1",
        "published_date": "2025-08-08T08:17:18+00:00",
        "updated_date": "2025-08-08T08:17:18+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Jian Wang",
            "Chaokang Jiang",
            "Haitao Xu"
        ],
        "tldr": "GMF-Drive introduces a novel end-to-end autonomous driving framework using gated mamba fusion with a spatial-aware BEV representation, achieving state-of-the-art performance by replacing transformers with efficient state-space models.",
        "tldr_zh": "GMF-Drive 提出了一种新型的端到端自动驾驶框架，该框架使用门控 Mamba 融合和空间感知 BEV 表示，通过用高效的状态空间模型取代 Transformer，实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation",
        "summary": "The fragmentation between high-level task semantics and low-level geometric\nfeatures remains a persistent challenge in robotic manipulation. While\nvision-language models (VLMs) have shown promise in generating affordance-aware\nvisual representations, the lack of semantic grounding in canonical spaces and\nreliance on manual annotations severely limit their ability to capture dynamic\nsemantic-affordance relationships. To address these, we propose Primitive-Aware\nSemantic Grounding (PASG), a closed-loop framework that introduces: (1)\nAutomatic primitive extraction through geometric feature aggregation, enabling\ncross-category detection of keypoints and axes; (2) VLM-driven semantic\nanchoring that dynamically couples geometric primitives with functional\naffordances and task-relevant description; (3) A spatial-semantic reasoning\nbenchmark and a fine-tuned VLM (Qwen2.5VL-PA). We demonstrate PASG's\neffectiveness in practical robotic manipulation tasks across diverse scenarios,\nachieving performance comparable to manual annotations. PASG achieves a\nfiner-grained semantic-affordance understanding of objects, establishing a\nunified paradigm for bridging geometric primitives with task semantics in\nrobotic manipulation.",
        "url": "http://arxiv.org/abs/2508.05976v1",
        "published_date": "2025-08-08T03:23:33+00:00",
        "updated_date": "2025-08-08T03:23:33+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Zhihao Zhu",
            "Yifan Zheng",
            "Siyu Pan",
            "Yaohui Jin",
            "Yao Mu"
        ],
        "tldr": "The paper introduces PASG, a closed-loop framework for robotic manipulation that combines automatic geometric primitive extraction with VLM-driven semantic anchoring, demonstrating performance comparable to manual annotation. It bridges the gap between low-level geometric features and high-level task semantics.",
        "tldr_zh": "该论文介绍了PASG，一个用于机器人操作的闭环框架，它结合了自动几何基元提取和VLM驱动的语义锚定，其性能与手动标注相当。它弥合了低级几何特征和高级任务语义之间的差距。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VISTA: Vision-Language Imitation of Situational Thinking and Attention for Human-Like Driver Focus in Dynamic Environments",
        "summary": "Driver visual attention prediction is a critical task in autonomous driving\nand human-computer interaction (HCI) research. Most prior studies focus on\nestimating attention allocation at a single moment in time, typically using\nstatic RGB images such as driving scene pictures. In this work, we propose a\nvision-language framework that models the changing landscape of drivers' gaze\nthrough natural language, using few-shot and zero-shot learning on single RGB\nimages. We curate and refine high-quality captions from the BDD-A dataset using\nhuman-in-the-loop feedback, then fine-tune LLaVA to align visual perception\nwith attention-centric scene understanding. Our approach integrates both\nlow-level cues and top-down context (e.g., route semantics, risk anticipation),\nenabling language-based descriptions of gaze behavior. We evaluate performance\nacross training regimes (few shot, and one-shot) and introduce domain-specific\nmetrics for semantic alignment and response diversity. Results show that our\nfine-tuned model outperforms general-purpose VLMs in attention shift detection\nand interpretability. To our knowledge, this is among the first attempts to\ngenerate driver visual attention allocation and shifting predictions in natural\nlanguage, offering a new direction for explainable AI in autonomous driving.\nOur approach provides a foundation for downstream tasks such as behavior\nforecasting, human-AI teaming, and multi-agent coordination.",
        "url": "http://arxiv.org/abs/2508.05852v1",
        "published_date": "2025-08-07T21:01:43+00:00",
        "updated_date": "2025-08-07T21:01:43+00:00",
        "categories": [
            "cs.CV",
            "I.5.4"
        ],
        "authors": [
            "Kaiser Hamid",
            "Khandakar Ashrafi Akbar",
            "Nade Liang"
        ],
        "tldr": "The paper introduces VISTA, a vision-language framework that uses natural language to model driver gaze in dynamic environments, improving attention shift detection and interpretability using few-shot and zero-shot learning.",
        "tldr_zh": "该论文介绍了VISTA，一个视觉-语言框架，它使用自然语言来模拟动态环境中的驾驶员视线，通过少量样本和零样本学习提高注意力转移检测和可解释性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Integrating Vision Foundation Models with Reinforcement Learning for Enhanced Object Interaction",
        "summary": "This paper presents a novel approach that integrates vision foundation models\nwith reinforcement learning to enhance object interaction capabilities in\nsimulated environments. By combining the Segment Anything Model (SAM) and\nYOLOv5 with a Proximal Policy Optimization (PPO) agent operating in the\nAI2-THOR simulation environment, we enable the agent to perceive and interact\nwith objects more effectively. Our comprehensive experiments, conducted across\nfour diverse indoor kitchen settings, demonstrate significant improvements in\nobject interaction success rates and navigation efficiency compared to a\nbaseline agent without advanced perception. The results show a 68% increase in\naverage cumulative reward, a 52.5% improvement in object interaction success\nrate, and a 33% increase in navigation efficiency. These findings highlight the\npotential of integrating foundation models with reinforcement learning for\ncomplex robotic tasks, paving the way for more sophisticated and capable\nautonomous agents.",
        "url": "http://arxiv.org/abs/2508.05838v1",
        "published_date": "2025-08-07T20:29:01+00:00",
        "updated_date": "2025-08-07T20:29:01+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.SY",
            "eess.SY",
            "68T07, 68T40, 90C40, 93E35",
            "I.2.6; I.2.9; I.2.10"
        ],
        "authors": [
            "Ahmad Farooq",
            "Kamran Iqbal"
        ],
        "tldr": "This paper integrates vision foundation models (SAM, YOLOv5) with a PPO agent in AI2-THOR, demonstrating improved object interaction and navigation. Experiments show significant performance gains compared to a baseline.",
        "tldr_zh": "本文将视觉基础模型（SAM，YOLOv5）与AI2-THOR中的PPO智能体集成，展示了改进的对象交互和导航能力。实验表明，与基线相比，性能有显著提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]