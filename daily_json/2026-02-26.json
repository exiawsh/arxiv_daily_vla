[
    {
        "title": "WeatherCity: Urban Scene Reconstruction with Controllable Multi-Weather Transformation",
        "summary": "Editable high-fidelity 4D scenes are crucial for autonomous driving, as they can be applied to end-to-end training and closed-loop simulation. However, existing reconstruction methods are primarily limited to replicating observed scenes and lack the capability for diverse weather simulation. While image-level weather editing methods tend to introduce scene artifacts and offer poor controllability over the weather effects. To address these limitations, we propose WeatherCity, a novel framework for 4D urban scene reconstruction and weather editing. Specifically, we leverage a text-guided image editing model to achieve flexible editing of image weather backgrounds. To tackle the challenge of multi-weather modeling, we introduce a novel weather Gaussian representation based on shared scene features and dedicated weather-specific decoders. This representation is further enhanced with a content consistency optimization, ensuring coherent modeling across different weather conditions. Additionally, we design a physics-driven model that simulates dynamic weather effects through particles and motion patterns. Extensive experiments on multiple datasets and various scenes demonstrate that WeatherCity achieves flexible controllability, high fidelity, and temporal consistency in 4D reconstruction and weather editing. Our framework not only enables fine-grained control over weather conditions (e.g., light rain and heavy snow) but also supports object-level manipulation within the scene.",
        "url": "http://arxiv.org/abs/2602.22096v1",
        "published_date": "2026-02-25T16:44:12+00:00",
        "updated_date": "2026-02-25T16:44:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenhua Wu",
            "Huai Guan",
            "Zhe Liu",
            "Hesheng Wang"
        ],
        "tldr": "WeatherCity reconstructs 4D urban scenes with controllable multi-weather transformation using text-guided image editing, weather Gaussian representation, and physics-driven effects, enabling fine-grained weather and object-level manipulation for autonomous driving applications.",
        "tldr_zh": "WeatherCity 使用文本引导的图像编辑、天气高斯表示和物理驱动效果重建具有可控多重天气变换的 4D 城市场景，从而实现精细的天气和物体级别的操作，以用于自动驾驶应用。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "MindDriver: Introducing Progressive Multimodal Reasoning for Autonomous Driving",
        "summary": "Vision-Language Models (VLM) exhibit strong reasoning capabilities, showing promise for end-to-end autonomous driving systems. Chain-of-Thought (CoT), as VLM's widely used reasoning strategy, is facing critical challenges. Existing textual CoT has a large gap between text semantic space and trajectory physical space. Although the recent approach utilizes future image to replace text as CoT process, it lacks clear planning-oriented objective guidance to generate images with accurate scene evolution. To address these, we innovatively propose MindDriver, a progressive multimodal reasoning framework that enables VLM to imitate human-like progressive thinking for autonomous driving. MindDriver presents semantic understanding, semantic-to-physical space imagination, and physical-space trajectory planning. To achieve aligned reasoning processes in MindDriver, we develop a feedback-guided automatic data annotation pipeline to generate aligned multimodal reasoning training data. Furthermore, we develop a progressive reinforcement fine-tuning method to optimize the alignment through progressive high- level reward-based learning. MindDriver demonstrates superior performance in both nuScences open-loop and Bench2Drive closed-loop evaluation. Codes are available at https://github.com/hotdogcheesewhite/MindDriver.",
        "url": "http://arxiv.org/abs/2602.21952v1",
        "published_date": "2026-02-25T14:34:50+00:00",
        "updated_date": "2026-02-25T14:34:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lingjun Zhang",
            "Yujian Yuan",
            "Changjie Wu",
            "Xinyuan Chang",
            "Xin Cai",
            "Shuang Zeng",
            "Linzhe Shi",
            "Sijin Wang",
            "Hang Zhang",
            "Mu Xu"
        ],
        "tldr": "MindDriver is introduced as a progressive multimodal reasoning framework for autonomous driving, addressing limitations of existing Vision-Language Models by incorporating semantic understanding, semantic-to-physical space imagination, and physical-space trajectory planning, with aligned data generation and reinforcement fine-tuning.",
        "tldr_zh": "MindDriver被提出作为一个用于自动驾驶的渐进式多模态推理框架，通过结合语义理解、语义到物理空间的想象以及物理空间轨迹规划，解决了现有视觉语言模型的局限性，并使用了对齐的数据生成和强化微调。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Send Less, Perceive More: Masked Quantized Point Cloud Communication for Loss-Tolerant Collaborative Perception",
        "summary": "Collaborative perception allows connected vehicles to overcome occlusions and limited viewpoints by sharing sensory information. However, existing approaches struggle to achieve high accuracy under strict bandwidth constraints and remain highly vulnerable to random transmission packet loss. We introduce QPoint2Comm, a quantized point-cloud communication framework that dramatically reduces bandwidth while preserving high-fidelity 3D information. Instead of transmitting intermediate features, QPoint2Comm directly communicates quantized point-cloud indices using a shared codebook, enabling efficient reconstruction with lower bandwidth than feature-based methods. To ensure robustness to possible communication packet loss, we employ a masked training strategy that simulates random packet loss, allowing the model to maintain strong performance even under severe transmission failures. In addition, a cascade attention fusion module is proposed to enhance multi-vehicle information integration. Extensive experiments on both simulated and real-world datasets demonstrate that QPoint2Comm sets a new state of the art in accuracy, communication efficiency, and resilience to packet loss.",
        "url": "http://arxiv.org/abs/2602.21667v1",
        "published_date": "2026-02-25T08:00:48+00:00",
        "updated_date": "2026-02-25T08:00:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sheng Xu",
            "Enshu Wang",
            "Hongfei Xue",
            "Jian Teng",
            "Bingyi Liu",
            "Yi Zhu",
            "Pu Wang",
            "Libing Wu",
            "Chunming Qiao"
        ],
        "tldr": "QPoint2Comm is introduced as a novel point cloud communication framework utilizing quantized point cloud indices and a masked training strategy to achieve high accuracy, communication efficiency, and resilience to packet loss in collaborative perception for autonomous vehicles.",
        "tldr_zh": "QPoint2Comm 是一种新型的点云通信框架，它利用量化的点云索引和掩蔽训练策略，在自动驾驶的协同感知中实现高精度、高通信效率和抗丢包能力。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "HorizonForge: Driving Scene Editing with Any Trajectories and Any Vehicles",
        "summary": "Controllable driving scene generation is critical for realistic and scalable autonomous driving simulation, yet existing approaches struggle to jointly achieve photorealism and precise control. We introduce HorizonForge, a unified framework that reconstructs scenes as editable Gaussian Splats and Meshes, enabling fine-grained 3D manipulation and language-driven vehicle insertion. Edits are rendered through a noise-aware video diffusion process that enforces spatial and temporal consistency, producing diverse scene variations in a single feed-forward pass without per-trajectory optimization. To standardize evaluation, we further propose HorizonSuite, a comprehensive benchmark spanning ego- and agent-level editing tasks such as trajectory modifications and object manipulation. Extensive experiments show that Gaussian-Mesh representation delivers substantially higher fidelity than alternative 3D representations, and that temporal priors from video diffusion are essential for coherent synthesis. Combining these findings, HorizonForge establishes a simple yet powerful paradigm for photorealistic, controllable driving simulation, achieving an 83.4% user-preference gain and a 25.19% FID improvement over the second best state-of-the-art method. Project page: https://horizonforge.github.io/ .",
        "url": "http://arxiv.org/abs/2602.21333v1",
        "published_date": "2026-02-24T20:03:47+00:00",
        "updated_date": "2026-02-24T20:03:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yifan Wang",
            "Francesco Pittaluga",
            "Zaid Tasneem",
            "Chenyu You",
            "Manmohan Chandraker",
            "Ziyu Jiang"
        ],
        "tldr": "HorizonForge introduces a novel framework for controllable driving scene generation using Gaussian Splats and Meshes, enabling photorealistic and editable simulations with significant improvements over existing methods, benchmarked on their new HorizonSuite.",
        "tldr_zh": "HorizonForge 提出了一个新颖的驾驶场景生成框架，使用高斯溅射和网格，实现了逼真的可编辑模拟，并在其新的 HorizonSuite 基准测试中，比现有方法有了显著改进。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning",
        "summary": "Vision-Language-Action (VLA) models are advancing autonomous driving by replacing modular pipelines with unified end-to-end architectures. However, current VLAs face two expensive requirements: (1) massive dataset collection, and (2) dense reasoning annotations. In this work, we address both challenges with \\modelname (\\textbf{No} \\textbf{R}easoning for \\textbf{D}riving). Compared to existing VLAs, \\modelname achieves competitive performance while being fine-tuned on $<$60\\% of the data and no reasoning annotations, resulting in 3$\\times$ fewer tokens. We identify that standard Group Relative Policy Optimization (GRPO) fails to yield significant improvements when applied to policies trained on such small, reasoning-free datasets. We show that this limitation stems from difficulty bias, which disproportionately penalizes reward signals from scenarios that produce high-variance rollouts within GRPO. \\modelname overcomes this by incorporating Dr.~GRPO, a recent algorithm designed to mitigate difficulty bias in LLMs. As a result, \\modelname achieves competitive performance on Waymo and NAVSIM with a fraction of the training data and no reasoning overhead, enabling more efficient autonomous systems.",
        "url": "http://arxiv.org/abs/2602.21172v1",
        "published_date": "2026-02-24T18:17:21+00:00",
        "updated_date": "2026-02-24T18:17:21+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Ishaan Rawal",
            "Shubh Gupta",
            "Yihan Hu",
            "Wei Zhan"
        ],
        "tldr": "The paper introduces NoRD, a VLA model for autonomous driving that achieves competitive performance with significantly less data and no reasoning annotations by addressing difficulty bias in Group Relative Policy Optimization.",
        "tldr_zh": "该论文介绍了NoRD，一种用于自动驾驶的VLA模型。它通过解决群体相对策略优化中的难度偏差，在显著减少数据量且无需推理标注的情况下，实现了具有竞争力的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Learning to Drive is a Free Gift: Large-Scale Label-Free Autonomy Pretraining from Unposed In-The-Wild Videos",
        "summary": "Ego-centric driving videos available online provide an abundant source of visual data for autonomous driving, yet their lack of annotations makes it difficult to learn representations that capture both semantic structure and 3D geometry. Recent advances in large feedforward spatial models demonstrate that point maps and ego-motion can be inferred in a single forward pass, suggesting a promising direction for scalable driving perception. We therefore propose a label-free, teacher-guided framework for learning autonomous driving representations directly from unposed videos. Unlike prior self-supervised approaches that focus primarily on frame-to-frame consistency, we posit that safe and reactive driving depends critically on temporal context. To this end, we leverage a feedforward architecture equipped with a lightweight autoregressive module, trained using multi-modal supervisory signals that guide the model to jointly predict current and future point maps, camera poses, semantic segmentation, and motion masks. Multi-modal teachers provide sequence-level pseudo-supervision, enabling LFG to learn a unified pseudo-4D representation from raw YouTube videos without poses, labels, or LiDAR. The resulting encoder not only transfers effectively to downstream autonomous driving planning on the NAVSIM benchmark, surpassing multi-camera and LiDAR baselines with only a single monocular camera, but also yields strong performance when evaluated on a range of semantic, geometric, and qualitative motion prediction tasks. These geometry and motion-aware features position LFG as a compelling video-centric foundation model for autonomous driving.",
        "url": "http://arxiv.org/abs/2602.22091v1",
        "published_date": "2026-02-25T16:38:53+00:00",
        "updated_date": "2026-02-25T16:38:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Matthew Strong",
            "Wei-Jer Chang",
            "Quentin Herau",
            "Jiezhi Yang",
            "Yihan Hu",
            "Chensheng Peng",
            "Wei Zhan"
        ],
        "tldr": "The paper introduces LFG, a label-free framework for pretraining autonomous driving representations from unposed, in-the-wild videos using multi-modal pseudo-supervision, achieving strong transfer performance on downstream tasks.",
        "tldr_zh": "该论文介绍了LFG，一个无需标签的框架，它利用多模态伪监督从无姿势的实际视频中预训练自动驾驶表征，并在下游任务中实现了强大的迁移性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AdaSpot: Spend Resolution Where It Matters for Precise Event Spotting",
        "summary": "Precise Event Spotting aims to localize fast-paced actions or events in videos with high temporal precision, a key task for applications in sports analytics, robotics, and autonomous systems. Existing methods typically process all frames uniformly, overlooking the inherent spatio-temporal redundancy in video data. This leads to redundant computation on non-informative regions while limiting overall efficiency. To remain tractable, they often spatially downsample inputs, losing fine-grained details crucial for precise localization. To address these limitations, we propose \\textbf{AdaSpot}, a simple yet effective framework that processes low-resolution videos to extract global task-relevant features while adaptively selecting the most informative region-of-interest in each frame for high-resolution processing. The selection is performed via an unsupervised, task-aware strategy that maintains spatio-temporal consistency across frames and avoids the training instability of learnable alternatives. This design preserves essential fine-grained visual cues with a marginal computational overhead compared to low-resolution-only baselines, while remaining far more efficient than uniform high-resolution processing. Experiments on standard PES benchmarks demonstrate that \\textbf{AdaSpot} achieves state-of-the-art performance under strict evaluation metrics (\\eg, $+3.96$ and $+2.26$ mAP$@0$ frames on Tennis and FineDiving), while also maintaining strong results under looser metrics. Code is available at: \\href{https://github.com/arturxe2/AdaSpot}{https://github.com/arturxe2/AdaSpot}.",
        "url": "http://arxiv.org/abs/2602.22073v1",
        "published_date": "2026-02-25T16:24:48+00:00",
        "updated_date": "2026-02-25T16:24:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Artur Xarles",
            "Sergio Escalera",
            "Thomas B. Moeslund",
            "Albert Clapés"
        ],
        "tldr": "AdaSpot addresses precise event spotting in videos by adaptively selecting informative regions for high-resolution processing, achieving state-of-the-art results with improved efficiency and maintaining spatio-temporal consistency.",
        "tldr_zh": "AdaSpot通过自适应地选择信息丰富的区域进行高分辨率处理，解决了视频中精确事件定位的问题，在提高效率的同时实现了最先进的性能，并保持了时空一致性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "World Guidance: World Modeling in Condition Space for Action Generation",
        "summary": "Leveraging future observation modeling to facilitate action generation presents a promising avenue for enhancing the capabilities of Vision-Language-Action (VLA) models. However, existing approaches struggle to strike a balance between maintaining efficient, predictable future representations and preserving sufficient fine-grained information to guide precise action generation. To address this limitation, we propose WoG (World Guidance), a framework that maps future observations into compact conditions by injecting them into the action inference pipeline. The VLA is then trained to simultaneously predict these compressed conditions alongside future actions, thereby achieving effective world modeling within the condition space for action inference. We demonstrate that modeling and predicting this condition space not only facilitates fine-grained action generation but also exhibits superior generalization capabilities. Moreover, it learns effectively from substantial human manipulation videos. Extensive experiments across both simulation and real-world environments validate that our method significantly outperforms existing methods based on future prediction. Project page is available at: https://selen-suyue.github.io/WoGNet/",
        "url": "http://arxiv.org/abs/2602.22010v1",
        "published_date": "2026-02-25T15:27:09+00:00",
        "updated_date": "2026-02-25T15:27:09+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Yue Su",
            "Sijin Chen",
            "Haixin Shi",
            "Mingyu Liu",
            "Zhengshen Zhang",
            "Ningyuan Huang",
            "Weiheng Zhong",
            "Zhengbang Zhu",
            "Yuxiao Liu",
            "Xihui Liu"
        ],
        "tldr": "The paper introduces WoG, a framework that maps future observations into compact condition spaces for action inference in Vision-Language-Action models, demonstrating improved fine-grained action generation and generalization.",
        "tldr_zh": "该论文介绍了一种名为WoG的框架，该框架将未来观察映射到紧凑的条件空间，用于视觉-语言-动作模型中的动作推理，从而展示了改进的细粒度动作生成和泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dream-SLAM: Dreaming the Unseen for Active SLAM in Dynamic Environments",
        "summary": "In addition to the core tasks of simultaneous localization and mapping (SLAM), active SLAM additionally in- volves generating robot actions that enable effective and efficient exploration of unknown environments. However, existing active SLAM pipelines are limited by three main factors. First, they inherit the restrictions of the underlying SLAM modules that they may be using. Second, their motion planning strategies are typically shortsighted and lack long-term vision. Third, most approaches struggle to handle dynamic scenes. To address these limitations, we propose a novel monocular active SLAM method, Dream-SLAM, which is based on dreaming cross-spatio-temporal images and semantically plausible structures of partially observed dynamic environments. The generated cross-spatio-temporal im- ages are fused with real observations to mitigate noise and data incompleteness, leading to more accurate camera pose estimation and a more coherent 3D scene representation. Furthermore, we integrate dreamed and observed scene structures to enable long- horizon planning, producing farsighted trajectories that promote efficient and thorough exploration. Extensive experiments on both public and self-collected datasets demonstrate that Dream-SLAM outperforms state-of-the-art methods in localization accuracy, mapping quality, and exploration efficiency. Source code will be publicly available upon paper acceptance.",
        "url": "http://arxiv.org/abs/2602.21967v1",
        "published_date": "2026-02-25T14:48:49+00:00",
        "updated_date": "2026-02-25T14:48:49+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Xiangqi Meng",
            "Pengxu Hou",
            "Zhenjun Zhao",
            "Javier Civera",
            "Daniel Cremers",
            "Hesheng Wang",
            "Haoang Li"
        ],
        "tldr": "Dream-SLAM addresses limitations in active SLAM by 'dreaming' cross-spatio-temporal images to improve localization, mapping, and exploration in dynamic environments using monocular vision and long-horizon planning.",
        "tldr_zh": "Dream-SLAM通过“梦境”跨时空图像来解决主动SLAM中的局限性，使用单目视觉和长程规划来提高动态环境中的定位、建图和探索能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Geometry-as-context: Modulating Explicit 3D in Scene-consistent Video Generation to Geometry Context",
        "summary": "Scene-consistent video generation aims to create videos that explore 3D scenes based on a camera trajectory. Previous methods rely on video generation models with external memory for consistency, or iterative 3D reconstruction and inpainting, which accumulate errors during inference due to incorrect intermediary outputs, non-differentiable processes, and separate models. To overcome these limitations, we introduce ``geometry-as-context\". It iteratively completes the following steps using an autoregressive camera-controlled video generation model: (1) estimates the geometry of the current view necessary for 3D reconstruction, and (2) simulates and restores novel view images rendered by the 3D scene. Under this multi-task framework, we develop the camera gated attention module to enhance the model's capability to effectively leverage camera poses. During the training phase, text contexts are utilized to ascertain whether geometric or RGB images should be generated. To ensure that the model can generate RGB-only outputs during inference, the geometry context is randomly dropped from the interleaved text-image-geometry training sequence. The method has been tested on scene video generation with one-direction and forth-and-back trajectories. The results show its superiority over previous approaches in maintaining scene consistency and camera control.",
        "url": "http://arxiv.org/abs/2602.21929v1",
        "published_date": "2026-02-25T14:09:03+00:00",
        "updated_date": "2026-02-25T14:09:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "JiaKui Hu",
            "Jialun Liu",
            "Liying Yang",
            "Xinliang Zhang",
            "Kaiwen Li",
            "Shuang Zeng",
            "Yuanwei Li",
            "Haibin Huang",
            "Chi Zhang",
            "Yanye Lu"
        ],
        "tldr": "This paper introduces a geometry-as-context approach for scene-consistent video generation from camera trajectories, using an autoregressive model that iteratively estimates geometry and renders novel views, demonstrating superior performance in maintaining scene consistency and camera control.",
        "tldr_zh": "该论文介绍了一种将几何信息作为上下文的方法，用于从相机轨迹生成场景一致的视频。该方法使用自回归模型迭代地估计几何形状并渲染新的视角，在保持场景一致性和相机控制方面表现出卓越的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UNet-Based Keypoint Regression for 3D Cone Localization in Autonomous Racing",
        "summary": "Accurate cone localization in 3D space is essential in autonomous racing for precise navigation around the track. Approaches that rely on traditional computer vision algorithms are sensitive to environmental variations, and neural networks are often trained on limited data and are infeasible to run in real time. We present a UNet-based neural network for keypoint detection on cones, leveraging the largest custom-labeled dataset we have assembled. Our approach enables accurate cone position estimation and the potential for color prediction. Our model achieves substantial improvements in keypoint accuracy over conventional methods. Furthermore, we leverage our predicted keypoints in the perception pipeline and evaluate the end-to-end autonomous system. Our results show high-quality performance across all metrics, highlighting the effectiveness of this approach and its potential for adoption in competitive autonomous racing systems.",
        "url": "http://arxiv.org/abs/2602.21904v1",
        "published_date": "2026-02-25T13:34:56+00:00",
        "updated_date": "2026-02-25T13:34:56+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Mariia Baidachna",
            "James Carty",
            "Aidan Ferguson",
            "Joseph Agrane",
            "Varad Kulkarni",
            "Aubrey Agub",
            "Michael Baxendale",
            "Aaron David",
            "Rachel Horton",
            "Elliott Atkinson"
        ],
        "tldr": "This paper presents a UNet-based approach for 3D cone localization in autonomous racing, claiming improved accuracy and real-time feasibility through a custom-labeled dataset and keypoint detection method. It also demonstrates the effectiveness of this approach in an end-to-end autonomous system.",
        "tldr_zh": "本文提出了一种基于UNet的方法，用于自动驾驶竞赛中的3D锥体定位，声称通过自定义标记的数据集和关键点检测方法提高了准确性和实时可行性。它还展示了这种方法在端到端自动驾驶系统中的有效性。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "XStreamVGGT: Extremely Memory-Efficient Streaming Vision Geometry Grounded Transformer with KV Cache Compression",
        "summary": "Learning-based 3D visual geometry models have significantly advanced with the advent of large-scale transformers. Among these, StreamVGGT leverages frame-wise causal attention to deliver robust and efficient streaming 3D reconstruction. However, it suffers from unbounded growth in the Key-Value (KV) cache due to the massive influx of vision tokens from multi-image and long-video inputs, leading to increased memory consumption and inference latency as input frames accumulate. This ultimately limits its scalability for long-horizon applications. To address this gap, we propose XStreamVGGT, a tuning-free approach that seamlessly integrates pruning and quantization to systematically compress the KV cache, enabling extremely memory-efficient streaming inference. Specifically, redundant KVs generated from multi-frame inputs are initially pruned to conform to a fixed KV memory budget using an efficient token-importance identification mechanism that maintains full compatibility with high-performance attention kernels (e.g., FlashAttention). Additionally, leveraging the inherent distribution patterns of KV tensors, we apply dimension-adaptive KV quantization within the pruning pipeline to further minimize memory overhead while preserving numerical accuracy. Extensive evaluations show that XStreamVGGT achieves mostly negligible performance degradation while substantially reducing memory usage by 4.42$\\times$ and accelerating inference by 5.48$\\times$, enabling practical and scalable streaming 3D applications. The code is available at https://github.com/ywh187/XStreamVGGT/.",
        "url": "http://arxiv.org/abs/2602.21780v1",
        "published_date": "2026-02-25T11:02:02+00:00",
        "updated_date": "2026-02-25T11:02:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zunhai Su",
            "Weihao Ye",
            "Hansen Feng",
            "Keyu Fan",
            "Jing Zhang",
            "Dahai Yu",
            "Zhengwu Liu",
            "Ngai Wong"
        ],
        "tldr": "XStreamVGGT introduces a pruning and quantization method to compress the KV cache in StreamVGGT, significantly reducing memory usage and accelerating inference for streaming 3D reconstruction, making it more scalable for long-horizon applications.",
        "tldr_zh": "XStreamVGGT 提出了一种剪枝和量化方法来压缩 StreamVGGT 中的 KV 缓存，从而显著减少内存使用并加速流式 3D 重建的推理，使其更具可扩展性，适用于长时程应用。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LiREC-Net: A Target-Free and Learning-Based Network for LiDAR, RGB, and Event Calibration",
        "summary": "Advanced autonomous systems rely on multi-sensor fusion for safer and more robust perception. To enable effective fusion, calibrating directly from natural driving scenes (i.e., target-free) with high accuracy is crucial for precise multi-sensor alignment. Existing learning-based calibration methods are typically designed for only a single pair of sensor modalities (i.e., a bi-modal setup). Unlike these methods, we propose LiREC-Net, a target-free, learning-based calibration network that jointly calibrates multiple sensor modality pairs, including LiDAR, RGB, and event data, within a unified framework. To reduce redundant computation and improve efficiency, we introduce a shared LiDAR representation that leverages features from both its 3D nature and projected depth map, ensuring better consistency across modalities. Trained and evaluated on established datasets, such as KITTI and DSEC, our LiREC-Net achieves competitive performance to bi-modal models and sets a new strong baseline for the tri-modal use case.",
        "url": "http://arxiv.org/abs/2602.21754v1",
        "published_date": "2026-02-25T10:08:14+00:00",
        "updated_date": "2026-02-25T10:08:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aditya Ranjan Dash",
            "Ramy Battrawy",
            "René Schuster",
            "Didier Stricker"
        ],
        "tldr": "The paper introduces LiREC-Net, a learning-based network for target-free calibration of LiDAR, RGB, and event data, achieving competitive performance and a new baseline for tri-modal calibration.",
        "tldr_zh": "该论文介绍了LiREC-Net，一个基于学习的免目标定网络，用于LiDAR、RGB和事件数据的校准，实现了具有竞争力的性能，并为三模态校准建立了一个新的基线。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SF3D-RGB: Scene Flow Estimation from Monocular Camera and Sparse LiDAR",
        "summary": "Scene flow estimation is an extremely important task in computer vision to support the perception of dynamic changes in the scene. For robust scene flow, learning-based approaches have recently achieved impressive results using either image-based or LiDAR-based modalities. However, these methods have tended to focus on the use of a single modality. To tackle these problems, we present a deep learning architecture, SF3D-RGB, that enables sparse scene flow estimation using 2D monocular images and 3D point clouds (e.g., acquired by LiDAR) as inputs. Our architecture is an end-to-end model that first encodes information from each modality into features and fuses them together. Then, the fused features enhance a graph matching module for better and more robust mapping matrix computation to generate an initial scene flow. Finally, a residual scene flow module further refines the initial scene flow. Our model is designed to strike a balance between accuracy and efficiency. Furthermore, experiments show that our proposed method outperforms single-modality methods and achieves better scene flow accuracy on real-world datasets while using fewer parameters compared to other state-of-the-art methods with fusion.",
        "url": "http://arxiv.org/abs/2602.21699v1",
        "published_date": "2026-02-25T09:03:42+00:00",
        "updated_date": "2026-02-25T09:03:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rajai Alhimdiat",
            "Ramy Battrawy",
            "René Schuster",
            "Didier Stricker",
            "Wesam Ashour"
        ],
        "tldr": "The paper introduces SF3D-RGB, a deep learning architecture for sparse scene flow estimation using monocular images and LiDAR point clouds, achieving better accuracy and efficiency compared to single-modality and fusion-based state-of-the-art methods.",
        "tldr_zh": "该论文介绍了SF3D-RGB，一种利用单目图像和激光雷达点云进行稀疏场景流估计的深度学习架构，与单模态和融合的先进方法相比，实现了更好的准确性和效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Space-Time Forecasting of Dynamic Scenes with Motion-aware Gaussian Grouping",
        "summary": "Forecasting dynamic scenes remains a fundamental challenge in computer vision, as limited observations make it difficult to capture coherent object-level motion and long-term temporal evolution. We present Motion Group-aware Gaussian Forecasting (MoGaF), a framework for long-term scene extrapolation built upon the 4D Gaussian Splatting representation. MoGaF introduces motion-aware Gaussian grouping and group-wise optimization to enforce physically consistent motion across both rigid and non-rigid regions, yielding spatially coherent dynamic representations. Leveraging this structured space-time representation, a lightweight forecasting module predicts future motion, enabling realistic and temporally stable scene evolution. Experiments on synthetic and real-world datasets demonstrate that MoGaF consistently outperforms existing baselines in rendering quality, motion plausibility, and long-term forecasting stability. Our project page is available at https://slime0519.github.io/mogaf",
        "url": "http://arxiv.org/abs/2602.21668v1",
        "published_date": "2026-02-25T08:04:07+00:00",
        "updated_date": "2026-02-25T08:04:07+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Junmyeong Lee",
            "Hoseung Choi",
            "Minsu Cho"
        ],
        "tldr": "The paper introduces Motion Group-aware Gaussian Forecasting (MoGaF), a novel framework for long-term dynamic scene forecasting using 4D Gaussian Splatting, achieving improved rendering quality, motion plausibility, and long-term stability.",
        "tldr_zh": "该论文提出了运动组感知高斯预测（MoGaF），一种使用4D高斯溅射进行长期动态场景预测的新框架，实现了改进的渲染质量、运动合理性和长期稳定性。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Self-Correcting VLA: Online Action Refinement via Sparse World Imagination",
        "summary": "Standard vision-language-action (VLA) models rely on fitting statistical data priors, limiting their robust understanding of underlying physical dynamics. Reinforcement learning enhances physical grounding through exploration yet typically relies on external reward signals that remain isolated from the agent's internal states. World action models have emerged as a promising paradigm that integrates imagination and control to enable predictive planning. However, they rely on implicit context modeling, lacking explicit mechanisms for self-improvement. To solve these problems, we propose Self-Correcting VLA (SC-VLA), which achieve self-improvement by intrinsically guiding action refinement through sparse imagination. We first design sparse world imagination by integrating auxiliary predictive heads to forecast current task progress and future trajectory trends, thereby constraining the policy to encode short-term physical evolution. Then we introduce the online action refinement module to reshape progress-dependent dense rewards, adjusting trajectory orientation based on the predicted sparse future states. Evaluations on challenging robot manipulation tasks from simulation benchmarks and real-world settings demonstrate that SC-VLA achieve state-of-the-art performance, yielding the highest task throughput with 16% fewer steps and a 9% higher success rate than the best-performing baselines, alongside a 14% gain in real-world experiments. Code is available at https://github.com/Kisaragi0/SC-VLA.",
        "url": "http://arxiv.org/abs/2602.21633v1",
        "published_date": "2026-02-25T06:58:06+00:00",
        "updated_date": "2026-02-25T06:58:06+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Chenyv Liu",
            "Wentao Tan",
            "Lei Zhu",
            "Fengling Li",
            "Jingjing Li",
            "Guoli Yang",
            "Heng Tao Shen"
        ],
        "tldr": "The paper introduces Self-Correcting VLA (SC-VLA), a method for improving vision-language-action models by integrating sparse world imagination with online action refinement, demonstrating state-of-the-art performance on robot manipulation tasks.",
        "tldr_zh": "该论文介绍了自校正VLA（SC-VLA），一种通过结合稀疏世界想象和在线行动改进来提升视觉-语言-行动模型的方法，并在机器人操作任务上展示了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SEF-MAP: Subspace-Decomposed Expert Fusion for Robust Multimodal HD Map Prediction",
        "summary": "High-definition (HD) maps are essential for autonomous driving, yet multi-modal fusion often suffers from inconsistency between camera and LiDAR modalities, leading to performance degradation under low-light conditions, occlusions, or sparse point clouds. To address this, we propose SEFMAP, a Subspace-Expert Fusion framework for robust multimodal HD map prediction. The key idea is to explicitly disentangle BEV features into four semantic subspaces: LiDAR-private, Image-private, Shared, and Interaction. Each subspace is assigned a dedicated expert, thereby preserving modality-specific cues while capturing cross-modal consensus. To adaptively combine expert outputs, we introduce an uncertainty-aware gating mechanism at the BEV-cell level, where unreliable experts are down-weighted based on predictive variance, complemented by a usage balance regularizer to prevent expert collapse. To enhance robustness in degraded conditions and promote role specialization, we further propose distribution-aware masking: during training, modality-drop scenarios are simulated using EMA-statistical surrogate features, and a specialization loss enforces distinct behaviors of private, shared, and interaction experts across complete and masked inputs. Experiments on nuScenes and Argoverse2 benchmarks demonstrate that SEFMAP achieves state-of-the-art performance, surpassing prior methods by +4.2% and +4.8% in mAP, respectively. SEF-MAPprovides a robust and effective solution for multi-modal HD map prediction under diverse and degraded conditions.",
        "url": "http://arxiv.org/abs/2602.21589v1",
        "published_date": "2026-02-25T05:32:23+00:00",
        "updated_date": "2026-02-25T05:32:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoxiang Fu",
            "Lingfeng Zhang",
            "Hao Li",
            "Ruibing Hu",
            "Zhengrong Li",
            "Guanjing Liu",
            "Zimu Tan",
            "Long Chen",
            "Hangjun Ye",
            "Xiaoshuai Hao"
        ],
        "tldr": "The paper introduces SEF-MAP, a subspace-expert fusion framework for robust HD map prediction that addresses inconsistencies between camera and LiDAR modalities, achieving state-of-the-art performance on nuScenes and Argoverse2 datasets.",
        "tldr_zh": "该论文介绍了SEF-MAP，一个用于鲁棒高清地图预测的子空间专家融合框架，解决了相机和激光雷达模态之间的不一致性，并在nuScenes和Argoverse2数据集上实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generalizing Visual Geometry Priors to Sparse Gaussian Occupancy Prediction",
        "summary": "Accurate 3D scene understanding is essential for embodied intelligence, with occupancy prediction emerging as a key task for reasoning about both objects and free space. Existing approaches largely rely on depth priors (e.g., DepthAnything) but make only limited use of 3D cues, restricting performance and generalization. Recently, visual geometry models such as VGGT have shown strong capability in providing rich 3D priors, but similar to monocular depth foundation models, they still operate at the level of visible surfaces rather than volumetric interiors, motivating us to explore how to more effectively leverage these increasingly powerful geometry priors for 3D occupancy prediction. We present GPOcc, a framework that leverages generalizable visual geometry priors (GPs) for monocular occupancy prediction. Our method extends surface points inward along camera rays to generate volumetric samples, which are represented as Gaussian primitives for probabilistic occupancy inference. To handle streaming input, we further design a training-free incremental update strategy that fuses per-frame Gaussians into a unified global representation. Experiments on Occ-ScanNet and EmbodiedOcc-ScanNet demonstrate significant gains: GPOcc improves mIoU by +9.99 in the monocular setting and +11.79 in the streaming setting over prior state of the art. Under the same depth prior, it achieves +6.73 mIoU while running 2.65$\\times$ faster. These results highlight that GPOcc leverages geometry priors more effectively and efficiently. Code will be released at https://github.com/JuIvyy/GPOcc.",
        "url": "http://arxiv.org/abs/2602.21552v1",
        "published_date": "2026-02-25T04:16:54+00:00",
        "updated_date": "2026-02-25T04:16:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Changqing Zhou",
            "Yueru Luo",
            "Changhao Chen"
        ],
        "tldr": "The paper introduces GPOcc, a novel framework for monocular occupancy prediction that leverages generalizable visual geometry priors and Gaussian primitives, achieving significant improvements in mIoU and speed compared to state-of-the-art methods.",
        "tldr_zh": "该论文介绍了GPOcc，一个用于单目占用预测的新框架，它利用了通用视觉几何先验和高斯原语，与最先进的方法相比，在 mIoU 和速度方面都取得了显著提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LiLo-VLA: Compositional Long-Horizon Manipulation via Linked Object-Centric Policies",
        "summary": "General-purpose robots must master long-horizon manipulation, defined as tasks involving multiple kinematic structure changes (e.g., attaching or detaching objects) in unstructured environments. While Vision-Language-Action (VLA) models offer the potential to master diverse atomic skills, they struggle with the combinatorial complexity of sequencing them and are prone to cascading failures due to environmental sensitivity. To address these challenges, we propose LiLo-VLA (Linked Local VLA), a modular framework capable of zero-shot generalization to novel long-horizon tasks without ever being trained on them. Our approach decouples transport from interaction: a Reaching Module handles global motion, while an Interaction Module employs an object-centric VLA to process isolated objects of interest, ensuring robustness against irrelevant visual features and invariance to spatial configurations. Crucially, this modularity facilitates robust failure recovery through dynamic replanning and skill reuse, effectively mitigating the cascading errors common in end-to-end approaches. We introduce a 21-task simulation benchmark consisting of two challenging suites: LIBERO-Long++ and Ultra-Long. In these simulations, LiLo-VLA achieves a 69% average success rate, outperforming Pi0.5 by 41% and OpenVLA-OFT by 67%. Furthermore, real-world evaluations across 8 long-horizon tasks demonstrate an average success rate of 85%. Project page: https://yy-gx.github.io/LiLo-VLA/.",
        "url": "http://arxiv.org/abs/2602.21531v1",
        "published_date": "2026-02-25T03:33:39+00:00",
        "updated_date": "2026-02-25T03:33:39+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "eess.SY"
        ],
        "authors": [
            "Yue Yang",
            "Shuo Cheng",
            "Yu Fang",
            "Homanga Bharadhwaj",
            "Mingyu Ding",
            "Gedas Bertasius",
            "Daniel Szafir"
        ],
        "tldr": "The paper introduces LiLo-VLA, a modular Vision-Language-Action framework for long-horizon robotic manipulation that decouples transport and interaction, achieving state-of-the-art results in simulation and real-world tasks.",
        "tldr_zh": "该论文介绍了一种名为LiLo-VLA的模块化视觉-语言-动作框架，用于长时程机器人操作。该框架将运输和交互解耦，并在模拟和现实世界任务中取得了最先进的成果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unified Unsupervised and Sparsely-Supervised 3D Object Detection by Semantic Pseudo-Labeling and Prototype Learning",
        "summary": "3D object detection is essential for autonomous driving and robotic perception, yet its reliance on large-scale manually annotated data limits scalability and adaptability. To reduce annotation dependency, unsupervised and sparsely-supervised paradigms have emerged. However, they face intertwined challenges: low-quality pseudo-labels, unstable feature mining, and a lack of a unified training framework. This paper proposes SPL, a unified training framework for both Unsupervised and Sparsely-Supervised 3D Object Detection via Semantic Pseudo-labeling and prototype Learning. SPL first generates high-quality pseudo-labels by integrating image semantics, point cloud geometry, and temporal cues, producing both 3D bounding boxes for dense objects and 3D point labels for sparse ones. These pseudo-labels are not used directly but as probabilistic priors within a novel, multi-stage prototype learning strategy. This strategy stabilizes feature representation learning through memory-based initialization and momentum-based prototype updating, effectively mining features from both labeled and unlabeled data. Extensive experiments on KITTI and nuScenes datasets demonstrate that SPL significantly outperforms state-of-the-art methods in both settings. Our work provides a robust and generalizable solution for learning 3D object detectors with minimal or no manual annotations.",
        "url": "http://arxiv.org/abs/2602.21484v1",
        "published_date": "2026-02-25T01:26:34+00:00",
        "updated_date": "2026-02-25T01:26:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yushen He"
        ],
        "tldr": "This paper introduces SPL, a unified framework for unsupervised and sparsely-supervised 3D object detection using semantic pseudo-labeling and prototype learning, demonstrating significant performance improvements on KITTI and nuScenes datasets.",
        "tldr_zh": "本文提出了SPL，一个统一的框架，通过语义伪标签和原型学习，用于无监督和稀疏监督的3D目标检测，并在KITTI和nuScenes数据集上展示了显著的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Uncertainty-Aware Diffusion Model for Multimodal Highway Trajectory Prediction via DDIM Sampling",
        "summary": "Accurate and uncertainty-aware trajectory prediction remains a core challenge for autonomous driving, driven by complex multi-agent interactions, diverse scene contexts and the inherently stochastic nature of future motion. Diffusion-based generative models have recently shown strong potential for capturing multimodal futures, yet existing approaches such as cVMD suffer from slow sampling, limited exploitation of generative diversity and brittle scenario encodings.\n  This work introduces cVMDx, an enhanced diffusion-based trajectory prediction framework that improves efficiency, robustness and multimodal predictive capability. Through DDIM sampling, cVMDx achieves up to a 100x reduction in inference time, enabling practical multi-sample generation for uncertainty estimation. A fitted Gaussian Mixture Model further provides tractable multimodal predictions from the generated trajectories. In addition, a CVQ-VAE variant is evaluated for scenario encoding. Experiments on the publicly available highD dataset show that cVMDx achieves higher accuracy and significantly improved efficiency over cVMD, enabling fully stochastic, multimodal trajectory prediction.",
        "url": "http://arxiv.org/abs/2602.21319v1",
        "published_date": "2026-02-24T19:40:37+00:00",
        "updated_date": "2026-02-24T19:40:37+00:00",
        "categories": [
            "cs.LG",
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Marion Neumeier",
            "Niklas Roßberg",
            "Michael Botsch",
            "Wolfgang Utschick"
        ],
        "tldr": "This paper introduces cVMDx, an improved diffusion-based trajectory prediction framework using DDIM sampling for faster inference and better multimodal prediction, demonstrating significant efficiency gains over cVMD on the highD dataset.",
        "tldr_zh": "该论文介绍了cVMDx，一种改进的基于扩散的轨迹预测框架，它使用DDIM采样来实现更快的推理和更好的多模态预测，并在highD数据集上展示了相对于cVMD的显著效率提升。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Squint: Fast Visual Reinforcement Learning for Sim-to-Real Robotics",
        "summary": "Visual reinforcement learning is appealing for robotics but expensive -- off-policy methods are sample-efficient yet slow; on-policy methods parallelize well but waste samples. Recent work has shown that off-policy methods can train faster than on-policy methods in wall-clock time for state-based control. Extending this to vision remains challenging, where high-dimensional input images complicate training dynamics and introduce substantial storage and encoding overhead. To address these challenges, we introduce Squint, a visual Soft Actor Critic method that achieves faster wall-clock training than prior visual off-policy and on-policy methods. Squint achieves this via parallel simulation, a distributional critic, resolution squinting, layer normalization, a tuned update-to-data ratio, and an optimized implementation. We evaluate on the SO-101 Task Set, a new suite of eight manipulation tasks in ManiSkill3 with heavy domain randomization, and demonstrate sim-to-real transfer to a real SO-101 robot. We train policies for 15 minutes on a single RTX 3090 GPU, with most tasks converging in under 6 minutes.",
        "url": "http://arxiv.org/abs/2602.21203v1",
        "published_date": "2026-02-24T18:58:11+00:00",
        "updated_date": "2026-02-24T18:58:11+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Abdulaziz Almuzairee",
            "Henrik I. Christensen"
        ],
        "tldr": "Squint, a visual Soft Actor Critic method, achieves faster wall-clock training than prior visual RL methods through parallel simulation, distributional critic, resolution squinting, and other optimizations, demonstrated on a new robotic manipulation task suite with sim-to-real transfer.",
        "tldr_zh": "Squint是一种视觉Soft Actor Critic方法，通过并行仿真、分布式的评论家、分辨率缩放和其他优化，实现了比以往视觉强化学习方法更快的实际训练时间，并在一个新的机器人操作任务套件上进行了演示，实现了从仿真到真实的迁移。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs",
        "summary": "Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: \\textit{reflection-in-action}, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and \\textit{reflection-on-action}, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.",
        "url": "http://arxiv.org/abs/2602.21198v1",
        "published_date": "2026-02-24T18:55:18+00:00",
        "updated_date": "2026-02-24T18:55:18+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Yining Hong",
            "Huang Huang",
            "Manling Li",
            "Li Fei-Fei",
            "Jiajun Wu",
            "Yejin Choi"
        ],
        "tldr": "This paper introduces Reflective Test-Time Planning, a method that allows Embodied LLMs to learn from mistakes through internal and external reflections, significantly improving performance on long-horizon robotic tasks. It incorporates reflection-in-action, reflection-on-action, and retrospective reflection.",
        "tldr_zh": "本文介绍了一种名为“反思性测试时规划”的方法，该方法允许具身LLM通过内部和外部反思从错误中学习，从而显著提高远程机器人任务的性能。 它结合了行动中反思、行动后反思和回顾性反思。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning",
        "summary": "While Vision-Language Models (VLMs) exhibit exceptional 2D visual understanding, their ability to comprehend and reason about 3D space--a cornerstone of spatial intelligence--remains superficial. Current methodologies attempt to bridge this domain gap either by relying on explicit 3D modalities or by augmenting VLMs with partial, view-conditioned geometric priors. However, such approaches hinder scalability and ultimately burden the language model with the ill-posed task of implicitly reconstructing holistic 3D geometry from sparse cues. In this paper, we argue that spatial intelligence can emerge inherently from 2D vision alone, rather than being imposed via explicit spatial instruction tuning. To this end, we introduce Spa3R, a self-supervised framework that learns a unified, view-invariant spatial representation directly from unposed multi-view images. Spa3R is built upon the proposed Predictive Spatial Field Modeling (PSFM) paradigm, where Spa3R learns to synthesize feature fields for arbitrary unseen views conditioned on a compact latent representation, thereby internalizing a holistic and coherent understanding of the underlying 3D scene. We further integrate the pre-trained Spa3R Encoder into existing VLMs via a lightweight adapter to form Spa3-VLM, effectively grounding language reasoning in a global spatial context. Experiments on the challenging VSI-Bench demonstrate that Spa3-VLM achieves state-of-the-art accuracy of 58.6% on 3D VQA, significantly outperforming prior methods. These results highlight PSFM as a scalable path toward advancing spatial intelligence. Code is available at https://github.com/hustvl/Spa3R.",
        "url": "http://arxiv.org/abs/2602.21186v1",
        "published_date": "2026-02-24T18:37:34+00:00",
        "updated_date": "2026-02-24T18:37:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoyi Jiang",
            "Liu Liu",
            "Xinjie Wang",
            "Yonghao He",
            "Wei Sui",
            "Zhizhong Su",
            "Wenyu Liu",
            "Xinggang Wang"
        ],
        "tldr": "The paper introduces Spa3R, a self-supervised framework that learns view-invariant spatial representations from multi-view images using Predictive Spatial Field Modeling (PSFM) to improve 3D visual reasoning in Vision-Language Models, achieving state-of-the-art results on 3D VQA.",
        "tldr_zh": "该论文介绍了Spa3R，一个自监督框架，它使用预测空间场建模(PSFM)从多视角图像中学习视角不变的空间表征，以提高视觉-语言模型中的3D视觉推理能力，并在3D VQA上实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GFPL: Generative Federated Prototype Learning for Resource-Constrained and Data-Imbalanced Vision Task",
        "summary": "Federated learning (FL) facilitates the secure utilization of decentralized images, advancing applications in medical image recognition and autonomous driving. However, conventional FL faces two critical challenges in real-world deployment: ineffective knowledge fusion caused by model updates biased toward majority-class features, and prohibitive communication overhead due to frequent transmissions of high-dimensional model parameters. Inspired by the human brain's efficiency in knowledge integration, we propose a novel Generative Federated Prototype Learning (GFPL) framework to address these issues. Within this framework, a prototype generation method based on Gaussian Mixture Model (GMM) captures the statistical information of class-wise features, while a prototype aggregation strategy using Bhattacharyya distance effectively fuses semantically similar knowledge across clients. In addition, these fused prototypes are leveraged to generate pseudo-features, thereby mitigating feature distribution imbalance across clients. To further enhance feature alignment during local training, we devise a dual-classifier architecture, optimized via a hybrid loss combining Dot Regression and Cross-Entropy. Extensive experiments on benchmarks show that GFPL improves model accuracy by 3.6% under imbalanced data settings while maintaining low communication cost.",
        "url": "http://arxiv.org/abs/2602.21873v1",
        "published_date": "2026-02-25T12:57:45+00:00",
        "updated_date": "2026-02-25T12:57:45+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Shiwei Lu",
            "Yuhang He",
            "Jiashuo Li",
            "Qiang Wang",
            "Yihong Gong"
        ],
        "tldr": "The paper introduces a Generative Federated Prototype Learning (GFPL) framework to address challenges in federated learning, specifically data imbalance and communication overhead, using prototype generation and aggregation techniques.",
        "tldr_zh": "该论文介绍了一种生成式联邦原型学习（GFPL）框架，旨在解决联邦学习中的数据不平衡和通信开销问题，使用原型生成和聚合技术。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]