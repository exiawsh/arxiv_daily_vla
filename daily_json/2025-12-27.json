[
    {
        "title": "RT-Focuser: A Real-Time Lightweight Model for Edge-side Image Deblurring",
        "summary": "Motion blur caused by camera or object movement severely degrades image quality and poses challenges for real-time applications such as autonomous driving, UAV perception, and medical imaging. In this paper, a lightweight U-shaped network tailored for real-time deblurring is presented and named RT-Focuser. To balance speed and accuracy, we design three key components: Lightweight Deblurring Block (LD) for edge-aware feature extraction, Multi-Level Integrated Aggregation module (MLIA) for encoder integration, and Cross-source Fusion Block (X-Fuse) for progressive decoder refinement. Trained on a single blurred input, RT-Focuser achieves 30.67 dB PSNR with only 5.85M parameters and 15.76 GMACs. It runs 6ms per frame on GPU and mobile, exceeds 140 FPS on both, showing strong potential for deployment on the edge. The official code and usage are available on: https://github.com/ReaganWu/RT-Focuser.",
        "url": "http://arxiv.org/abs/2512.21975v1",
        "published_date": "2025-12-26T10:41:25+00:00",
        "updated_date": "2025-12-26T10:41:25+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Zhuoyu Wu",
            "Wenhui Ou",
            "Qiawei Zheng",
            "Jiayan Yang",
            "Quanjun Wang",
            "Wenqi Fang",
            "Zheng Wang",
            "Yongkui Yang",
            "Heshan Li"
        ],
        "tldr": "RT-Focuser is a lightweight U-shaped network for real-time image deblurring, achieving good performance with low computational cost, making it suitable for edge deployment.",
        "tldr_zh": "RT-Focuser是一个轻量级的U型网络，用于实时图像去模糊，以低计算成本实现了良好的性能，使其适合边缘部署。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reloc-VGGT: Visual Re-localization with Geometry Grounded Transformer",
        "summary": "Visual localization has traditionally been formulated as a pair-wise pose regression problem. Existing approaches mainly estimate relative poses between two images and employ a late-fusion strategy to obtain absolute pose estimates. However, the late motion average is often insufficient for effectively integrating spatial information, and its accuracy degrades in complex environments. In this paper, we present the first visual localization framework that performs multi-view spatial integration through an early-fusion mechanism, enabling robust operation in both structured and unstructured environments. Our framework is built upon the VGGT backbone, which encodes multi-view 3D geometry, and we introduce a pose tokenizer and projection module to more effectively exploit spatial relationships from multiple database views. Furthermore, we propose a novel sparse mask attention strategy that reduces computational cost by avoiding the quadratic complexity of global attention, thereby enabling real-time performance at scale. Trained on approximately eight million posed image pairs, Reloc-VGGT demonstrates strong accuracy and remarkable generalization ability. Extensive experiments across diverse public datasets consistently validate the effectiveness and efficiency of our approach, delivering high-quality camera pose estimates in real time while maintaining robustness to unseen environments. Our code and models will be publicly released upon acceptance.https://github.com/dtc111111/Reloc-VGGT.",
        "url": "http://arxiv.org/abs/2512.21883v1",
        "published_date": "2025-12-26T06:12:17+00:00",
        "updated_date": "2025-12-26T06:12:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianchen Deng",
            "Wenhua Wu",
            "Kunzhen Wu",
            "Guangming Wang",
            "Siting Zhu",
            "Shenghai Yuan",
            "Xun Chen",
            "Guole Shen",
            "Zhe Liu",
            "Hesheng Wang"
        ],
        "tldr": "The paper introduces Reloc-VGGT, a novel visual re-localization framework using a geometry-grounded transformer with multi-view spatial integration and sparse mask attention for real-time, robust pose estimation in diverse environments.",
        "tldr_zh": "该论文介绍了 Reloc-VGGT，一种新颖的视觉重定位框架，它使用几何引导的Transformer，结合多视图空间整合和稀疏掩码注意力，可在各种环境中实现实时、稳健的姿态估计。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "End-to-End 3D Spatiotemporal Perception with Multimodal Fusion and V2X Collaboration",
        "summary": "Multi-view cooperative perception and multimodal fusion are essential for reliable 3D spatiotemporal understanding in autonomous driving, especially under occlusions, limited viewpoints, and communication delays in V2X scenarios. This paper proposes XET-V2X, a multi-modal fused end-to-end tracking framework for v2x collaboration that unifies multi-view multimodal sensing within a shared spatiotemporal representation. To efficiently align heterogeneous viewpoints and modalities, XET-V2X introduces a dual-layer spatial cross-attention module based on multi-scale deformable attention. Multi-view image features are first aggregated to enhance semantic consistency, followed by point cloud fusion guided by the updated spatial queries, enabling effective cross-modal interaction while reducing computational overhead. Experiments on the real-world V2X-Seq-SPD dataset and the simulated V2X-Sim-V2V and V2X-Sim-V2I benchmarks demonstrate consistent improvements in detection and tracking performance under varying communication delays. Both quantitative results and qualitative visualizations indicate that XET-V2X achieves robust and temporally stable perception in complex traffic scenarios.",
        "url": "http://arxiv.org/abs/2512.21831v1",
        "published_date": "2025-12-26T02:20:22+00:00",
        "updated_date": "2025-12-26T02:20:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenwei Yang",
            "Yibo Ai",
            "Weidong Zhang"
        ],
        "tldr": "This paper introduces XET-V2X, a multi-modal fused end-to-end tracking framework leveraging V2X collaboration for enhanced 3D spatiotemporal perception in autonomous driving, demonstrating improved performance in detection and tracking under communication delays.",
        "tldr_zh": "该论文介绍了XET-V2X，一个多模态融合的端到端跟踪框架，利用V2X协同来增强自动驾驶中的3D时空感知，并在通信延迟下展示了改进的检测和跟踪性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]