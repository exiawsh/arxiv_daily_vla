[
    {
        "title": "DriveAgent-R1: Advancing VLM-based Autonomous Driving with Hybrid Thinking and Active Perception",
        "summary": "Vision-Language Models (VLMs) are advancing autonomous driving, yet their\npotential is constrained by myopic decision-making and passive perception,\nlimiting reliability in complex environments. We introduce DriveAgent-R1 to\ntackle these challenges in long-horizon, high-level behavioral decision-making.\nDriveAgent-R1 features two core innovations: a Hybrid-Thinking framework that\nadaptively switches between efficient text-based and in-depth tool-based\nreasoning, and an Active Perception mechanism with a vision toolkit to\nproactively resolve uncertainties, thereby balancing decision-making efficiency\nand reliability. The agent is trained using a novel, three-stage progressive\nreinforcement learning strategy designed to master these hybrid capabilities.\nExtensive experiments demonstrate that DriveAgent-R1 achieves state-of-the-art\nperformance, outperforming even leading proprietary large multimodal models,\nsuch as Claude Sonnet 4. Ablation studies validate our approach and confirm\nthat the agent's decisions are robustly grounded in actively perceived visual\nevidence, paving a path toward safer and more intelligent autonomous systems.",
        "url": "http://arxiv.org/abs/2507.20879v1",
        "published_date": "2025-07-28T14:33:15+00:00",
        "updated_date": "2025-07-28T14:33:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weicheng Zheng",
            "Xiaofei Mao",
            "Nanfei Ye",
            "Pengxiang Li",
            "Kun Zhan",
            "Xianpeng Lang",
            "Hang Zhao"
        ],
        "tldr": "DriveAgent-R1 enhances VLM-based autonomous driving using a hybrid reasoning approach and active perception, achieving state-of-the-art performance in complex environments.",
        "tldr_zh": "DriveAgent-R1通过混合推理和主动感知增强了基于VLM的自动驾驶，在复杂环境中实现了最先进的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "VESPA: Towards un(Human)supervised Open-World Pointcloud Labeling for Autonomous Driving",
        "summary": "Data collection for autonomous driving is rapidly accelerating, but manual\nannotation, especially for 3D labels, remains a major bottleneck due to its\nhigh cost and labor intensity. Autolabeling has emerged as a scalable\nalternative, allowing the generation of labels for point clouds with minimal\nhuman intervention. While LiDAR-based autolabeling methods leverage geometric\ninformation, they struggle with inherent limitations of lidar data, such as\nsparsity, occlusions, and incomplete object observations. Furthermore, these\nmethods typically operate in a class-agnostic manner, offering limited semantic\ngranularity. To address these challenges, we introduce VESPA, a multimodal\nautolabeling pipeline that fuses the geometric precision of LiDAR with the\nsemantic richness of camera images. Our approach leverages vision-language\nmodels (VLMs) to enable open-vocabulary object labeling and to refine detection\nquality directly in the point cloud domain. VESPA supports the discovery of\nnovel categories and produces high-quality 3D pseudolabels without requiring\nground-truth annotations or HD maps. On Nuscenes dataset, VESPA achieves an AP\nof 52.95% for object discovery and up to 46.54% for multiclass object\ndetection, demonstrating strong performance in scalable 3D scene understanding.\nCode will be available upon acceptance.",
        "url": "http://arxiv.org/abs/2507.20397v1",
        "published_date": "2025-07-27T19:39:29+00:00",
        "updated_date": "2025-07-27T19:39:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Levente Tempfli",
            "Esteban Rivera",
            "Markus Lienkamp"
        ],
        "tldr": "VESPA is a multimodal autolabeling pipeline that fuses LiDAR and camera data using vision-language models for open-vocabulary 3D object labeling, achieving promising results on the NuScenes dataset without ground truth annotations or HD maps.",
        "tldr_zh": "VESPA是一种多模态自动标注流程，它融合了激光雷达和相机数据，利用视觉-语言模型进行开放词汇的3D物体标注，在NuScenes数据集上取得了有希望的结果，且无需真实标注或高清地图。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "GTAD: Global Temporal Aggregation Denoising Learning for 3D Semantic Occupancy Prediction",
        "summary": "Accurately perceiving dynamic environments is a fundamental task for\nautonomous driving and robotic systems. Existing methods inadequately utilize\ntemporal information, relying mainly on local temporal interactions between\nadjacent frames and failing to leverage global sequence information\neffectively. To address this limitation, we investigate how to effectively\naggregate global temporal features from temporal sequences, aiming to achieve\noccupancy representations that efficiently utilize global temporal information\nfrom historical observations. For this purpose, we propose a global temporal\naggregation denoising network named GTAD, introducing a global temporal\ninformation aggregation framework as a new paradigm for holistic 3D scene\nunderstanding. Our method employs an in-model latent denoising network to\naggregate local temporal features from the current moment and global temporal\nfeatures from historical sequences. This approach enables the effective\nperception of both fine-grained temporal information from adjacent frames and\nglobal temporal patterns from historical observations. As a result, it provides\na more coherent and comprehensive understanding of the environment. Extensive\nexperiments on the nuScenes and Occ3D-nuScenes benchmark and ablation studies\ndemonstrate the superiority of our method.",
        "url": "http://arxiv.org/abs/2507.20963v1",
        "published_date": "2025-07-28T16:18:29+00:00",
        "updated_date": "2025-07-28T16:18:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianhao Li",
            "Yang Li",
            "Mengtian Li",
            "Yisheng Deng",
            "Weifeng Ge"
        ],
        "tldr": "The paper introduces GTAD, a global temporal aggregation denoising network, for 3D semantic occupancy prediction in dynamic environments, aiming to improve performance by effectively leveraging global temporal information from historical observations.",
        "tldr_zh": "该论文介绍了一种名为GTAD的全局时间聚合去噪网络，用于动态环境中的3D语义占用预测，旨在通过有效利用历史观测中的全局时间信息来提高性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Automated 3D-GS Registration and Fusion via Skeleton Alignment and Gaussian-Adaptive Features",
        "summary": "In recent years, 3D Gaussian Splatting (3D-GS)-based scene representation\ndemonstrates significant potential in real-time rendering and training\nefficiency. However, most existing methods primarily focus on single-map\nreconstruction, while the registration and fusion of multiple 3D-GS sub-maps\nremain underexplored. Existing methods typically rely on manual intervention to\nselect a reference sub-map as a template and use point cloud matching for\nregistration. Moreover, hard-threshold filtering of 3D-GS primitives often\ndegrades rendering quality after fusion. In this paper, we present a novel\napproach for automated 3D-GS sub-map alignment and fusion, eliminating the need\nfor manual intervention while enhancing registration accuracy and fusion\nquality. First, we extract geometric skeletons across multiple scenes and\nleverage ellipsoid-aware convolution to capture 3D-GS attributes, facilitating\nrobust scene registration. Second, we introduce a multi-factor Gaussian fusion\nstrategy to mitigate the scene element loss caused by rigid thresholding.\nExperiments on the ScanNet-GSReg and our Coord datasets demonstrate the\neffectiveness of the proposed method in registration and fusion. For\nregistration, it achieves a 41.9\\% reduction in RRE on complex scenes, ensuring\nmore precise pose estimation. For fusion, it improves PSNR by 10.11 dB,\nhighlighting superior structural preservation. These results confirm its\nability to enhance scene alignment and reconstruction fidelity, ensuring more\nconsistent and accurate 3D scene representation for robotic perception and\nautonomous navigation.",
        "url": "http://arxiv.org/abs/2507.20480v1",
        "published_date": "2025-07-28T02:36:21+00:00",
        "updated_date": "2025-07-28T02:36:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shiyang Liu",
            "Dianyi Yang",
            "Yu Gao",
            "Bohan Ren",
            "Yi Yang",
            "Mengyin Fu"
        ],
        "tldr": "This paper introduces a novel method for automated registration and fusion of 3D Gaussian Splatting (3D-GS) sub-maps using skeleton alignment and Gaussian-adaptive features, improving registration accuracy and fusion quality without manual intervention.",
        "tldr_zh": "本文提出了一种新的方法，通过骨架对齐和高斯自适应特征自动注册和融合3D高斯溅射（3D-GS）子图，无需人工干预即可提高注册精度和融合质量。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Methods for the Segmentation of Reticular Structures Using 3D LiDAR Data: A Comparative Evaluation",
        "summary": "Reticular structures form the backbone of major infrastructure like bridges,\npylons, and airports, but their inspection and maintenance are costly and\nhazardous, often requiring human intervention. While prior research has focused\non fault detection via images or robotic platform design, the autonomous\nnavigation of robots within these structures is less explored. This study\naddresses that gap by proposing methods to detect navigable surfaces in truss\nstructures, enhancing the autonomy of climbing robots. The paper introduces\nseveral approaches for binary segmentation of navigable surfaces versus\nbackground from 3D point clouds of metallic trusses. These methods fall into\ntwo categories: analytical algorithms and deep learning models. The analytical\napproach features a custom algorithm that segments structures by analyzing the\neigendecomposition of planar patches in the point cloud. In parallel, advanced\ndeep learning models PointNet, PointNet++, MinkUNet34C, and PointTransformerV3\nare trained and evaluated for the same task. Comparative analysis shows that\nthe analytical algorithm offers easier parameter tuning and performance\ncomparable to deep learning models, which, while more computationally\nintensive, excel in segmentation accuracy. Notably, PointTransformerV3 achieves\na Mean Intersection Over Union (mIoU) of about 97%. The study demonstrates the\npromise of both analytical and deep learning methods for improving autonomous\nnavigation in complex truss environments. The results highlight the trade-offs\nbetween computational efficiency and segmentation performance, providing\nvaluable guidance for future research and practical applications in autonomous\ninfrastructure inspection and maintenance.",
        "url": "http://arxiv.org/abs/2507.20589v1",
        "published_date": "2025-07-28T07:55:26+00:00",
        "updated_date": "2025-07-28T07:55:26+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Francisco J. Soler Mora",
            "Adrián Peidró Vidal",
            "Marc Fabregat-Jaén",
            "Luis Payá Castelló",
            "Óscar Reinoso García"
        ],
        "tldr": "This paper compares analytical and deep learning methods for segmenting navigable surfaces on reticular structures from 3D LiDAR data to improve climbing robot autonomy, finding comparable performance with trade-offs between computational efficiency and accuracy.",
        "tldr_zh": "本文比较了使用3D激光雷达数据分割网状结构上可导航表面的解析方法和深度学习方法，以提高攀爬机器人的自主性，发现两者性能相当，但计算效率和准确性之间存在权衡。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Solving Scene Understanding for Autonomous Navigation in Unstructured Environments",
        "summary": "Autonomous vehicles are the next revolution in the automobile industry and\nthey are expected to revolutionize the future of transportation. Understanding\nthe scenario in which the autonomous vehicle will operate is critical for its\ncompetent functioning. Deep Learning has played a massive role in the progress\nthat has been made till date. Semantic Segmentation, the process of annotating\nevery pixel of an image with an object class, is one crucial part of this scene\ncomprehension using Deep Learning. It is especially useful in Autonomous\nDriving Research as it requires comprehension of drivable and non-drivable\nareas, roadside objects and the like. In this paper semantic segmentation has\nbeen performed on the Indian Driving Dataset which has been recently compiled\non the urban and rural roads of Bengaluru and Hyderabad. This dataset is more\nchallenging compared to other datasets like Cityscapes, since it is based on\nunstructured driving environments. It has a four level hierarchy and in this\npaper segmentation has been performed on the first level. Five different models\nhave been trained and their performance has been compared using the Mean\nIntersection over Union. These are UNET, UNET+RESNET50, DeepLabsV3, PSPNet and\nSegNet. The highest MIOU of 0.6496 has been achieved. The paper discusses the\ndataset, exploratory data analysis, preparation, implementation of the five\nmodels and studies the performance and compares the results achieved in the\nprocess.",
        "url": "http://arxiv.org/abs/2507.20389v1",
        "published_date": "2025-07-27T19:11:21+00:00",
        "updated_date": "2025-07-27T19:11:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Naveen Mathews Renji",
            "Kruthika K",
            "Manasa Keshavamurthy",
            "Pooja Kumari",
            "S. Rajarajeswari"
        ],
        "tldr": "This paper performs semantic segmentation on the Indian Driving Dataset (IDD), a dataset based on unstructured Indian roads, using five different deep learning models (UNET, UNET+RESNET50, DeepLabsV3, PSPNet and SegNet) and compares their performance.",
        "tldr_zh": "本文使用五种不同的深度学习模型 (UNET, UNET+RESNET50, DeepLabsV3, PSPNet 和 SegNet) 对印度驾驶数据集 (IDD) 进行语义分割，该数据集基于非结构化的印度道路，并比较了它们的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]