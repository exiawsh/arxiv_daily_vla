[
    {
        "title": "Out-of-Sight Trajectories: Tracking, Fusion, and Prediction",
        "summary": "Trajectory prediction is a critical task in computer vision and autonomous\nsystems, playing a key role in autonomous driving, robotics, surveillance, and\nvirtual reality. Existing methods often rely on complete and noise-free\nobservational data, overlooking the challenges associated with out-of-sight\nobjects and the inherent noise in sensor data caused by limited camera\ncoverage, obstructions, and the absence of ground truth for denoised\ntrajectories. These limitations pose safety risks and hinder reliable\nprediction in real-world scenarios. In this extended work, we present\nadvancements in Out-of-Sight Trajectory (OST), a novel task that predicts the\nnoise-free visual trajectories of out-of-sight objects using noisy sensor data.\nBuilding on our previous research, we broaden the scope of Out-of-Sight\nTrajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending\nits applicability to autonomous driving, robotics, surveillance, and virtual\nreality. Our enhanced Vision-Positioning Denoising Module leverages camera\ncalibration to establish a vision-positioning mapping, addressing the lack of\nvisual references, while effectively denoising noisy sensor data in an\nunsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB\ndatasets, our approach achieves state-of-the-art performance in both trajectory\ndenoising and prediction, significantly surpassing previous baselines.\nAdditionally, we introduce comparisons with traditional denoising methods, such\nas Kalman filtering, and adapt recent trajectory prediction models to our task,\nproviding a comprehensive benchmark. This work represents the first initiative\nto integrate vision-positioning projection for denoising noisy sensor\ntrajectories of out-of-sight agents, paving the way for future advances. The\ncode and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST",
        "url": "http://arxiv.org/abs/2509.15219v1",
        "published_date": "2025-09-18T17:59:16+00:00",
        "updated_date": "2025-09-18T17:59:16+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.MA",
            "cs.MM",
            "cs.RO",
            "68T45, 68U10, 68T07, 68T40, 93C85, 93E11, 62M20, 62M10, 68U05, 94A12",
            "F.2.2; I.2.9; I.2.10; I.4.1; I.4.8; I.4.9; I.5.4; I.3.7"
        ],
        "authors": [
            "Haichao Zhang",
            "Yi Xu",
            "Yun Fu"
        ],
        "tldr": "The paper introduces a novel approach, Out-of-Sight Trajectory (OST), for predicting and denoising trajectories of out-of-sight objects using noisy sensor data, particularly in the context of autonomous driving and robotics, achieving state-of-the-art results on public datasets.",
        "tldr_zh": "该论文提出了一种新颖的方法，即视野外轨迹（OST），用于预测和去噪视野外物体的轨迹，使用嘈杂的传感器数据，特别是在自动驾驶和机器人领域，并在公共数据集上取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation",
        "summary": "This paper presents RynnVLA-001, a vision-language-action(VLA) model built\nupon large-scale video generative pretraining from human demonstrations. We\npropose a novel two-stage pretraining methodology. The first stage, Ego-Centric\nVideo Generative Pretraining, trains an Image-to-Video model on 12M ego-centric\nmanipulation videos to predict future frames conditioned on an initial frame\nand a language instruction. The second stage, Human-Centric Trajectory-Aware\nModeling, extends this by jointly predicting future keypoint trajectories,\nthereby effectively bridging visual frame prediction with action prediction.\nFurthermore, to enhance action representation, we propose ActionVAE, a\nvariational autoencoder that compresses sequences of actions into compact\nlatent embeddings, reducing the complexity of the VLA output space. When\nfinetuned on the same downstream robotics datasets, RynnVLA-001 achieves\nsuperior performance over state-of-the-art baselines, demonstrating that the\nproposed pretraining strategy provides a more effective initialization for VLA\nmodels.",
        "url": "http://arxiv.org/abs/2509.15212v1",
        "published_date": "2025-09-18T17:58:02+00:00",
        "updated_date": "2025-09-18T17:58:02+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Yuming Jiang",
            "Siteng Huang",
            "Shengke Xue",
            "Yaxi Zhao",
            "Jun Cen",
            "Sicong Leng",
            "Kehan Li",
            "Jiayan Guo",
            "Kexiang Wang",
            "Mingxiu Chen",
            "Fan Wang",
            "Deli Zhao",
            "Xin Li"
        ],
        "tldr": "RynnVLA-001, a VLA model pre-trained on human demonstrations using a novel two-stage approach, achieves superior performance in robot manipulation tasks compared to state-of-the-art methods.",
        "tldr_zh": "RynnVLA-001是一个视觉-语言-动作（VLA）模型，它使用一种新颖的两阶段方法在人类演示上进行预训练，与最先进的方法相比，在机器人操作任务中实现了卓越的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UCorr: Wire Detection and Depth Estimation for Autonomous Drones",
        "summary": "In the realm of fully autonomous drones, the accurate detection of obstacles\nis paramount to ensure safe navigation and prevent collisions. Among these\nchallenges, the detection of wires stands out due to their slender profile,\nwhich poses a unique and intricate problem. To address this issue, we present\nan innovative solution in the form of a monocular end-to-end model for wire\nsegmentation and depth estimation. Our approach leverages a temporal\ncorrelation layer trained on synthetic data, providing the model with the\nability to effectively tackle the complex joint task of wire detection and\ndepth estimation. We demonstrate the superiority of our proposed method over\nexisting competitive approaches in the joint task of wire detection and depth\nestimation. Our results underscore the potential of our model to enhance the\nsafety and precision of autonomous drones, shedding light on its promising\napplications in real-world scenarios.",
        "url": "http://arxiv.org/abs/2509.14989v1",
        "published_date": "2025-09-18T14:21:52+00:00",
        "updated_date": "2025-09-18T14:21:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Benedikt Kolbeinsson",
            "Krystian Mikolajczyk"
        ],
        "tldr": "This paper introduces a monocular end-to-end model (UCorr) for wire segmentation and depth estimation using a temporal correlation layer trained on synthetic data, aiming to improve autonomous drone navigation.",
        "tldr_zh": "该论文介绍了一种单目端到端模型（UCorr），使用在合成数据上训练的时间相关层进行电线分割和深度估计，旨在提高自主无人机的导航能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation",
        "summary": "Mobile manipulation requires the coordinated control of a mobile base and a\nrobotic arm while simultaneously perceiving both global scene context and\nfine-grained object details. Existing single-view approaches often fail in\nunstructured environments due to limited fields of view, exploration, and\ngeneralization abilities. Moreover, classical controllers, although stable,\nstruggle with efficiency and manipulability near singularities. To address\nthese challenges, we propose M4Diffuser, a hybrid framework that integrates a\nMulti-View Diffusion Policy with a novel Reduced and Manipulability-aware QP\n(ReM-QP) controller for mobile manipulation. The diffusion policy leverages\nproprioceptive states and complementary camera perspectives with both\nclose-range object details and global scene context to generate task-relevant\nend-effector goals in the world frame. These high-level goals are then executed\nby the ReM-QP controller, which eliminates slack variables for computational\nefficiency and incorporates manipulability-aware preferences for robustness\nnear singularities. Comprehensive experiments in simulation and real-world\nenvironments show that M4Diffuser achieves 7 to 56 percent higher success rates\nand reduces collisions by 3 to 31 percent over baselines. Our approach\ndemonstrates robust performance for smooth whole-body coordination, and strong\ngeneralization to unseen tasks, paving the way for reliable mobile manipulation\nin unstructured environments. Details of the demo and supplemental material are\navailable on our project website https://sites.google.com/view/m4diffuser.",
        "url": "http://arxiv.org/abs/2509.14980v1",
        "published_date": "2025-09-18T14:09:53+00:00",
        "updated_date": "2025-09-18T14:09:53+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Ju Dong",
            "Lei Zhang",
            "Liding Zhang",
            "Yao Ling",
            "Yu Fu",
            "Kaixin Bai",
            "Zoltán-Csaba Márton",
            "Zhenshan Bing",
            "Zhaopeng Chen",
            "Alois Christian Knoll",
            "Jianwei Zhang"
        ],
        "tldr": "The paper introduces M4Diffuser, a novel framework integrating a Multi-View Diffusion Policy with a manipulability-aware QP controller for robust mobile manipulation, showing improved success rates and reduced collisions in complex environments.",
        "tldr_zh": "该论文介绍了M4Diffuser，一种新颖的框架，集成了多视角扩散策略和可操作性感知的QP控制器，用于稳健的移动操作，在复杂环境中显示出更高的成功率和更少的碰撞。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RoboEye: Enhancing 2D Robotic Object Identification with Selective 3D Geometric Keypoint Matching",
        "summary": "The rapidly growing number of product categories in large-scale e-commerce\nmakes accurate object identification for automated packing in warehouses\nsubstantially more difficult. As the catalog grows, intra-class variability and\na long tail of rare or visually similar items increase, and when combined with\ndiverse packaging, cluttered containers, frequent occlusion, and large\nviewpoint changes-these factors amplify discrepancies between query and\nreference images, causing sharp performance drops for methods that rely solely\non 2D appearance features. Thus, we propose RoboEye, a two-stage identification\nframework that dynamically augments 2D semantic features with domain-adapted 3D\nreasoning and lightweight adapters to bridge training deployment gaps. In the\nfirst stage, we train a large vision model to extract 2D features for\ngenerating candidate rankings. A lightweight 3D-feature-awareness module then\nestimates 3D feature quality and predicts whether 3D re-ranking is necessary,\npreventing performance degradation and avoiding unnecessary computation. When\ninvoked, the second stage uses our robot 3D retrieval transformer, comprising a\n3D feature extractor that produces geometry-aware dense features and a\nkeypoint-based matcher that computes keypoint-correspondence confidences\nbetween query and reference images instead of conventional cosine-similarity\nscoring. Experiments show that RoboEye improves Recall@1 by 7.1% over the prior\nstate of the art (RoboLLM). Moreover, RoboEye operates using only RGB images,\navoiding reliance on explicit 3D inputs and reducing deployment costs. The code\nused in this paper is publicly available at:\nhttps://github.com/longkukuhi/RoboEye.",
        "url": "http://arxiv.org/abs/2509.14966v1",
        "published_date": "2025-09-18T13:59:24+00:00",
        "updated_date": "2025-09-18T13:59:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Xingwu Zhang",
            "Guanxuan Li",
            "Zhuocheng Zhang",
            "Zijun Long"
        ],
        "tldr": "RoboEye improves 2D object identification in robotics by using a two-stage approach that selectively incorporates 3D geometric reasoning, achieving a 7.1% improvement over the state-of-the-art without requiring explicit 3D input.",
        "tldr_zh": "RoboEye通过一种选择性地结合3D几何推理的两阶段方法，改进了机器人技术中的2D物体识别，在不需要显式3D输入的情况下，比最先进的技术提高了7.1%。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Designing Latent Safety Filters using Pre-Trained Vision Models",
        "summary": "Ensuring safety of vision-based control systems remains a major challenge\nhindering their deployment in critical settings. Safety filters have gained\nincreased interest as effective tools for ensuring the safety of classical\ncontrol systems, but their applications in vision-based control settings have\nso far been limited. Pre-trained vision models (PVRs) have been shown to be\neffective perception backbones for control in various robotics domains. In this\npaper, we are interested in examining their effectiveness when used for\ndesigning vision-based safety filters. We use them as backbones for classifiers\ndefining failure sets, for Hamilton-Jacobi (HJ) reachability-based safety\nfilters, and for latent world models. We discuss the trade-offs between\ntraining from scratch, fine-tuning, and freezing the PVRs when training the\nmodels they are backbones for. We also evaluate whether one of the PVRs is\nsuperior across all tasks, evaluate whether learned world models or Q-functions\nare better for switching decisions to safe policies, and discuss practical\nconsiderations for deploying these PVRs on resource-constrained devices.",
        "url": "http://arxiv.org/abs/2509.14758v1",
        "published_date": "2025-09-18T09:06:37+00:00",
        "updated_date": "2025-09-18T09:06:37+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG",
            "cs.SY",
            "eess.SY"
        ],
        "authors": [
            "Ihab Tabbara",
            "Yuxuan Yang",
            "Ahmad Hamzeh",
            "Maxwell Astafyev",
            "Hussein Sibai"
        ],
        "tldr": "This paper explores using pre-trained vision models (PVRs) as backbones for designing vision-based safety filters in robotics, evaluating different training strategies and comparing various approaches for safe policy switching.",
        "tldr_zh": "本文探讨了使用预训练视觉模型（PVR）作为骨干网络来设计机器人中基于视觉的安全滤波器，评估了不同的训练策略，并比较了各种安全策略切换的方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Feature-aligned Motion Transformation for Efficient Dynamic Point Cloud Compression",
        "summary": "Dynamic point clouds are widely used in applications such as immersive\nreality, robotics, and autonomous driving. Efficient compression largely\ndepends on accurate motion estimation and compensation, yet the irregular\nstructure and significant local variations of point clouds make this task\nhighly challenging. Current methods often rely on explicit motion estimation,\nwhose encoded vectors struggle to capture intricate dynamics and fail to fully\nexploit temporal correlations. To overcome these limitations, we introduce a\nFeature-aligned Motion Transformation (FMT) framework for dynamic point cloud\ncompression. FMT replaces explicit motion vectors with a spatiotemporal\nalignment strategy that implicitly models continuous temporal variations, using\naligned features as temporal context within a latent-space conditional encoding\nframework. Furthermore, we design a random access (RA) reference strategy that\nenables bidirectional motion referencing and layered encoding, thereby\nsupporting frame-level parallel compression. Extensive experiments demonstrate\nthat our method surpasses D-DPCC and AdaDPCC in both encoding and decoding\nefficiency, while also achieving BD-Rate reductions of 20% and 9.4%,\nrespectively. These results highlight the effectiveness of FMT in jointly\nimproving compression efficiency and processing performance.",
        "url": "http://arxiv.org/abs/2509.14591v1",
        "published_date": "2025-09-18T03:51:06+00:00",
        "updated_date": "2025-09-18T03:51:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuan Deng",
            "Xiandong Meng",
            "Longguang Wang",
            "Tiange Zhang",
            "Xiaopeng Fan",
            "Debin Zhao"
        ],
        "tldr": "This paper introduces a Feature-aligned Motion Transformation (FMT) framework for efficient dynamic point cloud compression, achieving improved compression efficiency and processing performance compared to existing methods.",
        "tldr_zh": "该论文介绍了一种用于高效动态点云压缩的特征对齐运动变换（FMT）框架，与现有方法相比，该框架实现了更高的压缩效率和处理性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DiffVL: Diffusion-Based Visual Localization on 2D Maps via BEV-Conditioned GPS Denoising",
        "summary": "Accurate visual localization is crucial for autonomous driving, yet existing\nmethods face a fundamental dilemma: While high-definition (HD) maps provide\nhigh-precision localization references, their costly construction and\nmaintenance hinder scalability, which drives research toward\nstandard-definition (SD) maps like OpenStreetMap. Current SD-map-based\napproaches primarily focus on Bird's-Eye View (BEV) matching between images and\nmaps, overlooking a ubiquitous signal-noisy GPS. Although GPS is readily\navailable, it suffers from multipath errors in urban environments. We propose\nDiffVL, the first framework to reformulate visual localization as a GPS\ndenoising task using diffusion models. Our key insight is that noisy GPS\ntrajectory, when conditioned on visual BEV features and SD maps, implicitly\nencode the true pose distribution, which can be recovered through iterative\ndiffusion refinement. DiffVL, unlike prior BEV-matching methods (e.g.,\nOrienterNet) or transformer-based registration approaches, learns to reverse\nGPS noise perturbations by jointly modeling GPS, SD map, and visual signals,\nachieving sub-meter accuracy without relying on HD maps. Experiments on\nmultiple datasets demonstrate that our method achieves state-of-the-art\naccuracy compared to BEV-matching baselines. Crucially, our work proves that\ndiffusion models can enable scalable localization by treating noisy GPS as a\ngenerative prior-making a paradigm shift from traditional matching-based\nmethods.",
        "url": "http://arxiv.org/abs/2509.14565v1",
        "published_date": "2025-09-18T02:57:28+00:00",
        "updated_date": "2025-09-18T02:57:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Li Gao",
            "Hongyang Sun",
            "Liu Liu",
            "Yunhao Li",
            "Yang Cai"
        ],
        "tldr": "DiffVL uses a diffusion model to denoise GPS signals conditioned on visual BEV features and SD maps for accurate, scalable visual localization, outperforming BEV-matching baselines.",
        "tldr_zh": "DiffVL使用扩散模型，以视觉BEV特征和SD地图为条件，对GPS信号进行去噪，实现精确且可扩展的视觉定位，优于BEV匹配基线。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RLBind: Adversarial-Invariant Cross-Modal Alignment for Unified Robust Embeddings",
        "summary": "Unified multi-modal encoders that bind vision, audio, and other sensors into\na shared embedding space are attractive building blocks for robot perception\nand decision-making. However, on-robot deployment exposes the vision branch to\nadversarial and natural corruptions, making robustness a prerequisite for\nsafety. Prior defenses typically align clean and adversarial features within\nCLIP-style encoders and overlook broader cross-modal correspondence, yielding\nmodest gains and often degrading zero-shot transfer. We introduce RLBind, a\ntwo-stage adversarial-invariant cross-modal alignment framework for robust\nunified embeddings. Stage 1 performs unsupervised fine-tuning on\nclean-adversarial pairs to harden the visual encoder. Stage 2 leverages\ncross-modal correspondence by minimizing the discrepancy between\nclean/adversarial features and a text anchor, while enforcing class-wise\ndistributional alignment across modalities. Extensive experiments on Image,\nAudio, Thermal, and Video data show that RLBind consistently outperforms the\nLanguageBind backbone and standard fine-tuning baselines in both clean accuracy\nand norm-bounded adversarial robustness. By improving resilience without\nsacrificing generalization, RLBind provides a practical path toward safer\nmulti-sensor perception stacks for embodied robots in navigation, manipulation,\nand other autonomy settings.",
        "url": "http://arxiv.org/abs/2509.14383v1",
        "published_date": "2025-09-17T19:35:52+00:00",
        "updated_date": "2025-09-17T19:35:52+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Yuhong Lu"
        ],
        "tldr": "The paper introduces RLBind, a two-stage framework for creating robust, unified multi-modal embeddings by aligning clean and adversarial examples with text anchors, improving robustness and generalization for robot perception.",
        "tldr_zh": "该论文介绍了RLBind，一种两阶段框架，通过将干净的和对抗性样本与文本锚点对齐，创建稳健的统一多模态嵌入，从而提高机器人感知的稳健性和泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]