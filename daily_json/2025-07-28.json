[
    {
        "title": "MambaMap: Online Vectorized HD Map Construction using State Space Model",
        "summary": "High-definition (HD) maps are essential for autonomous driving, as they\nprovide precise road information for downstream tasks. Recent advances\nhighlight the potential of temporal modeling in addressing challenges like\nocclusions and extended perception range. However, existing methods either fail\nto fully exploit temporal information or incur substantial computational\noverhead in handling extended sequences. To tackle these challenges, we propose\nMambaMap, a novel framework that efficiently fuses long-range temporal features\nin the state space to construct online vectorized HD maps. Specifically,\nMambaMap incorporates a memory bank to store and utilize information from\nhistorical frames, dynamically updating BEV features and instance queries to\nimprove robustness against noise and occlusions. Moreover, we introduce a\ngating mechanism in the state space, selectively integrating dependencies of\nmap elements in high computational efficiency. In addition, we design\ninnovative multi-directional and spatial-temporal scanning strategies to\nenhance feature extraction at both BEV and instance levels. These strategies\nsignificantly boost the prediction accuracy of our approach while ensuring\nrobust temporal consistency. Extensive experiments on the nuScenes and\nArgoverse2 datasets demonstrate that our proposed MambaMap approach outperforms\nstate-of-the-art methods across various splits and perception ranges. Source\ncode will be available at https://github.com/ZiziAmy/MambaMap.",
        "url": "http://arxiv.org/abs/2507.20224v1",
        "published_date": "2025-07-27T11:09:27+00:00",
        "updated_date": "2025-07-27T11:09:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruizi Yang",
            "Xiaolu Liu",
            "Junbo Chen",
            "Jianke Zhu"
        ],
        "tldr": "MambaMap is a novel framework that efficiently constructs online vectorized HD maps by fusing long-range temporal features in a state space model, outperforming state-of-the-art methods on nuScenes and Argoverse2 datasets.",
        "tldr_zh": "MambaMap是一个新颖的框架，通过在状态空间模型中融合长程时间特征，高效地构建在线矢量化高清地图，并在nuScenes和Argoverse2数据集上优于最先进的方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots",
        "summary": "Humanoid robot technology is advancing rapidly, with manufacturers\nintroducing diverse heterogeneous visual perception modules tailored to\nspecific scenarios. Among various perception paradigms, occupancy-based\nrepresentation has become widely recognized as particularly suitable for\nhumanoid robots, as it provides both rich semantic and 3D geometric information\nessential for comprehensive environmental understanding. In this work, we\npresent Humanoid Occupancy, a generalized multimodal occupancy perception\nsystem that integrates hardware and software components, data acquisition\ndevices, and a dedicated annotation pipeline. Our framework employs advanced\nmulti-modal fusion techniques to generate grid-based occupancy outputs encoding\nboth occupancy status and semantic labels, thereby enabling holistic\nenvironmental understanding for downstream tasks such as task planning and\nnavigation. To address the unique challenges of humanoid robots, we overcome\nissues such as kinematic interference and occlusion, and establish an effective\nsensor layout strategy. Furthermore, we have developed the first panoramic\noccupancy dataset specifically for humanoid robots, offering a valuable\nbenchmark and resource for future research and development in this domain. The\nnetwork architecture incorporates multi-modal feature fusion and temporal\ninformation integration to ensure robust perception. Overall, Humanoid\nOccupancy delivers effective environmental perception for humanoid robots and\nestablishes a technical foundation for standardizing universal visual modules,\npaving the way for the widespread deployment of humanoid robots in complex\nreal-world scenarios.",
        "url": "http://arxiv.org/abs/2507.20217v2",
        "published_date": "2025-07-27T10:47:00+00:00",
        "updated_date": "2025-07-29T02:24:52+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Wei Cui",
            "Haoyu Wang",
            "Wenkang Qin",
            "Yijie Guo",
            "Gang Han",
            "Wen Zhao",
            "Jiahang Cao",
            "Zhang Zhang",
            "Jiaru Zhong",
            "Jingkai Sun",
            "Pihai Sun",
            "Shuai Shi",
            "Botuo Jiang",
            "Jiahao Ma",
            "Jiaxu Wang",
            "Hao Cheng",
            "Zhichao Liu",
            "Yang Wang",
            "Zheng Zhu",
            "Guan Huang",
            "Jian Tang",
            "Qiang Zhang"
        ],
        "tldr": "The paper introduces Humanoid Occupancy, a multimodal occupancy perception system for humanoid robots, including a novel dataset and sensor layout strategies to address humanoid-specific challenges, aiming to standardize visual modules for real-world deployment.",
        "tldr_zh": "该论文介绍了Humanoid Occupancy，一种用于人形机器人的多模态占用感知系统，包括一个新的数据集和传感器布局策略，以解决人形机器人特有的挑战，旨在标准化用于现实世界部署的视觉模块。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "LRR-Bench: Left, Right or Rotate? Vision-Language models Still Struggle With Spatial Understanding Tasks",
        "summary": "Real-world applications, such as autonomous driving and humanoid robot\nmanipulation, require precise spatial perception. However, it remains\nunderexplored how Vision-Language Models (VLMs) recognize spatial relationships\nand perceive spatial movement. In this work, we introduce a spatial evaluation\npipeline and construct a corresponding benchmark. Specifically, we categorize\nspatial understanding into two main types: absolute spatial understanding,\nwhich involves querying the absolute spatial position (e.g., left, right) of an\nobject within an image, and 3D spatial understanding, which includes movement\nand rotation. Notably, our dataset is entirely synthetic, enabling the\ngeneration of test samples at a low cost while also preventing dataset\ncontamination. We conduct experiments on multiple state-of-the-art VLMs and\nobserve that there is significant room for improvement in their spatial\nunderstanding abilities. Explicitly, in our experiments, humans achieve\nnear-perfect performance on all tasks, whereas current VLMs attain human-level\nperformance only on the two simplest tasks. For the remaining tasks, the\nperformance of VLMs is distinctly lower than that of humans. In fact, the\nbest-performing Vision-Language Models even achieve near-zero scores on\nmultiple tasks. The dataset and code are available on\nhttps://github.com/kong13661/LRR-Bench.",
        "url": "http://arxiv.org/abs/2507.20174v1",
        "published_date": "2025-07-27T08:31:24+00:00",
        "updated_date": "2025-07-27T08:31:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Fei Kong",
            "Jinhao Duan",
            "Kaidi Xu",
            "Zhenhua Guo",
            "Xiaofeng Zhu",
            "Xiaoshuang Shi"
        ],
        "tldr": "This paper introduces LRR-Bench, a synthetic benchmark for evaluating the spatial understanding abilities of Vision-Language Models, revealing significant deficiencies in their performance compared to humans, particularly in 3D spatial reasoning.",
        "tldr_zh": "该论文介绍了 LRR-Bench，一个用于评估视觉-语言模型空间理解能力的合成基准，揭示了它们在性能上与人类相比存在显著缺陷，尤其是在 3D 空间推理方面。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "NeuroVoxel-LM: Language-Aligned 3D Perception via Dynamic Voxelization and Meta-Embedding",
        "summary": "Recent breakthroughs in Visual Language Models (VLMs) and Multimodal Large\nLanguage Models (MLLMs) have significantly advanced 3D scene perception towards\nlanguage-driven cognition. However, existing 3D language models struggle with\nsparse, large-scale point clouds due to slow feature extraction and limited\nrepresentation accuracy. To address these challenges, we propose NeuroVoxel-LM,\na novel framework that integrates Neural Radiance Fields (NeRF) with dynamic\nresolution voxelization and lightweight meta-embedding. Specifically, we\nintroduce a Dynamic Resolution Multiscale Voxelization (DR-MSV) technique that\nadaptively adjusts voxel granularity based on geometric and structural\ncomplexity, reducing computational cost while preserving reconstruction\nfidelity. In addition, we propose the Token-level Adaptive Pooling for\nLightweight Meta-Embedding (TAP-LME) mechanism, which enhances semantic\nrepresentation through attention-based weighting and residual fusion.\nExperimental results demonstrate that DR-MSV significantly improves point cloud\nfeature extraction efficiency and accuracy, while TAP-LME outperforms\nconventional max-pooling in capturing fine-grained semantics from NeRF weights.",
        "url": "http://arxiv.org/abs/2507.20110v1",
        "published_date": "2025-07-27T03:11:08+00:00",
        "updated_date": "2025-07-27T03:11:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "I.4; I.5"
        ],
        "authors": [
            "Shiyu Liu",
            "Lianlei Shan"
        ],
        "tldr": "The paper introduces NeuroVoxel-LM, a novel framework for language-aligned 3D perception that uses dynamic voxelization and meta-embedding to improve efficiency and accuracy in processing large-scale point clouds. It aims to enhance 3D scene understanding for language-driven tasks by integrating NeRF with efficient feature extraction techniques.",
        "tldr_zh": "该论文介绍了NeuroVoxel-LM，一种新颖的语言对齐3D感知框架，它使用动态体素化和元嵌入来提高处理大规模点云的效率和准确性。它旨在通过将NeRF与高效的特征提取技术相结合，增强语言驱动任务的3D场景理解。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]