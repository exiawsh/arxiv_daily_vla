[
    {
        "title": "Risk-Aware World Model Predictive Control for Generalizable End-to-End Autonomous Driving",
        "summary": "With advances in imitation learning (IL) and large-scale driving datasets, end-to-end autonomous driving (E2E-AD) has made great progress recently. Currently, IL-based methods have become a mainstream paradigm: models rely on standard driving behaviors given by experts, and learn to minimize the discrepancy between their actions and expert actions. However, this objective of \"only driving like the expert\" suffers from limited generalization: when encountering rare or unseen long-tail scenarios outside the distribution of expert demonstrations, models tend to produce unsafe decisions in the absence of prior experience. This raises a fundamental question: Can an E2E-AD system make reliable decisions without any expert action supervision? Motivated by this, we propose a unified framework named Risk-aware World Model Predictive Control (RaWMPC) to address this generalization dilemma through robust control, without reliance on expert demonstrations. Practically, RaWMPC leverages a world model to predict the consequences of multiple candidate actions and selects low-risk actions through explicit risk evaluation. To endow the world model with the ability to predict the outcomes of risky driving behaviors, we design a risk-aware interaction strategy that systematically exposes the world model to hazardous behaviors, making catastrophic outcomes predictable and thus avoidable. Furthermore, to generate low-risk candidate actions at test time, we introduce a self-evaluation distillation method to distill riskavoidance capabilities from the well-trained world model into a generative action proposal network without any expert demonstration. Extensive experiments show that RaWMPC outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios, while providing superior decision interpretability.",
        "url": "http://arxiv.org/abs/2602.23259v1",
        "published_date": "2026-02-26T17:32:30+00:00",
        "updated_date": "2026-02-26T17:32:30+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Jiangxin Sun",
            "Feng Xue",
            "Teng Long",
            "Chang Liu",
            "Jian-Fang Hu",
            "Wei-Shi Zheng",
            "Nicu Sebe"
        ],
        "tldr": "This paper introduces Risk-aware World Model Predictive Control (RaWMPC), an end-to-end autonomous driving framework that leverages a world model and risk evaluation to make safe decisions, especially in unseen scenarios, without relying on expert demonstrations.",
        "tldr_zh": "本文介绍了风险感知世界模型预测控制（RaWMPC），这是一种端到端自动驾驶框架，它利用世界模型和风险评估来做出安全决策，尤其是在未见过的情况下，而无需依赖专家演示。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "UniScale: Unified Scale-Aware 3D Reconstruction for Multi-View Understanding via Prior Injection for Robotic Perception",
        "summary": "We present UniScale, a unified, scale-aware multi-view 3D reconstruction framework for robotic applications that flexibly integrates geometric priors through a modular, semantically informed design. In vision-based robotic navigation, the accurate extraction of environmental structure from raw image sequences is critical for downstream tasks. UniScale addresses this challenge with a single feed-forward network that jointly estimates camera intrinsics and extrinsics, scale-invariant depth and point maps, and the metric scale of a scene from multi-view images, while optionally incorporating auxiliary geometric priors when available. By combining global contextual reasoning with camera-aware feature representations, UniScale is able to recover the metric-scale of the scene. In robotic settings where camera intrinsics are known, they can be easily incorporated to improve performance, with additional gains obtained when camera poses are also available. This co-design enables robust, metric-aware 3D reconstruction within a single unified model. Importantly, UniScale does not require training from scratch, and leverages world priors exhibited in pre-existing models without geometric encoding strategies, making it particularly suitable for resource-constrained robotic teams. We evaluate UniScale on multiple benchmarks, demonstrating strong generalization and consistent performance across diverse environments. We will release our implementation upon acceptance.",
        "url": "http://arxiv.org/abs/2602.23224v1",
        "published_date": "2026-02-26T17:04:36+00:00",
        "updated_date": "2026-02-26T17:04:36+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Mohammad Mahdavian",
            "Gordon Tan",
            "Binbin Xu",
            "Yuan Ren",
            "Dongfeng Bai",
            "Bingbing Liu"
        ],
        "tldr": "UniScale is a unified framework for metric-scale 3D reconstruction from multi-view images, particularly suited for robotic applications, and allows incorporating geometric priors for improved performance.",
        "tldr_zh": "UniScale是一个统一的框架，用于从多视图图像中进行米制尺度的三维重建，特别适用于机器人应用，并且允许整合几何先验知识以提高性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents",
        "summary": "Human behaviors in the real world naturally encode rich, long-term contextual information that can be leveraged to train embodied agents for perception, understanding, and acting. However, existing capture systems typically rely on costly studio setups and wearable devices, limiting the large-scale collection of scene-conditioned human motion data in the wild. To address this, we propose EmbodMocap, a portable and affordable data collection pipeline using two moving iPhones. Our key idea is to jointly calibrate dual RGB-D sequences to reconstruct both humans and scenes within a unified metric world coordinate frame. The proposed method allows metric-scale and scene-consistent capture in everyday environments without static cameras or markers, bridging human motion and scene geometry seamlessly. Compared with optical capture ground truth, we demonstrate that the dual-view setting exhibits a remarkable ability to mitigate depth ambiguity, achieving superior alignment and reconstruction performance over single iphone or monocular models. Based on the collected data, we empower three embodied AI tasks: monocular human-scene-reconstruction, where we fine-tune on feedforward models that output metric-scale, world-space aligned humans and scenes; physics-based character animation, where we prove our data could be used to scale human-object interaction skills and scene-aware motion tracking; and robot motion control, where we train a humanoid robot via sim-to-real RL to replicate human motions depicted in videos. Experimental results validate the effectiveness of our pipeline and its contributions towards advancing embodied AI research.",
        "url": "http://arxiv.org/abs/2602.23205v1",
        "published_date": "2026-02-26T16:53:41+00:00",
        "updated_date": "2026-02-26T16:53:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenjia Wang",
            "Liang Pan",
            "Huaijin Pi",
            "Yuke Lou",
            "Xuqian Ren",
            "Yifan Wu",
            "Zhouyingcheng Liao",
            "Lei Yang",
            "Rishabh Dabral",
            "Christian Theobalt",
            "Taku Komura"
        ],
        "tldr": "The paper introduces EmbodMocap, a portable dual-iPhone system for capturing in-the-wild human-scene interactions to train embodied agents, demonstrating its effectiveness in tasks like monocular human-scene reconstruction, physics-based animation, and robot motion control.",
        "tldr_zh": "该论文介绍了EmbodMocap，一个便携式双iPhone系统，用于捕捉野外的人与场景交互，以训练具身智能体，并展示了其在单目人与场景重建、基于物理的动画和机器人运动控制等任务中的有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Latent Gaussian Splatting for 4D Panoptic Occupancy Tracking",
        "summary": "Capturing 4D spatiotemporal surroundings is crucial for the safe and reliable operation of robots in dynamic environments. However, most existing methods address only one side of the problem: they either provide coarse geometric tracking via bounding boxes, or detailed 3D structures like voxel-based occupancy that lack explicit temporal association. In this work, we present Latent Gaussian Splatting for 4D Panoptic Occupancy Tracking (LaGS) that advances spatiotemporal scene understanding in a holistic direction. Our approach incorporates camera-based end-to-end tracking with mask-based multi-view panoptic occupancy prediction, and addresses the key challenge of efficiently aggregating multi-view information into 3D voxel grids via a novel latent Gaussian splatting approach. Specifically, we first fuse observations into 3D Gaussians that serve as a sparse point-centric latent representation of the 3D scene, and then splat the aggregated features onto a 3D voxel grid that is decoded by a mask-based segmentation head. We evaluate LaGS on the Occ3D nuScenes and Waymo datasets, achieving state-of-the-art performance for 4D panoptic occupancy tracking. We make our code available at https://lags.cs.uni-freiburg.de/.",
        "url": "http://arxiv.org/abs/2602.23172v1",
        "published_date": "2026-02-26T16:34:49+00:00",
        "updated_date": "2026-02-26T16:34:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Maximilian Luz",
            "Rohit Mohan",
            "Thomas Nürnberg",
            "Yakov Miron",
            "Daniele Cattaneo",
            "Abhinav Valada"
        ],
        "tldr": "The paper introduces Latent Gaussian Splatting (LaGS) for 4D panoptic occupancy tracking, achieving state-of-the-art results on Occ3D nuScenes and Waymo datasets by fusing camera data into 3D Gaussians and splatting features onto a 3D voxel grid.",
        "tldr_zh": "本文提出了用于4D全景占用跟踪的潜在高斯溅射（LaGS）方法，通过将相机数据融合到3D高斯分布中，并将特征溅射到3D体素网格上，在Occ3D nuScenes和Waymo数据集上实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Encoder-Free Fourier-based 3D Large Multimodal Model",
        "summary": "Large Multimodal Models (LMMs) that process 3D data typically rely on heavy, pre-trained visual encoders to extract geometric features. While recent 2D LMMs have begun to eliminate such encoders for efficiency and scalability, extending this paradigm to 3D remains challenging due to the unordered and large-scale nature of point clouds. This leaves a critical unanswered question: How can we design an LMM that tokenizes unordered 3D data effectively and efficiently without a cumbersome encoder? We propose Fase3D, the first efficient encoder-free Fourier-based 3D scene LMM. Fase3D tackles the challenges of scalability and permutation invariance with a novel tokenizer that combines point cloud serialization and the Fast Fourier Transform (FFT) to approximate self-attention. This design enables an effective and computationally minimal architecture, built upon three key innovations: First, we represent large scenes compactly via structured superpoints. Second, our space-filling curve serialization followed by an FFT enables efficient global context modeling and graph-based token merging. Lastly, our Fourier-augmented LoRA adapters inject global frequency-aware interactions into the LLMs at a negligible cost. Fase3D achieves performance comparable to encoder-based 3D LMMs while being significantly more efficient in computation and parameters. Project website: https://tev-fbk.github.io/Fase3D.",
        "url": "http://arxiv.org/abs/2602.23153v1",
        "published_date": "2026-02-26T16:16:02+00:00",
        "updated_date": "2026-02-26T16:16:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Guofeng Mei",
            "Wei Lin",
            "Luigi Riz",
            "Yujiao Wu",
            "Yiming Wang",
            "Fabio Poiesi"
        ],
        "tldr": "Fase3D is a novel, efficient, encoder-free 3D Large Multimodal Model that leverages Fourier transforms for tokenizing point clouds, achieving comparable performance to encoder-based methods with significant computational and parameter efficiency.",
        "tldr_zh": "Fase3D 是一种新型、高效、无编码器的 3D 大型多模态模型，它利用傅里叶变换对点云进行标记化，在计算和参数效率方面表现优于基于编码器的方法，同时性能相当。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Align then Adapt: Rethinking Parameter-Efficient Transfer Learning in 4D Perception",
        "summary": "Point cloud video understanding is critical for robotics as it accurately encodes motion and scene interaction. We recognize that 4D datasets are far scarcer than 3D ones, which hampers the scalability of self-supervised 4D models. A promising alternative is to transfer 3D pre-trained models to 4D perception tasks. However, rigorous empirical analysis reveals two critical limitations that impede transfer capability: overfitting and the modality gap. To overcome these challenges, we develop a novel \"Align then Adapt\" (PointATA) paradigm that decomposes parameter-efficient transfer learning into two sequential stages. Optimal-transport theory is employed to quantify the distributional discrepancy between 3D and 4D datasets, enabling our proposed point align embedder to be trained in Stage 1 to alleviate the underlying modality gap. To mitigate overfitting, an efficient point-video adapter and a spatial-context encoder are integrated into the frozen 3D backbone to enhance temporal modeling capacity in Stage 2. Notably, with the above engineering-oriented designs, PointATA enables a pre-trained 3D model without temporal knowledge to reason about dynamic video content at a smaller parameter cost compared to previous work. Extensive experiments show that PointATA can match or even outperform strong full fine-tuning models, whilst enjoying the advantage of parameter efficiency, e.g. 97.21 \\% accuracy on 3D action recognition, $+8.7 \\%$ on 4 D action segmentation, and 84.06\\% on 4D semantic segmentation.",
        "url": "http://arxiv.org/abs/2602.23069v1",
        "published_date": "2026-02-26T14:58:59+00:00",
        "updated_date": "2026-02-26T14:58:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiding Sun",
            "Jihua Zhu",
            "Haozhe Cheng",
            "Chaoyi Lu",
            "Zhichuan Yang",
            "Lin Chen",
            "Yaonan Wang"
        ],
        "tldr": "The paper introduces PointATA, a parameter-efficient transfer learning method that aligns 3D pre-trained models to 4D point cloud video understanding tasks by addressing the modality gap and overfitting, achieving competitive performance with full fine-tuning.",
        "tldr_zh": "该论文介绍了PointATA，一种参数高效的迁移学习方法，通过解决模态差距和过拟合问题，将3D预训练模型对齐到4D点云视频理解任务，实现了与完全微调模型相当的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "WaterVideoQA: ASV-Centric Perception and Rule-Compliant Reasoning via Multi-Modal Agents",
        "summary": "While autonomous navigation has achieved remarkable success in passive perception (e.g., object detection and segmentation), it remains fundamentally constrained by a void in knowledge-driven, interactive environmental cognition. In the high-stakes domain of maritime navigation, the ability to bridge the gap between raw visual perception and complex cognitive reasoning is not merely an enhancement but a critical prerequisite for Autonomous Surface Vessels to execute safe and precise maneuvers. To this end, we present WaterVideoQA, the first large-scale, comprehensive Video Question Answering benchmark specifically engineered for all-waterway environments. This benchmark encompasses 3,029 video clips across six distinct waterway categories, integrating multifaceted variables such as volatile lighting and dynamic weather to rigorously stress-test ASV capabilities across a five-tier hierarchical cognitive framework. Furthermore, we introduce NaviMind, a pioneering multi-agent neuro-symbolic system designed for open-ended maritime reasoning. By synergizing Adaptive Semantic Routing, Situation-Aware Hierarchical Reasoning, and Autonomous Self-Reflective Verification, NaviMind transitions ASVs from superficial pattern matching to regulation-compliant, interpretable decision-making. Experimental results demonstrate that our framework significantly transcends existing baselines, establishing a new paradigm for intelligent, trustworthy interaction in dynamic maritime environments.",
        "url": "http://arxiv.org/abs/2602.22923v1",
        "published_date": "2026-02-26T12:12:40+00:00",
        "updated_date": "2026-02-26T12:12:40+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Runwei Guan",
            "Shaofeng Liang",
            "Ningwei Ouyang",
            "Weichen Fei",
            "Shanliang Yao",
            "Wei Dai",
            "Chenhao Ge",
            "Penglei Sun",
            "Xiaohui Zhu",
            "Tao Huang",
            "Ryan Wen Liu",
            "Hui Xiong"
        ],
        "tldr": "The paper introduces WaterVideoQA, a new large-scale video question answering benchmark for autonomous surface vessels (ASVs), and NaviMind, a multi-agent neuro-symbolic system for maritime reasoning, demonstrating significant improvements over existing baselines.",
        "tldr_zh": "该论文介绍了 WaterVideoQA，一个用于自主水面船只（ASV）的大规模视频问答基准，以及 NaviMind，一个用于海上推理的多智能体神经符号系统，展示了相对于现有基线的显著改进。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SceneTransporter: Optimal Transport-Guided Compositional Latent Diffusion for Single-Image Structured 3D Scene Generation",
        "summary": "We introduce SceneTransporter, an end-to-end framework for structured 3D scene generation from a single image. While existing methods generate part-level 3D objects, they often fail to organize these parts into distinct instances in open-world scenes. Through a debiased clustering probe, we reveal a critical insight: this failure stems from the lack of structural constraints within the model's internal assignment mechanism. Based on this finding, we reframe the task of structured 3D scene generation as a global correlation assignment problem. To solve this, SceneTransporter formulates and solves an entropic Optimal Transport (OT) objective within the denoising loop of the compositional DiT model. This formulation imposes two powerful structural constraints. First, the resulting transport plan gates cross-attention to enforce an exclusive, one-to-one routing of image patches to part-level 3D latents, preventing entanglement. Second, the competitive nature of the transport encourages the grouping of similar patches, a process that is further regularized by an edge-based cost, to form coherent objects and prevent fragmentation. Extensive experiments show that SceneTransporter outperforms existing methods on open-world scene generation, significantly improving instance-level coherence and geometric fidelity. Code and models will be publicly available at https://2019epwl.github.io/SceneTransporter/.",
        "url": "http://arxiv.org/abs/2602.22785v1",
        "published_date": "2026-02-26T09:19:59+00:00",
        "updated_date": "2026-02-26T09:19:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ling Wang",
            "Hao-Xiang Guo",
            "Xinzhou Wang",
            "Fuchun Sun",
            "Kai Sun",
            "Pengkun Liu",
            "Hang Xiao",
            "Zhong Wang",
            "Guangyuan Fu",
            "Eric Li",
            "Yang Liu",
            "Yikai Wang"
        ],
        "tldr": "SceneTransporter is a new framework for generating structured 3D scenes from a single image by using optimal transport within a latent diffusion model to enforce structural constraints and improve instance coherence.",
        "tldr_zh": "SceneTransporter是一个新的框架，用于从单张图像生成结构化的3D场景。它在潜在扩散模型中使用最优传输来强制结构约束，并提高实例的一致性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ArtPro: Self-Supervised Articulated Object Reconstruction with Adaptive Integration of Mobility Proposals",
        "summary": "Reconstructing articulated objects into high-fidelity digital twins is crucial for applications such as robotic manipulation and interactive simulation. Recent self-supervised methods using differentiable rendering frameworks like 3D Gaussian Splatting remain highly sensitive to the initial part segmentation. Their reliance on heuristic clustering or pre-trained models often causes optimization to converge to local minima, especially for complex multi-part objects. To address these limitations, we propose ArtPro, a novel self-supervised framework that introduces adaptive integration of mobility proposals. Our approach begins with an over-segmentation initialization guided by geometry features and motion priors, generating part proposals with plausible motion hypotheses. During optimization, we dynamically merge these proposals by analyzing motion consistency among spatial neighbors, while a collision-aware motion pruning mechanism prevents erroneous kinematic estimation. Extensive experiments on both synthetic and real-world objects demonstrate that ArtPro achieves robust reconstruction of complex multi-part objects, significantly outperforming existing methods in accuracy and stability.",
        "url": "http://arxiv.org/abs/2602.22666v1",
        "published_date": "2026-02-26T06:35:23+00:00",
        "updated_date": "2026-02-26T06:35:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuelu Li",
            "Zhaonan Wang",
            "Xiaogang Wang",
            "Lei Wu",
            "Manyi Li",
            "Changhe Tu"
        ],
        "tldr": "ArtPro is a novel self-supervised framework for reconstructing articulated objects using adaptive integration of mobility proposals, addressing limitations of prior methods in handling complex multi-part objects.",
        "tldr_zh": "ArtPro是一个新颖的自监督框架，用于重建铰接物体，它通过自适应地整合运动提议来解决现有方法在处理复杂多部件物体时的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DrivePTS: A Progressive Learning Framework with Textual and Structural Enhancement for Driving Scene Generation",
        "summary": "Synthesis of diverse driving scenes serves as a crucial data augmentation technique for validating the robustness and generalizability of autonomous driving systems. Current methods aggregate high-definition (HD) maps and 3D bounding boxes as geometric conditions in diffusion models for conditional scene generation. However, implicit inter-condition dependency causes generation failures when control conditions change independently. Additionally, these methods suffer from insufficient details in both semantic and structural aspects. Specifically, brief and view-invariant captions restrict semantic contexts, resulting in weak background modeling. Meanwhile, the standard denoising loss with uniform spatial weighting neglects foreground structural details, causing visual distortions and blurriness. To address these challenges, we propose DrivePTS, which incorporates three key innovations. Firstly, our framework adopts a progressive learning strategy to mitigate inter-dependency between geometric conditions, reinforced by an explicit mutual information constraint. Secondly, a Vision-Language Model is utilized to generate multi-view hierarchical descriptions across six semantic aspects, providing fine-grained textual guidance. Thirdly, a frequency-guided structure loss is introduced to strengthen the model's sensitivity to high-frequency elements, improving foreground structural fidelity. Extensive experiments demonstrate that our DrivePTS achieves state-of-the-art fidelity and controllability in generating diverse driving scenes. Notably, DrivePTS successfully generates rare scenes where prior methods fail, highlighting its strong generalization ability.",
        "url": "http://arxiv.org/abs/2602.22549v1",
        "published_date": "2026-02-26T02:42:14+00:00",
        "updated_date": "2026-02-26T02:42:14+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhechao Wang",
            "Yiming Zeng",
            "Lufan Ma",
            "Zeqing Fu",
            "Chen Bai",
            "Ziyao Lin",
            "Cheng Lu"
        ],
        "tldr": "DrivePTS introduces a progressive learning framework with enhanced textual and structural guidance to improve the fidelity and controllability of driving scene generation using diffusion models, addressing limitations in existing methods related to inter-condition dependency and insufficient details.",
        "tldr_zh": "DrivePTS 引入了一个渐进式学习框架，通过增强文本和结构引导，利用扩散模型改进驾驶场景生成的保真度和可控性，解决了现有方法在条件依赖性和细节不足方面的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]