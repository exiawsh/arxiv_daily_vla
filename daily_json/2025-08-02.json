[
    {
        "title": "IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation",
        "summary": "Visual navigation with an image as goal is a fundamental and challenging\nproblem. Conventional methods either rely on end-to-end RL learning or\nmodular-based policy with topological graph or BEV map as memory, which cannot\nfully model the geometric relationship between the explored 3D environment and\nthe goal image. In order to efficiently and accurately localize the goal image\nin 3D space, we build our navigation system upon the renderable 3D gaussian\n(3DGS) representation. However, due to the computational intensity of 3DGS\noptimization and the large search space of 6-DoF camera pose, directly\nleveraging 3DGS for image localization during agent exploration process is\nprohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D\nGaussian Localization framework for efficient and 3D-aware image-goal\nnavigation. Specifically, we incrementally update the scene representation as\nnew images arrive with feed-forward monocular prediction. Then we coarsely\nlocalize the goal by leveraging the geometric information for discrete space\nmatching, which can be equivalent to efficient 3D convolution. When the agent\nis close to the goal, we finally solve the fine target pose with optimization\nvia differentiable rendering. The proposed IGL-Nav outperforms existing\nstate-of-the-art methods by a large margin across diverse experimental\nconfigurations. It can also handle the more challenging free-view image-goal\nsetting and be deployed on real-world robotic platform using a cellphone to\ncapture goal image at arbitrary pose. Project page:\nhttps://gwxuan.github.io/IGL-Nav/.",
        "url": "http://arxiv.org/abs/2508.00823v1",
        "published_date": "2025-08-01T17:59:56+00:00",
        "updated_date": "2025-08-01T17:59:56+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Wenxuan Guo",
            "Xiuwei Xu",
            "Hang Yin",
            "Ziwei Wang",
            "Jianjiang Feng",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "tldr": "IGL-Nav is a novel framework for image-goal navigation that uses incremental 3D Gaussian representations for efficient localization, outperforming existing methods and enabling real-world robotic deployment with arbitrary goal image poses.",
        "tldr_zh": "IGL-Nav是一个新的图像目标导航框架，它使用增量3D高斯表示进行高效定位，优于现有方法，并能在现实世界中进行机器人部署，支持任意目标图像姿态。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Rethinking Backbone Design for Lightweight 3D Object Detection in LiDAR",
        "summary": "Recent advancements in LiDAR-based 3D object detection have significantly\naccelerated progress toward the realization of fully autonomous driving in\nreal-world environments. Despite achieving high detection performance, most of\nthe approaches still rely on a VGG-based or ResNet-based backbone for feature\nexploration, which increases the model complexity. Lightweight backbone design\nis well-explored for 2D object detection, but research on 3D object detection\nstill remains limited. In this work, we introduce Dense Backbone, a lightweight\nbackbone that combines the benefits of high processing speed, lightweight\narchitecture, and robust detection accuracy. We adapt multiple SoTA 3d object\ndetectors, such as PillarNet, with our backbone and show that with our\nbackbone, these models retain most of their detection capability at a\nsignificantly reduced computational cost. To our knowledge, this is the first\ndense-layer-based backbone tailored specifically for 3D object detection from\npoint cloud data. DensePillarNet, our adaptation of PillarNet, achieves a 29%\nreduction in model parameters and a 28% reduction in latency with just a 2%\ndrop in detection accuracy on the nuScenes test set. Furthermore, Dense\nBackbone's plug-and-play design allows straightforward integration into\nexisting architectures, requiring no modifications to other network components.",
        "url": "http://arxiv.org/abs/2508.00744v1",
        "published_date": "2025-08-01T16:19:51+00:00",
        "updated_date": "2025-08-01T16:19:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Adwait Chandorkar",
            "Hasan Tercan",
            "Tobias Meisen"
        ],
        "tldr": "This paper introduces Dense Backbone, a lightweight dense-layer-based backbone for LiDAR-based 3D object detection, showing reduced computational cost and comparable accuracy when integrated with existing architectures like PillarNet.",
        "tldr_zh": "本文介绍了Dense Backbone，一种用于基于激光雷达的3D目标检测的轻量级密集层骨干网络，在与PillarNet等现有架构集成时，可降低计算成本并保持相当的精度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "On-Device Diffusion Transformer Policy for Efficient Robot Manipulation",
        "summary": "Diffusion Policies have significantly advanced robotic manipulation tasks via\nimitation learning, but their application on resource-constrained mobile\nplatforms remains challenging due to computational inefficiency and extensive\nmemory footprint. In this paper, we propose LightDP, a novel framework\nspecifically designed to accelerate Diffusion Policies for real-time deployment\non mobile devices. LightDP addresses the computational bottleneck through two\ncore strategies: network compression of the denoising modules and reduction of\nthe required sampling steps. We first conduct an extensive computational\nanalysis on existing Diffusion Policy architectures, identifying the denoising\nnetwork as the primary contributor to latency. To overcome performance\ndegradation typically associated with conventional pruning methods, we\nintroduce a unified pruning and retraining pipeline, optimizing the model's\npost-pruning recoverability explicitly. Furthermore, we combine pruning\ntechniques with consistency distillation to effectively reduce sampling steps\nwhile maintaining action prediction accuracy. Experimental evaluations on the\nstandard datasets, \\ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that\nLightDP achieves real-time action prediction on mobile devices with competitive\nperformance, marking an important step toward practical deployment of\ndiffusion-based policies in resource-limited environments. Extensive real-world\nexperiments also show the proposed LightDP can achieve performance comparable\nto state-of-the-art Diffusion Policies.",
        "url": "http://arxiv.org/abs/2508.00697v1",
        "published_date": "2025-08-01T15:14:39+00:00",
        "updated_date": "2025-08-01T15:14:39+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Yiming Wu",
            "Huan Wang",
            "Zhenghao Chen",
            "Jianxin Pang",
            "Dong Xu"
        ],
        "tldr": "The paper introduces LightDP, a framework to compress and accelerate diffusion policies for real-time robot manipulation on mobile devices by pruning the denoising network and reducing sampling steps, achieving competitive performance on standard datasets and in real-world experiments.",
        "tldr_zh": "该论文介绍了 LightDP，一个用于压缩和加速扩散策略的框架，通过修剪去噪网络和减少采样步骤，实现移动设备上实时机器人操作。在标准数据集和真实世界实验中取得了有竞争力的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving",
        "summary": "Autonomous driving systems must operate reliably in safety-critical\nscenarios, particularly those involving unusual or complex behavior by\nVulnerable Road Users (VRUs). Identifying these edge cases in driving datasets\nis essential for robust evaluation and generalization, but retrieving such rare\nhuman behavior scenarios within the long tail of large-scale datasets is\nchallenging. To support targeted evaluation of autonomous driving systems in\ndiverse, human-centered scenarios, we propose a novel context-aware motion\nretrieval framework. Our method combines Skinned Multi-Person Linear\n(SMPL)-based motion sequences and corresponding video frames before encoding\nthem into a shared multimodal embedding space aligned with natural language.\nOur approach enables the scalable retrieval of human behavior and their context\nthrough text queries. This work also introduces our dataset WayMoCo, an\nextension of the Waymo Open Dataset. It contains automatically labeled motion\nand scene context descriptions derived from generated pseudo-ground-truth SMPL\nsequences and corresponding image data. Our approach outperforms\nstate-of-the-art models by up to 27.5% accuracy in motion-context retrieval,\nwhen evaluated on the WayMoCo dataset.",
        "url": "http://arxiv.org/abs/2508.00589v1",
        "published_date": "2025-08-01T12:41:52+00:00",
        "updated_date": "2025-08-01T12:41:52+00:00",
        "categories": [
            "cs.CV",
            "cs.CL",
            "cs.IR",
            "cs.RO",
            "68T45, 68P20, 68T10, 68T50, 68T07, 68T40",
            "I.2.10; I.4.8; I.2.9; H.3.3"
        ],
        "authors": [
            "Stefan Englmeier",
            "Max A. Büttner",
            "Katharina Winter",
            "Fabian B. Flohr"
        ],
        "tldr": "The paper introduces a context-aware motion retrieval framework for autonomous driving using multimodal embeddings of SMPL sequences, video frames, and natural language, enabling retrieval of rare human behavior scenarios via text queries, demonstrated on their new WayMoCo dataset.",
        "tldr_zh": "该论文介绍了一个用于自动驾驶的上下文感知运动检索框架，它使用 SMPL 序列、视频帧和自然语言的多模态嵌入，可以通过文本查询检索罕见的人类行为场景，并在他们新的 WayMoCo 数据集上进行了演示。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Uncertainty-Aware Likelihood Ratio Estimation for Pixel-Wise Out-of-Distribution Detection",
        "summary": "Semantic segmentation models trained on known object classes often fail in\nreal-world autonomous driving scenarios by confidently misclassifying unknown\nobjects. While pixel-wise out-of-distribution detection can identify unknown\nobjects, existing methods struggle in complex scenes where rare object classes\nare often confused with truly unknown objects. We introduce an\nuncertainty-aware likelihood ratio estimation method that addresses these\nlimitations. Our approach uses an evidential classifier within a likelihood\nratio test to distinguish between known and unknown pixel features from a\nsemantic segmentation model, while explicitly accounting for uncertainty.\nInstead of producing point estimates, our method outputs probability\ndistributions that capture uncertainty from both rare training examples and\nimperfect synthetic outliers. We show that by incorporating uncertainty in this\nway, outlier exposure can be leveraged more effectively. Evaluated on five\nstandard benchmark datasets, our method achieves the lowest average false\npositive rate (2.5%) among state-of-the-art while maintaining high average\nprecision (90.91%) and incurring only negligible computational overhead. Code\nis available at https://github.com/glasbruch/ULRE.",
        "url": "http://arxiv.org/abs/2508.00587v1",
        "published_date": "2025-08-01T12:39:16+00:00",
        "updated_date": "2025-08-01T12:39:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Marc Hölle",
            "Walter Kellermann",
            "Vasileios Belagiannis"
        ],
        "tldr": "This paper introduces an uncertainty-aware likelihood ratio estimation method for pixel-wise out-of-distribution detection in semantic segmentation, showing improved performance in identifying unknown objects in autonomous driving scenarios by explicitly modeling uncertainty.",
        "tldr_zh": "本文介绍了一种用于语义分割中像素级异常检测的、考虑不确定性的似然比估计方法。该方法通过显式地对不确定性进行建模，从而在自动驾驶场景中识别未知物体方面表现出更优的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CoProU-VO: Combining Projected Uncertainty for End-to-End Unsupervised Monocular Visual Odometry",
        "summary": "Visual Odometry (VO) is fundamental to autonomous navigation, robotics, and\naugmented reality, with unsupervised approaches eliminating the need for\nexpensive ground-truth labels. However, these methods struggle when dynamic\nobjects violate the static scene assumption, leading to erroneous pose\nestimations. We tackle this problem by uncertainty modeling, which is a\ncommonly used technique that creates robust masks to filter out dynamic objects\nand occlusions without requiring explicit motion segmentation. Traditional\nuncertainty modeling considers only single-frame information, overlooking the\nuncertainties across consecutive frames. Our key insight is that uncertainty\nmust be propagated and combined across temporal frames to effectively identify\nunreliable regions, particularly in dynamic scenes. To address this challenge,\nwe introduce Combined Projected Uncertainty VO (CoProU-VO), a novel end-to-end\napproach that combines target frame uncertainty with projected reference frame\nuncertainty using a principled probabilistic formulation. Built upon vision\ntransformer backbones, our model simultaneously learns depth, uncertainty\nestimation, and camera poses. Consequently, experiments on the KITTI and\nnuScenes datasets demonstrate significant improvements over previous\nunsupervised monocular end-to-end two-frame-based methods and exhibit strong\nperformance in challenging highway scenes where other approaches often fail.\nAdditionally, comprehensive ablation studies validate the effectiveness of\ncross-frame uncertainty propagation.",
        "url": "http://arxiv.org/abs/2508.00568v1",
        "published_date": "2025-08-01T12:09:42+00:00",
        "updated_date": "2025-08-01T12:09:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingchao Xie",
            "Oussema Dhaouadi",
            "Weirong Chen",
            "Johannes Meier",
            "Jacques Kaiser",
            "Daniel Cremers"
        ],
        "tldr": "The paper introduces CoProU-VO, a novel unsupervised monocular visual odometry approach that combines target and projected reference frame uncertainties to improve robustness in dynamic scenes, demonstrating significant improvements on KITTI and nuScenes datasets.",
        "tldr_zh": "该论文介绍了一种名为 CoProU-VO 的新型无监督单目视觉里程计方法，该方法结合了目标帧和投影参考帧的不确定性，以提高在动态场景中的鲁棒性，并在 KITTI 和 nuScenes 数据集上展示了显著的改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Bidirectional Action Sequence Learning for Long-term Action Anticipation with Large Language Models",
        "summary": "Video-based long-term action anticipation is crucial for early risk detection\nin areas such as automated driving and robotics. Conventional approaches\nextract features from past actions using encoders and predict future events\nwith decoders, which limits performance due to their unidirectional nature.\nThese methods struggle to capture semantically distinct sub-actions within a\nscene. The proposed method, BiAnt, addresses this limitation by combining\nforward prediction with backward prediction using a large language model.\nExperimental results on Ego4D demonstrate that BiAnt improves performance in\nterms of edit distance compared to baseline methods.",
        "url": "http://arxiv.org/abs/2508.00374v1",
        "published_date": "2025-08-01T07:07:24+00:00",
        "updated_date": "2025-08-01T07:07:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuji Sato",
            "Yasunori Ishii",
            "Takayoshi Yamashita"
        ],
        "tldr": "The paper introduces BiAnt, a novel method using bidirectional action sequence learning with a large language model for long-term action anticipation in videos, showing improved performance on Ego4D dataset.",
        "tldr_zh": "该论文介绍了一种名为BiAnt的新方法，该方法使用带有大型语言模型的双向动作序列学习来进行视频中的长期动作预测，并在Ego4D数据集上表现出性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective",
        "summary": "Collaborative perception shares information among different agents and helps\nsolving problems that individual agents may face, e.g., occlusions and small\nsensing range. Prior methods usually separate the multi-agent fusion and\nmulti-time fusion into two consecutive steps. In contrast, this paper proposes\nan efficient collaborative perception that aggregates the observations from\ndifferent agents (space) and different times into a unified spatio-temporal\nspace simultanesouly. The unified spatio-temporal space brings two benefits,\ni.e., efficient feature transmission and superior feature fusion. 1) Efficient\nfeature transmission: each static object yields a single observation in the\nspatial temporal space, and thus only requires transmission only once (whereas\nprior methods re-transmit all the object features multiple times). 2) superior\nfeature fusion: merging the multi-agent and multi-time fusion into a unified\nspatial-temporal aggregation enables a more holistic perspective, thereby\nenhancing perception performance in challenging scenarios. Consequently, our\nCollaborative perception with Spatio-temporal Transformer (CoST) gains\nimprovement in both efficiency and accuracy. Notably, CoST is not tied to any\nspecific method and is compatible with a majority of previous methods,\nenhancing their accuracy while reducing the transmission bandwidth.",
        "url": "http://arxiv.org/abs/2508.00359v1",
        "published_date": "2025-08-01T06:45:12+00:00",
        "updated_date": "2025-08-01T06:45:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zongheng Tang",
            "Yi Liu",
            "Yifan Sun",
            "Yulu Gao",
            "Jinyu Chen",
            "Runsheng Xu",
            "Si Liu"
        ],
        "tldr": "This paper introduces CoST, a collaborative perception method that unifies multi-agent and multi-time fusion in a spatio-temporal space, leading to efficient feature transmission and superior fusion for enhanced perception accuracy.",
        "tldr_zh": "该论文介绍了CoST，一种协同感知方法，它在时空空间中统一了多智能体和多时间的融合，从而实现了高效的特征传输和卓越的融合，从而提高了感知精度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Stable at Any Speed: Speed-Driven Multi-Object Tracking with Learnable Kalman Filtering",
        "summary": "Multi-object tracking (MOT) enables autonomous vehicles to continuously\nperceive dynamic objects, supplying essential temporal cues for prediction,\nbehavior understanding, and safe planning. However, conventional\ntracking-by-detection methods typically rely on static coordinate\ntransformations based on ego-vehicle poses, disregarding ego-vehicle\nspeed-induced variations in observation noise and reference frame changes,\nwhich degrades tracking stability and accuracy in dynamic, high-speed\nscenarios. In this paper, we investigate the critical role of ego-vehicle speed\nin MOT and propose a Speed-Guided Learnable Kalman Filter (SG-LKF) that\ndynamically adapts uncertainty modeling to ego-vehicle speed, significantly\nimproving stability and accuracy in highly dynamic scenarios. Central to SG-LKF\nis MotionScaleNet (MSNet), a decoupled token-mixing and channel-mixing MLP that\nadaptively predicts key parameters of SG-LKF. To enhance inter-frame\nassociation and trajectory continuity, we introduce a self-supervised\ntrajectory consistency loss jointly optimized with semantic and positional\nconstraints. Extensive experiments show that SG-LKF ranks first among all\nvision-based methods on KITTI 2D MOT with 79.59% HOTA, delivers strong results\non KITTI 3D MOT with 82.03% HOTA, and outperforms SimpleTrack by 2.2% AMOTA on\nnuScenes 3D MOT.",
        "url": "http://arxiv.org/abs/2508.00358v1",
        "published_date": "2025-08-01T06:42:33+00:00",
        "updated_date": "2025-08-01T06:42:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yan Gong",
            "Mengjun Chen",
            "Hao Liu",
            "Gao Yongsheng",
            "Lei Yang",
            "Naibang Wang",
            "Ziying Song",
            "Haoqun Ma"
        ],
        "tldr": "This paper introduces Speed-Guided Learnable Kalman Filter (SG-LKF) that dynamically adapts uncertainty modeling to ego-vehicle speed, significantly improving multi-object tracking stability and accuracy in autonomous driving scenarios.",
        "tldr_zh": "本文介绍了一种速度引导的可学习卡尔曼滤波器（SG-LKF），它可以动态地调整不确定性建模以适应自车速度，从而显著提高自动驾驶场景中多目标跟踪的稳定性和准确性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence",
        "summary": "Pedestrian detection models in autonomous driving systems often lack\nrobustness due to insufficient representation of dangerous pedestrian scenarios\nin training datasets. To address this limitation, we present a novel framework\nfor controllable pedestrian video editing in multi-view driving scenarios by\nintegrating video inpainting and human motion control techniques. Our approach\nbegins by identifying pedestrian regions of interest across multiple camera\nviews, expanding detection bounding boxes with a fixed ratio, and resizing and\nstitching these regions into a unified canvas while preserving cross-view\nspatial relationships. A binary mask is then applied to designate the editable\narea, within which pedestrian editing is guided by pose sequence control\nconditions. This enables flexible editing functionalities, including pedestrian\ninsertion, replacement, and removal. Extensive experiments demonstrate that our\nframework achieves high-quality pedestrian editing with strong visual realism,\nspatiotemporal coherence, and cross-view consistency. These results establish\nthe proposed method as a robust and versatile solution for multi-view\npedestrian video generation, with broad potential for applications in data\naugmentation and scenario simulation in autonomous driving.",
        "url": "http://arxiv.org/abs/2508.00299v1",
        "published_date": "2025-08-01T03:56:57+00:00",
        "updated_date": "2025-08-01T03:56:57+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Danzhen Fu",
            "Jiagao Hu",
            "Daiguo Zhou",
            "Fei Wang",
            "Zepeng Wang",
            "Wenhua Liao"
        ],
        "tldr": "The paper introduces a framework for controllable pedestrian video editing in multi-view driving scenarios using video inpainting and human motion control, enabling data augmentation and scenario simulation for autonomous driving.",
        "tldr_zh": "该论文介绍了一个可控的行人视频编辑框架，用于多视角驾驶场景，利用视频修复和人体运动控制技术，从而为自动驾驶实现数据增强和场景模拟。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents",
        "summary": "Aerial navigation is a fundamental yet underexplored capability in embodied\nintelligence, enabling agents to operate in large-scale, unstructured\nenvironments where traditional navigation paradigms fall short. However, most\nexisting research follows the Vision-and-Language Navigation (VLN) paradigm,\nwhich heavily depends on sequential linguistic instructions, limiting its\nscalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark\nfor large-scale Object Goal Navigation (ObjectNav) by aerial agents in\nopen-world environments, where agents operate based on high-level semantic\ngoals without relying on detailed instructional guidance as in VLN. UAV-ON\ncomprises 14 high-fidelity Unreal Engine environments with diverse semantic\nregions and complex spatial layouts, covering urban, natural, and mixed-use\nsettings. It defines 1270 annotated target objects, each characterized by an\ninstance-level instruction that encodes category, physical footprint, and\nvisual descriptors, allowing grounded reasoning. These instructions serve as\nsemantic goals, introducing realistic ambiguity and complex reasoning\nchallenges for aerial agents. To evaluate the benchmark, we implement several\nbaseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that\nintegrates instruction semantics with egocentric observations for long-horizon,\ngoal-directed exploration. Empirical results show that all baselines struggle\nin this setting, highlighting the compounded challenges of aerial navigation\nand semantic goal grounding. UAV-ON aims to advance research on scalable UAV\nautonomy driven by semantic goal descriptions in complex real-world\nenvironments.",
        "url": "http://arxiv.org/abs/2508.00288v1",
        "published_date": "2025-08-01T03:23:06+00:00",
        "updated_date": "2025-08-01T03:23:06+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Jianqiang Xiao",
            "Yuexuan Sun",
            "Yixin Shao",
            "Boxi Gan",
            "Rongqiang Liu",
            "Yanjing Wu",
            "Weili Gua",
            "Xiang Deng"
        ],
        "tldr": "The paper introduces UAV-ON, a new benchmark for aerial Object Goal Navigation in open-world environments, addressing the limitations of VLN by focusing on semantic goals for UAV autonomy.",
        "tldr_zh": "该论文介绍了UAV-ON，一个新的开放世界环境中的无人机目标导航基准，通过关注语义目标来实现无人机自主性，解决了视觉语言导航（VLN）的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting",
        "summary": "We introduce PointGauss, a novel point cloud-guided framework for real-time\nmulti-object segmentation in Gaussian Splatting representations. Unlike\nexisting methods that suffer from prolonged initialization and limited\nmulti-view consistency, our approach achieves efficient 3D segmentation by\ndirectly parsing Gaussian primitives through a point cloud segmentation-driven\npipeline. The key innovation lies in two aspects: (1) a point cloud-based\nGaussian primitive decoder that generates 3D instance masks within 1 minute,\nand (2) a GPU-accelerated 2D mask rendering system that ensures multi-view\nconsistency. Extensive experiments demonstrate significant improvements over\nprevious state-of-the-art methods, achieving performance gains of 1.89 to\n31.78% in multi-view mIoU, while maintaining superior computational efficiency.\nTo address the limitations of current benchmarks (single-object focus,\ninconsistent 3D evaluation, small scale, and partial coverage), we present\nDesktopObjects-360, a novel comprehensive dataset for 3D segmentation in\nradiance fields, featuring: (1) complex multi-object scenes, (2) globally\nconsistent 2D annotations, (3) large-scale training data (over 27 thousand 2D\nmasks), (4) full 360{\\deg} coverage, and (5) 3D evaluation masks.",
        "url": "http://arxiv.org/abs/2508.00259v1",
        "published_date": "2025-08-01T01:56:54+00:00",
        "updated_date": "2025-08-01T01:56:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wentao Sun",
            "Hanqing Xu",
            "Quanyun Wu",
            "Dedong Zhang",
            "Yiping Chen",
            "Lingfei Ma",
            "John S. Zelek",
            "Jonathan Li"
        ],
        "tldr": "PointGauss introduces a point cloud-guided framework for real-time multi-object segmentation in Gaussian Splatting, achieving significant performance improvements and introducing a novel dataset, DesktopObjects-360, to address limitations in current benchmarks.",
        "tldr_zh": "PointGauss 提出了一种基于点云引导的 Gaussian Splatting 实时多目标分割框架，显著提升了性能，并引入了一个名为 DesktopObjects-360 的新数据集，以解决当前基准测试的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs",
        "summary": "LiDAR-based 3D sensors provide point clouds, a canonical 3D representation\nused in various scene understanding tasks. Modern LiDARs face key challenges in\nseveral real-world scenarios, such as long-distance or low-albedo objects,\nproducing sparse or erroneous point clouds. These errors, which are rooted in\nthe noisy raw LiDAR measurements, get propagated to downstream perception\nmodels, resulting in potentially severe loss of accuracy. This is because\nconventional 3D processing pipelines do not retain any uncertainty information\nfrom the raw measurements when constructing point clouds.\n  We propose Probabilistic Point Clouds (PPC), a novel 3D scene representation\nwhere each point is augmented with a probability attribute that encapsulates\nthe measurement uncertainty (or confidence) in the raw data. We further\nintroduce inference approaches that leverage PPC for robust 3D object\ndetection; these methods are versatile and can be used as computationally\nlightweight drop-in modules in 3D inference pipelines. We demonstrate, via both\nsimulations and real captures, that PPC-based 3D inference methods outperform\nseveral baselines using LiDAR as well as camera-LiDAR fusion models, across\nchallenging indoor and outdoor scenarios involving small, distant, and\nlow-albedo objects, as well as strong ambient light.\n  Our project webpage is at https://bhavyagoyal.github.io/ppc .",
        "url": "http://arxiv.org/abs/2508.00169v1",
        "published_date": "2025-07-31T21:32:21+00:00",
        "updated_date": "2025-07-31T21:32:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bhavya Goyal",
            "Felipe Gutierrez-Barragan",
            "Wei Lin",
            "Andreas Velten",
            "Yin Li",
            "Mohit Gupta"
        ],
        "tldr": "This paper introduces Probabilistic Point Clouds (PPC), a novel 3D scene representation for LiDAR data that incorporates measurement uncertainty, improving robustness in 3D object detection, particularly in challenging conditions.",
        "tldr_zh": "本文介绍了一种新颖的3D场景表示方法，即概率点云（PPC），它将测量不确定性纳入LiDAR数据中，从而提高了3D目标检测的鲁棒性，尤其是在具有挑战性的条件下。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Cross-Dataset Semantic Segmentation Performance Analysis: Unifying NIST Point Cloud City Datasets for 3D Deep Learning",
        "summary": "This study analyzes semantic segmentation performance across heterogeneously\nlabeled point-cloud datasets relevant to public safety applications, including\npre-incident planning systems derived from lidar scans. Using NIST's Point\nCloud City dataset (Enfield and Memphis collections), we investigate challenges\nin unifying differently labeled 3D data. Our methodology employs a graded\nschema with the KPConv architecture, evaluating performance through IoU metrics\non safety-relevant features. Results indicate performance variability:\ngeometrically large objects (e.g. stairs, windows) achieve higher segmentation\nperformance, suggesting potential for navigational context, while smaller\nsafety-critical features exhibit lower recognition rates. Performance is\nimpacted by class imbalance and the limited geometric distinction of smaller\nobjects in typical lidar scans, indicating limitations in detecting certain\nsafety-relevant features using current point-cloud methods. Key identified\nchallenges include insufficient labeled data, difficulties in unifying class\nlabels across datasets, and the need for standardization. Potential directions\ninclude automated labeling and multi-dataset learning strategies. We conclude\nthat reliable point-cloud semantic segmentation for public safety necessitates\nstandardized annotation protocols and improved labeling techniques to address\ndata heterogeneity and the detection of small, safety-critical elements.",
        "url": "http://arxiv.org/abs/2508.00822v1",
        "published_date": "2025-08-01T17:59:02+00:00",
        "updated_date": "2025-08-01T17:59:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Alexander Nikitas Dimopoulos",
            "Joseph Grasso"
        ],
        "tldr": "This paper analyzes the performance of semantic segmentation on public safety-related point cloud datasets, highlighting challenges related to data heterogeneity and the detection of small, safety-critical objects. They suggest standardized annotation protocols and improved labeling techniques are needed.",
        "tldr_zh": "本文分析了语义分割在公共安全相关的点云数据集上的性能，强调了与数据异构性和小尺寸、安全关键物体检测相关的挑战。他们建议采用标准化的标注协议和改进的标注技术。",
        "relevance_score": 8,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging",
        "summary": "3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view\nimages. Such \"digital twins\" are useful for simulations, virtual reality,\nmarketing, robot policy fine-tuning, and part inspection. 3D object scanning\nusually requires multi-camera arrays, precise laser scanners, or robot\nwrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan,\na pipeline for producing high-quality 3D Gaussian Splat models using a\nbi-manual robot that grasps an object with one gripper and rotates the object\nwith respect to a stationary camera. The object is then re-grasped by a second\ngripper to expose surfaces that were occluded by the first gripper. We present\nthe Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as\nRAFT optical flow models to identify and isolate objects held by a robot\ngripper while removing the gripper and the background. We then modify the 3DGS\ntraining pipeline to support concatenated datasets with gripper occlusion,\nproducing an omni-directional (360 degree view) model of the object. We apply\nOmni-Scan to part defect inspection, finding that it can identify visual or\ngeometric defects in 12 different industrial and household objects with an\naverage accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be\nfound at https://berkeleyautomation.github.io/omni-scan/",
        "url": "http://arxiv.org/abs/2508.00354v1",
        "published_date": "2025-08-01T06:36:19+00:00",
        "updated_date": "2025-08-01T06:36:19+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Tianshuang Qiu",
            "Zehan Ma",
            "Karim El-Refai",
            "Hiya Shah",
            "Chung Min Kim",
            "Justin Kerr",
            "Ken Goldberg"
        ],
        "tldr": "This paper presents Omni-Scan, a bimanual robot system for creating high-quality 3D Gaussian Splat models of objects by re-grasping them to expose all surfaces and enhance defect inspection accuracy.",
        "tldr_zh": "本文介绍了Omni-Scan，一种双臂机器人系统，通过重新抓取物体以暴露所有表面，从而创建高质量的物体3D高斯溅射模型，并提高缺陷检测精度。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]