[
    {
        "title": "Uncertainty-aware Probabilistic 3D Human Motion Forecasting via Invertible Networks",
        "summary": "3D human motion forecasting aims to enable autonomous applications.\nEstimating uncertainty for each prediction (i.e., confidence based on\nprobability density or quantile) is essential for safety-critical contexts like\nhuman-robot collaboration to minimize risks. However, existing diverse motion\nforecasting approaches struggle with uncertainty quantification due to implicit\nprobabilistic representations hindering uncertainty modeling. We propose\nProbHMI, which introduces invertible networks to parameterize poses in a\ndisentangled latent space, enabling probabilistic dynamics modeling. A\nforecasting module then explicitly predicts future latent distributions,\nallowing effective uncertainty quantification. Evaluated on benchmarks, ProbHMI\nachieves strong performance for both deterministic and diverse prediction while\nvalidating uncertainty calibration, critical for risk-aware decision making.",
        "url": "http://arxiv.org/abs/2507.14694v1",
        "published_date": "2025-07-19T17:02:07+00:00",
        "updated_date": "2025-07-19T17:02:07+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Yue Ma",
            "Kanglei Zhou",
            "Fuyang Yu",
            "Frederick W. B. Li",
            "Xiaohui Liang"
        ],
        "tldr": "The paper introduces ProbHMI, a novel approach using invertible networks for uncertainty-aware 3D human motion forecasting, improving both accuracy and uncertainty quantification, crucial for safety-critical applications.",
        "tldr_zh": "该论文介绍了 ProbHMI，一种使用可逆网络进行不确定性感知 3D 人体运动预测的新方法，提高了准确性和不确定性量化，这对于安全至关重要的应用至关重要。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF",
        "summary": "3D semantic segmentation provides high-level scene understanding for\napplications in robotics, autonomous systems, \\textit{etc}. Traditional methods\nadapt exclusively to either task-specific goals (open-vocabulary segmentation)\nor scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the\nfirst method addressing the broader problem of 3D Open-Vocabulary Sub-concepts\nDiscovery, which aims to provide a 3D semantic segmentation that adapts to both\nthe scene and user queries. We build DiSCO-3D on Neural Fields representations,\ncombining unsupervised segmentation with weak open-vocabulary guidance. Our\nevaluations demonstrate that DiSCO-3D achieves effective performance in\nOpen-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in\nthe edge cases of both open-vocabulary and unsupervised segmentation.",
        "url": "http://arxiv.org/abs/2507.14596v1",
        "published_date": "2025-07-19T12:46:20+00:00",
        "updated_date": "2025-07-19T12:46:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Doriand Petit",
            "Steve Bourgeois",
            "Vincent Gay-Bellile",
            "Florian Chabot",
            "Loïc Barthe"
        ],
        "tldr": "DiSCO-3D is a new method for 3D open-vocabulary sub-concept discovery in NeRFs, combining unsupervised segmentation with weak open-vocabulary guidance for effective scene understanding based on user queries.",
        "tldr_zh": "DiSCO-3D是一种新的NeRF中3D开放词汇子概念发现方法，它结合了无监督分割和弱开放词汇引导，从而可以根据用户查询有效地理解场景。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions",
        "summary": "Understanding 3D scenes goes beyond simply recognizing objects; it requires\nreasoning about the spatial and semantic relationships between them. Current 3D\nscene-language models often struggle with this relational understanding,\nparticularly when visual embeddings alone do not adequately convey the roles\nand interactions of objects. In this paper, we introduce Descrip3D, a novel and\npowerful framework that explicitly encodes the relationships between objects\nusing natural language. Unlike previous methods that rely only on 2D and 3D\nembeddings, Descrip3D enhances each object with a textual description that\ncaptures both its intrinsic attributes and contextual relationships. These\nrelational cues are incorporated into the model through a dual-level\nintegration: embedding fusion and prompt-level injection. This allows for\nunified reasoning across various tasks such as grounding, captioning, and\nquestion answering, all without the need for task-specific heads or additional\nsupervision. When evaluated on five benchmark datasets, including ScanRefer,\nMulti3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms\nstrong baseline models, demonstrating the effectiveness of language-guided\nrelational representation for understanding complex indoor scenes.",
        "url": "http://arxiv.org/abs/2507.14555v1",
        "published_date": "2025-07-19T09:19:16+00:00",
        "updated_date": "2025-07-19T09:19:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jintang Xue",
            "Ganning Zhao",
            "Jie-En Yao",
            "Hong-En Chen",
            "Yue Hu",
            "Meida Chen",
            "Suya You",
            "C. -C. Jay Kuo"
        ],
        "tldr": "Descrip3D enhances 3D scene understanding by incorporating object-level textual descriptions of relationships, improving performance on tasks like grounding, captioning, and question answering. It outperforms baselines on five benchmark datasets by fusing embeddings and injecting prompts with relational cues.",
        "tldr_zh": "Descrip3D通过整合对象级别的关系文本描述来增强3D场景理解，提高了定位、图像描述和问答等任务的性能。通过融合嵌入和注入带有关系线索的提示，它在五个基准数据集上优于基线模型。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow",
        "summary": "This paper introduces a robust framework for motion segmentation and\negomotion estimation using event-based normal flow, tailored specifically for\nneuromorphic vision sensors. In contrast to traditional methods that rely\nheavily on optical flow or explicit depth estimation, our approach exploits the\nsparse, high-temporal-resolution event data and incorporates geometric\nconstraints between normal flow, scene structure, and inertial measurements.\nThe proposed optimization-based pipeline iteratively performs event\nover-segmentation, isolates independently moving objects via residual analysis,\nand refines segmentations using hierarchical clustering informed by motion\nsimilarity and temporal consistency. Experimental results on the EVIMO2v2\ndataset validate that our method achieves accurate segmentation and\ntranslational motion estimation without requiring full optical flow\ncomputation. This approach demonstrates significant advantages at object\nboundaries and offers considerable potential for scalable, real-time robotic\nand navigation applications.",
        "url": "http://arxiv.org/abs/2507.14500v1",
        "published_date": "2025-07-19T06:11:09+00:00",
        "updated_date": "2025-07-19T06:11:09+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Zhiyuan Hua",
            "Dehao Yuan",
            "Cornelia Fermüller"
        ],
        "tldr": "This paper presents a novel motion segmentation and egomotion estimation framework using event-based normal flow, leveraging geometric constraints and inertial measurements for robust performance in neuromorphic vision sensors, particularly for robotic and navigation applications.",
        "tldr_zh": "本文提出了一种新颖的运动分割和自运动估计框架，该框架使用基于事件的法向流，利用几何约束和惯性测量，在神经形态视觉传感器中实现鲁棒的性能，特别适用于机器人和导航应用。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving",
        "summary": "End-to-end autonomous driving requires adaptive and robust handling of\ncomplex and diverse traffic environments. However, prevalent single-mode\nplanning methods attempt to learn an overall policy while struggling to acquire\ndiversified driving skills to handle diverse scenarios. Therefore, this paper\nproposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework\nfeaturing a Global Expert, a Scene-Adaptive Experts Group, and equipped with a\nDual-aware Router. Specifically, the Global Expert is trained on the overall\ndataset, possessing robust performance. The Scene-Adaptive Experts are trained\non corresponding scene subsets, achieving adaptive performance. The Dual-aware\nRouter simultaneously considers scenario-level features and routing uncertainty\nto dynamically activate expert modules. Through the effective coupling of the\nGlobal Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,\nGEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS\noutperforms existing methods in the Bench2Drive closed-loop benchmark and\nachieves state-of-the-art performance in Driving Score and Success Rate, even\nwith only monocular vision input. Furthermore, ablation studies demonstrate\nsignificant improvements over the original single-expert baseline: 7.67% in\nDriving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The\ncode will be available at https://github.com/newbrains1/GEMINUS.",
        "url": "http://arxiv.org/abs/2507.14456v3",
        "published_date": "2025-07-19T03:04:28+00:00",
        "updated_date": "2025-07-23T08:26:59+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Chi Wan",
            "Yixin Cui",
            "Jiatong Du",
            "Shuo Yang",
            "Yulong Bai",
            "Yanjun Huang"
        ],
        "tldr": "The paper introduces GEMINUS, a Mixture-of-Experts framework for end-to-end autonomous driving that uses a global expert and scene-adaptive experts, guided by a dual-aware router, achieving state-of-the-art performance on Bench2Drive.",
        "tldr_zh": "该论文介绍了GEMINUS，一个用于端到端自动驾驶的混合专家框架，它使用全局专家和场景自适应专家，由双重感知路由器引导，在Bench2Drive上实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Semantic Segmentation based Scene Understanding in Autonomous Vehicles",
        "summary": "In recent years, the concept of artificial intelligence (AI) has become a\nprominent keyword because it is promising in solving complex tasks. The need\nfor human expertise in specific areas may no longer be needed because machines\nhave achieved successful results using artificial intelligence and can make the\nright decisions in critical situations. This process is possible with the help\nof deep learning (DL), one of the most popular artificial intelligence\ntechnologies. One of the areas in which the use of DL is used is in the\ndevelopment of self-driving cars, which is very effective and important. In\nthis work, we propose several efficient models to investigate scene\nunderstanding through semantic segmentation. We use the BDD100k dataset to\ninvestigate these models. Another contribution of this work is the usage of\nseveral Backbones as encoders for models. The obtained results show that\nchoosing the appropriate backbone has a great effect on the performance of the\nmodel for semantic segmentation. Better performance in semantic segmentation\nallows us to understand better the scene and the environment around the agent.\nIn the end, we analyze and evaluate the proposed models in terms of accuracy,\nmean IoU, and loss function, and the results show that these metrics are\nimproved.",
        "url": "http://arxiv.org/abs/2507.14303v1",
        "published_date": "2025-07-18T18:21:47+00:00",
        "updated_date": "2025-07-18T18:21:47+00:00",
        "categories": [
            "cs.CV",
            "I.4.8"
        ],
        "authors": [
            "Ehsan Rassekh"
        ],
        "tldr": "This paper explores semantic segmentation models for scene understanding in autonomous vehicles using the BDD100k dataset and different backbones, demonstrating the impact of backbone selection on performance.",
        "tldr_zh": "本文探讨了用于自动驾驶车辆场景理解的语义分割模型，使用了 BDD100k 数据集和不同的骨干网络，展示了骨干网络选择对性能的影响。",
        "relevance_score": 8,
        "novelty_claim_score": 5,
        "clarity_score": 7,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]