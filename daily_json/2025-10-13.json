[
    {
        "title": "Stability Under Scrutiny: Benchmarking Representation Paradigms for Online HD Mapping",
        "summary": "As one of the fundamental modules in autonomous driving, online\nhigh-definition (HD) maps have attracted significant attention due to their\ncost-effectiveness and real-time capabilities. Since vehicles always cruise in\nhighly dynamic environments, spatial displacement of onboard sensors inevitably\ncauses shifts in real-time HD mapping results, and such instability poses\nfundamental challenges for downstream tasks. However, existing online map\nconstruction models tend to prioritize improving each frame's mapping accuracy,\nwhile the mapping stability has not yet been systematically studied. To fill\nthis gap, this paper presents the first comprehensive benchmark for evaluating\nthe temporal stability of online HD mapping models. We propose a\nmulti-dimensional stability evaluation framework with novel metrics for\nPresence, Localization, and Shape Stability, integrated into a unified mean\nAverage Stability (mAS) score. Extensive experiments on 42 models and variants\nshow that accuracy (mAP) and stability (mAS) represent largely independent\nperformance dimensions. We further analyze the impact of key model design\nchoices on both criteria, identifying architectural and training factors that\ncontribute to high accuracy, high stability, or both. To encourage broader\nfocus on stability, we will release a public benchmark. Our work highlights the\nimportance of treating temporal stability as a core evaluation criterion\nalongside accuracy, advancing the development of more reliable autonomous\ndriving systems. The benchmark toolkit, code, and models will be available at\nhttps://stablehdmap.github.io/.",
        "url": "http://arxiv.org/abs/2510.10660v1",
        "published_date": "2025-10-12T15:33:45+00:00",
        "updated_date": "2025-10-12T15:33:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Shan",
            "Ruikai Li",
            "Han Jiang",
            "Yizhe Fan",
            "Ziyang Yan",
            "Bohan Li",
            "Xiaoshuai Hao",
            "Hao Zhao",
            "Zhiyong Cui",
            "Yilong Ren",
            "Haiyang Yu"
        ],
        "tldr": "This paper introduces a benchmark for evaluating the temporal stability of online HD mapping models, proposing new metrics and highlighting the importance of stability alongside accuracy for autonomous driving systems.",
        "tldr_zh": "本文提出了一个用于评估在线高清地图模型时间稳定性的基准，提出了新的指标，并强调了稳定性和准确性对于自动驾驶系统的重要性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "A Machine Learning Perspective on Automated Driving Corner Cases",
        "summary": "For high-stakes applications, like autonomous driving, a safe operation is\nnecessary to prevent harm, accidents, and failures. Traditionally, difficult\nscenarios have been categorized into corner cases and addressed individually.\nHowever, this example-based categorization is not scalable and lacks a data\ncoverage perspective, neglecting the generalization to training data of machine\nlearning models. In our work, we propose a novel machine learning approach that\ntakes the underlying data distribution into account. Based on our novel\nperspective, we present a framework for effective corner case recognition for\nperception on individual samples. In our evaluation, we show that our approach\n(i) unifies existing scenario-based corner case taxonomies under a\ndistributional perspective, (ii) achieves strong performance on corner case\ndetection tasks across standard benchmarks for which we extend established\nout-of-distribution detection benchmarks, and (iii) enables analysis of\ncombined corner cases via a newly introduced fog-augmented Lost & Found\ndataset. These results provide a principled basis for corner case recognition,\nunderlining our manual specification-free definition.",
        "url": "http://arxiv.org/abs/2510.10653v1",
        "published_date": "2025-10-12T15:18:12+00:00",
        "updated_date": "2025-10-12T15:18:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sebastian Schmidt",
            "Julius Körner",
            "Stephan Günnemann"
        ],
        "tldr": "This paper presents a machine learning framework for corner case recognition in autonomous driving by considering the underlying data distribution, unifying existing taxonomies and achieving strong detection performance.",
        "tldr_zh": "本文提出了一种基于机器学习的自动驾驶 corner case 识别框架，通过考虑底层数据分布，统一了现有分类，并实现了强大的检测性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "WorldMirror: Universal 3D World Reconstruction with Any-Prior Prompting",
        "summary": "We present WorldMirror, an all-in-one, feed-forward model for versatile 3D\ngeometric prediction tasks. Unlike existing methods constrained to image-only\ninputs or customized for a specific task, our framework flexibly integrates\ndiverse geometric priors, including camera poses, intrinsics, and depth maps,\nwhile simultaneously generating multiple 3D representations: dense point\nclouds, multi-view depth maps, camera parameters, surface normals, and 3D\nGaussians. This elegant and unified architecture leverages available prior\ninformation to resolve structural ambiguities and delivers geometrically\nconsistent 3D outputs in a single forward pass. WorldMirror achieves\nstate-of-the-art performance across diverse benchmarks from camera, point map,\ndepth, and surface normal estimation to novel view synthesis, while maintaining\nthe efficiency of feed-forward inference. Code and models will be publicly\navailable soon.",
        "url": "http://arxiv.org/abs/2510.10726v1",
        "published_date": "2025-10-12T17:59:09+00:00",
        "updated_date": "2025-10-12T17:59:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yifan Liu",
            "Zhiyuan Min",
            "Zhenwei Wang",
            "Junta Wu",
            "Tengfei Wang",
            "Yixuan Yuan",
            "Yawei Luo",
            "Chunchao Guo"
        ],
        "tldr": "WorldMirror is a feed-forward model for versatile 3D geometric prediction that integrates diverse geometric priors and generates multiple 3D representations in a unified architecture, achieving state-of-the-art performance.",
        "tldr_zh": "WorldMirror是一个通用的前馈模型，用于多功能的3D几何预测，它集成了各种几何先验，并在统一的架构中生成多个3D表示，从而实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SpikeGrasp: A Benchmark for 6-DoF Grasp Pose Detection from Stereo Spike Streams",
        "summary": "Most robotic grasping systems rely on converting sensor data into explicit 3D\npoint clouds, which is a computational step not found in biological\nintelligence. This paper explores a fundamentally different, neuro-inspired\nparadigm for 6-DoF grasp detection. We introduce SpikeGrasp, a framework that\nmimics the biological visuomotor pathway, processing raw, asynchronous events\nfrom stereo spike cameras, similarly to retinas, to directly infer grasp poses.\nOur model fuses these stereo spike streams and uses a recurrent spiking neural\nnetwork, analogous to high-level visual processing, to iteratively refine grasp\nhypotheses without ever reconstructing a point cloud. To validate this\napproach, we built a large-scale synthetic benchmark dataset. Experiments show\nthat SpikeGrasp surpasses traditional point-cloud-based baselines, especially\nin cluttered and textureless scenes, and demonstrates remarkable data\nefficiency. By establishing the viability of this end-to-end, neuro-inspired\napproach, SpikeGrasp paves the way for future systems capable of the fluid and\nefficient manipulation seen in nature, particularly for dynamic objects.",
        "url": "http://arxiv.org/abs/2510.10602v1",
        "published_date": "2025-10-12T13:36:40+00:00",
        "updated_date": "2025-10-12T13:36:40+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Zhuoheng Gao",
            "Jiyao Zhang",
            "Zhiyong Xie",
            "Hao Dong",
            "Zhaofei Yu",
            "Rongmei Chen",
            "Guozhang Chen",
            "Tiejun Huang"
        ],
        "tldr": "The paper introduces SpikeGrasp, a neuro-inspired framework that uses stereo spike streams and recurrent spiking neural networks for 6-DoF grasp pose detection, bypassing traditional point cloud reconstruction, and demonstrating superior performance in cluttered environments.",
        "tldr_zh": "该论文介绍了 SpikeGrasp，一个神经启发式框架，使用立体脉冲流和循环脉冲神经网络进行 6-DoF 抓取姿态检测，绕过了传统的点云重建，并在杂乱环境中表现出卓越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SuperEx: Enhancing Indoor Mapping and Exploration using Non-Line-of-Sight Perception",
        "summary": "Efficient exploration and mapping in unknown indoor environments is a\nfundamental challenge, with high stakes in time-critical settings. In current\nsystems, robot perception remains confined to line-of-sight; occluded regions\nremain unknown until physically traversed, leading to inefficient exploration\nwhen layouts deviate from prior assumptions. In this work, we bring\nnon-line-of-sight (NLOS) sensing to robotic exploration. We leverage\nsingle-photon LiDARs, which capture time-of-flight histograms that encode the\npresence of hidden objects - allowing robots to look around blind corners.\nRecent single-photon LiDARs have become practical and portable, enabling\ndeployment beyond controlled lab settings. Prior NLOS works target 3D\nreconstruction in static, lab-based scenarios, and initial efforts toward\nNLOS-aided navigation consider simplified geometries. We introduce SuperEx, a\nframework that integrates NLOS sensing directly into the mapping-exploration\nloop. SuperEx augments global map prediction with beyond-line-of-sight cues by\n(i) carving empty NLOS regions from timing histograms and (ii) reconstructing\noccupied structure via a two-step physics-based and data-driven approach that\nleverages structural regularities. Evaluations on complex simulated maps and\nthe real-world KTH Floorplan dataset show a 12% gain in mapping accuracy under\n< 30% coverage and improved exploration efficiency compared to line-of-sight\nbaselines, opening a path to reliable mapping beyond direct visibility.",
        "url": "http://arxiv.org/abs/2510.10506v1",
        "published_date": "2025-10-12T08:52:20+00:00",
        "updated_date": "2025-10-12T08:52:20+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Kush Garg",
            "Akshat Dave"
        ],
        "tldr": "The paper introduces SuperEx, a framework that integrates non-line-of-sight (NLOS) sensing using single-photon LiDARs into the mapping-exploration loop for robots, improving mapping accuracy and exploration efficiency in indoor environments.",
        "tldr_zh": "该论文介绍了SuperEx，一个将基于单光子激光雷达的非视距（NLOS）感知集成到机器人地图构建-探索循环中的框架，提高了室内环境中的地图构建精度和探索效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DAGLFNet:Deep Attention-Guided Global-Local Feature Fusion for Pseudo-Image Point Cloud Segmentation",
        "summary": "Environmental perception systems play a critical role in high-precision\nmapping and autonomous navigation, with LiDAR serving as a core sensor that\nprovides accurate 3D point cloud data. How to efficiently process unstructured\npoint clouds while extracting structured semantic information remains a\nsignificant challenge, and in recent years, numerous pseudo-image-based\nrepresentation methods have emerged to achieve a balance between efficiency and\nperformance. However, they often overlook the structural and semantic details\nof point clouds, resulting in limited feature fusion and discriminability. In\nthis work, we propose DAGLFNet, a pseudo-image-based semantic segmentation\nframework designed to extract discriminative features. First, the Global-Local\nFeature Fusion Encoding module is used to enhance the correlation among local\nfeatures within a set and capture global contextual information. Second, the\nMulti-Branch Feature Extraction network is employed to capture more\nneighborhood information and enhance the discriminability of contour features.\nFinally, a Feature Fusion via Deep Feature-guided Attention mechanism is\nintroduced to improve the precision of cross-channel feature fusion.\nExperimental evaluations show that DAGLFNet achieves 69.83\\% and 78.65\\% on the\nvalidation sets of SemanticKITTI and nuScenes, respectively. The method\nbalances high performance with real-time capability, demonstrating great\npotential for LiDAR-based real-time applications.",
        "url": "http://arxiv.org/abs/2510.10471v1",
        "published_date": "2025-10-12T06:35:03+00:00",
        "updated_date": "2025-10-12T06:35:03+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Chuang Chen",
            "Wenyi Ge"
        ],
        "tldr": "The paper introduces DAGLFNet, a pseudo-image-based LiDAR point cloud segmentation framework employing global-local feature fusion and attention mechanisms to improve segmentation accuracy and real-time performance, achieving state-of-the-art results on SemanticKITTI and nuScenes datasets.",
        "tldr_zh": "该论文介绍了DAGLFNet，一种基于伪图像的LiDAR点云分割框架，该框架采用全局-局部特征融合和注意力机制来提高分割精度和实时性能，并在SemanticKITTI和nuScenes数据集上取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MonoSE(3)-Diffusion: A Monocular SE(3) Diffusion Framework for Robust Camera-to-Robot Pose Estimation",
        "summary": "We propose MonoSE(3)-Diffusion, a monocular SE(3) diffusion framework that\nformulates markerless, image-based robot pose estimation as a conditional\ndenoising diffusion process. The framework consists of two processes: a\nvisibility-constrained diffusion process for diverse pose augmentation and a\ntimestep-aware reverse process for progressive pose refinement. The diffusion\nprocess progressively perturbs ground-truth poses to noisy transformations for\ntraining a pose denoising network. Importantly, we integrate visibility\nconstraints into the process, ensuring the transformations remain within the\ncamera field of view. Compared to the fixed-scale perturbations used in current\nmethods, the diffusion process generates in-view and diverse training poses,\nthereby improving the network generalization capability. Furthermore, the\nreverse process iteratively predicts the poses by the denoising network and\nrefines pose estimates by sampling from the diffusion posterior of current\ntimestep, following a scheduled coarse-to-fine procedure. Moreover, the\ntimestep indicates the transformation scales, which guide the denoising network\nto achieve more accurate pose predictions. The reverse process demonstrates\nhigher robustness than direct prediction, benefiting from its timestep-aware\nrefinement scheme. Our approach demonstrates improvements across two benchmarks\n(DREAM and RoboKeyGen), achieving a notable AUC of 66.75 on the most\nchallenging dataset, representing a 32.3% gain over the state-of-the-art.",
        "url": "http://arxiv.org/abs/2510.10434v1",
        "published_date": "2025-10-12T03:57:30+00:00",
        "updated_date": "2025-10-12T03:57:30+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Kangjian Zhu",
            "Haobo Jiang",
            "Yigong Zhang",
            "Jianjun Qian",
            "Jian Yang",
            "Jin Xie"
        ],
        "tldr": "The paper introduces MonoSE(3)-Diffusion, a monocular SE(3) diffusion framework for robust camera-to-robot pose estimation using a visibility-constrained diffusion process for pose augmentation and a timestep-aware reverse process for refinement, achieving significant improvements over state-of-the-art methods.",
        "tldr_zh": "该论文提出了MonoSE(3)-Diffusion，一个单目SE(3)扩散框架，用于稳健的相机到机器人姿态估计。该框架使用可见性约束的扩散过程进行姿态增强，并使用时间步感知逆过程进行姿态细化，相对于现有技术取得了显著的改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PointMAC: Meta-Learned Adaptation for Robust Test-Time Point Cloud Completion",
        "summary": "Point cloud completion is essential for robust 3D perception in\nsafety-critical applications such as robotics and augmented reality. However,\nexisting models perform static inference and rely heavily on inductive biases\nlearned during training, limiting their ability to adapt to novel structural\npatterns and sensor-induced distortions at test time. To address this\nlimitation, we propose PointMAC, a meta-learned framework for robust test-time\nadaptation in point cloud completion. It enables sample-specific refinement\nwithout requiring additional supervision. Our method optimizes the completion\nmodel under two self-supervised auxiliary objectives that simulate structural\nand sensor-level incompleteness. A meta-auxiliary learning strategy based on\nModel-Agnostic Meta-Learning (MAML) ensures that adaptation driven by auxiliary\nobjectives is consistently aligned with the primary completion task. During\ninference, we adapt the shared encoder on-the-fly by optimizing auxiliary\nlosses, with the decoder kept fixed. To further stabilize adaptation, we\nintroduce Adaptive $\\lambda$-Calibration, a meta-learned mechanism for\nbalancing gradients between primary and auxiliary objectives. Extensive\nexperiments on synthetic, simulated, and real-world datasets demonstrate that\nPointMAC achieves state-of-the-art results by refining each sample individually\nto produce high-quality completions. To the best of our knowledge, this is the\nfirst work to apply meta-auxiliary test-time adaptation to point cloud\ncompletion.",
        "url": "http://arxiv.org/abs/2510.10365v1",
        "published_date": "2025-10-11T23:13:17+00:00",
        "updated_date": "2025-10-11T23:13:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Linlian Jiang",
            "Rui Ma",
            "Li Gu",
            "Ziqiang Wang",
            "Xinxin Zuo",
            "Yang Wang"
        ],
        "tldr": "PointMAC introduces a meta-learned test-time adaptation framework for point cloud completion, using self-supervised auxiliary objectives and a meta-auxiliary learning strategy to improve robustness in novel and distorted environments.",
        "tldr_zh": "PointMAC 提出了一种用于点云补全的元学习测试时自适应框架，它使用自监督辅助目标和元辅助学习策略来提高在新的和扭曲的环境中的鲁棒性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]