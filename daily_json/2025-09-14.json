[
    {
        "title": "OpenUrban3D: Annotation-Free Open-Vocabulary Semantic Segmentation of Large-Scale Urban Point Clouds",
        "summary": "Open-vocabulary semantic segmentation enables models to recognize and segment\nobjects from arbitrary natural language descriptions, offering the flexibility\nto handle novel, fine-grained, or functionally defined categories beyond fixed\nlabel sets. While this capability is crucial for large-scale urban point clouds\nthat support applications such as digital twins, smart city management, and\nurban analytics, it remains largely unexplored in this domain. The main\nobstacles are the frequent absence of high-quality, well-aligned multi-view\nimagery in large-scale urban point cloud datasets and the poor generalization\nof existing three-dimensional (3D) segmentation pipelines across diverse urban\nenvironments with substantial variation in geometry, scale, and appearance. To\naddress these challenges, we present OpenUrban3D, the first 3D open-vocabulary\nsemantic segmentation framework for large-scale urban scenes that operates\nwithout aligned multi-view images, pre-trained point cloud segmentation\nnetworks, or manual annotations. Our approach generates robust semantic\nfeatures directly from raw point clouds through multi-view, multi-granularity\nrendering, mask-level vision-language feature extraction, and sample-balanced\nfusion, followed by distillation into a 3D backbone model. This design enables\nzero-shot segmentation for arbitrary text queries while capturing both semantic\nrichness and geometric priors. Extensive experiments on large-scale urban\nbenchmarks, including SensatUrban and SUM, show that OpenUrban3D achieves\nsignificant improvements in both segmentation accuracy and cross-scene\ngeneralization over existing methods, demonstrating its potential as a flexible\nand scalable solution for 3D urban scene understanding.",
        "url": "http://arxiv.org/abs/2509.10842v1",
        "published_date": "2025-09-13T15:03:28+00:00",
        "updated_date": "2025-09-13T15:03:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chongyu Wang",
            "Kunlei Jing",
            "Jihua Zhu",
            "Di Wang"
        ],
        "tldr": "The paper introduces OpenUrban3D, a novel annotation-free framework for open-vocabulary semantic segmentation of large-scale urban point clouds, demonstrating improved accuracy and generalization capabilities without relying on multi-view images or pre-trained networks.",
        "tldr_zh": "该论文介绍了 OpenUrban3D，一种新颖的无需标注的开放词汇大规模城市点云语义分割框架，展示了在不依赖多视角图像或预训练网络的情况下，提高了准确性和泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Nav-R1: Reasoning and Navigation in Embodied Scenes",
        "summary": "Embodied navigation requires agents to integrate perception, reasoning, and\naction for robust interaction in complex 3D environments. Existing approaches\noften suffer from incoherent and unstable reasoning traces that hinder\ngeneralization across diverse environments, and difficulty balancing\nlong-horizon semantic reasoning with low-latency control for real-time\nnavigation. To address these challenges, we propose Nav-R1, an embodied\nfoundation model that unifies reasoning in embodied environments. We first\nconstruct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought\n(CoT) for embodied tasks, which enables cold-start initialization with\nstructured reasoning. Building on this foundation, we design a GRPO-based\nreinforcement learning framework with three complementary rewards: format,\nunderstanding, and navigation, to improve structural adherence, semantic\ngrounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow\nreasoning paradigm, decoupling deliberate semantic reasoning from low-latency\nreactive control for efficient yet coherent navigation. Extensive evaluations\non embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms\nstrong baselines, with over 8% average improvement in reasoning and navigation\nperformance. Real-world deployment on a mobile robot further validates its\nrobustness under limited onboard resources. Code:\nhttps://github.com/AIGeeksGroup/Nav-R1. Website:\nhttps://aigeeksgroup.github.io/Nav-R1.",
        "url": "http://arxiv.org/abs/2509.10884v1",
        "published_date": "2025-09-13T16:31:03+00:00",
        "updated_date": "2025-09-13T16:31:03+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Qingxiang Liu",
            "Ting Huang",
            "Zeyu Zhang",
            "Hao Tang"
        ],
        "tldr": "Nav-R1 is an embodied foundation model that unifies reasoning and navigation through a large-scale CoT dataset, GRPO-based reinforcement learning, and a Fast-in-Slow reasoning paradigm, demonstrating significant improvements on embodied AI benchmarks and real-world robot deployment.",
        "tldr_zh": "Nav-R1是一个具身基础模型，通过大规模CoT数据集、基于GRPO的强化学习和一种Fast-in-Slow推理范式，统一了推理和导航。它在具身AI基准测试和真实机器人部署中表现出显著的改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]