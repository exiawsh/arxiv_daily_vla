[
    {
        "title": "A Contrastive Learning Framework Empowered by Attention-based Feature Adaptation for Street-View Image Classification",
        "summary": "Street-view image attribute classification is a vital downstream task of image classification, enabling applications such as autonomous driving, urban analytics, and high-definition map construction. It remains computationally demanding whether training from scratch, initialising from pre-trained weights, or fine-tuning large models. Although pre-trained vision-language models such as CLIP offer rich image representations, existing adaptation or fine-tuning methods often rely on their global image embeddings, limiting their ability to capture fine-grained, localised attributes essential in complex, cluttered street scenes. To address this, we propose CLIP-MHAdapter, a variant of the current lightweight CLIP adaptation paradigm that appends a bottleneck MLP equipped with multi-head self-attention operating on patch tokens to model inter-patch dependencies. With approximately 1.4 million trainable parameters, CLIP-MHAdapter achieves superior or competitive accuracy across eight attribute classification tasks on the Global StreetScapes dataset, attaining new state-of-the-art results while maintaining low computational cost. The code is available at https://github.com/SpaceTimeLab/CLIP-MHAdapter.",
        "url": "http://arxiv.org/abs/2602.16590v1",
        "published_date": "2026-02-18T16:41:32+00:00",
        "updated_date": "2026-02-18T16:41:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Qi You",
            "Yitai Cheng",
            "Zichao Zeng",
            "James Haworth"
        ],
        "tldr": "The paper introduces CLIP-MHAdapter, a lightweight adaptation method for CLIP using multi-head self-attention on patch tokens, achieving state-of-the-art results on street-view image attribute classification with low computational cost.",
        "tldr_zh": "该论文介绍了CLIP-MHAdapter，一种轻量级的CLIP适配方法，使用多头自注意力机制处理图像块令牌，在低计算成本下实现了街景图像属性分类的最先进结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Benchmarking Adversarial Robustness and Adversarial Training Strategies for Object Detection",
        "summary": "Object detection models are critical components of automated systems, such as autonomous vehicles and perception-based robots, but their sensitivity to adversarial attacks poses a serious security risk. Progress in defending these models lags behind classification, hindered by a lack of standardized evaluation. It is nearly impossible to thoroughly compare attack or defense methods, as existing work uses different datasets, inconsistent efficiency metrics, and varied measures of perturbation cost. This paper addresses this gap by investigating three key questions: (1) How can we create a fair benchmark to impartially compare attacks? (2) How well do modern attacks transfer across different architectures, especially from Convolutional Neural Networks to Vision Transformers? (3) What is the most effective adversarial training strategy for robust defense? To answer these, we first propose a unified benchmark framework focused on digital, non-patch-based attacks. This framework introduces specific metrics to disentangle localization and classification errors and evaluates attack cost using multiple perceptual metrics. Using this benchmark, we conduct extensive experiments on state-of-the-art attacks and a wide range of detectors. Our findings reveal two major conclusions: first, modern adversarial attacks against object detection models show a significant lack of transferability to transformer-based architectures. Second, we demonstrate that the most robust adversarial training strategy leverages a dataset composed of a mix of high-perturbation attacks with different objectives (e.g., spatial and semantic), which outperforms training on any single attack.",
        "url": "http://arxiv.org/abs/2602.16494v1",
        "published_date": "2026-02-18T14:33:58+00:00",
        "updated_date": "2026-02-18T14:33:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Alexis Winter",
            "Jean-Vincent Martini",
            "Romaric Audigier",
            "Angelique Loesch",
            "Bertrand Luvison"
        ],
        "tldr": "This paper introduces a benchmark for evaluating adversarial robustness in object detection, finding that attacks poorly transfer to transformer architectures and that mixed-attack adversarial training is most effective.",
        "tldr_zh": "本文提出了一个用于评估目标检测中对抗鲁棒性的基准，发现攻击在Transformer架构上的迁移性较差，并且混合攻击对抗训练最为有效。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]