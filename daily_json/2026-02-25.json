[
    {
        "title": "UFO: Unifying Feed-Forward and Optimization-based Methods for Large Driving Scene Modeling",
        "summary": "Dynamic driving scene reconstruction is critical for autonomous driving simulation and closed-loop learning. While recent feed-forward methods have shown promise for 3D reconstruction, they struggle with long-range driving sequences due to quadratic complexity in sequence length and challenges in modeling dynamic objects over extended durations. We propose UFO, a novel recurrent paradigm that combines the benefits of optimization-based and feed-forward methods for efficient long-range 4D reconstruction. Our approach maintains a 4D scene representation that is iteratively refined as new observations arrive, using a visibility-based filtering mechanism to select informative scene tokens and enable efficient processing of long sequences. For dynamic objects, we introduce an object pose-guided modeling approach that supports accurate long-range motion capture. Experiments on the Waymo Open Dataset demonstrate that our method significantly outperforms both per-scene optimization and existing feed-forward methods across various sequence lengths. Notably, our approach can reconstruct 16-second driving logs within 0.5 second while maintaining superior visual quality and geometric accuracy.",
        "url": "http://arxiv.org/abs/2602.20943v1",
        "published_date": "2026-02-24T14:24:50+00:00",
        "updated_date": "2026-02-24T14:24:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaiyuan Tan",
            "Yingying Shen",
            "Mingfei Tu",
            "Haohui Zhu",
            "Bing Wang",
            "Guang Chen",
            "Hangjun Ye",
            "Haiyang Sun"
        ],
        "tldr": "The paper introduces UFO, a recurrent method combining optimization and feed-forward approaches for efficient long-range 4D driving scene reconstruction, demonstrating improved performance and speed on the Waymo Open Dataset.",
        "tldr_zh": "该论文介绍了一种名为UFO的循环方法，它结合了优化和前馈方法，用于高效的远程4D驾驶场景重建，并在Waymo开放数据集上展示了改进的性能和速度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "VGGDrive: Empowering Vision-Language Models with Cross-View Geometric Grounding for Autonomous Driving",
        "summary": "The significance of cross-view 3D geometric modeling capabilities for autonomous driving is self-evident, yet existing Vision-Language Models (VLMs) inherently lack this capability, resulting in their mediocre performance. While some promising approaches attempt to mitigate this by constructing Q&A data for auxiliary training, they still fail to fundamentally equip VLMs with the ability to comprehensively handle diverse evaluation protocols. We thus chart a new course, advocating for the infusion of VLMs with the cross-view geometric grounding of mature 3D foundation models, closing this critical capability gap in autonomous driving. In this spirit, we propose a novel architecture, VGGDrive, which empowers Vision-language models with cross-view Geometric Grounding for autonomous Driving. Concretely, to bridge the cross-view 3D geometric features from the frozen visual 3D model with the VLM's 2D visual features, we introduce a plug-and-play Cross-View 3D Geometric Enabler (CVGE). The CVGE decouples the base VLM architecture and effectively empowers the VLM with 3D features through a hierarchical adaptive injection mechanism. Extensive experiments show that VGGDrive enhances base VLM performance across five autonomous driving benchmarks, including tasks like cross-view risk perception, motion prediction, and trajectory planning. It's our belief that mature 3D foundation models can empower autonomous driving tasks through effective integration, and we hope our initial exploration demonstrates the potential of this paradigm to the autonomous driving community.",
        "url": "http://arxiv.org/abs/2602.20794v1",
        "published_date": "2026-02-24T11:33:44+00:00",
        "updated_date": "2026-02-24T11:33:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jie Wang",
            "Guang Li",
            "Zhijian Huang",
            "Chenxu Dang",
            "Hangjun Ye",
            "Yahong Han",
            "Long Chen"
        ],
        "tldr": "The paper introduces VGGDrive, a novel architecture that enhances VLMs for autonomous driving by incorporating cross-view 3D geometric grounding from 3D foundation models, improving performance on various autonomous driving benchmarks.",
        "tldr_zh": "该论文介绍了VGGDrive，一种新颖的架构，通过结合来自3D基础模型的跨视角3D几何信息，增强了VLMs在自动驾驶方面的性能，并在多个自动驾驶基准测试中提高了性能。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "GA-Drive: Geometry-Appearance Decoupled Modeling for Free-viewpoint Driving Scene Generatio",
        "summary": "A free-viewpoint, editable, and high-fidelity driving simulator is crucial for training and evaluating end-to-end autonomous driving systems. In this paper, we present GA-Drive, a novel simulation framework capable of generating camera views along user-specified novel trajectories through Geometry-Appearance Decoupling and Diffusion-Based Generation. Given a set of images captured along a recorded trajectory and the corresponding scene geometry, GA-Drive synthesizes novel pseudo-views using geometry information. These pseudo-views are then transformed into photorealistic views using a trained video diffusion model. In this way, we decouple the geometry and appearance of scenes. An advantage of such decoupling is its support for appearance editing via state-of-the-art video-to-video editing techniques, while preserving the underlying geometry, enabling consistent edits across both original and novel trajectories. Extensive experiments demonstrate that GA-Drive substantially outperforms existing methods in terms of NTA-IoU, NTL-IoU, and FID scores.",
        "url": "http://arxiv.org/abs/2602.20673v1",
        "published_date": "2026-02-24T08:22:42+00:00",
        "updated_date": "2026-02-24T08:22:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Zhang",
            "Lue Fan",
            "Qitai Wang",
            "Wenbo Li",
            "Zehuan Wu",
            "Lewei Lu",
            "Zhaoxiang Zhang",
            "Hongsheng Li"
        ],
        "tldr": "GA-Drive is a novel driving simulation framework that decouples scene geometry and appearance using diffusion models to generate free-viewpoint, editable, and high-fidelity driving scenes, outperforming existing methods.",
        "tldr_zh": "GA-Drive 是一种新型驾驶模拟框架，它使用扩散模型解耦场景几何和外观，以生成自由视点、可编辑和高保真的驾驶场景，性能优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Object-Scene-Camera Decomposition and Recomposition for Data-Efficient Monocular 3D Object Detection",
        "summary": "Monocular 3D object detection (M3OD) is intrinsically ill-posed, hence training a high-performance deep learning based M3OD model requires a humongous amount of labeled data with complicated visual variation from diverse scenes, variety of objects and camera poses.However, we observe that, due to strong human bias, the three independent entities, i.e., object, scene, and camera pose, are always tightly entangled when an image is captured to construct training data. More specifically, specific 3D objects are always captured in particular scenes with fixed camera poses, and hence lacks necessary diversity. Such tight entanglement induces the challenging issues of insufficient utilization and overfitting to uniform training data. To mitigate this, we propose an online object-scene-camera decomposition and recomposition data manipulation scheme to more efficiently exploit the training data. We first fully decompose training images into textured 3D object point models and background scenes in an efficient computation and storage manner. We then continuously recompose new training images in each epoch by inserting the 3D objects into the freespace of the background scenes, and rendering them with perturbed camera poses from textured 3D point representation. In this way, the refreshed training data in all epochs can cover the full spectrum of independent object, scene, and camera pose combinations. This scheme can serve as a plug-and-play component to boost M3OD models, working flexibly with both fully and sparsely supervised settings. In the sparsely-supervised setting, objects closest to the ego-camera for all instances are sparsely annotated. We then can flexibly increase the annotated objects to control annotation cost. For validation, our method is widely applied to five representative M3OD models and evaluated on both the KITTI and the more complicated Waymo datasets.",
        "url": "http://arxiv.org/abs/2602.20627v1",
        "published_date": "2026-02-24T07:22:58+00:00",
        "updated_date": "2026-02-24T07:22:58+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Zhaonian Kuang",
            "Rui Ding",
            "Meng Yang",
            "Xinhu Zheng",
            "Gang Hua"
        ],
        "tldr": "This paper introduces a data augmentation method for monocular 3D object detection (M3OD) that decomposes training images into objects, scenes, and camera poses, then recomposes them to generate more diverse training data, addressing the issue of entanglement and improving model performance, especially in sparsely-supervised settings.",
        "tldr_zh": "本文提出了一种单目3D目标检测(M3OD)的数据增强方法，该方法将训练图像分解为目标、场景和相机姿态，然后重新组合它们以生成更多样化的训练数据，解决了纠缠问题并提高了模型性能，尤其是在稀疏监督设置中。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Efficient and Explainable End-to-End Autonomous Driving via Masked Vision-Language-Action Diffusion",
        "summary": "Large Language Models (LLMs) and Vision-Language Models (VLMs) have emerged as promising candidates for end-to-end autonomous driving. However, these models typically face challenges in inference latency, action precision, and explainability. Existing autoregressive approaches struggle with slow token-by-token generation, while prior diffusion-based planners often rely on verbose, general-purpose language tokens that lack explicit geometric structure. In this work, we propose Masked Vision-Language-Action Diffusion for Autonomous Driving (MVLAD-AD), a novel framework designed to bridge the gap between efficient planning and semantic explainability via a masked vision-language-action diffusion model. Unlike methods that force actions into the language space, we introduce a discrete action tokenization strategy that constructs a compact codebook of kinematically feasible waypoints from real-world driving distributions. Moreover, we propose geometry-aware embedding learning to ensure that embeddings in the latent space approximate physical geometric metrics. Finally, an action-priority decoding strategy is introduced to prioritize trajectory generation. Extensive experiments on nuScenes and derived benchmarks demonstrate that MVLAD-AD achieves superior efficiency and outperforms state-of-the-art autoregressive and diffusion baselines in planning precision, while providing high-fidelity and explainable reasoning.",
        "url": "http://arxiv.org/abs/2602.20577v1",
        "published_date": "2026-02-24T05:59:10+00:00",
        "updated_date": "2026-02-24T05:59:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaru Zhang",
            "Manav Gagvani",
            "Can Cui",
            "Juntong Peng",
            "Ruqi Zhang",
            "Ziran Wang"
        ],
        "tldr": "The paper introduces a novel Masked Vision-Language-Action Diffusion (MVLAD-AD) framework for efficient and explainable autonomous driving, using discrete action tokenization and geometry-aware embedding learning to improve planning precision and reasoning.",
        "tldr_zh": "该论文介绍了一种新的Masked Vision-Language-Action Diffusion (MVLAD-AD)框架，用于高效且可解释的自动驾驶。该框架采用离散动作标记化和几何感知嵌入学习，以提高规划精度和可解释的推理能力。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "An interactive enhanced driving dataset for autonomous driving",
        "summary": "The evolution of autonomous driving towards full automation demands robust interactive capabilities; however, the development of Vision-Language-Action (VLA) models is constrained by the sparsity of interactive scenarios and inadequate multimodal alignment in existing data. To this end, this paper proposes the Interactive Enhanced Driving Dataset (IEDD). We develop a scalable pipeline to mine million-level interactive segments from naturalistic driving data based on interactive trajectories, and design metrics to quantify the interaction processes. Furthermore, the IEDD-VQA dataset is constructed by generating synthetic Bird's Eye View (BEV) videos where semantic actions are strictly aligned with structured language. Benchmark results evaluating ten mainstream Vision Language Models (VLMs) are provided to demonstrate the dataset's reuse value in assessing and fine-tuning the reasoning capabilities of autonomous driving models.",
        "url": "http://arxiv.org/abs/2602.20575v1",
        "published_date": "2026-02-24T05:57:18+00:00",
        "updated_date": "2026-02-24T05:57:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haojie Feng",
            "Peizhi Zhang",
            "Mengjie Tian",
            "Xinrui Zhang",
            "Zhuoren Li",
            "Junpeng Huang",
            "Xiurong Wang",
            "Junfan Zhu",
            "Jianzhou Wang",
            "Dongxiao Yin",
            "Lu Xiong"
        ],
        "tldr": "The paper introduces a new interactive driving dataset (IEDD) with million-level interactive segments mined from naturalistic driving data, designed for training and evaluating Vision-Language-Action (VLA) models in autonomous driving.",
        "tldr_zh": "该论文介绍了一个新的交互式驾驶数据集 (IEDD)，该数据集包含从自然驾驶数据中挖掘的百万级交互式片段，专为训练和评估自动驾驶中的视觉-语言-动作 (VLA) 模型而设计。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "BFA++: Hierarchical Best-Feature-Aware Token Prune for Multi-View Vision Language Action Model",
        "summary": "Vision-Language-Action (VLA) models have achieved significant breakthroughs by leveraging Large Vision Language Models (VLMs) to jointly interpret instructions and visual inputs. However, the substantial increase in visual tokens, particularly from multi-view inputs, poses serious challenges to real-time robotic manipulation. Existing acceleration techniques for VLMs, such as token pruning, often result in degraded performance when directly applied to VLA models, as they overlook the relationships between different views and fail to account for the dynamic and task-specific characteristics of robotic operation. To address this, we propose BFA++, a dynamic token pruning framework designed specifically for VLA models. BFA++ introduces a hierarchical pruning strategy guided by two-level importance predictors: an intra-view predictor highlights task-relevant regions within each image to suppress spatial noise, while an inter-view predictor identifies critical camera views throughout different manipulation phases to reduce cross-view redundancy. This design enables efficient token selection while preserving essential visual cues, resulting in improved computational efficiency and higher manipulation success rates. Evaluations on the RoboTwin benchmark and real-world robotic tasks demonstrate that BFA++ consistently outperforms existing methods. BFA++ improves the success rate by about 10% on both the π0 and RDT models, achieving speedup of 1.8X and 1.5X, respectively. Our results highlight that context-sensitive and task-aware token pruning serves as a more effective strategy than full visual processing, enabling faster inference and improved manipulation accuracy in real-world robotic systems.",
        "url": "http://arxiv.org/abs/2602.20566v1",
        "published_date": "2026-02-24T05:31:52+00:00",
        "updated_date": "2026-02-24T05:31:52+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Haosheng Li",
            "Weixin Mao",
            "Zihan Lan",
            "Hongwei Xiong",
            "Hongan Wang",
            "Chenyang Si",
            "Ziwei Liu",
            "Xiaoming Deng",
            "Hua Chen"
        ],
        "tldr": "The paper introduces BFA++, a hierarchical token pruning framework for VLA models that improves computational efficiency and manipulation success rates by dynamically selecting important visual tokens within and across multiple camera views, outperforming existing methods on robotic benchmarks.",
        "tldr_zh": "该论文介绍了 BFA++，一种用于 VLA 模型的分层令牌修剪框架，通过动态选择内部和跨多个摄像机视图的重要视觉令牌来提高计算效率和操作成功率，在机器人基准测试中优于现有方法。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction",
        "summary": "We propose tttLRM, a novel large 3D reconstruction model that leverages a Test-Time Training (TTT) layer to enable long-context, autoregressive 3D reconstruction with linear computational complexity, further scaling the model's capability. Our framework efficiently compresses multiple image observations into the fast weights of the TTT layer, forming an implicit 3D representation in the latent space that can be decoded into various explicit formats, such as Gaussian Splats (GS) for downstream applications. The online learning variant of our model supports progressive 3D reconstruction and refinement from streaming observations. We demonstrate that pretraining on novel view synthesis tasks effectively transfers to explicit 3D modeling, resulting in improved reconstruction quality and faster convergence. Extensive experiments show that our method achieves superior performance in feedforward 3D Gaussian reconstruction compared to state-of-the-art approaches on both objects and scenes.",
        "url": "http://arxiv.org/abs/2602.20160v1",
        "published_date": "2026-02-23T18:59:45+00:00",
        "updated_date": "2026-02-23T18:59:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chen Wang",
            "Hao Tan",
            "Wang Yifan",
            "Zhiqin Chen",
            "Yuheng Liu",
            "Kalyan Sunkavalli",
            "Sai Bi",
            "Lingjie Liu",
            "Yiwei Hu"
        ],
        "tldr": "The paper introduces tttLRM, a test-time training method for efficient, long-context autoregressive 3D reconstruction, achieving superior performance in Gaussian Splat reconstruction.",
        "tldr_zh": "该论文介绍了tttLRM，一种用于高效、长上下文自回归3D重建的测试时训练方法，并在高斯溅射重建中实现了卓越的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Event-Aided Sharp Radiance Field Reconstruction for Fast-Flying Drones",
        "summary": "Fast-flying aerial robots promise rapid inspection under limited battery constraints, with direct applications in infrastructure inspection, terrain exploration, and search and rescue. However, high speeds lead to severe motion blur in images and induce significant drift and noise in pose estimates, making dense 3D reconstruction with Neural Radiance Fields (NeRFs) particularly challenging due to their high sensitivity to such degradations. In this work, we present a unified framework that leverages asynchronous event streams alongside motion-blurred frames to reconstruct high-fidelity radiance fields from agile drone flights. By embedding event-image fusion into NeRF optimization and jointly refining event-based visual-inertial odometry priors using both event and frame modalities, our method recovers sharp radiance fields and accurate camera trajectories without ground-truth supervision. We validate our approach on both synthetic data and real-world sequences captured by a fast-flying drone. Despite highly dynamic drone flights, where RGB frames are severely degraded by motion blur and pose priors become unreliable, our method reconstructs high-fidelity radiance fields and preserves fine scene details, delivering a performance gain of over 50% on real-world data compared to state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2602.21101v1",
        "published_date": "2026-02-24T17:02:56+00:00",
        "updated_date": "2026-02-24T17:02:56+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Rong Zou",
            "Marco Cannici",
            "Davide Scaramuzza"
        ],
        "tldr": "This paper introduces a method for reconstructing high-fidelity radiance fields from fast-flying drone footage using event cameras to overcome motion blur and noisy pose estimation, achieving significant performance gains over state-of-the-art methods.",
        "tldr_zh": "本文提出了一种利用事件相机从快速飞行的无人机镜头中重建高保真辐射场的方法，以克服运动模糊和噪声姿态估计，与最先进的方法相比，实现了显著的性能提升。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LST-SLAM: A Stereo Thermal SLAM System for Kilometer-Scale Dynamic Environments",
        "summary": "Thermal cameras offer strong potential for robot perception under challenging illumination and weather conditions. However, thermal Simultaneous Localization and Mapping (SLAM) remains difficult due to unreliable feature extraction, unstable motion tracking, and inconsistent global pose and map construction, particularly in dynamic large-scale outdoor environments. To address these challenges, we propose LST-SLAM, a novel large-scale stereo thermal SLAM system that achieves robust performance in complex, dynamic scenes. Our approach combines self-supervised thermal feature learning, stereo dual-level motion tracking, and geometric pose optimization. We also introduce a semantic-geometric hybrid constraint that suppresses potentially dynamic features lacking strong inter-frame geometric consistency. Furthermore, we develop an online incremental bag-of-words model for loop closure detection, coupled with global pose optimization to mitigate accumulated drift. Extensive experiments on kilometer-scale dynamic thermal datasets show that LST-SLAM significantly outperforms recent representative SLAM systems, including AirSLAM and DROID-SLAM, in both robustness and accuracy.",
        "url": "http://arxiv.org/abs/2602.20925v1",
        "published_date": "2026-02-24T14:04:54+00:00",
        "updated_date": "2026-02-24T14:04:54+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Zeyu Jiang",
            "Kuan Xu",
            "Changhao Chen"
        ],
        "tldr": "LST-SLAM is a novel stereo thermal SLAM system that achieves robust and accurate performance in kilometer-scale dynamic environments by combining self-supervised feature learning, dual-level motion tracking, geometric pose optimization, and a semantic-geometric hybrid constraint. It outperforms existing SLAM systems in thermal datasets.",
        "tldr_zh": "LST-SLAM 是一种新型的立体热成像SLAM系统，通过结合自监督特征学习、双层运动跟踪、几何姿态优化和语义-几何混合约束，在千米级动态环境中实现了鲁棒且精确的性能。它在热成像数据集上的表现优于现有的SLAM系统。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RU4D-SLAM: Reweighting Uncertainty in Gaussian Splatting SLAM for 4D Scene Reconstruction",
        "summary": "Combining 3D Gaussian splatting with Simultaneous Localization and Mapping (SLAM) has gained popularity as it enables continuous 3D environment reconstruction during motion. However, existing methods struggle in dynamic environments, particularly moving objects complicate 3D reconstruction and, in turn, hinder reliable tracking. The emergence of 4D reconstruction, especially 4D Gaussian splatting, offers a promising direction for addressing these challenges, yet its potential for 4D-aware SLAM remains largely underexplored. Along this direction, we propose a robust and efficient framework, namely Reweighting Uncertainty in Gaussian Splatting SLAM (RU4D-SLAM) for 4D scene reconstruction, that introduces temporal factors into spatial 3D representation while incorporating uncertainty-aware perception of scene changes, blurred image synthesis, and dynamic scene reconstruction. We enhance dynamic scene representation by integrating motion blur rendering, and improve uncertainty-aware tracking by extending per-pixel uncertainty modeling, which is originally designed for static scenarios, to handle blurred images. Furthermore, we propose a semantic-guided reweighting mechanism for per-pixel uncertainty estimation in dynamic scenes, and introduce a learnable opacity weight to support adaptive 4D mapping. Extensive experiments on standard benchmarks demonstrate that our method substantially outperforms state-of-the-art approaches in both trajectory accuracy and 4D scene reconstruction, particularly in dynamic environments with moving objects and low-quality inputs. Code available: https://ru4d-slam.github.io",
        "url": "http://arxiv.org/abs/2602.20807v1",
        "published_date": "2026-02-24T11:47:43+00:00",
        "updated_date": "2026-02-24T11:47:43+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Yangfan Zhao",
            "Hanwei Zhang",
            "Ke Huang",
            "Qiufeng Wang",
            "Zhenzhou Shao",
            "Dengyu Wu"
        ],
        "tldr": "The paper introduces RU4D-SLAM, a novel SLAM framework using 4D Gaussian splatting with uncertainty-aware perception and motion blur rendering, demonstrating improved trajectory accuracy and 4D reconstruction in dynamic environments.",
        "tldr_zh": "本文介绍了RU4D-SLAM，一种新颖的SLAM框架，它使用4D高斯溅射，具有不确定性感知和运动模糊渲染，在动态环境中展示了改进的轨迹精度和4D重建。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RAYNOVA: 3D-Geometry-Free Auto-Regressive Driving World Modeling with Unified Spatio-Temporal Representation",
        "summary": "World foundation models aim to simulate the evolution of the real world with physically plausible behavior. Unlike prior methods that handle spatial and temporal correlations separately, we propose RAYNOVA, a geometry-free world model that employs a dual-causal autoregressive framework. It follows both scale-wise and temporal topological orders in the autoregressive process, and leverages global attention for unified 4D spatio-temporal reasoning. Different from existing works that impose strong 3D geometric priors, RAYNOVA constructs an isotropic spatio-temporal representation across views, frames, and scales based on relative Plücker-ray positional encoding, enabling robust generalization to diverse camera setups and ego motions. We further introduce a recurrent training paradigm to alleviate distribution drift in long-horizon video generation. RAYNOVA achieves state-of-the-art multi-view video generation results on nuScenes, while offering higher throughput and strong controllability under diverse input conditions, generalizing to novel views and camera configurations without explicit 3D scene representation. Our code will be released at http://yichen928.github.io/raynova.",
        "url": "http://arxiv.org/abs/2602.20685v1",
        "published_date": "2026-02-24T08:41:40+00:00",
        "updated_date": "2026-02-24T08:41:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yichen Xie",
            "Chensheng Peng",
            "Mazen Abdelfattah",
            "Yihan Hu",
            "Jiezhi Yang",
            "Eric Higgins",
            "Ryan Brigden",
            "Masayoshi Tomizuka",
            "Wei Zhan"
        ],
        "tldr": "RAYNOVA is a novel geometry-free world model for autonomous driving that uses a dual-causal autoregressive framework with unified spatio-temporal representation, achieving state-of-the-art multi-view video generation results and strong generalization capabilities.",
        "tldr_zh": "RAYNOVA是一种新颖的无几何世界模型，用于自动驾驶，它采用具有统一时空表示的双因果自回归框架，实现了最先进的多视图视频生成结果和强大的泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SD4R: Sparse-to-Dense Learning for 3D Object Detection with 4D Radar",
        "summary": "4D radar measurements offer an affordable and weather-robust solution for 3D perception. However, the inherent sparsity and noise of radar point clouds present significant challenges for accurate 3D object detection, underscoring the need for effective and robust point clouds densification. Despite recent progress, existing densification methods often fail to address the extreme sparsity of 4D radar point clouds and exhibit limited robustness when processing scenes with a small number of points. In this paper, we propose SD4R, a novel framework that transforms sparse radar point clouds into dense representations. SD4R begins by utilizing a foreground point generator (FPG) to mitigate noise propagation and produce densified point clouds. Subsequently, a logit-query encoder (LQE) enhances conventional pillarization, resulting in robust feature representations. Through these innovations, our SD4R demonstrates strong capability in both noise reduction and foreground point densification. Extensive experiments conducted on the publicly available View-of-Delft dataset demonstrate that SD4R achieves state-of-the-art performance. Source code is available at https://github.com/lancelot0805/SD4R.",
        "url": "http://arxiv.org/abs/2602.20653v1",
        "published_date": "2026-02-24T07:57:19+00:00",
        "updated_date": "2026-02-24T07:57:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaokai Bai",
            "Jiahao Cheng",
            "Songkai Wang",
            "Yixuan Luo",
            "Lianqing Zheng",
            "Xiaohan Zhang",
            "Si-Yuan Cao",
            "Hui-Liang Shen"
        ],
        "tldr": "The paper introduces SD4R, a novel framework for 3D object detection using 4D radar data, which addresses sparsity and noise challenges through foreground point generation and logit-query encoding, achieving state-of-the-art results on the View-of-Delft dataset.",
        "tldr_zh": "该论文介绍了SD4R，一种使用4D雷达数据进行3D物体检测的新框架，它通过前景点生成和logit查询编码解决了稀疏性和噪声问题，并在View-of-Delft数据集上取得了最先进的成果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Boosting Instance Awareness via Cross-View Correlation with 4D Radar and Camera for 3D Object Detection",
        "summary": "4D millimeter-wave radar has emerged as a promising sensing modality for autonomous driving due to its robustness and affordability. However, its sparse and weak geometric cues make reliable instance activation difficult, limiting the effectiveness of existing radar-camera fusion paradigms. BEV-level fusion offers global scene understanding but suffers from weak instance focus, while perspective-level fusion captures instance details but lacks holistic context. To address these limitations, we propose SIFormer, a scene-instance aware transformer for 3D object detection using 4D radar and camera. SIFormer first suppresses background noise during view transformation through segmentation- and depth-guided localization. It then introduces a cross-view activation mechanism that injects 2D instance cues into BEV space, enabling reliable instance awareness under weak radar geometry. Finally, a transformer-based fusion module aggregates complementary image semantics and radar geometry for robust perception. As a result, with the aim of enhancing instance awareness, SIFormer bridges the gap between the two paradigms, combining their complementary strengths to address inherent sparse nature of radar and improve detection accuracy. Experiments demonstrate that SIFormer achieves state-of-the-art performance on View-of-Delft, TJ4DRadSet and NuScenes datasets. Source code is available at github.com/shawnnnkb/SIFormer.",
        "url": "http://arxiv.org/abs/2602.20632v1",
        "published_date": "2026-02-24T07:25:53+00:00",
        "updated_date": "2026-02-24T07:25:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiaokai Bai",
            "Lianqing Zheng",
            "Si-Yuan Cao",
            "Xiaohan Zhang",
            "Zhe Wu",
            "Beinan Yu",
            "Fang Wang",
            "Jie Bai",
            "Hui-Liang Shen"
        ],
        "tldr": "This paper introduces SIFormer, a novel scene-instance aware transformer for 3D object detection using 4D radar and camera data, designed to improve instance awareness by bridging BEV-level and perspective-level fusion paradigms.",
        "tldr_zh": "本文介绍了一种名为SIFormer的新型场景实例感知Transformer，它使用4D雷达和相机数据进行3D目标检测，旨在通过桥接BEV级别和透视级别的融合范例来提高实例感知。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CLIPoint3D: Language-Grounded Few-Shot Unsupervised 3D Point Cloud Domain Adaptation",
        "summary": "Recent vision-language models (VLMs) such as CLIP demonstrate impressive cross-modal reasoning, extending beyond images to 3D perception. Yet, these models remain fragile under domain shifts, especially when adapting from synthetic to real-world point clouds. Conventional 3D domain adaptation approaches rely on heavy trainable encoders, yielding strong accuracy but at the cost of efficiency. We introduce CLIPoint3D, the first framework for few-shot unsupervised 3D point cloud domain adaptation built upon CLIP. Our approach projects 3D samples into multiple depth maps and exploits the frozen CLIP backbone, refined through a knowledge-driven prompt tuning scheme that integrates high-level language priors with geometric cues from a lightweight 3D encoder. To adapt task-specific features effectively, we apply parameter-efficient fine-tuning to CLIP's encoders and design an entropy-guided view sampling strategy for selecting confident projections. Furthermore, an optimal transport-based alignment loss and an uncertainty-aware prototype alignment loss collaboratively bridge source-target distribution gaps while maintaining class separability. Extensive experiments on PointDA-10 and GraspNetPC-10 benchmarks show that CLIPoint3D achieves consistent 3-16% accuracy gains over both CLIP-based and conventional encoder-based baselines. Codes are available at https://github.com/SarthakM320/CLIPoint3D.",
        "url": "http://arxiv.org/abs/2602.20409v1",
        "published_date": "2026-02-23T23:17:12+00:00",
        "updated_date": "2026-02-23T23:17:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mainak Singha",
            "Sarthak Mehrotra",
            "Paolo Casari",
            "Subhasis Chaudhuri",
            "Elisa Ricci",
            "Biplab Banerjee"
        ],
        "tldr": "CLIPoint3D is a novel framework for few-shot unsupervised 3D point cloud domain adaptation using CLIP, achieving significant accuracy gains by combining prompt tuning, efficient fine-tuning, and distribution alignment losses.",
        "tldr_zh": "CLIPoint3D是一个新颖的框架，用于使用CLIP进行少样本无监督的3D点云域适应。它通过结合提示调整、高效微调和分布对齐损失，实现了显著的精度提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniLACT: Depth-Aware RGB Latent Action Learning for Vision-Language-Action Models",
        "summary": "Latent action representations learned from unlabeled videos have recently emerged as a promising paradigm for pretraining vision-language-action (VLA) models without explicit robot action supervision. However, latent actions derived solely from RGB observations primarily encode appearance-driven dynamics and lack explicit 3D geometric structure, which is essential for precise and contact-rich manipulation. To address this limitation, we introduce UniLACT, a transformer-based VLA model that incorporates geometric structure through depth-aware latent pretraining, enabling downstream policies to inherit stronger spatial priors. To facilitate this process, we propose UniLARN, a unified latent action learning framework based on inverse and forward dynamics objectives that learns a shared embedding space for RGB and depth while explicitly modeling their cross-modal interactions. This formulation produces modality-specific and unified latent action representations that serve as pseudo-labels for the depth-aware pretraining of UniLACT. Extensive experiments in both simulation and real-world settings demonstrate the effectiveness of depth-aware unified latent action representations. UniLACT consistently outperforms RGB-based latent action baselines under in-domain and out-of-domain pretraining regimes, as well as on both seen and unseen manipulation tasks.",
        "url": "http://arxiv.org/abs/2602.20231v1",
        "published_date": "2026-02-23T18:41:41+00:00",
        "updated_date": "2026-02-23T18:41:41+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Manish Kumar Govind",
            "Dominick Reilly",
            "Pu Wang",
            "Srijan Das"
        ],
        "tldr": "UniLACT is a VLA model that uses depth-aware latent pretraining to improve spatial reasoning for robotic manipulation, outperforming RGB-based methods in simulation and real-world experiments.",
        "tldr_zh": "UniLACT是一种视觉-语言-动作模型，它使用深度感知的潜在预训练来改善机器人操作的空间推理能力，并在仿真和真实世界实验中优于基于RGB的方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning",
        "summary": "Solving long-horizon tasks requires robots to integrate high-level semantic reasoning with low-level physical interaction. While vision-language models (VLMs) and video generation models can decompose tasks and imagine outcomes, they often lack the physical grounding necessary for real-world execution. We introduce NovaPlan, a hierarchical framework that unifies closed-loop VLM and video planning with geometrically grounded robot execution for zero-shot long-horizon manipulation. At the high level, a VLM planner decomposes tasks into sub-goals and monitors robot execution in a closed loop, enabling the system to recover from single-step failures through autonomous re-planning. To compute low-level robot actions, we extract and utilize both task-relevant object keypoints and human hand poses as kinematic priors from the generated videos, and employ a switching mechanism to choose the better one as a reference for robot actions, maintaining stable execution even under heavy occlusion or depth inaccuracy. We demonstrate the effectiveness of NovaPlan on three long-horizon tasks and the Functional Manipulation Benchmark (FMB). Our results show that NovaPlan can perform complex assembly tasks and exhibit dexterous error recovery behaviors without any prior demonstrations or training. Project page: https://nova-plan.github.io/",
        "url": "http://arxiv.org/abs/2602.20119v1",
        "published_date": "2026-02-23T18:35:18+00:00",
        "updated_date": "2026-02-23T18:35:18+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Jiahui Fu",
            "Junyu Nan",
            "Lingfeng Sun",
            "Hongyu Li",
            "Jianing Qian",
            "Jennifer L. Barry",
            "Kris Kitani",
            "George Konidaris"
        ],
        "tldr": "NovaPlan is a hierarchical framework using VLMs and video planning with geometrically grounded robot execution for zero-shot long-horizon manipulation, achieving complex assembly and error recovery without training.",
        "tldr_zh": "NovaPlan是一个分层框架，它使用视觉语言模型（VLMs）和视频规划与几何基础的机器人执行来实现零样本长时程操作，无需训练即可完成复杂的组装和错误恢复。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]