[
    {
        "title": "SegSplat: Feed-forward Gaussian Splatting and Open-Set Semantic Segmentation",
        "summary": "We have introduced SegSplat, a novel framework designed to bridge the gap between rapid, feed-forward 3D reconstruction and rich, open-vocabulary semantic understanding. By constructing a compact semantic memory bank from multi-view 2D foundation model features and predicting discrete semantic indices alongside geometric and appearance attributes for each 3D Gaussian in a single pass, SegSplat efficiently imbues scenes with queryable semantics. Our experiments demonstrate that SegSplat achieves geometric fidelity comparable to state-of-the-art feed-forward 3D Gaussian Splatting methods while simultaneously enabling robust open-set semantic segmentation, crucially \\textit{without} requiring any per-scene optimization for semantic feature integration. This work represents a significant step towards practical, on-the-fly generation of semantically aware 3D environments, vital for advancing robotic interaction, augmented reality, and other intelligent systems.",
        "url": "http://arxiv.org/abs/2511.18386v1",
        "published_date": "2025-11-23T10:26:38+00:00",
        "updated_date": "2025-11-23T10:26:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Peter Siegel",
            "Federico Tombari",
            "Marc Pollefeys",
            "Daniel Barath"
        ],
        "tldr": "SegSplat introduces a feed-forward method for 3D Gaussian Splatting that incorporates open-set semantic segmentation by leveraging multi-view 2D foundation model features, enabling rapid generation of semantically aware 3D environments without per-scene optimization.",
        "tldr_zh": "SegSplat 提出了一种前馈的 3D 高斯溅射方法，通过利用多视图 2D 基础模型特征，集成了开放集语义分割，无需针对每个场景进行优化即可快速生成具有语义信息的 3D 环境。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "SwiftVGGT: A Scalable Visual Geometry Grounded Transformer for Large-Scale Scenes",
        "summary": "3D reconstruction in large-scale scenes is a fundamental task in 3D perception, but the inherent trade-off between accuracy and computational efficiency remains a significant challenge. Existing methods either prioritize speed and produce low-quality results, or achieve high-quality reconstruction at the cost of slow inference times. In this paper, we propose SwiftVGGT, a training-free method that significantly reduce inference time while preserving high-quality dense 3D reconstruction. To maintain global consistency in large-scale scenes, SwiftVGGT performs loop closure without relying on the external Visual Place Recognition (VPR) model. This removes redundant computation and enables accurate reconstruction over kilometer-scale environments. Furthermore, we propose a simple yet effective point sampling method to align neighboring chunks using a single Sim(3)-based Singular Value Decomposition (SVD) step. This eliminates the need for the Iteratively Reweighted Least Squares (IRLS) optimization commonly used in prior work, leading to substantial speed-ups. We evaluate SwiftVGGT on multiple datasets and show that it achieves state-of-the-art reconstruction quality while requiring only 33% of the inference time of recent VGGT-based large-scale reconstruction approaches.",
        "url": "http://arxiv.org/abs/2511.18290v1",
        "published_date": "2025-11-23T05:03:49+00:00",
        "updated_date": "2025-11-23T05:03:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jungho Lee",
            "Minhyeok Lee",
            "Sunghun Yang",
            "Minseok Kang",
            "Sangyoun Lee"
        ],
        "tldr": "The paper introduces SwiftVGGT, a training-free method for large-scale 3D reconstruction that achieves state-of-the-art quality with significantly reduced inference time by optimizing loop closure and point sampling.",
        "tldr_zh": "该论文介绍了一种名为 SwiftVGGT 的免训练方法，用于大规模 3D 重建，通过优化闭环和点采样，在显著减少推理时间的同时实现了最先进的质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniFlow: Towards Zero-Shot LiDAR Scene Flow for Autonomous Vehicles via Cross-Domain Generalization",
        "summary": "LiDAR scene flow is the task of estimating per-point 3D motion between consecutive point clouds. Recent methods achieve centimeter-level accuracy on popular autonomous vehicle (AV) datasets, but are typically only trained and evaluated on a single sensor. In this paper, we aim to learn general motion priors that transfer to diverse and unseen LiDAR sensors. However, prior work in LiDAR semantic segmentation and 3D object detection demonstrate that naively training on multiple datasets yields worse performance than single dataset models. Interestingly, we find that this conventional wisdom does not hold for motion estimation, and that state-of-the-art scene flow methods greatly benefit from cross-dataset training. We posit that low-level tasks such as motion estimation may be less sensitive to sensor configuration; indeed, our analysis shows that models trained on fast-moving objects (e.g., from highway datasets) perform well on fast-moving objects, even across different datasets. Informed by our analysis, we propose UniFlow, a family of feedforward models that unifies and trains on multiple large-scale LiDAR scene flow datasets with diverse sensor placements and point cloud densities. Our frustratingly simple solution establishes a new state-of-the-art on Waymo and nuScenes, improving over prior work by 5.1% and 35.2% respectively. Moreover, UniFlow achieves state-of-the-art accuracy on unseen datasets like TruckScenes, outperforming prior TruckScenes-specific models by 30.1%.",
        "url": "http://arxiv.org/abs/2511.18254v1",
        "published_date": "2025-11-23T02:51:42+00:00",
        "updated_date": "2025-11-23T02:51:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siyi Li",
            "Qingwen Zhang",
            "Ishan Khatri",
            "Kyle Vedder",
            "Deva Ramanan",
            "Neehar Peri"
        ],
        "tldr": "The paper introduces UniFlow, a cross-dataset trained LiDAR scene flow model that achieves state-of-the-art performance on multiple datasets, including unseen ones, by generalizing motion priors across diverse LiDAR sensors.",
        "tldr_zh": "该论文介绍了UniFlow，一个跨数据集训练的激光雷达场景流模型，通过泛化不同激光雷达传感器之间的运动先验，在多个数据集（包括未见过的数据集）上实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EgoControl: Controllable Egocentric Video Generation via 3D Full-Body Poses",
        "summary": "Egocentric video generation with fine-grained control through body motion is a key requirement towards embodied AI agents that can simulate, predict, and plan actions. In this work, we propose EgoControl, a pose-controllable video diffusion model trained on egocentric data. We train a video prediction model to condition future frame generation on explicit 3D body pose sequences. To achieve precise motion control, we introduce a novel pose representation that captures both global camera dynamics and articulated body movements, and integrate it through a dedicated control mechanism within the diffusion process. Given a short sequence of observed frames and a sequence of target poses, EgoControl generates temporally coherent and visually realistic future frames that align with the provided pose control. Experimental results demonstrate that EgoControl produces high-quality, pose-consistent egocentric videos, paving the way toward controllable embodied video simulation and understanding.",
        "url": "http://arxiv.org/abs/2511.18173v1",
        "published_date": "2025-11-22T19:56:39+00:00",
        "updated_date": "2025-11-22T19:56:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Enrico Pallotta",
            "Sina Mokhtarzadeh Azar",
            "Lars Doorenbos",
            "Serdar Ozsoy",
            "Umar Iqbal",
            "Juergen Gall"
        ],
        "tldr": "EgoControl is a pose-controllable video diffusion model for egocentric video generation, conditioning future frame generation on explicit 3D body pose sequences, which is important for embodied AI agents.",
        "tldr_zh": "EgoControl是一个姿态可控的自中心视频生成扩散模型，它通过显式的3D身体姿态序列来调节未来帧的生成，这对具身AI Agent非常重要。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AVERY: Adaptive VLM Split Computing through Embodied Self-Awareness for Efficient Disaster Response Systems",
        "summary": "Unmanned Aerial Vehicles (UAVs) in disaster response require complex, queryable intelligence that on-board CNNs cannot provide. While Vision-Language Models (VLMs) offer this semantic reasoning, their high resource demands make on-device deployment infeasible, and naive cloud offloading fails under the low-bandwidth networks common in disaster zones. We present AVERY, a framework that enables VLM deployment through adaptive split computing. We advance the split computing paradigm beyond traditional depth-wise partitioning by introducing a functional, cognitive-inspired dual-stream split that separates the VLM into a high-frequency, low-resolution \"context stream\" for real-time awareness and a low-frequency, high-fidelity \"insight stream\" for deep analysis. A lightweight, self-aware on-board controller manages this architecture, monitoring network conditions and operator intent to dynamically select from pre-trained compression models, navigating the fundamental accuracy-throughput trade-off. Evaluated using the VLM LISA-7B across an edge-cloud scenario under fluctuating network conditions, AVERY consistently outperforms static configurations, achieving 11.2% higher accuracy than raw image compression and 93.98% lower energy consumption compared to full-edge execution, thereby enhancing mission efficiency and enabling real-time, queryable intelligence on resource-constrained platforms in dynamic environments.",
        "url": "http://arxiv.org/abs/2511.18151v1",
        "published_date": "2025-11-22T18:42:04+00:00",
        "updated_date": "2025-11-22T18:42:04+00:00",
        "categories": [
            "cs.DC",
            "cs.AR",
            "cs.CV",
            "cs.LG",
            "cs.NI"
        ],
        "authors": [
            "Rajat Bhattacharjya",
            "Sing-Yao Wu",
            "Hyunwoo Oh",
            "Chaewon Nam",
            "Suyeon Koo",
            "Mohsen Imani",
            "Elaheh Bozorgzadeh",
            "Nikil Dutt"
        ],
        "tldr": "The paper introduces AVERY, an adaptive split computing framework for deploying VLMs on UAVs in disaster response by dynamically partitioning the model into context and insight streams based on network conditions and operator intent, achieving better accuracy and lower energy consumption compared to static configurations.",
        "tldr_zh": "该论文介绍了AVERY，一个自适应分割计算框架，用于在灾难响应中将VLM部署在无人机上。它根据网络条件和操作员意图，将模型动态分割成上下文流和洞察流，与静态配置相比，实现了更高的精度和更低的能耗。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]