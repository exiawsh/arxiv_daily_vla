[
    {
        "title": "When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs",
        "summary": "Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.",
        "url": "http://arxiv.org/abs/2602.17659v1",
        "published_date": "2026-02-19T18:59:20+00:00",
        "updated_date": "2026-02-19T18:59:20+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Yu Fang",
            "Yuchun Feng",
            "Dong Jing",
            "Jiaqi Liu",
            "Yue Yang",
            "Zhenyu Wei",
            "Daniel Szafir",
            "Mingyu Ding"
        ],
        "tldr": "The paper introduces LIBERO-CF, a benchmark for evaluating counterfactual failures in Vision-Language-Action models, and proposes Counterfactual Action Guidance (CAG) to mitigate these failures, showing significant improvements in language following and task success.",
        "tldr_zh": "该论文介绍了LIBERO-CF，一个用于评估视觉-语言-动作模型中反事实失败的基准，并提出了反事实动作指导（CAG）来减轻这些失败，在语言跟随和任务成功方面显示出显著的改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]