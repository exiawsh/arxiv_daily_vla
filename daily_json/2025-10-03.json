[
    {
        "title": "Learning to Generate Object Interactions with Physics-Guided Video Diffusion",
        "summary": "Recent models for video generation have achieved remarkable progress and are\nnow deployed in film, social media production, and advertising. Beyond their\ncreative potential, such models also hold promise as world simulators for\nrobotics and embodied decision making. Despite strong advances, however,\ncurrent approaches still struggle to generate physically plausible object\ninteractions and lack physics-grounded control mechanisms. To address this\nlimitation, we introduce KineMask, an approach for physics-guided video\ngeneration that enables realistic rigid body control, interactions, and\neffects. Given a single image and a specified object velocity, our method\ngenerates videos with inferred motions and future object interactions. We\npropose a two-stage training strategy that gradually removes future motion\nsupervision via object masks. Using this strategy we train video diffusion\nmodels (VDMs) on synthetic scenes of simple interactions and demonstrate\nsignificant improvements of object interactions in real scenes. Furthermore,\nKineMask integrates low-level motion control with high-level textual\nconditioning via predictive scene descriptions, leading to effective support\nfor synthesis of complex dynamical phenomena. Extensive experiments show that\nKineMask achieves strong improvements over recent models of comparable size.\nAblation studies further highlight the complementary roles of low- and\nhigh-level conditioning in VDMs. Our code, model, and data will be made\npublicly available.",
        "url": "http://arxiv.org/abs/2510.02284v1",
        "published_date": "2025-10-02T17:56:46+00:00",
        "updated_date": "2025-10-02T17:56:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "David Romero",
            "Ariana Bermudez",
            "Hao Li",
            "Fabio Pizzati",
            "Ivan Laptev"
        ],
        "tldr": "The paper introduces KineMask, a physics-guided video diffusion model for generating videos with realistic object interactions, controlled by object velocities and textual descriptions. It improves upon existing video generation models, particularly in physical plausibility.",
        "tldr_zh": "该论文介绍了KineMask，一种物理引导的视频扩散模型，用于生成具有真实物体交互的视频，并通过物体速度和文本描述进行控制。它改进了现有的视频生成模型，尤其是在物理合理性方面。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning",
        "summary": "We study view-invariant imitation learning by explicitly conditioning\npolicies on camera extrinsics. Using Plucker embeddings of per-pixel rays, we\nshow that conditioning on extrinsics significantly improves generalization\nacross viewpoints for standard behavior cloning policies, including ACT,\nDiffusion Policy, and SmolVLA. To evaluate policy robustness under realistic\nviewpoint shifts, we introduce six manipulation tasks in RoboSuite and\nManiSkill that pair \"fixed\" and \"randomized\" scene variants, decoupling\nbackground cues from camera pose. Our analysis reveals that policies without\nextrinsics often infer camera pose using visual cues from static backgrounds in\nfixed scenes; this shortcut collapses when workspace geometry or camera\nplacement shifts. Conditioning on extrinsics restores performance and yields\nrobust RGB-only control without depth. We release the tasks, demonstrations,\nand code at https://ripl.github.io/know_your_camera/ .",
        "url": "http://arxiv.org/abs/2510.02268v1",
        "published_date": "2025-10-02T17:47:06+00:00",
        "updated_date": "2025-10-02T17:47:06+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Tianchong Jiang",
            "Jingtian Ji",
            "Xiangshan Tan",
            "Jiading Fang",
            "Anand Bhattad",
            "Vitor Guizilini",
            "Matthew R. Walter"
        ],
        "tldr": "The paper introduces a view-invariant imitation learning approach by conditioning policies on camera extrinsics, demonstrated on manipulation tasks, showing improved generalization and robustness against viewpoint shifts compared to standard behavior cloning methods.",
        "tldr_zh": "该论文介绍了一种通过将策略建立在相机外参之上来实现视角不变的模仿学习方法。该方法在操作任务中进行了演示，与标准行为克隆方法相比，显示出更好的泛化能力和对视角变化的鲁棒性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstruction",
        "summary": "This work proposed a 3D autoencoder architecture, named LiLa-Net, which\nencodes efficient features from real traffic environments, employing only the\nLiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle,\nequipped with Velodyne LiDAR. The system leverage skip connections concept to\nimprove the performance without using extensive resources as the\nstate-of-the-art architectures. Key changes include reducing the number of\nencoder layers and simplifying the skip connections, while still producing an\nefficient and representative latent space which allows to accurately\nreconstruct the original point cloud. Furthermore, an effective balance has\nbeen achieved between the information carried by the skip connections and the\nlatent encoding, leading to improved reconstruction quality without\ncompromising performance. Finally, the model demonstrates strong generalization\ncapabilities, successfully reconstructing objects unrelated to the original\ntraffic environment.",
        "url": "http://arxiv.org/abs/2510.02028v1",
        "published_date": "2025-10-02T14:00:20+00:00",
        "updated_date": "2025-10-02T14:00:20+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Mario Resino",
            "Borja Pérez",
            "Jaime Godoy",
            "Abdulla Al-Kaff",
            "Fernando García"
        ],
        "tldr": "LiLa-Net, a lightweight LiDAR autoencoder, is proposed for efficient 3D point cloud reconstruction in traffic environments. It achieves good reconstruction quality and generalization with reduced computational resources.",
        "tldr_zh": "LiLa-Net是一个轻量级的激光雷达自动编码器，旨在高效地重建交通环境中的3D点云。它以较低的计算资源实现了良好的重建质量和泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Calibrating the Full Predictive Class Distribution of 3D Object Detectors for Autonomous Driving",
        "summary": "In autonomous systems, precise object detection and uncertainty estimation\nare critical for self-aware and safe operation. This work addresses confidence\ncalibration for the classification task of 3D object detectors. We argue that\nit is necessary to regard the calibration of the full predictive confidence\ndistribution over all classes and deduce a metric which captures the\ncalibration of dominant and secondary class predictions. We propose two\nauxiliary regularizing loss terms which introduce either calibration of the\ndominant prediction or the full prediction vector as a training goal. We\nevaluate a range of post-hoc and train-time methods for CenterPoint, PillarNet\nand DSVT-Pillar and find that combining our loss term, which regularizes for\ncalibration of the full class prediction, and isotonic regression lead to the\nbest calibration of CenterPoint and PillarNet with respect to both dominant and\nsecondary class predictions. We further find that DSVT-Pillar can not be\njointly calibrated for dominant and secondary predictions using the same\nmethod.",
        "url": "http://arxiv.org/abs/2510.01829v1",
        "published_date": "2025-10-02T09:22:03+00:00",
        "updated_date": "2025-10-02T09:22:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Cornelius Schröder",
            "Marius-Raphael Schlüter",
            "Markus Lienkamp"
        ],
        "tldr": "This paper addresses the calibration of the full predictive class distribution for 3D object detectors in autonomous driving, proposing novel loss terms and evaluating them on various models.",
        "tldr_zh": "本文解决了自动驾驶中3D目标检测器完整预测类别分布的校准问题，提出了新的损失项并在各种模型上进行了评估。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VLA-R1: Enhancing Reasoning in Vision-Language-Action Models",
        "summary": "Vision-Language-Action (VLA) models aim to unify perception, language\nunderstanding, and action generation, offering strong cross-task and\ncross-scene generalization with broad impact on embodied AI. However, current\nVLA models often lack explicit step-by-step reasoning, instead emitting final\nactions without considering affordance constraints or geometric relations.\nTheir post-training pipelines also rarely reinforce reasoning quality, relying\nprimarily on supervised fine-tuning with weak reward design. To address these\nchallenges, we present VLA-R1, a reasoning-enhanced VLA that integrates\nReinforcement Learning from Verifiable Rewards (RLVR) with Group Relative\nPolicy Optimization (GRPO) to systematically optimize both reasoning and\nexecution. Specifically, we design an RLVR-based post-training strategy with\nverifiable rewards for region alignment, trajectory consistency, and output\nformatting, thereby strengthening reasoning robustness and execution accuracy.\nMoreover, we develop VLA-CoT-13K, a high-quality dataset that provides\nchain-of-thought supervision explicitly aligned with affordance and trajectory\nannotations. Furthermore, extensive evaluations on in-domain, out-of-domain,\nsimulation, and real-robot platforms demonstrate that VLA-R1 achieves superior\ngeneralization and real-world performance compared to prior VLA methods. We\nplan to release the model, code, and dataset following the publication of this\nwork. Code: https://github.com/GigaAI-research/VLA-R1. Website:\nhttps://gigaai-research.github.io/VLA-R1.",
        "url": "http://arxiv.org/abs/2510.01623v1",
        "published_date": "2025-10-02T02:54:03+00:00",
        "updated_date": "2025-10-02T02:54:03+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Angen Ye",
            "Zeyu Zhang",
            "Boyuan Wang",
            "Xiaofeng Wang",
            "Dapeng Zhang",
            "Zheng Zhu"
        ],
        "tldr": "The paper introduces VLA-R1, a reasoning-enhanced Vision-Language-Action model that uses Reinforcement Learning from Verifiable Rewards and Group Relative Policy Optimization to improve reasoning and execution, validated with a new dataset and experiments across various platforms.",
        "tldr_zh": "该论文介绍了VLA-R1，一个增强推理的视觉-语言-动作模型，它使用来自可验证奖励的强化学习和群体相对策略优化来改进推理和执行，并通过一个新的数据集和跨多个平台的实验进行了验证。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ActiveUMI: Robotic Manipulation with Active Perception from Robot-Free Human Demonstrations",
        "summary": "We present ActiveUMI, a framework for a data collection system that transfers\nin-the-wild human demonstrations to robots capable of complex bimanual\nmanipulation. ActiveUMI couples a portable VR teleoperation kit with sensorized\ncontrollers that mirror the robot's end-effectors, bridging human-robot\nkinematics via precise pose alignment. To ensure mobility and data quality, we\nintroduce several key techniques, including immersive 3D model rendering, a\nself-contained wearable computer, and efficient calibration methods.\nActiveUMI's defining feature is its capture of active, egocentric perception.\nBy recording an operator's deliberate head movements via a head-mounted\ndisplay, our system learns the crucial link between visual attention and\nmanipulation. We evaluate ActiveUMI on six challenging bimanual tasks. Policies\ntrained exclusively on ActiveUMI data achieve an average success rate of 70\\%\non in-distribution tasks and demonstrate strong generalization, retaining a\n56\\% success rate when tested on novel objects and in new environments. Our\nresults demonstrate that portable data collection systems, when coupled with\nlearned active perception, provide an effective and scalable pathway toward\ncreating generalizable and highly capable real-world robot policies.",
        "url": "http://arxiv.org/abs/2510.01607v1",
        "published_date": "2025-10-02T02:44:21+00:00",
        "updated_date": "2025-10-02T02:44:21+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Qiyuan Zeng",
            "Chengmeng Li",
            "Jude St. John",
            "Zhongyi Zhou",
            "Junjie Wen",
            "Guorui Feng",
            "Yichen Zhu",
            "Yi Xu"
        ],
        "tldr": "The paper introduces ActiveUMI, a VR teleoperation system for collecting robot manipulation data with active perception by recording the operator's head movements. Policies trained on this data achieve high success rates and generalization in bimanual tasks.",
        "tldr_zh": "该论文介绍了一种名为ActiveUMI的VR遥操作系统，通过记录操作员的头部运动来收集具有主动感知能力的机器人操作数据。基于此数据训练的策略在双臂操作任务中实现了较高的成功率和泛化能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation",
        "summary": "Robots must adapt to diverse human instructions and operate safely in\nunstructured, open-world environments. Recent Vision-Language models (VLMs)\noffer strong priors for grounding language and perception, but remain difficult\nto steer for navigation due to differences in action spaces and pretraining\nobjectives that hamper transferability to robotics tasks. Towards addressing\nthis, we introduce VENTURA, a vision-language navigation system that finetunes\ninternet-pretrained image diffusion models for path planning. Instead of\ndirectly predicting low-level actions, VENTURA generates a path mask (i.e. a\nvisual plan) in image space that captures fine-grained, context-aware\nnavigation behaviors. A lightweight behavior-cloning policy grounds these\nvisual plans into executable trajectories, yielding an interface that follows\nnatural language instructions to generate diverse robot behaviors. To scale\ntraining, we supervise on path masks derived from self-supervised tracking\nmodels paired with VLM-augmented captions, avoiding manual pixel-level\nannotation or highly engineered data collection setups. In extensive real-world\nevaluations, VENTURA outperforms state-of-the-art foundation model baselines on\nobject reaching, obstacle avoidance, and terrain preference tasks, improving\nsuccess rates by 33% and reducing collisions by 54% across both seen and unseen\nscenarios. Notably, we find that VENTURA generalizes to unseen combinations of\ndistinct tasks, revealing emergent compositional capabilities. Videos, code,\nand additional materials: https://venturapath.github.io",
        "url": "http://arxiv.org/abs/2510.01388v1",
        "published_date": "2025-10-01T19:21:28+00:00",
        "updated_date": "2025-10-01T19:21:28+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Arthur Zhang",
            "Xiangyun Meng",
            "Luca Calliari",
            "Dong-Ki Kim",
            "Shayegan Omidshafiei",
            "Joydeep Biswas",
            "Ali Agha",
            "Amirreza Shaban"
        ],
        "tldr": "VENTURA is a vision-language navigation system that finetunes image diffusion models to generate path masks for robots, outperforming state-of-the-art methods in real-world tasks and demonstrating compositional generalization.",
        "tldr_zh": "VENTURA是一个视觉语言导航系统，通过微调图像扩散模型生成机器人路径掩码，在真实世界的任务中优于现有方法，并展示了组合泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MultiModal Action Conditioned Video Generation",
        "summary": "Current video models fail as world model as they lack fine-graiend control.\nGeneral-purpose household robots require real-time fine motor control to handle\ndelicate tasks and urgent situations. In this work, we introduce fine-grained\nmultimodal actions to capture such precise control. We consider senses of\nproprioception, kinesthesia, force haptics, and muscle activation. Such\nmultimodal senses naturally enables fine-grained interactions that are\ndifficult to simulate with text-conditioned generative models. To effectively\nsimulate fine-grained multisensory actions, we develop a feature learning\nparadigm that aligns these modalities while preserving the unique information\neach modality provides. We further propose a regularization scheme to enhance\ncausality of the action trajectory features in representing intricate\ninteraction dynamics. Experiments show that incorporating multimodal senses\nimproves simulation accuracy and reduces temporal drift. Extensive ablation\nstudies and downstream applications demonstrate the effectiveness and\npracticality of our work.",
        "url": "http://arxiv.org/abs/2510.02287v1",
        "published_date": "2025-10-02T17:57:06+00:00",
        "updated_date": "2025-10-02T17:57:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yichen Li",
            "Antonio Torralba"
        ],
        "tldr": "This paper introduces a multimodal action-conditioned video generation model that leverages proprioception, kinesthesia, force haptics, and muscle activation to enable fine-grained control for robotic tasks, showing improved simulation accuracy and reduced temporal drift.",
        "tldr_zh": "本文介绍了一种多模态动作条件视频生成模型，该模型利用本体感受、动觉、力触觉和肌肉激活来实现机器人任务的精细控制，实验表明该模型提高了仿真精度并减少了时间漂移。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]