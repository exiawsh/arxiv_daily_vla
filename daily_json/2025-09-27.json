[
    {
        "title": "WoW: Towards a World omniscient World model Through Embodied Interaction",
        "summary": "Humans develop an understanding of intuitive physics through active\ninteraction with the world. This approach is in stark contrast to current video\nmodels, such as Sora, which rely on passive observation and therefore struggle\nwith grasping physical causality. This observation leads to our central\nhypothesis: authentic physical intuition of the world model must be grounded in\nextensive, causally rich interactions with the real world. To test this\nhypothesis, we present WoW, a 14-billion-parameter generative world model\ntrained on 2 million robot interaction trajectories. Our findings reveal that\nthe model's understanding of physics is a probabilistic distribution of\nplausible outcomes, leading to stochastic instabilities and physical\nhallucinations. Furthermore, we demonstrate that this emergent capability can\nbe actively constrained toward physical realism by SOPHIA, where\nvision-language model agents evaluate the DiT-generated output and guide its\nrefinement by iteratively evolving the language instructions. In addition, a\nco-trained Inverse Dynamics Model translates these refined plans into\nexecutable robotic actions, thus closing the imagination-to-action loop. We\nestablish WoWBench, a new benchmark focused on physical consistency and causal\nreasoning in video, where WoW achieves state-of-the-art performance in both\nhuman and autonomous evaluation, demonstrating strong ability in physical\ncausality, collision dynamics, and object permanence. Our work provides\nsystematic evidence that large-scale, real-world interaction is a cornerstone\nfor developing physical intuition in AI. Models, data, and benchmarks will be\nopen-sourced.",
        "url": "http://arxiv.org/abs/2509.22642v1",
        "published_date": "2025-09-26T17:59:07+00:00",
        "updated_date": "2025-09-26T17:59:07+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Xiaowei Chi",
            "Peidong Jia",
            "Chun-Kai Fan",
            "Xiaozhu Ju",
            "Weishi Mi",
            "Kevin Zhang",
            "Zhiyuan Qin",
            "Wanxin Tian",
            "Kuangzhi Ge",
            "Hao Li",
            "Zezhong Qian",
            "Anthony Chen",
            "Qiang Zhou",
            "Yueru Jia",
            "Jiaming Liu",
            "Yong Dai",
            "Qingpo Wuwu",
            "Chengyu Bai",
            "Yu-Kai Wang",
            "Ying Li",
            "Lizhang Chen",
            "Yong Bao",
            "Zhiyuan Jiang",
            "Jiacheng Zhu",
            "Kai Tang",
            "Ruichuan An",
            "Yulin Luo",
            "Qiuxuan Feng",
            "Siyuan Zhou",
            "Chi-min Chan",
            "Chengkai Hou",
            "Wei Xue",
            "Sirui Han",
            "Yike Guo",
            "Shanghang Zhang",
            "Jian Tang"
        ],
        "tldr": "This paper introduces WoW, a large-scale generative world model trained with robot interaction data, demonstrating improved physical intuition and causal reasoning in video through active refinement with vision-language models and an inverse dynamics model.",
        "tldr_zh": "该论文介绍了WoW，一个使用机器人交互数据训练的大规模生成世界模型，通过视觉语言模型和逆动力学模型的主动改进，展示了视频中改进的物理直觉和因果推理。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "FantasyWorld: Geometry-Consistent World Modeling via Unified Video and 3D Prediction",
        "summary": "High-quality 3D world models are pivotal for embodied intelligence and\nArtificial General Intelligence (AGI), underpinning applications such as AR/VR\ncontent creation and robotic navigation. Despite the established strong\nimaginative priors, current video foundation models lack explicit 3D grounding\ncapabilities, thus being limited in both spatial consistency and their utility\nfor downstream 3D reasoning tasks. In this work, we present FantasyWorld, a\ngeometry-enhanced framework that augments frozen video foundation models with a\ntrainable geometric branch, enabling joint modeling of video latents and an\nimplicit 3D field in a single forward pass. Our approach introduces\ncross-branch supervision, where geometry cues guide video generation and video\npriors regularize 3D prediction, thus yielding consistent and generalizable\n3D-aware video representations. Notably, the resulting latents from the\ngeometric branch can potentially serve as versatile representations for\ndownstream 3D tasks such as novel view synthesis and navigation, without\nrequiring per-scene optimization or fine-tuning. Extensive experiments show\nthat FantasyWorld effectively bridges video imagination and 3D perception,\noutperforming recent geometry-consistent baselines in multi-view coherence and\nstyle consistency. Ablation studies further confirm that these gains stem from\nthe unified backbone and cross-branch information exchange.",
        "url": "http://arxiv.org/abs/2509.21657v1",
        "published_date": "2025-09-25T22:24:23+00:00",
        "updated_date": "2025-09-25T22:24:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yixiang Dai",
            "Fan Jiang",
            "Chiyu Wang",
            "Mu Xu",
            "Yonggang Qi"
        ],
        "tldr": "FantasyWorld enhances video foundation models with a geometric branch for joint video and 3D modeling, achieving geometry-consistent video representations without per-scene optimization and demonstrating improved multi-view coherence.",
        "tldr_zh": "FantasyWorld通过几何分支增强视频基础模型，实现视频和3D的联合建模。它无需针对特定场景进行优化，即可生成几何一致的视频表示，并展示了改进的多视角连贯性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Residual Vector Quantization For Communication-Efficient Multi-Agent Perception",
        "summary": "Multi-agent collaborative perception (CP) improves scene understanding by\nsharing information across connected agents such as autonomous vehicles,\nunmanned aerial vehicles, and robots. Communication bandwidth, however,\nconstrains scalability. We present ReVQom, a learned feature codec that\npreserves spatial identity while compressing intermediate features. ReVQom is\nan end-to-end method that compresses feature dimensions via a simple bottleneck\nnetwork followed by multi-stage residual vector quantization (RVQ). This allows\nonly per-pixel code indices to be transmitted, reducing payloads from 8192 bits\nper pixel (bpp) of uncompressed 32-bit float features to 6-30 bpp per agent\nwith minimal accuracy loss. On DAIR-V2X real-world CP dataset, ReVQom achieves\n273x compression at 30 bpp to 1365x compression at 6 bpp. At 18 bpp (455x),\nReVQom matches or outperforms raw-feature CP, and at 6-12 bpp it enables\nultra-low-bandwidth operation with graceful degradation. ReVQom allows\nefficient and accurate multi-agent collaborative perception with a step toward\npractical V2X deployment.",
        "url": "http://arxiv.org/abs/2509.21464v1",
        "published_date": "2025-09-25T19:30:27+00:00",
        "updated_date": "2025-09-25T19:30:27+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Dereje Shenkut",
            "B. V. K Vijaya Kumar"
        ],
        "tldr": "The paper introduces ReVQom, a residual vector quantization method for compressing intermediate features in multi-agent collaborative perception, achieving significant bandwidth reduction with minimal accuracy loss.",
        "tldr_zh": "该论文介绍了 ReVQom，一种残差向量量化方法，用于压缩多智能体协同感知中的中间特征，从而在最小化精度损失的同时显著降低带宽。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Pixel Motion Diffusion is What We Need for Robot Control",
        "summary": "We present DAWN (Diffusion is All We Need for robot control), a unified\ndiffusion-based framework for language-conditioned robotic manipulation that\nbridges high-level motion intent and low-level robot action via structured\npixel motion representation. In DAWN, both the high-level and low-level\ncontrollers are modeled as diffusion processes, yielding a fully trainable,\nend-to-end system with interpretable intermediate motion abstractions. DAWN\nachieves state-of-the-art results on the challenging CALVIN benchmark,\ndemonstrating strong multi-task performance, and further validates its\neffectiveness on MetaWorld. Despite the substantial domain gap between\nsimulation and reality and limited real-world data, we demonstrate reliable\nreal-world transfer with only minimal finetuning, illustrating the practical\nviability of diffusion-based motion abstractions for robotic control. Our\nresults show the effectiveness of combining diffusion modeling with\nmotion-centric representations as a strong baseline for scalable and robust\nrobot learning. Project page: https://nero1342.github.io/DAWN/",
        "url": "http://arxiv.org/abs/2509.22652v1",
        "published_date": "2025-09-26T17:59:59+00:00",
        "updated_date": "2025-09-26T17:59:59+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "E-Ro Nguyen",
            "Yichi Zhang",
            "Kanchana Ranasinghe",
            "Xiang Li",
            "Michael S. Ryoo"
        ],
        "tldr": "The paper introduces DAWN, a diffusion-based framework for language-conditioned robotic manipulation using pixel motion representation, achieving state-of-the-art results and demonstrating real-world transferability.",
        "tldr_zh": "本文介绍了DAWN，一个基于扩散模型的语言条件机器人操作框架，使用像素运动表示，实现了最先进的结果，并展示了现实世界的迁移能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation",
        "summary": "We present See, Point, Fly (SPF), a training-free aerial vision-and-language\nnavigation (AVLN) framework built atop vision-language models (VLMs). SPF is\ncapable of navigating to any goal based on any type of free-form instructions\nin any kind of environment. In contrast to existing VLM-based approaches that\ntreat action prediction as a text generation task, our key insight is to\nconsider action prediction for AVLN as a 2D spatial grounding task. SPF\nharnesses VLMs to decompose vague language instructions into iterative\nannotation of 2D waypoints on the input image. Along with the predicted\ntraveling distance, SPF transforms predicted 2D waypoints into 3D displacement\nvectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the\ntraveling distance to facilitate more efficient navigation. Notably, SPF\nperforms navigation in a closed-loop control manner, enabling UAVs to follow\ndynamic targets in dynamic environments. SPF sets a new state of the art in DRL\nsimulation benchmark, outperforming the previous best method by an absolute\nmargin of 63%. In extensive real-world evaluations, SPF outperforms strong\nbaselines by a large margin. We also conduct comprehensive ablation studies to\nhighlight the effectiveness of our design choice. Lastly, SPF shows remarkable\ngeneralization to different VLMs. Project page: https://spf-web.pages.dev",
        "url": "http://arxiv.org/abs/2509.22653v1",
        "published_date": "2025-09-26T17:59:59+00:00",
        "updated_date": "2025-09-26T17:59:59+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Chih Yao Hu",
            "Yang-Sen Lin",
            "Yuna Lee",
            "Chih-Hai Su",
            "Jie-Ying Lee",
            "Shr-Ruei Tsai",
            "Chin-Yang Lin",
            "Kuan-Wen Chen",
            "Tsung-Wei Ke",
            "Yu-Lun Liu"
        ],
        "tldr": "The paper introduces See, Point, Fly (SPF), a training-free vision-language navigation framework for UAVs that uses VLMs to decompose instructions into 2D waypoints, achieving state-of-the-art results in both simulation and real-world environments.",
        "tldr_zh": "该论文介绍了See, Point, Fly (SPF)，一个无需训练的无人机视觉语言导航框架，该框架使用视觉语言模型将指令分解为2D航点，并在仿真和真实环境中都取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CCNeXt: An Effective Self-Supervised Stereo Depth Estimation Approach",
        "summary": "Depth Estimation plays a crucial role in recent applications in robotics,\nautonomous vehicles, and augmented reality. These scenarios commonly operate\nunder constraints imposed by computational power. Stereo image pairs offer an\neffective solution for depth estimation since it only needs to estimate the\ndisparity of pixels in image pairs to determine the depth in a known rectified\nsystem. Due to the difficulty in acquiring reliable ground-truth depth data\nacross diverse scenarios, self-supervised techniques emerge as a solution,\nparticularly when large unlabeled datasets are available. We propose a novel\nself-supervised convolutional approach that outperforms existing\nstate-of-the-art Convolutional Neural Networks (CNNs) and Vision Transformers\n(ViTs) while balancing computational cost. The proposed CCNeXt architecture\nemploys a modern CNN feature extractor with a novel windowed epipolar\ncross-attention module in the encoder, complemented by a comprehensive redesign\nof the depth estimation decoder. Our experiments demonstrate that CCNeXt\nachieves competitive metrics on the KITTI Eigen Split test data while being\n10.18$\\times$ faster than the current best model and achieves state-of-the-art\nresults in all metrics in the KITTI Eigen Split Improved Ground Truth and\nDriving Stereo datasets when compared to recently proposed techniques. To\nensure complete reproducibility, our project is accessible at\n\\href{https://github.com/alelopes/CCNext}{\\texttt{https://github.com/alelopes/CCNext}}.",
        "url": "http://arxiv.org/abs/2509.22627v1",
        "published_date": "2025-09-26T17:51:28+00:00",
        "updated_date": "2025-09-26T17:51:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Alexandre Lopes",
            "Roberto Souza",
            "Helio Pedrini"
        ],
        "tldr": "This paper introduces CCNeXt, a novel self-supervised convolutional approach for stereo depth estimation that achieves state-of-the-art results on KITTI datasets with improved computational efficiency compared to existing CNNs and ViTs.",
        "tldr_zh": "本文介绍了一种名为CCNeXt的新型自监督卷积立体深度估计方法，该方法在KITTI数据集上取得了最先进的结果，并且与现有的CNN和ViT相比，计算效率更高。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EfficientDepth: A Fast and Detail-Preserving Monocular Depth Estimation Model",
        "summary": "Monocular depth estimation (MDE) plays a pivotal role in various computer\nvision applications, such as robotics, augmented reality, and autonomous\ndriving. Despite recent advancements, existing methods often fail to meet key\nrequirements for 3D reconstruction and view synthesis, including geometric\nconsistency, fine details, robustness to real-world challenges like reflective\nsurfaces, and efficiency for edge devices. To address these challenges, we\nintroduce a novel MDE system, called EfficientDepth, which combines a\ntransformer architecture with a lightweight convolutional decoder, as well as a\nbimodal density head that allows the network to estimate detailed depth maps.\nWe train our model on a combination of labeled synthetic and real images, as\nwell as pseudo-labeled real images, generated using a high-performing MDE\nmethod. Furthermore, we employ a multi-stage optimization strategy to improve\ntraining efficiency and produce models that emphasize geometric consistency and\nfine detail. Finally, in addition to commonly used objectives, we introduce a\nloss function based on LPIPS to encourage the network to produce detailed depth\nmaps. Experimental results demonstrate that EfficientDepth achieves performance\ncomparable to or better than existing state-of-the-art models, with\nsignificantly reduced computational resources.",
        "url": "http://arxiv.org/abs/2509.22527v1",
        "published_date": "2025-09-26T16:05:43+00:00",
        "updated_date": "2025-09-26T16:05:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Andrii Litvynchuk",
            "Ivan Livinsky",
            "Anand Ravi",
            "Nima Kalantari",
            "Andrii Tsarov"
        ],
        "tldr": "EfficientDepth is a new monocular depth estimation model combining a transformer architecture with a lightweight convolutional decoder and a bimodal density head, achieving state-of-the-art performance with significantly reduced computational resources.",
        "tldr_zh": "EfficientDepth是一种新的单目深度估计模型，它结合了Transformer架构、轻量级卷积解码器和双峰密度头，以显著降低的计算资源实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation",
        "summary": "The safety and reliability of embodied agents rely on accurate and unbiased\nvisual perception. However, existing benchmarks mainly emphasize generalization\nand robustness under perturbations, while systematic quantification of visual\nbias remains scarce. This gap limits a deeper understanding of how perception\ninfluences decision-making stability. To address this issue, we propose\nRoboView-Bias, the first benchmark specifically designed to systematically\nquantify visual bias in robotic manipulation, following a principle of factor\nisolation. Leveraging a structured variant-generation framework and a\nperceptual-fairness validation protocol, we create 2,127 task instances that\nenable robust measurement of biases induced by individual visual factors and\ntheir interactions. Using this benchmark, we systematically evaluate three\nrepresentative embodied agents across two prevailing paradigms and report three\nkey findings: (i) all agents exhibit significant visual biases, with camera\nviewpoint being the most critical factor; (ii) agents achieve their highest\nsuccess rates on highly saturated colors, indicating inherited visual\npreferences from underlying VLMs; and (iii) visual biases show strong,\nasymmetric coupling, with viewpoint strongly amplifying color-related bias.\nFinally, we demonstrate that a mitigation strategy based on a semantic\ngrounding layer substantially reduces visual bias by approximately 54.5\\% on\nMOKA. Our results highlight that systematic analysis of visual bias is a\nprerequisite for developing safe and reliable general-purpose embodied agents.",
        "url": "http://arxiv.org/abs/2509.22356v1",
        "published_date": "2025-09-26T13:53:25+00:00",
        "updated_date": "2025-09-26T13:53:25+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Enguang Liu",
            "Siyuan Liang",
            "Liming Lu",
            "Xiyu Zeng",
            "Xiaochun Cao",
            "Aishan Liu",
            "Shuchao Pang"
        ],
        "tldr": "The paper introduces RoboView-Bias, a novel benchmark for quantifying visual biases in embodied agents for robotic manipulation, revealing significant biases and proposing a mitigation strategy.",
        "tldr_zh": "本文介绍了RoboView-Bias，这是一个新的基准，用于量化具身智能体在机器人操作中的视觉偏差，揭示了显著的偏差，并提出了一种缓解策略。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniMapGen: A Generative Framework for Large-Scale Map Construction from Multi-modal Data",
        "summary": "Large-scale map construction is foundational for critical applications such\nas autonomous driving and navigation systems. Traditional large-scale map\nconstruction approaches mainly rely on costly and inefficient special data\ncollection vehicles and labor-intensive annotation processes. While existing\nsatellite-based methods have demonstrated promising potential in enhancing the\nefficiency and coverage of map construction, they exhibit two major\nlimitations: (1) inherent drawbacks of satellite data (e.g., occlusions,\noutdatedness) and (2) inefficient vectorization from perception-based methods,\nresulting in discontinuous and rough roads that require extensive\npost-processing. This paper presents a novel generative framework, UniMapGen,\nfor large-scale map construction, offering three key innovations: (1)\nrepresenting lane lines as \\textbf{discrete sequence} and establishing an\niterative strategy to generate more complete and smooth map vectors than\ntraditional perception-based methods. (2) proposing a flexible architecture\nthat supports \\textbf{multi-modal} inputs, enabling dynamic selection among\nBEV, PV, and text prompt, to overcome the drawbacks of satellite data. (3)\ndeveloping a \\textbf{state update} strategy for global continuity and\nconsistency of the constructed large-scale map. UniMapGen achieves\nstate-of-the-art performance on the OpenSatMap dataset. Furthermore, UniMapGen\ncan infer occluded roads and predict roads missing from dataset annotations.\nOur code will be released.",
        "url": "http://arxiv.org/abs/2509.22262v1",
        "published_date": "2025-09-26T12:26:33+00:00",
        "updated_date": "2025-09-26T12:26:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yujian Yuan",
            "Changjie Wu",
            "Xinyuan Chang",
            "Sijin Wang",
            "Hang Zhang",
            "Shiyi Liang",
            "Shuang Zeng",
            "Mu Xu"
        ],
        "tldr": "UniMapGen is a novel generative framework for large-scale map construction from multi-modal data (BEV, PV, text) that addresses limitations of satellite-based methods by representing lane lines as discrete sequences and using a state update strategy for global consistency.",
        "tldr_zh": "UniMapGen是一个新的生成式框架，用于从多模态数据（BEV，PV，文本）构建大规模地图，通过将车道线表示为离散序列并使用状态更新策略以实现全局一致性，从而解决了基于卫星的方法的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Self-Supervised Point Cloud Completion based on Multi-View Augmentations of Single Partial Point Cloud",
        "summary": "Point cloud completion aims to reconstruct complete shapes from partial\nobservations. Although current methods have achieved remarkable performance,\nthey still have some limitations: Supervised methods heavily rely on ground\ntruth, which limits their generalization to real-world datasets due to the\nsynthetic-to-real domain gap. Unsupervised methods require complete point\nclouds to compose unpaired training data, and weakly-supervised methods need\nmulti-view observations of the object. Existing self-supervised methods\nfrequently produce unsatisfactory predictions due to the limited capabilities\nof their self-supervised signals. To overcome these challenges, we propose a\nnovel self-supervised point cloud completion method. We design a set of novel\nself-supervised signals based on multi-view augmentations of the single partial\npoint cloud. Additionally, to enhance the model's learning ability, we first\nincorporate Mamba into self-supervised point cloud completion task, encouraging\nthe model to generate point clouds with better quality. Experiments on\nsynthetic and real-world datasets demonstrate that our method achieves\nstate-of-the-art results.",
        "url": "http://arxiv.org/abs/2509.22132v1",
        "published_date": "2025-09-26T09:53:55+00:00",
        "updated_date": "2025-09-26T09:53:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingjing Lu",
            "Huilong Pi",
            "Yunchuan Qin",
            "Zhuo Tang",
            "Ruihui Li"
        ],
        "tldr": "This paper introduces a novel self-supervised point cloud completion method using multi-view augmentations and Mamba to overcome limitations of existing supervised, unsupervised, and weakly-supervised approaches, achieving state-of-the-art results.",
        "tldr_zh": "本文提出了一种新的自监督点云补全方法，该方法利用多视角增强和 Mamba 来克服现有监督、无监督和弱监督方法的局限性，并实现了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DynaNav: Dynamic Feature and Layer Selection for Efficient Visual Navigation",
        "summary": "Visual navigation is essential for robotics and embodied AI. However,\nexisting foundation models, particularly those with transformer decoders,\nsuffer from high computational overhead and lack interpretability, limiting\ntheir deployment in resource-tight scenarios. To address this, we propose\nDynaNav, a Dynamic Visual Navigation framework that adapts feature and layer\nselection based on scene complexity. It employs a trainable hard feature\nselector for sparse operations, enhancing efficiency and interpretability.\nAdditionally, we integrate feature selection into an early-exit mechanism, with\nBayesian Optimization determining optimal exit thresholds to reduce\ncomputational cost. Extensive experiments in real-world-based datasets and\nsimulated environments demonstrate the effectiveness of DynaNav. Compared to\nViNT, DynaNav achieves a 2.26x reduction in FLOPs, 42.3% lower inference time,\nand 32.8% lower memory usage, while improving navigation performance across\nfour public datasets.",
        "url": "http://arxiv.org/abs/2509.21930v1",
        "published_date": "2025-09-26T06:15:31+00:00",
        "updated_date": "2025-09-26T06:15:31+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Jiahui Wang",
            "Changhao Chen"
        ],
        "tldr": "DynaNav introduces a dynamic feature and layer selection framework for efficient visual navigation, achieving significant FLOPs reduction, faster inference, and lower memory usage while improving performance compared to ViNT.",
        "tldr_zh": "DynaNav 引入了一个动态特征和层选择框架，用于高效的视觉导航，与 ViNT 相比，实现了显著的 FLOPs 减少、更快的推理速度和更低的内存使用，同时提高了性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation",
        "summary": "Embodied action planning is a core challenge in robotics, requiring models to\ngenerate precise actions from visual observations and language instructions.\nWhile video generation world models are promising, their reliance on\npixel-level reconstruction often introduces visual redundancies that hinder\naction decoding and generalization. Latent world models offer a compact,\nmotion-aware representation, but overlook the fine-grained details critical for\nprecise manipulation. To overcome these limitations, we propose MoWM, a\nmixture-of-world-model framework that fuses representations from hybrid world\nmodels for embodied action planning. Our approach uses motion-aware\nrepresentations from a latent model as a high-level prior, which guides the\nextraction of fine-grained visual features from the pixel space model. This\ndesign allows MoWM to highlight the informative visual details needed for\naction decoding. Extensive evaluations on the CALVIN benchmark demonstrate that\nour method achieves state-of-the-art task success rates and superior\ngeneralization. We also provide a comprehensive analysis of the strengths of\neach feature space, offering valuable insights for future research in embodied\nplanning. The code is available at: https://github.com/tsinghua-fib-lab/MoWM.",
        "url": "http://arxiv.org/abs/2509.21797v1",
        "published_date": "2025-09-26T02:54:36+00:00",
        "updated_date": "2025-09-26T02:54:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yu Shang",
            "Yangcheng Yu",
            "Xin Zhang",
            "Xin Jin",
            "Haisheng Su",
            "Wei Wu",
            "Yong Li"
        ],
        "tldr": "The paper introduces MoWM, a mixture-of-world-model framework that combines latent and pixel-space world models for improved embodied action planning, achieving state-of-the-art results on the CALVIN benchmark.",
        "tldr_zh": "该论文介绍了MoWM，一个混合世界模型的框架，结合了潜在空间和像素空间的世界模型，以改进具身动作规划，并在CALVIN基准测试上取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE",
        "summary": "Video-based world models hold significant potential for generating\nhigh-quality embodied manipulation data. However, current video generation\nmethods struggle to achieve stable long-horizon generation: classical\ndiffusion-based approaches often suffer from temporal inconsistency and visual\ndrift over multiple rollouts, while autoregressive methods tend to compromise\non visual detail. To solve this, we introduce LongScape, a hybrid framework\nthat adaptively combines intra-chunk diffusion denoising with inter-chunk\nautoregressive causal generation. Our core innovation is an action-guided,\nvariable-length chunking mechanism that partitions video based on the semantic\ncontext of robotic actions. This ensures each chunk represents a complete,\ncoherent action, enabling the model to flexibly generate diverse dynamics. We\nfurther introduce a Context-aware Mixture-of-Experts (CMoE) framework that\nadaptively activates specialized experts for each chunk during generation,\nguaranteeing high visual quality and seamless chunk transitions. Extensive\nexperimental results demonstrate that our method achieves stable and consistent\nlong-horizon generation over extended rollouts. Our code is available at:\nhttps://github.com/tsinghua-fib-lab/Longscape.",
        "url": "http://arxiv.org/abs/2509.21790v1",
        "published_date": "2025-09-26T02:47:05+00:00",
        "updated_date": "2025-09-26T02:47:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yu Shang",
            "Lei Jin",
            "Yiding Ma",
            "Xin Zhang",
            "Chen Gao",
            "Wei Wu",
            "Yong Li"
        ],
        "tldr": "LongScape introduces a hybrid video generation framework using action-guided chunking and context-aware Mixture-of-Experts (CMoE) to achieve stable and consistent long-horizon generation for embodied manipulation, addressing limitations of existing diffusion and autoregressive methods.",
        "tldr_zh": "LongScape 引入了一种混合视频生成框架，该框架使用动作引导的分块和上下文感知的混合专家 (CMoE)，以实现稳定且一致的具身操作长时程生成，解决了现有扩散和自回归方法的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "What Happens Next? Anticipating Future Motion by Generating Point Trajectories",
        "summary": "We consider the problem of forecasting motion from a single image, i.e.,\npredicting how objects in the world are likely to move, without the ability to\nobserve other parameters such as the object velocities or the forces applied to\nthem. We formulate this task as conditional generation of dense trajectory\ngrids with a model that closely follows the architecture of modern video\ngenerators but outputs motion trajectories instead of pixels. This approach\ncaptures scene-wide dynamics and uncertainty, yielding more accurate and\ndiverse predictions than prior regressors and generators. We extensively\nevaluate our method on simulated data, demonstrate its effectiveness on\ndownstream applications such as robotics, and show promising accuracy on\nreal-world intuitive physics datasets. Although recent state-of-the-art video\ngenerators are often regarded as world models, we show that they struggle with\nforecasting motion from a single image, even in simple physical scenarios such\nas falling blocks or mechanical object interactions, despite fine-tuning on\nsuch data. We show that this limitation arises from the overhead of generating\npixels rather than directly modeling motion.",
        "url": "http://arxiv.org/abs/2509.21592v1",
        "published_date": "2025-09-25T21:03:56+00:00",
        "updated_date": "2025-09-25T21:03:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Gabrijel Boduljak",
            "Laurynas Karazija",
            "Iro Laina",
            "Christian Rupprecht",
            "Andrea Vedaldi"
        ],
        "tldr": "The paper introduces a method for forecasting object motion from a single image by generating dense trajectory grids, outperforming video generators by directly modeling motion instead of pixels. It demonstrates effectiveness on simulated data, robotics applications, and real-world physics datasets.",
        "tldr_zh": "本文提出了一种从单张图像预测物体运动的方法，通过生成密集轨迹网格，直接建模运动而非像素，优于视频生成器。它在模拟数据、机器人应用和真实物理数据集上展示了有效性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]