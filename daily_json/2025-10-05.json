[
    {
        "title": "LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization",
        "summary": "LIBERO has emerged as a widely adopted benchmark for evaluating\nVision-Language-Action (VLA) models; however, its current training and\nevaluation settings are problematic, often leading to inflated performance\nestimates and preventing fair model comparison. To address these issues, we\nintroduce LIBERO-PRO, an extended LIBERO benchmark that systematically\nevaluates model performance under reasonable perturbations across four\ndimensions: manipulated objects, initial states, task instructions, and\nenvironments. Experimental results reveal that, although existing models\nachieve over 90% accuracy under the standard LIBERO evaluation, their\nperformance collapses to 0.0% under our generalized setting. Crucially, this\ndiscrepancy exposes the models' reliance on rote memorization of action\nsequences and environment layouts from the training set, rather than genuine\ntask understanding or environmental perception. For instance, models persist in\nexecuting grasping actions when the target object is replaced with irrelevant\nitems, and their outputs remain unchanged even when given corrupted\ninstructions or even messy tokens. These findings expose the severe flaws in\ncurrent evaluation practices, and we call on the community to abandon\nmisleading methodologies in favor of robust assessments of model generalization\nand comprehension. Our code is available at:\nhttps://github.com/Zxy-MLlab/LIBERO-PRO.",
        "url": "http://arxiv.org/abs/2510.03827v1",
        "published_date": "2025-10-04T14:56:40+00:00",
        "updated_date": "2025-10-04T14:56:40+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Xueyang Zhou",
            "Yangming Xu",
            "Guiyao Tie",
            "Yongchao Chen",
            "Guowen Zhang",
            "Duanfeng Chu",
            "Pan Zhou",
            "Lichao Sun"
        ],
        "tldr": "The paper introduces LIBERO-PRO, a more robust benchmark for Vision-Language-Action models that exposes the limitations of existing models in generalization and comprehension by demonstrating their reliance on memorization rather than genuine understanding.",
        "tldr_zh": "该论文介绍了LIBERO-PRO，一个更强大的视觉-语言-动作模型基准，通过揭示现有模型对记忆的依赖而非真正的理解，从而暴露了它们在泛化和理解方面的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EmbodiSwap for Zero-Shot Robot Imitation Learning",
        "summary": "We introduce EmbodiSwap - a method for producing photorealistic synthetic\nrobot overlays over human video. We employ EmbodiSwap for zero-shot imitation\nlearning, bridging the embodiment gap between in-the-wild ego-centric human\nvideo and a target robot embodiment. We train a closed-loop robot manipulation\npolicy over the data produced by EmbodiSwap. We make novel use of V-JEPA as a\nvisual backbone, repurposing V-JEPA from the domain of video understanding to\nimitation learning over synthetic robot videos. Adoption of V-JEPA outperforms\nalternative vision backbones more conventionally used within robotics. In\nreal-world tests, our zero-shot trained V-JEPA model achieves an $82\\%$ success\nrate, outperforming a few-shot trained $\\pi_0$ network as well as $\\pi_0$\ntrained over data produced by EmbodiSwap. We release (i) code for generating\nthe synthetic robot overlays which takes as input human videos and an arbitrary\nrobot URDF and generates a robot dataset, (ii) the robot dataset we synthesize\nover EPIC-Kitchens, HOI4D and Ego4D, and (iii) model checkpoints and inference\ncode, to facilitate reproducible research and broader adoption.",
        "url": "http://arxiv.org/abs/2510.03706v1",
        "published_date": "2025-10-04T07:11:20+00:00",
        "updated_date": "2025-10-04T07:11:20+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Eadom Dessalene",
            "Pavan Mantripragada",
            "Michael Maynord",
            "Yiannis Aloimonos"
        ],
        "tldr": "The paper introduces EmbodiSwap, a method for generating synthetic robot overlays on human videos for zero-shot robot imitation learning, leveraging V-JEPA as a visual backbone, achieving promising real-world results and releasing code, datasets, and models.",
        "tldr_zh": "该论文介绍了EmbodiSwap，一种在人类视频上生成合成机器人覆盖层的方法，用于零样本机器人模仿学习，利用V-JEPA作为视觉主干，实现了有希望的真实世界结果，并发布了代码、数据集和模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Referring Expression Comprehension for Small Objects",
        "summary": "Referring expression comprehension (REC) aims to localize the target object\ndescribed by a natural language expression. Recent advances in vision-language\nlearning have led to significant performance improvements in REC tasks.\nHowever, localizing extremely small objects remains a considerable challenge\ndespite its importance in real-world applications such as autonomous driving.\nTo address this issue, we introduce a novel dataset and method for REC\ntargeting small objects. First, we present the small object REC (SOREC)\ndataset, which consists of 100,000 pairs of referring expressions and\ncorresponding bounding boxes for small objects in driving scenarios. Second, we\npropose the progressive-iterative zooming adapter (PIZA), an adapter module for\nparameter-efficient fine-tuning that enables models to progressively zoom in\nand localize small objects. In a series of experiments, we apply PIZA to\nGroundingDINO and demonstrate a significant improvement in accuracy on the\nSOREC dataset. Our dataset, codes and pre-trained models are publicly available\non the project page.",
        "url": "http://arxiv.org/abs/2510.03701v1",
        "published_date": "2025-10-04T06:50:02+00:00",
        "updated_date": "2025-10-04T06:50:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kanoko Goto",
            "Takumi Hirose",
            "Mahiro Ukai",
            "Shuhei Kurita",
            "Nakamasa Inoue"
        ],
        "tldr": "This paper introduces a new dataset (SOREC) and method (PIZA) for referring expression comprehension of small objects in driving scenarios, demonstrating improved accuracy over existing methods when applied to GroundingDINO.",
        "tldr_zh": "本文介绍了一个新的数据集（SOREC）和方法（PIZA），用于驾驶场景中小物体的指代表达式理解，并展示了将其应用于GroundingDINO时相对于现有方法的准确性提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SketchPlan: Diffusion Based Drone Planning From Human Sketches",
        "summary": "We propose SketchPlan, a diffusion-based planner that interprets 2D\nhand-drawn sketches over depth images to generate 3D flight paths for drone\nnavigation. SketchPlan comprises two components: a SketchAdapter that learns to\nmap the human sketches to projected 2D paths, and DiffPath, a diffusion model\nthat infers 3D trajectories from 2D projections and a first person view depth\nimage. Our model achieves zero-shot sim-to-real transfer, generating accurate\nand safe flight paths in previously unseen real-world environments. To train\nthe model, we build a synthetic dataset of 32k flight paths using a diverse set\nof photorealistic 3D Gaussian Splatting scenes. We automatically label the data\nby computing 2D projections of the 3D flight paths onto the camera plane, and\nuse this to train the DiffPath diffusion model. However, since real human 2D\nsketches differ significantly from ideal 2D projections, we additionally label\n872 of the 3D flight paths with real human sketches and use this to train the\nSketchAdapter to infer the 2D projection from the human sketch. We demonstrate\nSketchPlan's effectiveness in both simulated and real-world experiments, and\nshow through ablations that training on a mix of human labeled and auto-labeled\ndata together with a modular design significantly boosts its capabilities to\ncorrectly interpret human intent and infer 3D paths. In real-world drone tests,\nSketchPlan achieved 100\\% success in low/medium clutter and 40\\% in unseen\nhigh-clutter environments, outperforming key ablations by 20-60\\% in task\ncompletion.",
        "url": "http://arxiv.org/abs/2510.03545v1",
        "published_date": "2025-10-03T22:31:24+00:00",
        "updated_date": "2025-10-03T22:31:24+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Sixten Norelius",
            "Aaron O. Feldman",
            "Mac Schwager"
        ],
        "tldr": "SketchPlan is a diffusion-based drone planner that translates 2D hand-drawn sketches into 3D flight paths using depth images, demonstrating sim-to-real transfer with promising real-world results.",
        "tldr_zh": "SketchPlan是一个基于扩散的无人机规划器，它使用深度图像将2D手绘草图转换为3D飞行路径，展示了从仿真到现实的迁移能力，并具有良好的实际效果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]