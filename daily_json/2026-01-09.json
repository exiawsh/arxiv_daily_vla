[
    {
        "title": "UniLiPs: Unified LiDAR Pseudo-Labeling with Geometry-Grounded Dynamic Scene Decomposition",
        "summary": "Unlabeled LiDAR logs, in autonomous driving applications, are inherently a gold mine of dense 3D geometry hiding in plain sight - yet they are almost useless without human labels, highlighting a dominant cost barrier for autonomous-perception research. In this work we tackle this bottleneck by leveraging temporal-geometric consistency across LiDAR sweeps to lift and fuse cues from text and 2D vision foundation models directly into 3D, without any manual input. We introduce an unsupervised multi-modal pseudo-labeling method relying on strong geometric priors learned from temporally accumulated LiDAR maps, alongside with a novel iterative update rule that enforces joint geometric-semantic consistency, and vice-versa detecting moving objects from inconsistencies. Our method simultaneously produces 3D semantic labels, 3D bounding boxes, and dense LiDAR scans, demonstrating robust generalization across three datasets. We experimentally validate that our method compares favorably to existing semantic segmentation and object detection pseudo-labeling methods, which often require additional manual supervision. We confirm that even a small fraction of our geometrically consistent, densified LiDAR improves depth prediction by 51.5% and 22.0% MAE in the 80-150 and 150-250 meters range, respectively.",
        "url": "http://arxiv.org/abs/2601.05105v1",
        "published_date": "2026-01-08T16:52:28+00:00",
        "updated_date": "2026-01-08T16:52:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Filippo Ghilotti",
            "Samuel Brucker",
            "Nahku Saidy",
            "Matteo Matteucci",
            "Mario Bijelic",
            "Felix Heide"
        ],
        "tldr": "The paper introduces an unsupervised multi-modal pseudo-labeling method for LiDAR data using temporal-geometric consistency and foundation models to generate 3D semantic labels, bounding boxes, and dense LiDAR scans, improving depth prediction.",
        "tldr_zh": "该论文介绍了一种无监督的多模态伪标签方法，利用时间几何一致性和基础模型对激光雷达数据进行处理，生成3D语义标签、边界框和密集的激光雷达扫描，从而提高深度预测。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "UniDrive-WM: Unified Understanding, Planning and Generation World Model For Autonomous Driving",
        "summary": "World models have become central to autonomous driving, where accurate scene understanding and future prediction are crucial for safe control. Recent work has explored using vision-language models (VLMs) for planning, yet existing approaches typically treat perception, prediction, and planning as separate modules. We propose UniDrive-WM, a unified VLM-based world model that jointly performs driving-scene understanding, trajectory planning, and trajectory-conditioned future image generation within a single architecture. UniDrive-WM's trajectory planner predicts a future trajectory, which conditions a VLM-based image generator to produce plausible future frames. These predictions provide additional supervisory signals that enhance scene understanding and iteratively refine trajectory generation. We further compare discrete and continuous output representations for future image prediction, analyzing their influence on downstream driving performance. Experiments on the challenging Bench2Drive benchmark show that UniDrive-WM produces high-fidelity future images and improves planning performance by 5.9% in L2 trajectory error and 9.2% in collision rate over the previous best method. These results demonstrate the advantages of tightly integrating VLM-driven reasoning, planning, and generative world modeling for autonomous driving. The project page is available at https://unidrive-wm.github.io/UniDrive-WM .",
        "url": "http://arxiv.org/abs/2601.04453v1",
        "published_date": "2026-01-07T23:49:52+00:00",
        "updated_date": "2026-01-07T23:49:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhexiao Xiong",
            "Xin Ye",
            "Burhan Yaman",
            "Sheng Cheng",
            "Yiren Lu",
            "Jingru Luo",
            "Nathan Jacobs",
            "Liu Ren"
        ],
        "tldr": "UniDrive-WM is a unified VLM-based world model for autonomous driving that jointly performs scene understanding, trajectory planning, and future image generation, achieving state-of-the-art performance on the Bench2Drive benchmark.",
        "tldr_zh": "UniDrive-WM是一个统一的、基于VLM的自动驾驶世界模型，它联合执行场景理解、轨迹规划和未来图像生成，并在Bench2Drive基准测试中实现了最先进的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "3D-Agent:Tri-Modal Multi-Agent Collaboration for Scalable 3D Object Annotation",
        "summary": "Driven by applications in autonomous driving robotics and augmented reality 3D object annotation presents challenges beyond 2D annotation including spatial complexity occlusion and viewpoint inconsistency Existing approaches based on single models often struggle to address these issues effectively We propose Tri MARF a novel framework that integrates tri modal inputs including 2D multi view images textual descriptions and 3D point clouds within a multi agent collaborative architecture to enhance large scale 3D annotation Tri MARF consists of three specialized agents a vision language model agent for generating multi view descriptions an information aggregation agent for selecting optimal descriptions and a gating agent that aligns textual semantics with 3D geometry for refined captioning Extensive experiments on Objaverse LVIS Objaverse XL and ABO demonstrate that Tri MARF substantially outperforms existing methods achieving a CLIPScore of 88 point 7 compared to prior state of the art methods retrieval accuracy of 45 point 2 and 43 point 8 on ViLT R at 5 and a throughput of up to 12000 objects per hour on a single NVIDIA A100 GPU",
        "url": "http://arxiv.org/abs/2601.04404v1",
        "published_date": "2026-01-07T21:23:05+00:00",
        "updated_date": "2026-01-07T21:23:05+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jusheng Zhang",
            "Yijia Fan",
            "Zimo Wen",
            "Jian Wang",
            "Keze Wang"
        ],
        "tldr": "The paper introduces Tri-MARF, a multi-agent framework that integrates 2D multi-view images, textual descriptions, and 3D point clouds for scalable 3D object annotation, achieving state-of-the-art performance and high throughput.",
        "tldr_zh": "该论文介绍了一种名为Tri-MARF的多智能体框架，它集成了2D多视角图像、文本描述和3D点云，用于可扩展的3D对象标注，实现了最先进的性能和高吞吐量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
        "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\n  We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
        "url": "http://arxiv.org/abs/2601.05172v1",
        "published_date": "2026-01-08T17:59:42+00:00",
        "updated_date": "2026-01-08T17:59:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Haoyu Zhao",
            "Akide Liu",
            "Zeyu Zhang",
            "Weijie Wang",
            "Feng Chen",
            "Ruihan Zhu",
            "Gholamreza Haffari",
            "Bohan Zhuang"
        ],
        "tldr": "The paper introduces Chain-of-View (CoV), a training-free framework for embodied question answering that enables VLMs to actively select viewpoints for improved spatial reasoning in 3D environments, showing significant performance gains on multiple benchmarks.",
        "tldr_zh": "该论文介绍了Chain-of-View (CoV)，一种无需训练的具身问答框架，使VLM能够主动选择视角，从而提高在3D环境中的空间推理能力，并在多个基准测试中表现出显著的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Driving on Registers",
        "summary": "We present DrivoR, a simple and efficient transformer-based architecture for end-to-end autonomous driving. Our approach builds on pretrained Vision Transformers (ViTs) and introduces camera-aware register tokens that compress multi-camera features into a compact scene representation, significantly reducing downstream computation without sacrificing accuracy. These tokens drive two lightweight transformer decoders that generate and then score candidate trajectories. The scoring decoder learns to mimic an oracle and predicts interpretable sub-scores representing aspects such as safety, comfort, and efficiency, enabling behavior-conditioned driving at inference. Despite its minimal design, DrivoR outperforms or matches strong contemporary baselines across NAVSIM-v1, NAVSIM-v2, and the photorealistic closed-loop HUGSIM benchmark. Our results show that a pure-transformer architecture, combined with targeted token compression, is sufficient for accurate, efficient, and adaptive end-to-end driving. Code and checkpoints will be made available via the project page.",
        "url": "http://arxiv.org/abs/2601.05083v1",
        "published_date": "2026-01-08T16:28:24+00:00",
        "updated_date": "2026-01-08T16:28:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Ellington Kirby",
            "Alexandre Boulch",
            "Yihong Xu",
            "Yuan Yin",
            "Gilles Puy",
            "Éloi Zablocki",
            "Andrei Bursuc",
            "Spyros Gidaris",
            "Renaud Marlet",
            "Florent Bartoccioni",
            "Anh-Quan Cao",
            "Nermin Samet",
            "Tuan-Hung VU",
            "Matthieu Cord"
        ],
        "tldr": "DrivoR is a transformer-based autonomous driving architecture that uses camera-aware register tokens to compress multi-camera features for efficient trajectory generation and scoring, achieving strong performance on multiple benchmarks.",
        "tldr_zh": "DrivoR是一种基于Transformer的自动驾驶架构，它使用相机感知的注册token来压缩多相机特征，以实现高效的轨迹生成和评分，并在多个基准测试中取得了良好的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SparseLaneSTP: Leveraging Spatio-Temporal Priors with Sparse Transformers for 3D Lane Detection",
        "summary": "3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.",
        "url": "http://arxiv.org/abs/2601.04968v1",
        "published_date": "2026-01-08T14:16:11+00:00",
        "updated_date": "2026-01-08T14:16:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Maximilian Pittner",
            "Joel Janai",
            "Mario Faigle",
            "Alexandru Paul Condurache"
        ],
        "tldr": "SparseLaneSTP introduces a novel sparse lane transformer incorporating spatio-temporal priors and a new dataset for 3D lane detection, achieving state-of-the-art performance.",
        "tldr_zh": "SparseLaneSTP 提出了一种新的稀疏车道 Transformer，结合了时空先验和新的 3D 车道检测数据集，实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
        "summary": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
        "url": "http://arxiv.org/abs/2601.04754v1",
        "published_date": "2026-01-08T09:20:46+00:00",
        "updated_date": "2026-01-08T09:20:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yen-Jen Chiou",
            "Wei-Tse Cheng",
            "Yuan-Fu Yang"
        ],
        "tldr": "ProFuse is an efficient framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting that uses cross-view clustering and dense correspondence to achieve faster semantic attachment without render-supervised fine-tuning.",
        "tldr_zh": "ProFuse是一个用于开放词汇三维场景理解的高效框架，它利用三维高斯溅射，通过跨视角聚类和密集对应关系来实现更快的语义连接，而无需渲染监督微调。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "UNIC: Learning Unified Multimodal Extrinsic Contact Estimation",
        "summary": "Contact-rich manipulation requires reliable estimation of extrinsic contacts-the interactions between a grasped object and its environment which provide essential contextual information for planning, control, and policy learning. However, existing approaches often rely on restrictive assumptions, such as predefined contact types, fixed grasp configurations, or camera calibration, that hinder generalization to novel objects and deployment in unstructured environments. In this paper, we present UNIC, a unified multimodal framework for extrinsic contact estimation that operates without any prior knowledge or camera calibration. UNIC directly encodes visual observations in the camera frame and integrates them with proprioceptive and tactile modalities in a fully data-driven manner. It introduces a unified contact representation based on scene affordance maps that captures diverse contact formations and employs a multimodal fusion mechanism with random masking, enabling robust multimodal representation learning. Extensive experiments demonstrate that UNIC performs reliably. It achieves a 9.6 mm average Chamfer distance error on unseen contact locations, performs well on unseen objects, remains robust under missing modalities, and adapts to dynamic camera viewpoints. These results establish extrinsic contact estimation as a practical and versatile capability for contact-rich manipulation.",
        "url": "http://arxiv.org/abs/2601.04356v1",
        "published_date": "2026-01-07T19:43:16+00:00",
        "updated_date": "2026-01-07T19:43:16+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Zhengtong Xu",
            "Yuki Shirai"
        ],
        "tldr": "The paper introduces UNIC, a unified multimodal framework for estimating extrinsic contacts in contact-rich manipulation without prior knowledge or camera calibration, using visual, proprioceptive, and tactile modalities.",
        "tldr_zh": "该论文介绍了一种名为UNIC的统一多模态框架，用于估计富接触操作中的外部接触，无需先验知识或相机校准，利用视觉、本体感受和触觉模态。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ImLoc: Revisiting Visual Localization with Image-based Representation",
        "summary": "Existing visual localization methods are typically either 2D image-based, which are easy to build and maintain but limited in effective geometric reasoning, or 3D structure-based, which achieve high accuracy but require a centralized reconstruction and are difficult to update. In this work, we revisit visual localization with a 2D image-based representation and propose to augment each image with estimated depth maps to capture the geometric structure. Supported by the effective use of dense matchers, this representation is not only easy to build and maintain, but achieves highest accuracy in challenging conditions. With compact compression and a GPU-accelerated LO-RANSAC implementation, the whole pipeline is efficient in both storage and computation and allows for a flexible trade-off between accuracy and highest memory efficiency. Our method achieves a new state-of-the-art accuracy on various standard benchmarks and outperforms existing memory-efficient methods at comparable map sizes. Code will be available at https://github.com/cvg/Hierarchical-Localization.",
        "url": "http://arxiv.org/abs/2601.04185v1",
        "published_date": "2026-01-07T18:51:51+00:00",
        "updated_date": "2026-01-07T18:51:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xudong Jiang",
            "Fangjinhua Wang",
            "Silvano Galliani",
            "Christoph Vogel",
            "Marc Pollefeys"
        ],
        "tldr": "This paper proposes a novel image-based visual localization method using depth map augmentation, achieving state-of-the-art accuracy with efficient storage and computation, outperforming existing memory-efficient approaches.",
        "tldr_zh": "本文提出了一种新的基于图像的视觉定位方法，该方法使用深度图增强，实现了最先进的精度，同时具有高效的存储和计算能力，优于现有的内存高效方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]