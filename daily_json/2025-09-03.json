[
    {
        "title": "AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving",
        "summary": "Vision-Language-Action (VLA) models in autonomous driving systems have\nrecently demonstrated transformative potential by integrating multimodal\nperception with decision-making capabilities. However, the interpretability and\ncoherence of the decision process and the plausibility of action sequences\nremain largely underexplored. To address these issues, we propose\nAutoDrive-R$^2$, a novel VLA framework that enhances both reasoning and\nself-reflection capabilities of autonomous driving systems through\nchain-of-thought (CoT) processing and reinforcement learning (RL).\nSpecifically, we first propose an innovative CoT dataset named nuScenesR$^2$-6K\nfor supervised fine-tuning, which effectively builds cognitive bridges between\ninput information and output trajectories through a four-step logical chain\nwith self-reflection for validation. Moreover, to maximize both reasoning and\nself-reflection during the RL stage, we further employ the Group Relative\nPolicy Optimization (GRPO) algorithm within a physics-grounded reward framework\nthat incorporates spatial alignment, vehicle dynamic, and temporal smoothness\ncriteria to ensure reliable and realistic trajectory planning. Extensive\nevaluation results across both nuScenes and Waymo datasets demonstrates the\nstate-of-the-art performance and robust generalization capacity of our proposed\nmethod.",
        "url": "http://arxiv.org/abs/2509.01944v1",
        "published_date": "2025-09-02T04:32:24+00:00",
        "updated_date": "2025-09-02T04:32:24+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Zhenlong Yuan",
            "Jing Tang",
            "Jinguo Luo",
            "Rui Chen",
            "Chengxuan Qian",
            "Lei Sun",
            "Xiangxiang Chu",
            "Yujun Cai",
            "Dapeng Zhang",
            "Shuo Li"
        ],
        "tldr": "This paper introduces AutoDrive-R², a novel VLA framework for autonomous driving that enhances reasoning and self-reflection using chain-of-thought processing, reinforcement learning, and a new dataset. It achieves state-of-the-art performance on nuScenes and Waymo datasets.",
        "tldr_zh": "本文介绍了 AutoDrive-R²，一种用于自动驾驶的新型 VLA 框架，通过使用思维链处理、强化学习和一个新数据集来增强推理和自我反思能力。 它在 nuScenes 和 Waymo 数据集上实现了最先进的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "FastVGGT: Training-Free Acceleration of Visual Geometry Transformer",
        "summary": "Foundation models for 3D vision have recently demonstrated remarkable\ncapabilities in 3D perception. However, scaling these models to long-sequence\nimage inputs remains a significant challenge due to inference-time\ninefficiency. In this work, we present a detailed analysis of VGGT, a\nstate-of-the-art feed-forward visual geometry model and identify its primary\nbottleneck. Visualization further reveals a token collapse phenomenon in the\nattention maps. Motivated by these findings, we explore the potential of token\nmerging in the feed-forward visual geometry model. Owing to the unique\narchitectural and task-specific properties of 3D models, directly applying\nexisting merging techniques proves challenging. To this end, we propose\nFastVGGT, which, for the first time, leverages token merging in the 3D domain\nthrough a training-free mechanism for accelerating VGGT. we devise a unique\ntoken partitioning strategy tailored to 3D architectures and tasks, effectively\neliminating redundant computation while preserving VGGT's powerful\nreconstruction capacity. Extensive experiments on multiple 3D geometry\nbenchmarks validate the effectiveness of our approach. Notably, with 1000 input\nimages, FastVGGT achieves a 4x speedup over VGGT while mitigating error\naccumulation in long-sequence scenarios. These findings underscore the\npotential of token merging as a principled solution for scalable 3D vision\nsystems. Code is available at: https://mystorm16.github.io/fastvggt/.",
        "url": "http://arxiv.org/abs/2509.02560v1",
        "published_date": "2025-09-02T17:54:21+00:00",
        "updated_date": "2025-09-02T17:54:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "You Shen",
            "Zhipeng Zhang",
            "Yansong Qu",
            "Liujuan Cao"
        ],
        "tldr": "This paper introduces FastVGGT, a training-free token merging technique to accelerate VGGT for 3D vision, achieving a 4x speedup with 1000 input images while maintaining reconstruction capacity.",
        "tldr_zh": "本文介绍了 FastVGGT，一种无需训练的令牌合并技术，旨在加速 VGGT 在 3D 视觉中的应用。该方法在处理 1000 张输入图像时实现了 4 倍的加速，同时保持了重建能力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "2nd Place Solution for CVPR2024 E2E Challenge: End-to-End Autonomous Driving Using Vision Language Model",
        "summary": "End-to-end autonomous driving has drawn tremendous attention recently. Many\nworks focus on using modular deep neural networks to construct the end-to-end\narchi-tecture. However, whether using powerful large language models (LLM),\nespecially multi-modality Vision Language Models (VLM) could benefit the\nend-to-end driving tasks remain a question. In our work, we demonstrate that\ncombining end-to-end architectural design and knowledgeable VLMs yield\nimpressive performance on the driving tasks. It is worth noting that our method\nonly uses a single camera and is the best camera-only solution across the\nleaderboard, demonstrating the effectiveness of vision-based driving approach\nand the potential for end-to-end driving tasks.",
        "url": "http://arxiv.org/abs/2509.02659v1",
        "published_date": "2025-09-02T17:52:29+00:00",
        "updated_date": "2025-09-02T17:52:29+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Zilong Guo",
            "Yi Luo",
            "Long Sha",
            "Dongxu Wang",
            "Panqu Wang",
            "Chenyang Xu",
            "Yi Yang"
        ],
        "tldr": "This paper presents a vision-language model (VLM) based end-to-end autonomous driving system that achieved 2nd place in the CVPR2024 E2E challenge, highlighting the potential of VLMs for this task, especially in camera-only setups.",
        "tldr_zh": "本文介绍了一种基于视觉语言模型（VLM）的端到端自动驾驶系统，该系统在CVPR2024 E2E挑战赛中获得第二名，突出了VLM在该任务中的潜力，尤其是在纯相机设置中。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Manipulation as in Simulation: Enabling Accurate Geometry Perception in Robots",
        "summary": "Modern robotic manipulation primarily relies on visual observations in a 2D\ncolor space for skill learning but suffers from poor generalization. In\ncontrast, humans, living in a 3D world, depend more on physical properties-such\nas distance, size, and shape-than on texture when interacting with objects.\nSince such 3D geometric information can be acquired from widely available depth\ncameras, it appears feasible to endow robots with similar perceptual\ncapabilities. Our pilot study found that using depth cameras for manipulation\nis challenging, primarily due to their limited accuracy and susceptibility to\nvarious types of noise. In this work, we propose Camera Depth Models (CDMs) as\na simple plugin on daily-use depth cameras, which take RGB images and raw depth\nsignals as input and output denoised, accurate metric depth. To achieve this,\nwe develop a neural data engine that generates high-quality paired data from\nsimulation by modeling a depth camera's noise pattern. Our results show that\nCDMs achieve nearly simulation-level accuracy in depth prediction, effectively\nbridging the sim-to-real gap for manipulation tasks. Notably, our experiments\ndemonstrate, for the first time, that a policy trained on raw simulated depth,\nwithout the need for adding noise or real-world fine-tuning, generalizes\nseamlessly to real-world robots on two challenging long-horizon tasks involving\narticulated, reflective, and slender objects, with little to no performance\ndegradation. We hope our findings will inspire future research in utilizing\nsimulation data and 3D information in general robot policies.",
        "url": "http://arxiv.org/abs/2509.02530v1",
        "published_date": "2025-09-02T17:29:38+00:00",
        "updated_date": "2025-09-02T17:29:38+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Minghuan Liu",
            "Zhengbang Zhu",
            "Xiaoshen Han",
            "Peng Hu",
            "Haotong Lin",
            "Xinyao Li",
            "Jingxiao Chen",
            "Jiafeng Xu",
            "Yichu Yang",
            "Yunfeng Lin",
            "Xinghang Li",
            "Yong Yu",
            "Weinan Zhang",
            "Tao Kong",
            "Bingyi Kang"
        ],
        "tldr": "The paper introduces Camera Depth Models (CDMs) that denoise and improve the accuracy of depth data from commodity depth cameras using a neural data engine trained on simulated depth camera noise. This enables sim-to-real transfer of robot manipulation policies without real-world fine-tuning.",
        "tldr_zh": "该论文介绍了相机深度模型（CDMs），它使用在模拟深度相机噪声上训练的神经数据引擎来去噪并提高来自商品深度相机的深度数据的准确性。这使得机器人操作策略能够实现从模拟到真实的转换，而无需进行实际微调。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SynthGenNet: a self-supervised approach for test-time generalization using synthetic multi-source domain mixing of street view images",
        "summary": "Unstructured urban environments present unique challenges for scene\nunderstanding and generalization due to their complex and diverse layouts. We\nintroduce SynthGenNet, a self-supervised student-teacher architecture designed\nto enable robust test-time domain generalization using synthetic multi-source\nimagery. Our contributions include the novel ClassMix++ algorithm, which blends\nlabeled data from various synthetic sources while maintaining semantic\nintegrity, enhancing model adaptability. We further employ Grounded Mask\nConsistency Loss (GMC), which leverages source ground truth to improve\ncross-domain prediction consistency and feature alignment. The Pseudo-Label\nGuided Contrastive Learning (PLGCL) mechanism is integrated into the student\nnetwork to facilitate domain-invariant feature learning through iterative\nknowledge distillation from the teacher network. This self-supervised strategy\nimproves prediction accuracy, addresses real-world variability, bridges the\nsim-to-real domain gap, and reliance on labeled target data, even in complex\nurban areas. Outcomes show our model outperforms the state-of-the-art (relying\non single source) by achieving 50% Mean Intersection-Over-Union (mIoU) value on\nreal-world datasets like Indian Driving Dataset (IDD).",
        "url": "http://arxiv.org/abs/2509.02287v1",
        "published_date": "2025-09-02T13:08:03+00:00",
        "updated_date": "2025-09-02T13:08:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pushpendra Dhakara",
            "Prachi Chachodhia",
            "Vaibhav Kumar"
        ],
        "tldr": "SynthGenNet is a self-supervised method for test-time domain generalization on street view images using synthetic data and novel techniques like ClassMix++, GMC, and PLGCL, achieving significant mIoU improvements on real-world datasets.",
        "tldr_zh": "SynthGenNet 是一种自监督方法，使用合成数据和 ClassMix++、GMC 和 PLGCL 等创新技术，在街景图像上进行测试时域泛化，并在真实世界的数据集上实现了显着的 mIoU 改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Omnidirectional Spatial Modeling from Correlated Panoramas",
        "summary": "Omnidirectional scene understanding is vital for various downstream\napplications, such as embodied AI, autonomous driving, and immersive\nenvironments, yet remains challenging due to geometric distortion and complex\nspatial relations in 360{\\deg} imagery. Existing omnidirectional methods\nachieve scene understanding within a single frame while neglecting cross-frame\ncorrelated panoramas. To bridge this gap, we introduce \\textbf{CFpano}, the\n\\textbf{first} benchmark dataset dedicated to cross-frame correlated panoramas\nvisual question answering in the holistic 360{\\deg} scenes. CFpano consists of\nover 2700 images together with over 8000 question-answer pairs, and the\nquestion types include both multiple choice and open-ended VQA. Building upon\nour CFpano, we further present \\methodname, a multi-modal large language model\n(MLLM) fine-tuned with Group Relative Policy Optimization (GRPO) and a set of\ntailored reward functions for robust and consistent reasoning with cross-frame\ncorrelated panoramas. Benchmark experiments with existing MLLMs are conducted\nwith our CFpano. The experimental results demonstrate that \\methodname achieves\nstate-of-the-art performance across both multiple-choice and open-ended VQA\ntasks, outperforming strong baselines on all major reasoning categories\n(\\textbf{+5.37\\%} in overall performance). Our analyses validate the\neffectiveness of GRPO and establish a new benchmark for panoramic scene\nunderstanding.",
        "url": "http://arxiv.org/abs/2509.02164v1",
        "published_date": "2025-09-02T10:14:55+00:00",
        "updated_date": "2025-09-02T10:14:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinshen Zhang",
            "Tongxi Fu",
            "Xu Zheng"
        ],
        "tldr": "The paper introduces CFpano, a new benchmark dataset for cross-frame correlated panorama visual question answering, and presents a fine-tuned MLLM, \\methodname, which achieves state-of-the-art performance on the dataset.",
        "tldr_zh": "该论文介绍了一个新的基准数据集 CFpano，用于跨帧相关的全景视觉问答，并提出了一个微调的多模态大型语言模型 \\methodname，该模型在该数据集上实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enabling Federated Object Detection for Connected Autonomous Vehicles: A Deployment-Oriented Evaluation",
        "summary": "Object detection is crucial for Connected Autonomous Vehicles (CAVs) to\nperceive their surroundings and make safe driving decisions. Centralized\ntraining of object detection models often achieves promising accuracy, fast\nconvergence, and simplified training process, but it falls short in\nscalability, adaptability, and privacy-preservation. Federated learning (FL),\nby contrast, enables collaborative, privacy-preserving, and continuous training\nacross naturally distributed CAV fleets. However, deploying FL in real-world\nCAVs remains challenging due to the substantial computational demands of\ntraining and inference, coupled with highly diverse operating conditions.\nPractical deployment must address three critical factors: (i) heterogeneity\nfrom non-IID data distributions, (ii) constrained onboard computing hardware,\nand (iii) environmental variability such as lighting and weather, alongside\nsystematic evaluation to ensure reliable performance. This work introduces the\nfirst holistic deployment-oriented evaluation of FL-based object detection in\nCAVs, integrating model performance, system-level resource profiling, and\nenvironmental robustness. Using state-of-the-art detectors, YOLOv5, YOLOv8,\nYOLOv11, and Deformable DETR, evaluated on the KITTI, BDD100K, and nuScenes\ndatasets, we analyze trade-offs between detection accuracy, computational cost,\nand resource usage under diverse resolutions, batch sizes, weather and lighting\nconditions, and dynamic client participation, paving the way for robust FL\ndeployment in CAVs.",
        "url": "http://arxiv.org/abs/2509.01868v1",
        "published_date": "2025-09-02T01:23:50+00:00",
        "updated_date": "2025-09-02T01:23:50+00:00",
        "categories": [
            "cs.CV",
            "cs.DC"
        ],
        "authors": [
            "Komala Subramanyam Cherukuri",
            "Kewei Sha",
            "Zhenhua Huang"
        ],
        "tldr": "This paper presents a deployment-oriented evaluation of federated learning (FL) for object detection in connected autonomous vehicles (CAVs), considering heterogeneity, constrained hardware, and environmental variability using YOLO and DETR variants on popular datasets.",
        "tldr_zh": "本文针对互联自动驾驶车辆（CAV）中的联邦学习（FL）目标检测提出了一个面向部署的评估，考虑了异构性、受限硬件和环境变化，使用了YOLO和DETR变体在流行数据集上进行了实验。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Articulated Object Estimation in the Wild",
        "summary": "Understanding the 3D motion of articulated objects is essential in robotic\nscene understanding, mobile manipulation, and motion planning. Prior methods\nfor articulation estimation have primarily focused on controlled settings,\nassuming either fixed camera viewpoints or direct observations of various\nobject states, which tend to fail in more realistic unconstrained environments.\nIn contrast, humans effortlessly infer articulation by watching others\nmanipulate objects. Inspired by this, we introduce ArtiPoint, a novel\nestimation framework that can infer articulated object models under dynamic\ncamera motion and partial observability. By combining deep point tracking with\na factor graph optimization framework, ArtiPoint robustly estimates articulated\npart trajectories and articulation axes directly from raw RGB-D videos. To\nfoster future research in this domain, we introduce Arti4D, the first\nego-centric in-the-wild dataset that captures articulated object interactions\nat a scene level, accompanied by articulation labels and ground-truth camera\nposes. We benchmark ArtiPoint against a range of classical and learning-based\nbaselines, demonstrating its superior performance on Arti4D. We make code and\nArti4D publicly available at https://artipoint.cs.uni-freiburg.de.",
        "url": "http://arxiv.org/abs/2509.01708v1",
        "published_date": "2025-09-01T18:34:17+00:00",
        "updated_date": "2025-09-01T18:34:17+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Abdelrhman Werby",
            "Martin Büchner",
            "Adrian Röfer",
            "Chenguang Huang",
            "Wolfram Burgard",
            "Abhinav Valada"
        ],
        "tldr": "The paper introduces ArtiPoint, a novel framework for articulated object estimation from RGB-D videos under dynamic camera motion and partial observability, along with Arti4D, a new ego-centric in-the-wild dataset for this task.",
        "tldr_zh": "该论文介绍了ArtiPoint，一种新颖的框架，用于在动态相机运动和部分可观察性下从RGB-D视频中估计铰接对象，以及Arti4D，一个新的针对此任务的以自我为中心的野外数据集。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]