[
    {
        "title": "RDT2: Exploring the Scaling Limit of UMI Data Towards Zero-Shot Cross-Embodiment Generalization",
        "summary": "Vision-Language-Action (VLA) models hold promise for generalist robotics but currently struggle with data scarcity, architectural inefficiencies, and the inability to generalize across different hardware platforms. We introduce RDT2, a robotic foundation model built upon a 7B parameter VLM designed to enable zero-shot deployment on novel embodiments for open-vocabulary tasks. To achieve this, we collected one of the largest open-source robotic datasets--over 10,000 hours of demonstrations in diverse families--using an enhanced, embodiment-agnostic Universal Manipulation Interface (UMI). Our approach employs a novel three-stage training recipe that aligns discrete linguistic knowledge with continuous control via Residual Vector Quantization (RVQ), flow-matching, and distillation for real-time inference. Consequently, RDT2 becomes one of the first models that simultaneously zero-shot generalizes to unseen objects, scenes, instructions, and even robotic platforms. Besides, it outperforms state-of-the-art baselines in dexterous, long-horizon, and dynamic downstream tasks like playing table tennis. See https://rdt-robotics.github.io/rdt2/ for more information.",
        "url": "http://arxiv.org/abs/2602.03310v1",
        "published_date": "2026-02-03T09:38:23+00:00",
        "updated_date": "2026-02-03T09:38:23+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Songming Liu",
            "Bangguo Li",
            "Kai Ma",
            "Lingxuan Wu",
            "Hengkai Tan",
            "Xiao Ouyang",
            "Hang Su",
            "Jun Zhu"
        ],
        "tldr": "RDT2 is a robotic foundation model utilizing a large-scale dataset and a novel three-stage training recipe to achieve zero-shot cross-embodiment generalization in robotic tasks, outperforming existing methods.",
        "tldr_zh": "RDT2是一个机器人基础模型，它利用大规模数据集和一种新的三阶段训练方法，在机器人任务中实现零样本跨形态泛化，性能优于现有方法。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "JRDB-Pose3D: A Multi-person 3D Human Pose and Shape Estimation Dataset for Robotics",
        "summary": "Real-world scenes are inherently crowded. Hence, estimating 3D poses of all nearby humans, tracking their movements over time, and understanding their activities within social and environmental contexts are essential for many applications, such as autonomous driving, robot perception, robot navigation, and human-robot interaction. However, most existing 3D human pose estimation datasets primarily focus on single-person scenes or are collected in controlled laboratory environments, which restricts their relevance to real-world applications. To bridge this gap, we introduce JRDB-Pose3D, which captures multi-human indoor and outdoor environments from a mobile robotic platform. JRDB-Pose3D provides rich 3D human pose annotations for such complex and dynamic scenes, including SMPL-based pose annotations with consistent body-shape parameters and track IDs for each individual over time. JRDB-Pose3D contains, on average, 5-10 human poses per frame, with some scenes featuring up to 35 individuals simultaneously. The proposed dataset presents unique challenges, including frequent occlusions, truncated bodies, and out-of-frame body parts, which closely reflect real-world environments. Moreover, JRDB-Pose3D inherits all available annotations from the JRDB dataset, such as 2D pose, information about social grouping, activities, and interactions, full-scene semantic masks with consistent human- and object-level tracking, and detailed annotations for each individual, such as age, gender, and race, making it a holistic dataset for a wide range of downstream perception and human-centric understanding tasks.",
        "url": "http://arxiv.org/abs/2602.03064v1",
        "published_date": "2026-02-03T03:46:27+00:00",
        "updated_date": "2026-02-03T03:46:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Sandika Biswas",
            "Kian Izadpanah",
            "Hamid Rezatofighi"
        ],
        "tldr": "The paper introduces JRDB-Pose3D, a new dataset for multi-person 3D human pose and shape estimation in real-world robotic scenarios, addressing the limitations of existing datasets by providing rich annotations in complex and dynamic environments.",
        "tldr_zh": "本文介绍了 JRDB-Pose3D，一个新的数据集，用于在真实机器人场景中进行多人 3D 人体姿势和形状估计，通过在复杂和动态环境中提供丰富的注释，解决了现有数据集的局限性。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "BridgeV2W: Bridging Video Generation Models to Embodied World Models via Embodiment Masks",
        "summary": "Embodied world models have emerged as a promising paradigm in robotics, most of which leverage large-scale Internet videos or pretrained video generation models to enrich visual and motion priors. However, they still face key challenges: a misalignment between coordinate-space actions and pixel-space videos, sensitivity to camera viewpoint, and non-unified architectures across embodiments. To this end, we present BridgeV2W, which converts coordinate-space actions into pixel-aligned embodiment masks rendered from the URDF and camera parameters. These masks are then injected into a pretrained video generation model via a ControlNet-style pathway, which aligns the action control signals with predicted videos, adds view-specific conditioning to accommodate camera viewpoints, and yields a unified world model architecture across embodiments. To mitigate overfitting to static backgrounds, BridgeV2W further introduces a flow-based motion loss that focuses on learning dynamic and task-relevant regions. Experiments on single-arm (DROID) and dual-arm (AgiBot-G1) datasets, covering diverse and challenging conditions with unseen viewpoints and scenes, show that BridgeV2W improves video generation quality compared to prior state-of-the-art methods. We further demonstrate the potential of BridgeV2W on downstream real-world tasks, including policy evaluation and goal-conditioned planning. More results can be found on our project website at https://BridgeV2W.github.io .",
        "url": "http://arxiv.org/abs/2602.03793v1",
        "published_date": "2026-02-03T17:56:28+00:00",
        "updated_date": "2026-02-03T17:56:28+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Yixiang Chen",
            "Peiyan Li",
            "Jiabing Yang",
            "Keji He",
            "Xiangnan Wu",
            "Yuan Xu",
            "Kai Wang",
            "Jing Liu",
            "Nianfeng Liu",
            "Yan Huang",
            "Liang Wang"
        ],
        "tldr": "The paper introduces BridgeV2W, a method that uses embodiment masks to align actions with predicted videos in a video generation model for robotics, addressing challenges in coordinate-space action misalignment, viewpoint sensitivity, and unified architecture. It shows improved video generation and demonstrates potential in policy evaluation and goal-conditioned planning.",
        "tldr_zh": "该论文介绍了BridgeV2W，一种使用具身掩码将动作与机器人视频生成模型中的预测视频对齐的方法，解决了坐标空间动作不对齐、视点敏感性和统一架构的挑战。它展示了改进的视频生成，并证明了其在策略评估和目标条件规划中的潜力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization",
        "summary": "The advent of Vision-Language-Action (VLA) models represents a significant leap for embodied intelligence, yet their immense computational demands critically hinder deployment on resource-constrained robotic platforms. Intuitively, low-bit quantization is a prevalent and preferred technique for large-scale model compression. However, we find that a systematic analysis of VLA model's quantization is fundamentally lacking. We argue that naively applying uniform-bit quantization from Large Language Models (LLMs) to robotics is flawed, as these methods prioritize passive data fidelity while ignoring how minor action deviations compound into catastrophic task failures. To bridge this gap, we introduce QVLA, the first action-centric quantization framework specifically designed for embodied control. In a sharp departure from the rigid, uniform-bit quantization of LLM-based methods, QVLA introduces a highly granular, channel-wise bit allocation strategy. Its core mechanism is to directly measure the final action-space sensitivity when quantizing each individual channel to various bit-widths. This process yields a precise, per-channel importance metric that guides a global optimization, which elegantly unifies quantization and pruning (0-bit) into a single, cohesive framework. Extensive evaluations on different baselines demonstrate the superiority of our approach. In the LIBERO, the quantization version of OpenVLA-OFT with our method requires only 29.2% of the original model's VRAM while maintaining 98.9% of its original performance and achieving a 1.49x speedup. This translates to a 22.6% performance improvement over the LLM-derived method SmoothQuant. Our work establishes a new, principled foundation for compressing VLA models in robotics, paving the way for deploying powerful, large-scale models on real-world hardware. Code will be released.",
        "url": "http://arxiv.org/abs/2602.03782v1",
        "published_date": "2026-02-03T17:43:45+00:00",
        "updated_date": "2026-02-03T17:43:45+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Yuhao Xu",
            "Yantai Yang",
            "Zhenyang Fan",
            "Yufan Liu",
            "Yuming Li",
            "Bing Li",
            "Zhipeng Zhang"
        ],
        "tldr": "The paper introduces QVLA, a novel action-centric quantization framework for Vision-Language-Action (VLA) models that uses channel-wise bit allocation based on action-space sensitivity, achieving significant compression and speedup while maintaining performance.",
        "tldr_zh": "该论文介绍了一种名为QVLA的新的面向动作的视觉-语言-动作(VLA)模型量化框架，该框架基于动作空间敏感性使用通道式比特分配，从而在保持性能的同时实现了显著的压缩和加速。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Sequential Neural Network with Spatial-Temporal Attention and Linear LSTM for Robust Lane Detection Using Multi-Frame Images",
        "summary": "Lane detection is a crucial perception task for all levels of automated vehicles (AVs) and Advanced Driver Assistance Systems, particularly in mixed-traffic environments where AVs must interact with human-driven vehicles (HDVs) and challenging traffic scenarios. Current methods lack versatility in delivering accurate, robust, and real-time compatible lane detection, especially vision-based methods often neglect critical regions of the image and their spatial-temporal (ST) salience, leading to poor performance in difficult circumstances such as serious occlusion and dazzle lighting. This study introduces a novel sequential neural network model with a spatial-temporal attention mechanism to focus on key features of lane lines and exploit salient ST correlations among continuous image frames. The proposed model, built on a standard encoder-decoder structure and common neural network backbones, is trained and evaluated on three large-scale open-source datasets. Extensive experiments demonstrate the strength and robustness of the proposed model, outperforming state-of-the-art methods in various testing scenarios. Furthermore, with the ST attention mechanism, the developed sequential neural network models exhibit fewer parameters and reduced Multiply-Accumulate Operations (MACs) compared to baseline sequential models, highlighting their computational efficiency. Relevant data, code, and models are released at https://doi.org/10.4121/4619cab6-ae4a-40d5-af77-582a77f3d821.",
        "url": "http://arxiv.org/abs/2602.03669v1",
        "published_date": "2026-02-03T15:51:29+00:00",
        "updated_date": "2026-02-03T15:51:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "eess.IV"
        ],
        "authors": [
            "Sandeep Patil",
            "Yongqi Dong",
            "Haneen Farah",
            "Hans Hellendoorn"
        ],
        "tldr": "This paper introduces a novel sequential neural network with spatial-temporal attention and Linear LSTM for robust and efficient lane detection, demonstrating state-of-the-art performance on multiple datasets.",
        "tldr_zh": "本文提出了一种新颖的序列神经网络，具有时空注意力机制和线性LSTM，用于稳健高效的 lane 检测，并在多个数据集上展示了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AffordanceGrasp-R1:Leveraging Reasoning-Based Affordance Segmentation with Reinforcement Learning for Robotic Grasping",
        "summary": "We introduce AffordanceGrasp-R1, a reasoning-driven affordance segmentation framework for robotic grasping that combines a chain-of-thought (CoT) cold-start strategy with reinforcement learning to enhance deduction and spatial grounding. In addition, we redesign the grasping pipeline to be more context-aware by generating grasp candidates from the global scene point cloud and subsequently filtering them using instruction-conditioned affordance masks. Extensive experiments demonstrate that AffordanceGrasp-R1 consistently outperforms state-of-the-art (SOTA) methods on benchmark datasets, and real-world robotic grasping evaluations further validate its robustness and generalization under complex language-conditioned manipulation scenarios.",
        "url": "http://arxiv.org/abs/2602.03547v1",
        "published_date": "2026-02-03T14:00:56+00:00",
        "updated_date": "2026-02-03T14:00:56+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Dingyi Zhou",
            "Mu He",
            "Zhuowei Fang",
            "Xiangtong Yao",
            "Yinlong Liu",
            "Alois Knoll",
            "Hu Cao"
        ],
        "tldr": "AffordanceGrasp-R1 combines reasoning-based affordance segmentation with reinforcement learning for robotic grasping, achieving SOTA performance and demonstrating robustness in real-world, language-conditioned manipulation.",
        "tldr_zh": "AffordanceGrasp-R1结合了基于推理的认知分割和强化学习来进行机器人抓取，实现了最先进的性能，并在真实的，语言条件下的操作中展示了鲁棒性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HetroD: A High-Fidelity Drone Dataset and Benchmark for Autonomous Driving in Heterogeneous Traffic",
        "summary": "We present HetroD, a dataset and benchmark for developing autonomous driving systems in heterogeneous environments. HetroD targets the critical challenge of navi- gating real-world heterogeneous traffic dominated by vulner- able road users (VRUs), including pedestrians, cyclists, and motorcyclists that interact with vehicles. These mixed agent types exhibit complex behaviors such as hook turns, lane splitting, and informal right-of-way negotiation. Such behaviors pose significant challenges for autonomous vehicles but remain underrepresented in existing datasets focused on structured, lane-disciplined traffic. To bridge the gap, we collect a large- scale drone-based dataset to provide a holistic observation of traffic scenes with centimeter-accurate annotations, HD maps, and traffic signal states. We further develop a modular toolkit for extracting per-agent scenarios to support downstream task development. In total, the dataset comprises over 65.4k high- fidelity agent trajectories, 70% of which are from VRUs. HetroD supports modeling of VRU behaviors in dense, het- erogeneous traffic and provides standardized benchmarks for forecasting, planning, and simulation tasks. Evaluation results reveal that state-of-the-art prediction and planning models struggle with the challenges presented by our dataset: they fail to predict lateral VRU movements, cannot handle unstructured maneuvers, and exhibit limited performance in dense and multi-agent scenarios, highlighting the need for more robust approaches to heterogeneous traffic. See our project page for more examples: https://hetroddata.github.io/HetroD/",
        "url": "http://arxiv.org/abs/2602.03447v1",
        "published_date": "2026-02-03T12:12:47+00:00",
        "updated_date": "2026-02-03T12:12:47+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Yu-Hsiang Chen",
            "Wei-Jer Chang",
            "Christian Kotulla",
            "Thomas Keutgens",
            "Steffen Runde",
            "Tobias Moers",
            "Christoph Klas",
            "Wei Zhan",
            "Masayoshi Tomizuka",
            "Yi-Ting Chen"
        ],
        "tldr": "The paper introduces HetroD, a large-scale drone-based dataset and benchmark for autonomous driving in heterogeneous traffic environments with a focus on vulnerable road users (VRUs) and their complex behaviors. It highlights the limitations of current models in handling such scenarios and provides tools for downstream task development.",
        "tldr_zh": "该论文介绍了HetroD，一个大规模的无人机数据集和基准，用于异构交通环境中的自动驾驶，重点关注弱势道路使用者（VRU）及其复杂行为。它强调了当前模型在处理此类场景中的局限性，并为下游任务开发提供了工具。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PlanTRansformer: Unified Prediction and Planning with Goal-conditioned Transformer",
        "summary": "Trajectory prediction and planning are fundamental yet disconnected components in autonomous driving. Prediction models forecast surrounding agent motion under unknown intentions, producing multimodal distributions, while planning assumes known ego objectives and generates deterministic trajectories. This mismatch creates a critical bottleneck: prediction lacks supervision for agent intentions, while planning requires this information. Existing prediction models, despite strong benchmarking performance, often remain disconnected from planning constraints such as collision avoidance and dynamic feasibility. We introduce Plan TRansformer (PTR), a unified Gaussian Mixture Transformer framework integrating goal-conditioned prediction, dynamic feasibility, interaction awareness, and lane-level topology reasoning. A teacher-student training strategy progressively masks surrounding agent commands during training to align with inference conditions where agent intentions are unavailable. PTR achieves 4.3%/3.5% improvement in marginal/joint mAP compared to the baseline Motion Transformer (MTR) and 15.5% planning error reduction at 5s horizon compared to GameFormer. The architecture-agnostic design enables application to diverse Transformer-based prediction models. Project Website: https://github.com/SelzerConst/PlanTRansformer",
        "url": "http://arxiv.org/abs/2602.03376v1",
        "published_date": "2026-02-03T10:55:05+00:00",
        "updated_date": "2026-02-03T10:55:05+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Constantin Selzer",
            "Fabina B. Flohr"
        ],
        "tldr": "PlanTRansformer unifies trajectory prediction and planning in autonomous driving using a goal-conditioned Transformer framework, improving both prediction accuracy and planning performance by incorporating intention awareness and dynamic feasibility.",
        "tldr_zh": "PlanTRansformer 使用目标条件Transformer框架统一了自动驾驶中的轨迹预测和规划，通过结合意图感知和动态可行性，提高了预测准确性和规划性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multi-Resolution Alignment for Voxel Sparsity in Camera-Based 3D Semantic Scene Completion",
        "summary": "Camera-based 3D semantic scene completion (SSC) offers a cost-effective solution for assessing the geometric occupancy and semantic labels of each voxel in the surrounding 3D scene with image inputs, providing a voxel-level scene perception foundation for the perception-prediction-planning autonomous driving systems. Although significant progress has been made in existing methods, their optimization rely solely on the supervision from voxel labels and face the challenge of voxel sparsity as a large portion of voxels in autonomous driving scenarios are empty, which limits both optimization efficiency and model performance. To address this issue, we propose a \\textit{Multi-Resolution Alignment (MRA)} approach to mitigate voxel sparsity in camera-based 3D semantic scene completion, which exploits the scene and instance level alignment across multi-resolution 3D features as auxiliary supervision. Specifically, we first propose the Multi-resolution View Transformer module, which projects 2D image features into multi-resolution 3D features and aligns them at the scene level through fusing discriminative seed features. Furthermore, we design the Cubic Semantic Anisotropy module to identify the instance-level semantic significance of each voxel, accounting for the semantic differences of a specific voxel against its neighboring voxels within a cubic area. Finally, we devise a Critical Distribution Alignment module, which selects critical voxels as instance-level anchors with the guidance of cubic semantic anisotropy, and applies a circulated loss for auxiliary supervision on the critical feature distribution consistency across different resolutions. The code is available at https://github.com/PKU-ICST-MIPL/MRA_TIP.",
        "url": "http://arxiv.org/abs/2602.03371v1",
        "published_date": "2026-02-03T10:46:51+00:00",
        "updated_date": "2026-02-03T10:46:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhiwen Yang",
            "Yuxin Peng"
        ],
        "tldr": "This paper introduces a Multi-Resolution Alignment (MRA) approach to address voxel sparsity in camera-based 3D semantic scene completion by exploiting scene and instance-level alignment across multi-resolution 3D features as auxiliary supervision.",
        "tldr_zh": "本文提出了一种多分辨率对齐（MRA）方法，通过利用多分辨率3D特征之间的场景和实例级对齐作为辅助监督，来解决基于相机的3D语义场景补全中的体素稀疏性问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Z3D: Zero-Shot 3D Visual Grounding from Images",
        "summary": "3D visual grounding (3DVG) aims to localize objects in a 3D scene based on natural language queries. In this work, we explore zero-shot 3DVG from multi-view images alone, without requiring any geometric supervision or object priors. We introduce Z3D, a universal grounding pipeline that flexibly operates on multi-view images while optionally incorporating camera poses and depth maps. We identify key bottlenecks in prior zero-shot methods causing significant performance degradation and address them with (i) a state-of-the-art zero-shot 3D instance segmentation method to generate high-quality 3D bounding box proposals and (ii) advanced reasoning via prompt-based segmentation, which utilizes full capabilities of modern VLMs. Extensive experiments on the ScanRefer and Nr3D benchmarks demonstrate that our approach achieves state-of-the-art performance among zero-shot methods. Code is available at https://github.com/col14m/z3d .",
        "url": "http://arxiv.org/abs/2602.03361v1",
        "published_date": "2026-02-03T10:35:18+00:00",
        "updated_date": "2026-02-03T10:35:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nikita Drozdov",
            "Andrey Lemeshko",
            "Nikita Gavrilov",
            "Anton Konushin",
            "Danila Rukhovich",
            "Maksim Kolodiazhnyi"
        ],
        "tldr": "The paper introduces Z3D, a zero-shot 3D visual grounding pipeline from multi-view images, achieving state-of-the-art performance on ScanRefer and Nr3D benchmarks by addressing limitations in previous methods with improved 3D instance segmentation and prompt-based segmentation using VLMs.",
        "tldr_zh": "该论文介绍了 Z3D，一个从多视图图像进行零样本 3D 视觉定位的流程。通过改进的 3D 实例分割和利用 VLM 的基于提示的分割，解决了先前方法的局限性，并在 ScanRefer 和 Nr3D 基准测试中实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "LEVIO: Lightweight Embedded Visual Inertial Odometry for Resource-Constrained Devices",
        "summary": "Accurate, infrastructure-less sensor systems for motion tracking are essential for mobile robotics and augmented reality (AR) applications. The most popular state-of-the-art visual-inertial odometry (VIO) systems, however, are too computationally demanding for resource-constrained hardware, such as micro-drones and smart glasses. This work presents LEVIO, a fully featured VIO pipeline optimized for ultra-low-power compute platforms, allowing six-degrees-of-freedom (DoF) real-time sensing. LEVIO incorporates established VIO components such as Oriented FAST and Rotated BRIEF (ORB) feature tracking and bundle adjustment, while emphasizing a computationally efficient architecture with parallelization and low memory usage to suit embedded microcontrollers and low-power systems-on-chip (SoCs). The paper proposes and details the algorithmic design choices and the hardware-software co-optimization approach, and presents real-time performance on resource-constrained hardware. LEVIO is validated on a parallel-processing ultra-low-power RISC-V SoC, achieving 20 FPS while consuming less than 100 mW, and benchmarked against public VIO datasets, offering a compelling balance between efficiency and accuracy. To facilitate reproducibility and adoption, the complete implementation is released as open-source.",
        "url": "http://arxiv.org/abs/2602.03294v1",
        "published_date": "2026-02-03T09:20:57+00:00",
        "updated_date": "2026-02-03T09:20:57+00:00",
        "categories": [
            "cs.CV",
            "cs.RO",
            "eess.IV"
        ],
        "authors": [
            "Jonas Kühne",
            "Christian Vogt",
            "Michele Magno",
            "Luca Benini"
        ],
        "tldr": "The paper introduces LEVIO, a computationally efficient VIO pipeline optimized for resource-constrained devices, achieving real-time performance with low power consumption and open-source availability.",
        "tldr_zh": "该论文介绍了LEVIO，一种为资源受限设备优化的计算高效的VIO流程，实现了低功耗的实时性能，并开源。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation",
        "summary": "Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA's autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is https://shanpoyang654.github.io/InstaDrive/page.html.",
        "url": "http://arxiv.org/abs/2602.03242v1",
        "published_date": "2026-02-03T08:22:13+00:00",
        "updated_date": "2026-02-03T08:22:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhuoran Yang",
            "Xi Guo",
            "Chenjing Ding",
            "Chiyu Wang",
            "Wei Wu",
            "Yanyong Zhang"
        ],
        "tldr": "InstaDrive is a novel world model framework for autonomous driving video generation that enhances realism by enforcing instance-level temporal consistency and spatial geometric fidelity through an Instance Flow Guider and a Spatial Geometric Aligner. It achieves SOTA results and improves downstream tasks.",
        "tldr_zh": "InstaDrive 是一种新颖的自动驾驶视频生成世界模型框架，通过 Instance Flow Guider 和 Spatial Geometric Aligner 增强了真实感，从而强制执行实例级别的时序一致性和空间几何保真度。它实现了 SOTA 结果并改进了下游任务。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask",
        "summary": "Autonomous driving relies on robust models trained on large-scale, high-quality multi-view driving videos. Although world models provide a cost-effective solution for generating realistic driving data, they often suffer from identity drift, where the same object changes its appearance or category across frames due to the absence of instance-level temporal constraints. We introduce ConsisDrive, an identity-preserving driving world model designed to enforce temporal consistency at the instance level. Our framework incorporates two key components: (1) Instance-Masked Attention, which applies instance identity masks and trajectory masks within attention blocks to ensure that visual tokens interact only with their corresponding instance features across spatial and temporal dimensions, thereby preserving object identity consistency; and (2) Instance-Masked Loss, which adaptively emphasizes foreground regions with probabilistic instance masking, reducing background noise while maintaining overall scene fidelity. By integrating these mechanisms, ConsisDrive achieves state-of-the-art driving video generation quality and demonstrates significant improvements in downstream autonomous driving tasks on the nuScenes dataset. Our project page is https://shanpoyang654.github.io/ConsisDrive/page.html.",
        "url": "http://arxiv.org/abs/2602.03213v1",
        "published_date": "2026-02-03T07:28:44+00:00",
        "updated_date": "2026-02-03T07:28:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhuoran Yang",
            "Yanyong Zhang"
        ],
        "tldr": "ConsisDrive is a driving world model that preserves object identity across frames by using instance-masked attention and loss, improving driving video generation quality and downstream autonomous driving tasks.",
        "tldr_zh": "ConsisDrive 是一种驾驶世界模型，它通过使用实例掩码的注意力和损失来保持跨帧的对象身份，从而提高驾驶视频生成质量和下游自动驾驶任务的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Edge-Optimized Vision-Language Models for Underground Infrastructure Assessment",
        "summary": "Autonomous inspection of underground infrastructure, such as sewer and culvert systems, is critical to public safety and urban sustainability. Although robotic platforms equipped with visual sensors can efficiently detect structural deficiencies, the automated generation of human-readable summaries from these detections remains a significant challenge, especially on resource-constrained edge devices. This paper presents a novel two-stage pipeline for end-to-end summarization of underground deficiencies, combining our lightweight RAPID-SCAN segmentation model with a fine-tuned Vision-Language Model (VLM) deployed on an edge computing platform. The first stage employs RAPID-SCAN (Resource-Aware Pipeline Inspection and Defect Segmentation using Compact Adaptive Network), achieving 0.834 F1-score with only 0.64M parameters for efficient defect segmentation. The second stage utilizes a fine-tuned Phi-3.5 VLM that generates concise, domain-specific summaries in natural language from the segmentation outputs. We introduce a curated dataset of inspection images with manually verified descriptions for VLM fine-tuning and evaluation. To enable real-time performance, we employ post-training quantization with hardware-specific optimization, achieving significant reductions in model size and inference latency without compromising summarization quality. We deploy and evaluate our complete pipeline on a mobile robotic platform, demonstrating its effectiveness in real-world inspection scenarios. Our results show the potential of edge-deployable integrated AI systems to bridge the gap between automated defect detection and actionable insights for infrastructure maintenance, paving the way for more scalable and autonomous inspection solutions.",
        "url": "http://arxiv.org/abs/2602.03742v1",
        "published_date": "2026-02-03T17:03:46+00:00",
        "updated_date": "2026-02-03T17:03:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Johny J. Lopez",
            "Md Meftahul Ferdaus",
            "Mahdi Abdelguerfi"
        ],
        "tldr": "This paper presents an edge-optimized vision-language pipeline for automated underground infrastructure assessment, using a lightweight segmentation model and a fine-tuned VLM to generate human-readable summaries of defects on a mobile robot.",
        "tldr_zh": "本文提出了一种边缘优化的视觉-语言流水线，用于自动评估地下基础设施。该流水线使用轻量级分割模型和微调后的 VLM，在移动机器人上生成人类可读的缺陷摘要。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 7
    }
]