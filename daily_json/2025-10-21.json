[
    {
        "title": "SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries",
        "summary": "Semantic occupancy has emerged as a powerful representation in world models\nfor its ability to capture rich spatial semantics. However, most existing\noccupancy world models rely on static and fixed embeddings or grids, which\ninherently limit the flexibility of perception. Moreover, their ``in-place\nclassification\" over grids exhibits a potential misalignment with the dynamic\nand continuous nature of real scenarios.In this paper, we propose SparseWorld,\na novel 4D occupancy world model that is flexible, adaptive, and efficient,\npowered by sparse and dynamic queries. We propose a Range-Adaptive Perception\nmodule, in which learnable queries are modulated by the ego vehicle states and\nenriched with temporal-spatial associations to enable extended-range\nperception. To effectively capture the dynamics of the scene, we design a\nState-Conditioned Forecasting module, which replaces classification-based\nforecasting with regression-guided formulation, precisely aligning the dynamic\nqueries with the continuity of the 4D environment. In addition, We specifically\ndevise a Temporal-Aware Self-Scheduling training strategy to enable smooth and\nefficient training. Extensive experiments demonstrate that SparseWorld achieves\nstate-of-the-art performance across perception, forecasting, and planning\ntasks. Comprehensive visualizations and ablation studies further validate the\nadvantages of SparseWorld in terms of flexibility, adaptability, and\nefficiency. The code is available at https://github.com/MSunDYY/SparseWorld.",
        "url": "http://arxiv.org/abs/2510.17482v1",
        "published_date": "2025-10-20T12:26:25+00:00",
        "updated_date": "2025-10-20T12:26:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Chenxu Dang",
            "Haiyan Liu",
            "Guangjun Bao",
            "Pei An",
            "Xinyue Tang",
            "Jie Ma",
            "Bingchuan Sun",
            "Yan Wang"
        ],
        "tldr": "SparseWorld is a novel 4D occupancy world model for autonomous driving that uses sparse, dynamic queries and a regression-guided formulation to improve perception, forecasting, and planning by better capturing scene dynamics and extending range.",
        "tldr_zh": "SparseWorld是一种新颖的4D占据世界模型，用于自动驾驶，它使用稀疏、动态的查询和回归引导的公式，通过更好地捕获场景动态并扩展范围来改善感知、预测和规划。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment",
        "summary": "Conventional end-to-end (E2E) driving models are effective at generating\nphysically plausible trajectories, but often fail to generalize to long-tail\nscenarios due to the lack of essential world knowledge to understand and reason\nabout surrounding environments. In contrast, Vision-Language-Action (VLA)\nmodels leverage world knowledge to handle challenging cases, but their limited\n3D reasoning capability can lead to physically infeasible actions. In this work\nwe introduce DiffVLA++, an enhanced autonomous driving framework that\nexplicitly bridges cognitive reasoning and E2E planning through metric-guided\nalignment. First, we build a VLA module directly generating semantically\ngrounded driving trajectories. Second, we design an E2E module with a dense\ntrajectory vocabulary that ensures physical feasibility. Third, and most\ncritically, we introduce a metric-guided trajectory scorer that guides and\naligns the outputs of the VLA and E2E modules, thereby integrating their\ncomplementary strengths. The experiment on the ICCV 2025 Autonomous Grand\nChallenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.",
        "url": "http://arxiv.org/abs/2510.17148v1",
        "published_date": "2025-10-20T04:49:14+00:00",
        "updated_date": "2025-10-20T04:49:14+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Yu Gao",
            "Yiru Wang",
            "Anqing Jiang",
            "Heng Yuwen",
            "Wang Shuo",
            "Sun Hao",
            "Wang Jijun"
        ],
        "tldr": "DiffVLA++ combines Vision-Language-Action (VLA) models with end-to-end (E2E) driving using a metric-guided trajectory scorer to improve autonomous driving performance, achieving a high score on the ICCV 2025 Autonomous Grand Challenge leaderboard.",
        "tldr_zh": "DiffVLA++通过度量引导的轨迹评分器，结合了视觉-语言-动作（VLA）模型和端到端（E2E）驾驶，以提高自动驾驶性能，并在 ICCV 2025 自动驾驶大赛排行榜上取得了高分。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads",
        "summary": "4D panoptic segmentation in a streaming setting is critical for highly\ndynamic environments, such as evacuating dense crowds and autonomous driving in\ncomplex scenarios, where real-time, fine-grained perception within a\nconstrained time budget is essential. In this paper, we introduce\n4DSegStreamer, a novel framework that employs a Dual-Thread System to\nefficiently process streaming frames. The framework is general and can be\nseamlessly integrated into existing 3D and 4D segmentation methods to enable\nreal-time capability. It also demonstrates superior robustness compared to\nexisting streaming perception approaches, particularly under high FPS\nconditions. The system consists of a predictive thread and an inference thread.\nThe predictive thread leverages historical motion and geometric information to\nextract features and forecast future dynamics. The inference thread ensures\ntimely prediction for incoming frames by aligning with the latest memory and\ncompensating for ego-motion and dynamic object movements. We evaluate\n4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and\nnuScenes datasets. Comprehensive experiments demonstrate the effectiveness of\nour approach, particularly in accurately predicting dynamic objects in complex\nscenes.",
        "url": "http://arxiv.org/abs/2510.17664v1",
        "published_date": "2025-10-20T15:37:49+00:00",
        "updated_date": "2025-10-20T15:37:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ling Liu",
            "Jun Tian",
            "Li Yi"
        ],
        "tldr": "The paper introduces 4DSegStreamer, a dual-thread system for real-time 4D panoptic segmentation, demonstrating robustness and accuracy in dynamic environments on datasets like HOI4D, SemanticKITTI, and nuScenes.",
        "tldr_zh": "该论文介绍了4DSegStreamer，一个用于实时4D全景分割的双线程系统，并在HOI4D、SemanticKITTI和nuScenes等数据集上展示了其在动态环境中的鲁棒性和准确性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception",
        "summary": "Recent 3D feed-forward models, such as the Visual Geometry Grounded\nTransformer (VGGT), have shown strong capability in inferring 3D attributes of\nstatic scenes. However, since they are typically trained on static datasets,\nthese models often struggle in real-world scenarios involving complex dynamic\nelements, such as moving humans or deformable objects like umbrellas. To\naddress this limitation, we introduce PAGE-4D, a feedforward model that extends\nVGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and\npoint cloud reconstruction -- all without post-processing. A central challenge\nin multi-task 4D reconstruction is the inherent conflict between tasks:\naccurate camera pose estimation requires suppressing dynamic regions, while\ngeometry reconstruction requires modeling them. To resolve this tension, we\npropose a dynamics-aware aggregator that disentangles static and dynamic\ninformation by predicting a dynamics-aware mask -- suppressing motion cues for\npose estimation while amplifying them for geometry reconstruction. Extensive\nexperiments show that PAGE-4D consistently outperforms the original VGGT in\ndynamic scenarios, achieving superior results in camera pose estimation,\nmonocular and video depth estimation, and dense point map reconstruction.",
        "url": "http://arxiv.org/abs/2510.17568v1",
        "published_date": "2025-10-20T14:17:16+00:00",
        "updated_date": "2025-10-20T14:17:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaichen Zhou",
            "Yuhan Wang",
            "Grace Chen",
            "Xinhai Chang",
            "Gaspard Beaudouin",
            "Fangneng Zhan",
            "Paul Pu Liang",
            "Mengyu Wang"
        ],
        "tldr": "PAGE-4D is a feedforward model extending VGGT for dynamic 4D scene reconstruction, addressing the conflict between pose estimation and geometry reconstruction using a dynamics-aware aggregator.",
        "tldr_zh": "PAGE-4D是一个前馈模型，扩展了VGGT用于动态4D场景重建，通过使用动态感知聚合器来解决姿态估计和几何重建之间的冲突。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception",
        "summary": "Deploying real-time spatial perception on edge devices requires efficient\nmulti-task models that leverage complementary task information while minimizing\ncomputational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel\nmulti-task learning framework designed for semantic segmentation and depth,\nedge, and surface normal estimation from a single monocular image. Unlike\nconventional approaches that rely on independent single-task models or shared\nencoder-decoder architectures, M2H introduces a Window-Based Cross-Task\nAttention Module that enables structured feature exchange while preserving\ntask-specific details, improving prediction consistency across tasks. Built on\na lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time\ndeployment and serves as the foundation for monocular spatial perception\nsystems supporting 3D scene graph construction in dynamic environments.\nComprehensive evaluations show that M2H outperforms state-of-the-art multi-task\nmodels on NYUDv2, surpasses single-task depth and semantic baselines on\nHypersim, and achieves superior performance on the Cityscapes dataset, all\nwhile maintaining computational efficiency on laptop hardware. Beyond\nbenchmarks, M2H is validated on real-world data, demonstrating its practicality\nin spatial perception tasks.",
        "url": "http://arxiv.org/abs/2510.17363v1",
        "published_date": "2025-10-20T10:03:31+00:00",
        "updated_date": "2025-10-20T10:03:31+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "U. V. B. L Udugama",
            "George Vosselman",
            "Francesco Nex"
        ],
        "tldr": "The paper introduces M2H, a multi-task learning framework with a window-based cross-task attention module, optimized for real-time monocular spatial perception tasks like semantic segmentation, depth estimation, and surface normal estimation. It outperforms state-of-the-art methods with computational efficiency.",
        "tldr_zh": "该论文介绍了一种名为M2H的多任务学习框架，它采用基于窗口的跨任务注意力模块，针对单目空间感知任务（如语义分割、深度估计和表面法线估计）进行了实时优化。该方法在计算效率方面优于现有技术水平。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models",
        "summary": "Current autonomous driving systems rely on specialized models for perceiving\nand predicting motion, which demonstrate reliable performance in standard\nconditions. However, generalizing cost-effectively to diverse real-world\nscenarios remains a significant challenge. To address this, we propose\nPlug-and-Forecast (PnF), a plug-and-play approach that augments existing motion\nforecasting models with multimodal large language models (MLLMs). PnF builds on\nthe insight that natural language provides a more effective way to describe and\nhandle complex scenarios, enabling quick adaptation to targeted behaviors. We\ndesign prompts to extract structured scene understanding from MLLMs and distill\nthis information into learnable embeddings to augment existing behavior\nprediction models. Our method leverages the zero-shot reasoning capabilities of\nMLLMs to achieve significant improvements in motion prediction performance,\nwhile requiring no fine-tuning -- making it practical to adopt. We validate our\napproach on two state-of-the-art motion forecasting models using the Waymo Open\nMotion Dataset and the nuScenes Dataset, demonstrating consistent performance\nimprovements across both benchmarks.",
        "url": "http://arxiv.org/abs/2510.17274v1",
        "published_date": "2025-10-20T08:01:29+00:00",
        "updated_date": "2025-10-20T08:01:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Katie Luo",
            "Jingwei Ji",
            "Tong He",
            "Runsheng Xu",
            "Yichen Xie",
            "Dragomir Anguelov",
            "Mingxing Tan"
        ],
        "tldr": "The paper introduces Plug-and-Forecast (PnF), a method to enhance motion forecasting models by integrating multimodal large language models (MLLMs) without fine-tuning, demonstrating improved performance on Waymo and nuScenes datasets.",
        "tldr_zh": "该论文介绍了Plug-and-Forecast (PnF)，一种通过集成多模态大型语言模型(MLLM)来增强运动预测模型的方法，无需微调，并在Waymo和nuScenes数据集上展示了性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding",
        "summary": "Three-dimensional (3D) point clouds are becoming increasingly vital in\napplications such as autonomous driving, augmented reality, and immersive\ncommunication, demanding real-time processing and low latency. However, their\nlarge data volumes and bandwidth constraints hinder the deployment of\nhigh-quality services in resource-limited environments. Progres- sive coding,\nwhich allows for decoding at varying levels of detail, provides an alternative\nby allowing initial partial decoding with subsequent refinement. Although\nrecent learning-based point cloud geometry coding methods have achieved notable\nsuccess, their fixed latent representation does not support progressive\ndecoding. To bridge this gap, we propose ProDAT, a novel density-aware\ntail-drop mechanism for progressive point cloud coding. By leveraging density\ninformation as a guidance signal, latent features and coordinates are decoded\nadaptively based on their significance, therefore achieving progressive\ndecoding at multiple bitrates using one single model. Experimental results on\nbenchmark datasets show that the proposed ProDAT not only enables progressive\ncoding but also achieves superior coding efficiency compared to\nstate-of-the-art learning-based coding techniques, with over 28.6% BD-rate\nimprovement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet",
        "url": "http://arxiv.org/abs/2510.17068v1",
        "published_date": "2025-10-20T00:50:16+00:00",
        "updated_date": "2025-10-20T00:50:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhe Luo",
            "Wenjing Jia",
            "Stuart Perry"
        ],
        "tldr": "The paper introduces ProDAT, a density-aware tail-drop mechanism for progressive point cloud coding, enabling efficient progressive decoding with improved coding efficiency compared to state-of-the-art methods.",
        "tldr_zh": "该论文介绍了一种名为ProDAT的密度感知尾部丢弃机制，用于渐进式点云编码，与最先进的方法相比，它能够实现高效的渐进式解码并提高编码效率。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding",
        "summary": "Multimodal 3D grounding has garnered considerable interest in Vision-Language\nModels (VLMs) \\cite{yin2025spatial} for advancing spatial reasoning in complex\nenvironments. However, these models suffer from a severe \"2D semantic bias\"\nthat arises from over-reliance on 2D image features for coarse localization,\nlargely disregarding 3D geometric inputs and resulting in suboptimal fusion\nperformance. In this paper, we propose a novel training framework called\nWhat-Where Representation Re-Forming (W2R2) to tackle this issue via\ndisentangled representation learning and targeted shortcut suppression. Our\napproach fundamentally reshapes the model's internal space by designating 2D\nfeatures as semantic beacons for \"What\" identification and 3D features as\nspatial anchors for \"Where\" localization, enabling precise 3D grounding without\nmodifying inference architecture. Key components include a dual-objective loss\nfunction with an Alignment Loss that supervises fused predictions using adapted\ncross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes\noverly effective 2D-dominant pseudo-outputs via a margin-based mechanism.\nExperiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of\nW2R2, with significant gains in localization accuracy and robustness,\nparticularly in cluttered outdoor scenes.",
        "url": "http://arxiv.org/abs/2510.17034v1",
        "published_date": "2025-10-19T22:40:18+00:00",
        "updated_date": "2025-10-19T22:40:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yutong Zhong"
        ],
        "tldr": "This paper proposes a novel training framework (W2R2) to improve 3D grounding in Vision-Language Models by disentangling 2D semantic features and 3D geometric features, leading to better localization accuracy, especially in cluttered scenes.",
        "tldr_zh": "本文提出了一种新的训练框架（W2R2），通过解耦2D语义特征和3D几何特征来提高视觉语言模型中的3D定位能力，从而提高定位精度，尤其是在杂乱的场景中。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats",
        "summary": "Commercial plant phenotyping systems using fixed cameras cannot perceive many\nplant details due to leaf occlusion. In this paper, we present Botany-Bot, a\nsystem for building detailed \"annotated digital twins\" of living plants using\ntwo stereo cameras, a digital turntable inside a lightbox, an industrial robot\narm, and 3D segmentated Gaussian Splat models. We also present robot algorithms\nfor manipulating leaves to take high-resolution indexable images of occluded\ndetails such as stem buds and the underside/topside of leaves. Results from\nexperiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,\ndetect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and\ntake detailed overside/underside images with 77.3% accuracy. Code, videos, and\ndatasets are available at https://berkeleyautomation.github.io/Botany-Bot/.",
        "url": "http://arxiv.org/abs/2510.17783v1",
        "published_date": "2025-10-20T17:42:20+00:00",
        "updated_date": "2025-10-20T17:42:20+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Simeon Adebola",
            "Chung Min Kim",
            "Justin Kerr",
            "Shuangyu Xie",
            "Prithvi Akella",
            "Jose Luis Susa Rincon",
            "Eugen Solowjow",
            "Ken Goldberg"
        ],
        "tldr": "The paper introduces Botany-Bot, a robotic system using Gaussian Splats and leaf manipulation for detailed digital twin creation of plants, enabling the capture of previously occluded structures with promising accuracy.",
        "tldr_zh": "该论文介绍了Botany-Bot，一个使用高斯溅射和叶片操作的机器人系统，用于创建植物的详细数字孪生模型，能够以有希望的精度捕获先前被遮挡的结构。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    },
    {
        "title": "From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors",
        "summary": "Existing vision-language-action (VLA) models act in 3D real-world but are\ntypically built on 2D encoders, leaving a spatial reasoning gap that limits\ngeneralization and adaptability. Recent 3D integration techniques for VLAs\neither require specialized sensors and transfer poorly across modalities, or\ninject weak cues that lack geometry and degrade vision-language alignment. In\nthis work, we introduce FALCON (From Spatial to Action), a novel paradigm that\ninjects rich 3D spatial tokens into the action head. FALCON leverages spatial\nfoundation models to deliver strong geometric priors from RGB alone, and\nincludes an Embodied Spatial Model that can optionally fuse depth, or pose for\nhigher fidelity when available, without retraining or architectural changes. To\npreserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced\nAction Head rather than being concatenated into the vision-language backbone.\nThese designs enable FALCON to address limitations in spatial representation,\nmodality transferability, and alignment. In comprehensive evaluations across\nthree simulation benchmarks and eleven real-world tasks, our proposed FALCON\nachieves state-of-the-art performance, consistently surpasses competitive\nbaselines, and remains robust under clutter, spatial-prompt conditioning, and\nvariations in object scale and height.",
        "url": "http://arxiv.org/abs/2510.17439v1",
        "published_date": "2025-10-20T11:26:45+00:00",
        "updated_date": "2025-10-20T11:26:45+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Zhengshen Zhang",
            "Hao Li",
            "Yalun Dai",
            "Zhengbang Zhu",
            "Lei Zhou",
            "Chenchen Liu",
            "Dong Wang",
            "Francis E. H. Tay",
            "Sijin Chen",
            "Ziwei Liu",
            "Yuxiao Liu",
            "Xinghang Li",
            "Pan Zhou"
        ]
    }
]