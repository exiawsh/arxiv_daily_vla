[
    {
        "title": "Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction",
        "summary": "End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.",
        "url": "http://arxiv.org/abs/2601.20720v1",
        "published_date": "2026-01-28T15:53:32+00:00",
        "updated_date": "2026-01-28T15:53:32+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Matej Halinkovic",
            "Nina Masarykova",
            "Alexey Vinel",
            "Marek Galinski"
        ],
        "tldr": "Li-ViP3D++ introduces a query-gated deformable fusion method for end-to-end perception and trajectory prediction in autonomous driving, achieving improved performance on nuScenes with faster inference time compared to its predecessor.",
        "tldr_zh": "Li-ViP3D++ 提出了一种查询门控可变形融合方法，用于自动驾驶中的端到端感知和轨迹预测，在 nuScenes 上取得了更好的性能，并且推理时间比其前身更快。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Quartet of Diffusions: Structure-Aware Point Cloud Generation through Part and Symmetry Guidance",
        "summary": "We introduce the Quartet of Diffusions, a structure-aware point cloud generation framework that explicitly models part composition and symmetry. Unlike prior methods that treat shape generation as a holistic process or only support part composition, our approach leverages four coordinated diffusion models to learn distributions of global shape latents, symmetries, semantic parts, and their spatial assembly. This structured pipeline ensures guaranteed symmetry, coherent part placement, and diverse, high-quality outputs. By disentangling the generative process into interpretable components, our method supports fine-grained control over shape attributes, enabling targeted manipulation of individual parts while preserving global consistency. A central global latent further reinforces structural coherence across assembled parts. Our experiments show that the Quartet achieves state-of-the-art performance. To our best knowledge, this is the first 3D point cloud generation framework that fully integrates and enforces both symmetry and part priors throughout the generative process.",
        "url": "http://arxiv.org/abs/2601.20425v1",
        "published_date": "2026-01-28T09:33:14+00:00",
        "updated_date": "2026-01-28T09:33:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenliang Zhou",
            "Fangcheng Zhong",
            "Weihao Xia",
            "Albert Miao",
            "Canberk Baykal",
            "Cengiz Oztireli"
        ],
        "tldr": "The paper introduces a novel point cloud generation framework called Quartet of Diffusions that uses four coordinated diffusion models to generate 3D shapes with guaranteed symmetry, coherent part placement, and high quality, outperforming existing methods.",
        "tldr_zh": "该论文介绍了一种名为Quartet of Diffusions的新型点云生成框架，该框架使用四个协调的扩散模型来生成具有保证对称性、连贯的部件放置和高质量的3D形状，优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CURVE: Learning Causality-Inspired Invariant Representations for Robust Scene Understanding via Uncertainty-Guided Regularization",
        "summary": "Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.",
        "url": "http://arxiv.org/abs/2601.20355v1",
        "published_date": "2026-01-28T08:15:56+00:00",
        "updated_date": "2026-01-28T08:15:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yue Liang",
            "Jiatong Du",
            "Ziyi Yang",
            "Yanjun Huang",
            "Hong Chen"
        ],
        "tldr": "The paper introduces CURVE, a causality-inspired framework for learning invariant scene graph representations to improve out-of-distribution generalization in scene understanding by suppressing environment-specific relations using uncertainty-guided regularization.",
        "tldr_zh": "本文介绍了一种名为CURVE的因果关系启发框架，该框架通过使用不确定性引导的正则化来抑制特定于环境的关系，从而学习不变的场景图表示，以提高场景理解中的分布外泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction",
        "summary": "We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT. Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics. Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures. Finally, we conduct a suite of experiments which includes showing VGGT-SLAM 2.0 can easily be adapted for open-set object detection and demonstrating real time performance while running online onboard a ground robot using a Jetson Thor. We also test in environments ranging from cluttered indoor apartments and office scenes to a 4,200 square foot barn, and we also demonstrate VGGT-SLAM 2.0 achieves the highest accuracy on the TUM dataset with about 23 percent less pose error than VGGT-SLAM. Code will be released upon publication.",
        "url": "http://arxiv.org/abs/2601.19887v1",
        "published_date": "2026-01-27T18:54:29+00:00",
        "updated_date": "2026-01-27T18:54:29+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Dominic Maggio",
            "Luca Carlone"
        ],
        "tldr": "VGGT-SLAM 2.0 improves upon VGGT-SLAM by addressing drift and planar degeneracy, leveraging attention layers for loop closure, and demonstrating real-time performance on a robot, achieving state-of-the-art accuracy on the TUM dataset.",
        "tldr_zh": "VGGT-SLAM 2.0 通过解决漂移和平面退化，利用注意力层进行闭环检测，并在机器人上展示实时性能，从而改进了 VGGT-SLAM，并在 TUM 数据集上实现了最先进的精度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TRACER: Texture-Robust Affordance Chain-of-Thought for Deformable-Object Refinement",
        "summary": "The central challenge in robotic manipulation of deformable objects lies in aligning high-level semantic instructions with physical interaction points under complex appearance and texture variations. Due to near-infinite degrees of freedom, complex dynamics, and heterogeneous patterns, existing vision-based affordance prediction methods often suffer from boundary overflow and fragmented functional regions. To address these issues, we propose TRACER, a Texture-Robust Affordance Chain-of-thought with dEformable-object Refinement framework, which establishes a cross-hierarchical mapping from hierarchical semantic reasoning to appearance-robust and physically consistent functional region refinement. Specifically, a Tree-structured Affordance Chain-of-Thought (TA-CoT) is formulated to decompose high-level task intentions into hierarchical sub-task semantics, providing consistent guidance across various execution stages. To ensure spatial integrity, a Spatial-Constrained Boundary Refinement (SCBR) mechanism is introduced to suppress prediction spillover, guiding the perceptual response to converge toward authentic interaction manifolds. Furthermore, an Interactive Convergence Refinement Flow (ICRF) is developed to aggregate discrete pixels corrupted by appearance noise, significantly enhancing the spatial continuity and physical plausibility of the identified functional regions. Extensive experiments conducted on the Fine-AGDDO15 dataset and a real-world robotic platform demonstrate that TRACER significantly improves affordance grounding precision across diverse textures and patterns inherent to deformable objects. More importantly, it enhances the success rate of long-horizon tasks, effectively bridging the gap between high-level semantic reasoning and low-level physical execution. The source code and dataset will be made publicly available at https://github.com/Dikay1/TRACER.",
        "url": "http://arxiv.org/abs/2601.20208v1",
        "published_date": "2026-01-28T03:12:18+00:00",
        "updated_date": "2026-01-28T03:12:18+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Wanjun Jia",
            "Kang Li",
            "Fan Yang",
            "Mengfei Duan",
            "Wenrui Chen",
            "Yiming Jiang",
            "Hui Zhang",
            "Kailun Yang",
            "Zhiyong Li",
            "Yaonan Wang"
        ],
        "tldr": "The paper introduces TRACER, a novel framework for robotic manipulation of deformable objects that uses a texture-robust affordance chain-of-thought approach to refine functional region identification and improve long-horizon task success.",
        "tldr_zh": "该论文介绍了一种名为TRACER 的新框架，用于机器人操作可变形物体，它采用了一种纹理鲁棒的感知链式思考方法来改进功能区域的识别，并提高长时程任务的成功率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]