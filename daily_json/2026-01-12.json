[
    {
        "title": "MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation",
        "summary": "Most existing 3D referring expression segmentation (3DRES) methods rely on dense, high-quality point clouds, while real-world agents such as robots and mobile phones operate with only a few sparse RGB views and strict latency constraints. We introduce Multi-view 3D Referring Expression Segmentation (MV-3DRES), where the model must recover scene structure and segment the referred object directly from sparse multi-view images. Traditional two-stage pipelines, which first reconstruct a point cloud and then perform segmentation, often yield low-quality geometry, produce coarse or degraded target regions, and run slowly. We propose the Multimodal Visual Geometry Grounded Transformer (MVGGT), an efficient end-to-end framework that integrates language information into sparse-view geometric reasoning through a dual-branch design. Training in this setting exposes a critical optimization barrier, termed Foreground Gradient Dilution (FGD), where sparse 3D signals lead to weak supervision. To resolve this, we introduce Per-view No-target Suppression Optimization (PVSO), which provides stronger and more balanced gradients across views, enabling stable and efficient learning. To support consistent evaluation, we build MVRefer, a benchmark that defines standardized settings and metrics for MV-3DRES. Experiments show that MVGGT establishes the first strong baseline and achieves both high accuracy and fast inference, outperforming existing alternatives. Code and models are publicly available at https://mvggt.github.io.",
        "url": "http://arxiv.org/abs/2601.06874v1",
        "published_date": "2026-01-11T11:44:07+00:00",
        "updated_date": "2026-01-11T11:44:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Changli Wu",
            "Haodong Wang",
            "Jiayi Ji",
            "Yutian Yao",
            "Chunsai Du",
            "Jihua Kang",
            "Yanwei Fu",
            "Liujuan Cao"
        ],
        "tldr": "This paper introduces MV-3DRES, a new task of 3D referring expression segmentation from sparse multi-view images, and proposes MVGGT, an efficient end-to-end framework, along with a novel optimization technique and a new benchmark dataset.",
        "tldr_zh": "本文介绍了MV-3DRES，一个从稀疏多视图图像中进行3D指代表达式分割的新任务。同时，提出了MVGGT，一种高效的端到端框架，以及一种新的优化技术和一个新的基准数据集。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "ObjSplat: Geometry-Aware Gaussian Surfels for Active Object Reconstruction",
        "summary": "Autonomous high-fidelity object reconstruction is fundamental for creating digital assets and bridging the simulation-to-reality gap in robotics. We present ObjSplat, an active reconstruction framework that leverages Gaussian surfels as a unified representation to progressively reconstruct unknown objects with both photorealistic appearance and accurate geometry. Addressing the limitations of conventional opacity or depth-based cues, we introduce a geometry-aware viewpoint evaluation pipeline that explicitly models back-face visibility and occlusion-aware multi-view covisibility, reliably identifying under-reconstructed regions even on geometrically complex objects. Furthermore, to overcome the limitations of greedy planning strategies, ObjSplat employs a next-best-path (NBP) planner that performs multi-step lookahead on a dynamically constructed spatial graph. By jointly optimizing information gain and movement cost, this planner generates globally efficient trajectories. Extensive experiments in simulation and on real-world cultural artifacts demonstrate that ObjSplat produces physically consistent models within minutes, achieving superior reconstruction fidelity and surface completeness while significantly reducing scan time and path length compared to state-of-the-art approaches. Project page: https://li-yuetao.github.io/ObjSplat-page/ .",
        "url": "http://arxiv.org/abs/2601.06997v1",
        "published_date": "2026-01-11T17:14:33+00:00",
        "updated_date": "2026-01-11T17:14:33+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Yuetao Li",
            "Zhizhou Jia",
            "Yu Zhang",
            "Qun Hao",
            "Shaohui Zhang"
        ],
        "tldr": "ObjSplat introduces a geometry-aware Gaussian surfel-based active reconstruction framework with a next-best-path planner for efficient and high-fidelity 3D object reconstruction, demonstrating superior performance on complex objects in both simulation and real-world scenarios.",
        "tldr_zh": "ObjSplat 提出了一种基于几何感知高斯曲面的主动重建框架，该框架采用最佳路径规划器，能够高效、高保真地进行三维物体重建。在模拟和真实场景下对复杂物体的实验表明，该方法具有优越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SpatialNav: Leveraging Spatial Scene Graphs for Zero-Shot Vision-and-Language Navigation",
        "summary": "Although learning-based vision-and-language navigation (VLN) agents can learn spatial knowledge implicitly from large-scale training data, zero-shot VLN agents lack this process, relying primarily on local observations for navigation, which leads to inefficient exploration and a significant performance gap. To deal with the problem, we consider a zero-shot VLN setting that agents are allowed to fully explore the environment before task execution. Then, we construct the Spatial Scene Graph (SSG) to explicitly capture global spatial structure and semantics in the explored environment. Based on the SSG, we introduce SpatialNav, a zero-shot VLN agent that integrates an agent-centric spatial map, a compass-aligned visual representation, and a remote object localization strategy for efficient navigation. Comprehensive experiments in both discrete and continuous environments demonstrate that SpatialNav significantly outperforms existing zero-shot agents and clearly narrows the gap with state-of-the-art learning-based methods. Such results highlight the importance of global spatial representations for generalizable navigation.",
        "url": "http://arxiv.org/abs/2601.06806v1",
        "published_date": "2026-01-11T08:39:19+00:00",
        "updated_date": "2026-01-11T08:39:19+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Jiwen Zhang",
            "Zejun Li",
            "Siyuan Wang",
            "Xiangyu Shi",
            "Zhongyu Wei",
            "Qi Wu"
        ],
        "tldr": "The paper introduces SpatialNav, a zero-shot Vision-and-Language Navigation (VLN) agent that uses a Spatial Scene Graph (SSG) to explicitly capture global spatial structure and semantics, achieving significant performance improvements over existing zero-shot agents.",
        "tldr_zh": "该论文介绍了一种名为SpatialNav的零样本视觉语言导航（VLN）代理，它使用空间场景图（SSG）来显式捕获全局空间结构和语义信息，从而显著提高了现有零样本代理的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]