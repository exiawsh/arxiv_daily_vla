[
    {
        "title": "Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks",
        "summary": "Deploying multiple machine learning models on resource-constrained robotic\nplatforms for different perception tasks often results in redundant\ncomputations, large memory footprints, and complex integration challenges. In\nresponse, this work presents Visual Perception Engine (VPEngine), a modular\nframework designed to enable efficient GPU usage for visual multitasking while\nmaintaining extensibility and developer accessibility. Our framework\narchitecture leverages a shared foundation model backbone that extracts image\nrepresentations, which are efficiently shared, without any unnecessary GPU-CPU\nmemory transfers, across multiple specialized task-specific model heads running\nin parallel. This design eliminates the computational redundancy inherent in\nfeature extraction component when deploying traditional sequential models while\nenabling dynamic task prioritization based on application demands. We\ndemonstrate our framework's capabilities through an example implementation\nusing DINOv2 as the foundation model with multiple task (depth, object\ndetection and semantic segmentation) heads, achieving up to 3x speedup compared\nto sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine\noffers efficient GPU utilization and maintains a constant memory footprint\nwhile allowing per-task inference frequencies to be adjusted dynamically during\nruntime. The framework is written in Python and is open source with ROS2 C++\n(Humble) bindings for ease of use by the robotics community across diverse\nrobotic platforms. Our example implementation demonstrates end-to-end real-time\nperformance at $\\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized\nmodels.",
        "url": "http://arxiv.org/abs/2508.11584v1",
        "published_date": "2025-08-15T16:42:23+00:00",
        "updated_date": "2025-08-15T16:42:23+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Jakub Łucki",
            "Jonathan Becktor",
            "Georgios Georgakis",
            "Robert Royce",
            "Shehryar Khattak"
        ],
        "tldr": "The paper introduces VPEngine, a modular framework for efficient multi-task visual perception on robots, leveraging a shared foundation model backbone and dynamic task prioritization to achieve significant speedups and efficient GPU utilization.",
        "tldr_zh": "该论文介绍了VPEngine，一个用于机器人高效多任务视觉感知的模块化框架，它利用共享的基础模型骨干和动态任务优先级来实现显著的加速和高效的GPU利用率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Relative Position Matters: Trajectory Prediction and Planning with Polar Representation",
        "summary": "Trajectory prediction and planning in autonomous driving are highly\nchallenging due to the complexity of predicting surrounding agents' movements\nand planning the ego agent's actions in dynamic environments. Existing methods\nencode map and agent positions and decode future trajectories in Cartesian\ncoordinates. However, modeling the relationships between the ego vehicle and\nsurrounding traffic elements in Cartesian space can be suboptimal, as it does\nnot naturally capture the varying influence of different elements based on\ntheir relative distances and directions. To address this limitation, we adopt\nthe Polar coordinate system, where positions are represented by radius and\nangle. This representation provides a more intuitive and effective way to model\nspatial changes and relative relationships, especially in terms of distance and\ndirectional influence. Based on this insight, we propose Polaris, a novel\nmethod that operates entirely in Polar coordinates, distinguishing itself from\nconventional Cartesian-based approaches. By leveraging the Polar\nrepresentation, this method explicitly models distance and direction variations\nand captures relative relationships through dedicated encoding and refinement\nmodules, enabling more structured and spatially aware trajectory prediction and\nplanning. Extensive experiments on the challenging prediction (Argoverse 2) and\nplanning benchmarks (nuPlan) demonstrate that Polaris achieves state-of-the-art\nperformance.",
        "url": "http://arxiv.org/abs/2508.11492v1",
        "published_date": "2025-08-15T14:15:11+00:00",
        "updated_date": "2025-08-15T14:15:11+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Bozhou Zhang",
            "Nan Song",
            "Bingzhao Gao",
            "Li Zhang"
        ],
        "tldr": "The paper introduces Polaris, a novel trajectory prediction and planning method for autonomous driving that leverages Polar coordinates to explicitly model distance and direction variations, achieving state-of-the-art performance on Argoverse 2 and nuPlan benchmarks.",
        "tldr_zh": "该论文介绍了一种名为Polaris的新型自动驾驶轨迹预测和规划方法，该方法利用极坐标来显式地建模距离和方向的变化，在Argoverse 2和nuPlan基准测试中实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Perception in Plan: Coupled Perception and Planning for End-to-End Autonomous Driving",
        "summary": "End-to-end autonomous driving has achieved remarkable advancements in recent\nyears. Existing methods primarily follow a perception-planning paradigm, where\nperception and planning are executed sequentially within a fully differentiable\nframework for planning-oriented optimization. We further advance this paradigm\nthrough a perception-in-plan framework design, which integrates perception into\nthe planning process. This design facilitates targeted perception guided by\nevolving planning objectives over time, ultimately enhancing planning\nperformance. Building on this insight, we introduce VeteranAD, a coupled\nperception and planning framework for end-to-end autonomous driving. By\nincorporating multi-mode anchored trajectories as planning priors, the\nperception module is specifically designed to gather traffic elements along\nthese trajectories, enabling comprehensive and targeted perception. Planning\ntrajectories are then generated based on both the perception results and the\nplanning priors. To make perception fully serve planning, we adopt an\nautoregressive strategy that progressively predicts future trajectories while\nfocusing on relevant regions for targeted perception at each step. With this\nsimple yet effective design, VeteranAD fully unleashes the potential of\nplanning-oriented end-to-end methods, leading to more accurate and reliable\ndriving behavior. Extensive experiments on the NAVSIM and Bench2Drive datasets\ndemonstrate that our VeteranAD achieves state-of-the-art performance.",
        "url": "http://arxiv.org/abs/2508.11488v1",
        "published_date": "2025-08-15T14:05:57+00:00",
        "updated_date": "2025-08-15T14:05:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bozhou Zhang",
            "Jingyu Li",
            "Nan Song",
            "Li Zhang"
        ],
        "tldr": "The paper introduces VeteranAD, a novel end-to-end autonomous driving framework that integrates perception into the planning process using an autoregressive strategy for targeted perception, achieving state-of-the-art results.",
        "tldr_zh": "该论文介绍了VeteranAD，一种新颖的端到端自动驾驶框架，它通过使用自回归策略进行有针对性的感知，将感知整合到规划过程中，从而实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving",
        "summary": "Autonomous driving requires rich contextual comprehension and precise\npredictive reasoning to navigate dynamic and complex environments safely.\nVision-Language Models (VLMs) and Driving World Models (DWMs) have\nindependently emerged as powerful recipes addressing different aspects of this\nchallenge. VLMs provide interpretability and robust action prediction through\ntheir ability to understand multi-modal context, while DWMs excel in generating\ndetailed and plausible future driving scenarios essential for proactive\nplanning. Integrating VLMs with DWMs is an intuitive, promising, yet\nunderstudied strategy to exploit the complementary strengths of accurate\nbehavioral prediction and realistic scene generation. Nevertheless, this\nintegration presents notable challenges, particularly in effectively connecting\naction-level decisions with high-fidelity pixel-level predictions and\nmaintaining computational efficiency. In this paper, we propose ImagiDrive, a\nnovel end-to-end autonomous driving framework that integrates a VLM-based\ndriving agent with a DWM-based scene imaginer to form a unified\nimagination-and-planning loop. The driving agent predicts initial driving\ntrajectories based on multi-modal inputs, guiding the scene imaginer to\ngenerate corresponding future scenarios. These imagined scenarios are\nsubsequently utilized to iteratively refine the driving agent's planning\ndecisions. To address efficiency and predictive accuracy challenges inherent in\nthis integration, we introduce an early stopping mechanism and a trajectory\nselection strategy. Extensive experimental validation on the nuScenes and\nNAVSIM datasets demonstrates the robustness and superiority of ImagiDrive over\nprevious alternatives under both open-loop and closed-loop conditions.",
        "url": "http://arxiv.org/abs/2508.11428v1",
        "published_date": "2025-08-15T12:06:55+00:00",
        "updated_date": "2025-08-15T12:06:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingyu Li",
            "Bozhou Zhang",
            "Xin Jin",
            "Jiankang Deng",
            "Xiatian Zhu",
            "Li Zhang"
        ],
        "tldr": "ImagiDrive is a novel autonomous driving framework integrating a VLM-based driving agent with a DWM-based scene imaginer for improved planning and robustness, validated on nuScenes and NAVSIM datasets.",
        "tldr_zh": "ImagiDrive是一个新的自动驾驶框架，它集成了基于VLM的驾驶代理和基于DWM的场景想象器，以提高规划和鲁棒性，并在nuScenes和NAVSIM数据集上进行了验证。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model",
        "summary": "Understanding and recognizing human-object interaction (HOI) is a pivotal\napplication in AR/VR and robotics. Recent open-vocabulary HOI detection\napproaches depend exclusively on large language models for richer textual\nprompts, neglecting their inherent 3D spatial understanding capabilities. To\naddress this shortcoming, we introduce HOID-R1, the first HOI detection\nframework that integrates chain-of-thought (CoT) guided supervised fine-tuning\n(SFT) with group relative policy optimization (GRPO) within a reinforcement\nlearning (RL) paradigm. Specifically, we initially apply SFT to imbue the model\nwith essential reasoning capabilities, forcing the model to articulate its\nthought process in the output. Subsequently, we integrate GRPO to leverage\nmulti-reward signals for policy optimization, thereby enhancing alignment\nacross diverse modalities. To mitigate hallucinations in the CoT reasoning, we\nintroduce an \"MLLM-as-a-judge\" mechanism that supervises the CoT outputs,\nfurther improving generalization. Extensive experiments show that HOID-R1\nachieves state-of-the-art performance on HOI detection benchmarks and\noutperforms existing methods in open-world generalization to novel scenarios.",
        "url": "http://arxiv.org/abs/2508.11350v1",
        "published_date": "2025-08-15T09:28:57+00:00",
        "updated_date": "2025-08-15T09:28:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenhao Zhang",
            "Hanqing Wang",
            "Xiangyu Zeng",
            "Ziyu Cheng",
            "Jiaxin Liu",
            "Haoyu Yan",
            "Zhirui Liu",
            "Kaiyang Ji",
            "Tianxiang Gui",
            "Ke Hu",
            "Kangyi Chen",
            "Yahao Fan",
            "Mokai Pan"
        ],
        "tldr": "The paper introduces HOID-R1, a reinforcement learning framework for open-world human-object interaction detection that integrates supervised fine-tuning with group relative policy optimization, leveraging multimodal large language models for improved reasoning and generalization.",
        "tldr_zh": "该论文介绍了HOID-R1，一个用于开放世界人-物交互检测的强化学习框架，它结合了监督式微调与群体相对策略优化，利用多模态大型语言模型来提高推理和泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking",
        "summary": "3D multi-object tracking is a critical and challenging task in the field of\nautonomous driving. A common paradigm relies on modeling individual object\nmotion, e.g., Kalman filters, to predict trajectories. While effective in\nsimple scenarios, this approach often struggles in crowded environments or with\ninaccurate detections, as it overlooks the rich geometric relationships between\nobjects. This highlights the need to leverage spatial cues. However, existing\ngeometry-aware methods can be susceptible to interference from irrelevant\nobjects, leading to ambiguous features and incorrect associations. To address\nthis, we propose focusing on cue-consistency: identifying and matching stable\nspatial patterns over time. We introduce the Dynamic Scene Cue-Consistency\nTracker (DSC-Track) to implement this principle. Firstly, we design a unified\nspatiotemporal encoder using Point Pair Features (PPF) to learn discriminative\ntrajectory embeddings while suppressing interference. Secondly, our\ncue-consistency transformer module explicitly aligns consistent feature\nrepresentations between historical tracks and current detections. Finally, a\ndynamic update mechanism preserves salient spatiotemporal information for\nstable online tracking. Extensive experiments on the nuScenes and Waymo Open\nDatasets validate the effectiveness and robustness of our approach. On the\nnuScenes benchmark, for instance, our method achieves state-of-the-art\nperformance, reaching 73.2% and 70.3% AMOTA on the validation and test sets,\nrespectively.",
        "url": "http://arxiv.org/abs/2508.11323v1",
        "published_date": "2025-08-15T08:48:13+00:00",
        "updated_date": "2025-08-15T08:48:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haonan Zhang",
            "Xinyao Wang",
            "Boxi Wu",
            "Tu Zheng",
            "Wang Yunhua",
            "Zheng Yang"
        ],
        "tldr": "The paper introduces DSC-Track, a novel 3D multi-object tracking method that leverages dynamic scene cue-consistency by using Point Pair Features, a cue-consistency transformer, and a dynamic update mechanism to achieve state-of-the-art performance on nuScenes and Waymo datasets.",
        "tldr_zh": "该论文介绍了DSC-Track，一种新颖的3D多目标跟踪方法，通过利用动态场景线索一致性，使用点对特征、线索一致性变换器和动态更新机制，在nuScenes和Waymo数据集上实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scene Graph-Guided Proactive Replanning for Failure-Resilient Embodied Agent",
        "summary": "When humans perform everyday tasks, we naturally adjust our actions based on\nthe current state of the environment. For instance, if we intend to put\nsomething into a drawer but notice it is closed, we open it first. However,\nmany autonomous robots lack this adaptive awareness. They often follow\npre-planned actions that may overlook subtle yet critical changes in the scene,\nwhich can result in actions being executed under outdated assumptions and\neventual failure. While replanning is critical for robust autonomy, most\nexisting methods respond only after failures occur, when recovery may be\ninefficient or infeasible. While proactive replanning holds promise for\npreventing failures in advance, current solutions often rely on manually\ndesigned rules and extensive supervision. In this work, we present a proactive\nreplanning framework that detects and corrects failures at subtask boundaries\nby comparing scene graphs constructed from current RGB-D observations against\nreference graphs extracted from successful demonstrations. When the current\nscene fails to align with reference trajectories, a lightweight reasoning\nmodule is activated to diagnose the mismatch and adjust the plan. Experiments\nin the AI2-THOR simulator demonstrate that our approach detects semantic and\nspatial mismatches before execution failures occur, significantly improving\ntask success and robustness.",
        "url": "http://arxiv.org/abs/2508.11286v1",
        "published_date": "2025-08-15T07:48:51+00:00",
        "updated_date": "2025-08-15T07:48:51+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Che Rin Yu",
            "Daewon Chae",
            "Dabin Seo",
            "Sangwon Lee",
            "Hyeongwoo Im",
            "Jinkyu Kim"
        ],
        "tldr": "The paper introduces a scene graph-guided proactive replanning framework for embodied agents that detects and corrects potential failures by comparing current RGB-D observations with reference scene graphs from successful demonstrations, improving task success and robustness.",
        "tldr_zh": "该论文介绍了一个场景图引导的主动重规划框架，用于具身智能体，通过比较当前的 RGB-D 观测与来自成功演示的参考场景图，检测并纠正潜在的失败，从而提高任务的成功率和鲁棒性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds",
        "summary": "Domain generalization in 3D segmentation is a critical challenge in deploying\nmodels to unseen environments. Current methods mitigate the domain shift by\naugmenting the data distribution of point clouds. However, the model learns\nglobal geometric patterns in point clouds while ignoring the category-level\ndistribution and alignment. In this paper, a category-level geometry learning\nframework is proposed to explore the domain-invariant geometric features for\ndomain generalized 3D semantic segmentation. Specifically, Category-level\nGeometry Embedding (CGE) is proposed to perceive the fine-grained geometric\nproperties of point cloud features, which constructs the geometric properties\nof each class and couples geometric embedding to semantic learning. Secondly,\nGeometric Consistent Learning (GCL) is proposed to simulate the latent 3D\ndistribution and align the category-level geometric embeddings, allowing the\nmodel to focus on the geometric invariant information to improve\ngeneralization. Experimental results verify the effectiveness of the proposed\nmethod, which has very competitive segmentation accuracy compared with the\nstate-of-the-art domain generalized point cloud methods.",
        "url": "http://arxiv.org/abs/2508.11265v1",
        "published_date": "2025-08-15T07:02:08+00:00",
        "updated_date": "2025-08-15T07:02:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pei He",
            "Lingling Li",
            "Licheng Jiao",
            "Ronghua Shang",
            "Fang Liu",
            "Shuang Wang",
            "Xu Liu",
            "Wenping Ma"
        ],
        "tldr": "This paper proposes a category-level geometry learning framework for domain generalization in 3D point cloud segmentation, aiming to improve accuracy in unseen environments by focusing on domain-invariant geometric features.",
        "tldr_zh": "本文提出了一种类别级别的几何学习框架，用于三维点云分割中的领域泛化，旨在通过关注领域不变的几何特征来提高在未见环境中的准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving",
        "summary": "Re-Identification (ReID) is a critical technology in intelligent perception\nsystems, especially within autonomous driving, where onboard cameras must\nidentify pedestrians across views and time in real-time to support safe\nnavigation and trajectory prediction. However, the presence of uncertain or\nmissing input modalities--such as RGB, infrared, sketches, or textual\ndescriptions--poses significant challenges to conventional ReID approaches.\nWhile large-scale pre-trained models offer strong multimodal semantic modeling\ncapabilities, their computational overhead limits practical deployment in\nresource-constrained environments. To address these challenges, we propose a\nlightweight Uncertainty Modal Modeling (UMM) framework, which integrates a\nmultimodal token mapper, synthetic modality augmentation strategy, and\ncross-modal cue interactive learner. Together, these components enable unified\nfeature representation, mitigate the impact of missing modalities, and extract\ncomplementary information across different data types. Additionally, UMM\nleverages CLIP's vision-language alignment ability to fuse multimodal inputs\nefficiently without extensive finetuning. Experimental results demonstrate that\nUMM achieves strong robustness, generalization, and computational efficiency\nunder uncertain modality conditions, offering a scalable and practical solution\nfor pedestrian re-identification in autonomous driving scenarios.",
        "url": "http://arxiv.org/abs/2508.11218v1",
        "published_date": "2025-08-15T04:50:27+00:00",
        "updated_date": "2025-08-15T04:50:27+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Jialin Li",
            "Shuqi Wu",
            "Ning Wang"
        ],
        "tldr": "The paper introduces a lightweight CLIP-based Uncertainty Modal Modeling (UMM) framework for pedestrian re-identification in autonomous driving, addressing the challenge of missing or uncertain input modalities and computational constraints.",
        "tldr_zh": "该论文介绍了一种轻量级的基于CLIP的不确定性模态建模(UMM)框架，用于自动驾驶中的行人重识别，解决了输入模态缺失或不确定以及计算约束的挑战。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector",
        "summary": "Monocular 3D object detectors, while effective on data from one ego camera\nheight, struggle with unseen or out-of-distribution camera heights. Existing\nmethods often rely on Plucker embeddings, image transformations or data\naugmentation. This paper takes a step towards this understudied problem by\nfirst investigating the impact of camera height variations on state-of-the-art\n(SoTA) Mono3D models. With a systematic analysis on the extended CARLA dataset\nwith multiple camera heights, we observe that depth estimation is a primary\nfactor influencing performance under height variations. We mathematically prove\nand also empirically observe consistent negative and positive trends in mean\ndepth error of regressed and ground-based depth models, respectively, under\ncamera height changes. To mitigate this, we propose Camera Height Robust\nMonocular 3D Detector (CHARM3R), which averages both depth estimates within the\nmodel. CHARM3R improves generalization to unseen camera heights by more than\n$45\\%$, achieving SoTA performance on the CARLA dataset. Codes and Models at\nhttps://github.com/abhi1kumar/CHARM3R",
        "url": "http://arxiv.org/abs/2508.11185v1",
        "published_date": "2025-08-15T03:27:17+00:00",
        "updated_date": "2025-08-15T03:27:17+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Abhinav Kumar",
            "Yuliang Guo",
            "Zhihao Zhang",
            "Xinyu Huang",
            "Liu Ren",
            "Xiaoming Liu"
        ],
        "tldr": "This paper addresses the problem of monocular 3D object detection performance degradation under varying camera heights by analyzing depth estimation errors and proposing CHARM3R, a method that averages regressed and ground-based depth estimates to improve generalization. The method achieves SoTA performance on the CARLA dataset.",
        "tldr_zh": "本文解决了单目3D物体检测在不同相机高度下性能下降的问题，通过分析深度估计误差，提出了CHARM3R，一种平均回归和基于地面的深度估计的方法，以提高泛化能力。该方法在CARLA数据集上取得了SoTA性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning",
        "summary": "Recent advances have shown that video generation models can enhance robot\nlearning by deriving effective robot actions through inverse dynamics. However,\nthese methods heavily depend on the quality of generated data and struggle with\nfine-grained manipulation due to the lack of environment feedback. While\nvideo-based reinforcement learning improves policy robustness, it remains\nconstrained by the uncertainty of video generation and the challenges of\ncollecting large-scale robot datasets for training diffusion models. To address\nthese limitations, we propose GenFlowRL, which derives shaped rewards from\ngenerated flow trained from diverse cross-embodiment datasets. This enables\nlearning generalizable and robust policies from diverse demonstrations using\nlow-dimensional, object-centric features. Experiments on 10 manipulation tasks,\nboth in simulation and real-world cross-embodiment evaluations, demonstrate\nthat GenFlowRL effectively leverages manipulation features extracted from\ngenerated object-centric flow, consistently achieving superior performance\nacross diverse and challenging scenarios. Our Project Page:\nhttps://colinyu1.github.io/genflowrl",
        "url": "http://arxiv.org/abs/2508.11049v1",
        "published_date": "2025-08-14T20:19:20+00:00",
        "updated_date": "2025-08-14T20:19:20+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Kelin Yu",
            "Sheng Zhang",
            "Harshit Soora",
            "Furong Huang",
            "Heng Huang",
            "Pratap Tokekar",
            "Ruohan Gao"
        ],
        "tldr": "GenFlowRL improves visual reinforcement learning by deriving shaped rewards from generated object-centric flow trained on diverse cross-embodiment datasets, achieving superior performance in manipulation tasks.",
        "tldr_zh": "GenFlowRL通过从各种跨具身数据集中训练的生成对象中心流中获得塑造的奖励，从而改进了视觉强化学习，并在操作任务中实现了卓越的性能。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Hyperspectral vs. RGB for Pedestrian Segmentation in Urban Driving Scenes: A Comparative Study",
        "summary": "Pedestrian segmentation in automotive perception systems faces critical\nsafety challenges due to metamerism in RGB imaging, where pedestrians and\nbackgrounds appear visually indistinguishable.. This study investigates the\npotential of hyperspectral imaging (HSI) for enhanced pedestrian segmentation\nin urban driving scenarios using the Hyperspectral City v2 (H-City) dataset. We\ncompared standard RGB against two dimensionality-reduction approaches by\nconverting 128-channel HSI data into three-channel representations: Principal\nComponent Analysis (PCA) and optimal band selection using Contrast\nSignal-to-Noise Ratio with Joint Mutual Information Maximization (CSNR-JMIM).\nThree semantic segmentation models were evaluated: U-Net, DeepLabV3+, and\nSegFormer. CSNR-JMIM consistently outperformed RGB with an average improvements\nof 1.44% in Intersection over Union (IoU) and 2.18% in F1-score for pedestrian\nsegmentation. Rider segmentation showed similar gains with 1.43% IoU and 2.25%\nF1-score improvements. These improved performance results from enhanced\nspectral discrimination of optimally selected HSI bands effectively reducing\nfalse positives. This study demonstrates robust pedestrian segmentation through\noptimal HSI band selection, showing significant potential for safety-critical\nautomotive applications.",
        "url": "http://arxiv.org/abs/2508.11301v1",
        "published_date": "2025-08-15T08:10:19+00:00",
        "updated_date": "2025-08-15T08:10:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiarong Li",
            "Imad Ali Shah",
            "Enda Ward",
            "Martin Glavin",
            "Edward Jones",
            "Brian Deegan"
        ],
        "tldr": "This paper compares hyperspectral imaging (HSI) with RGB for pedestrian and rider segmentation in urban driving, finding that dimensionality-reduced HSI using CSNR-JMIM outperforms RGB, improving IoU and F1-score.",
        "tldr_zh": "本文比较了高光谱成像 (HSI) 和 RGB 在城市驾驶中行人及骑行者分割方面的性能，发现使用 CSNR-JMIM 降维的 HSI 优于 RGB，提高了 IoU 和 F1 分数。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]