[
    {
        "title": "PAGS: Priority-Adaptive Gaussian Splatting for Dynamic Driving Scenes",
        "summary": "Reconstructing dynamic 3D urban scenes is crucial for autonomous driving, yet\ncurrent methods face a stark trade-off between fidelity and computational cost.\nThis inefficiency stems from their semantically agnostic design, which\nallocates resources uniformly, treating static backgrounds and safety-critical\nobjects with equal importance. To address this, we introduce Priority-Adaptive\nGaussian Splatting (PAGS), a framework that injects task-aware semantic\npriorities directly into the 3D reconstruction and rendering pipeline. PAGS\nintroduces two core contributions: (1) Semantically-Guided Pruning and\nRegularization strategy, which employs a hybrid importance metric to\naggressively simplify non-critical scene elements while preserving fine-grained\ndetails on objects vital for navigation. (2) Priority-Driven Rendering\npipeline, which employs a priority-based depth pre-pass to aggressively cull\noccluded primitives and accelerate the final shading computations. Extensive\nexperiments on the Waymo and KITTI datasets demonstrate that PAGS achieves\nexceptional reconstruction quality, particularly on safety-critical objects,\nwhile significantly reducing training time and boosting rendering speeds to\nover 350 FPS.",
        "url": "http://arxiv.org/abs/2510.12282v1",
        "published_date": "2025-10-14T08:36:09+00:00",
        "updated_date": "2025-10-14T08:36:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ying A",
            "Wenzhang Sun",
            "Chang Zeng",
            "Chunfeng Wang",
            "Hao Li",
            "Jianxun Cui"
        ],
        "tldr": "PAGS introduces a priority-adaptive Gaussian Splatting framework for dynamic driving scenes that improves reconstruction quality of critical objects while reducing computational cost by prioritizing semantic information. It achieves over 350 FPS rendering speeds.",
        "tldr_zh": "PAGS 引入了一种优先级自适应的高斯溅射框架，用于动态驾驶场景，通过优先考虑语义信息，提高关键物体的重建质量，同时降低计算成本。它实现了超过 350 FPS 的渲染速度。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "MCOP: Multi-UAV Collaborative Occupancy Prediction",
        "summary": "Unmanned Aerial Vehicle (UAV) swarm systems necessitate efficient\ncollaborative perception mechanisms for diverse operational scenarios. Current\nBird's Eye View (BEV)-based approaches exhibit two main limitations:\nbounding-box representations fail to capture complete semantic and geometric\ninformation of the scene, and their performance significantly degrades when\nencountering undefined or occluded objects. To address these limitations, we\npropose a novel multi-UAV collaborative occupancy prediction framework. Our\nframework effectively preserves 3D spatial structures and semantics through\nintegrating a Spatial-Aware Feature Encoder and Cross-Agent Feature\nIntegration. To enhance efficiency, we further introduce Altitude-Aware Feature\nReduction to compactly represent scene information, along with a Dual-Mask\nPerceptual Guidance mechanism to adaptively select features and reduce\ncommunication overhead. Due to the absence of suitable benchmark datasets, we\nextend three datasets for evaluation: two virtual datasets (Air-to-Pred-Occ and\nUAV3D-Occ) and one real-world dataset (GauUScene-Occ). Experiments results\ndemonstrate that our method achieves state-of-the-art accuracy, significantly\noutperforming existing collaborative methods while reducing communication\noverhead to only a fraction of previous approaches.",
        "url": "http://arxiv.org/abs/2510.12679v1",
        "published_date": "2025-10-14T16:17:42+00:00",
        "updated_date": "2025-10-14T16:17:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zefu Lin",
            "Wenbo Chen",
            "Xiaojuan Jin",
            "Yuran Yang",
            "Lue Fan",
            "Yixin Zhang",
            "Yufeng Zhang",
            "Zhaoxiang Zhang"
        ],
        "tldr": "This paper introduces MCOP, a multi-UAV collaborative occupancy prediction framework that uses spatial-aware feature encoding, cross-agent feature integration, altitude-aware feature reduction, and dual-mask perceptual guidance to improve accuracy and reduce communication overhead compared to BEV-based methods.",
        "tldr_zh": "该论文介绍了MCOP，一种多无人机协同占据预测框架，它使用空间感知特征编码、跨代理特征集成、高度感知特征减少和双掩码感知引导，与基于BEV的方法相比，提高了准确性并降低了通信开销。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving",
        "summary": "End-to-end autonomous driving models trained solely with imitation learning\n(IL) often suffer from poor generalization. In contrast, reinforcement learning\n(RL) promotes exploration through reward maximization but faces challenges such\nas sample inefficiency and unstable convergence. A natural solution is to\ncombine IL and RL. Moving beyond the conventional two-stage paradigm (IL\npretraining followed by RL fine-tuning), we propose CoIRL-AD, a competitive\ndual-policy framework that enables IL and RL agents to interact during\ntraining. CoIRL-AD introduces a competition-based mechanism that facilitates\nknowledge exchange while preventing gradient conflicts. Experiments on the\nnuScenes dataset show an 18% reduction in collision rate compared to baselines,\nalong with stronger generalization and improved performance on long-tail\nscenarios. Code is available at: https://github.com/SEU-zxj/CoIRL-AD.",
        "url": "http://arxiv.org/abs/2510.12560v1",
        "published_date": "2025-10-14T14:21:52+00:00",
        "updated_date": "2025-10-14T14:21:52+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Xiaoji Zheng",
            "Ziyuan Yang",
            "Yanhao Chen",
            "Yuhang Peng",
            "Yuanrong Tang",
            "Gengyuan Liu",
            "Bokui Chen",
            "Jiangtao Gong"
        ],
        "tldr": "The paper introduces CoIRL-AD, a collaborative-competitive imitation-reinforcement learning framework with dual-policy training, which improves autonomous driving performance with a significant reduction in collision rate on the nuScenes dataset.",
        "tldr_zh": "该论文提出了CoIRL-AD，一个协作竞争的模仿-强化学习框架，具有双策略训练，通过在nuScenes数据集上显著降低碰撞率来提高自动驾驶性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Scene Coordinate Reconstruction Priors",
        "summary": "Scene coordinate regression (SCR) models have proven to be powerful implicit\nscene representations for 3D vision, enabling visual relocalization and\nstructure-from-motion. SCR models are trained specifically for one scene. If\ntraining images imply insufficient multi-view constraints SCR models\ndegenerate. We present a probabilistic reinterpretation of training SCR models,\nwhich allows us to infuse high-level reconstruction priors. We investigate\nmultiple such priors, ranging from simple priors over the distribution of\nreconstructed depth values to learned priors over plausible scene coordinate\nconfigurations. For the latter, we train a 3D point cloud diffusion model on a\nlarge corpus of indoor scans. Our priors push predicted 3D scene points towards\nplausible geometry at each training step to increase their likelihood. On three\nindoor datasets our priors help learning better scene representations,\nresulting in more coherent scene point clouds, higher registration rates and\nbetter camera poses, with a positive effect on down-stream tasks such as novel\nview synthesis and camera relocalization.",
        "url": "http://arxiv.org/abs/2510.12387v1",
        "published_date": "2025-10-14T11:13:31+00:00",
        "updated_date": "2025-10-14T11:13:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenjing Bian",
            "Axel Barroso-Laguna",
            "Tommaso Cavallari",
            "Victor Adrian Prisacariu",
            "Eric Brachmann"
        ],
        "tldr": "The paper introduces a probabilistic reinterpretation of scene coordinate regression (SCR) training, incorporating reconstruction priors (depth distribution and learned scene coordinate configurations via a 3D point cloud diffusion model) to improve scene representation learning, resulting in better 3D reconstruction and camera pose estimation.",
        "tldr_zh": "该论文提出了一种场景坐标回归(SCR)训练的概率重新解释方法，通过结合重建先验（深度分布和使用3D点云扩散模型学习到的场景坐标配置）来改进场景表示学习，从而产生更好的3D重建和相机姿态估计。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "CurriFlow: Curriculum-Guided Depth Fusion with Optical Flow-Based Temporal Alignment for 3D Semantic Scene Completion",
        "summary": "Semantic Scene Completion (SSC) aims to infer complete 3D geometry and\nsemantics from monocular images, serving as a crucial capability for\ncamera-based perception in autonomous driving. However, existing SSC methods\nrelying on temporal stacking or depth projection often lack explicit motion\nreasoning and struggle with occlusions and noisy depth supervision. We propose\nCurriFlow, a novel semantic occupancy prediction framework that integrates\noptical flow-based temporal alignment with curriculum-guided depth fusion.\nCurriFlow employs a multi-level fusion strategy to align segmentation, visual,\nand depth features across frames using pre-trained optical flow, thereby\nimproving temporal consistency and dynamic object understanding. To enhance\ngeometric robustness, a curriculum learning mechanism progressively transitions\nfrom sparse yet accurate LiDAR depth to dense but noisy stereo depth during\ntraining, ensuring stable optimization and seamless adaptation to real-world\ndeployment. Furthermore, semantic priors from the Segment Anything Model (SAM)\nprovide category-agnostic supervision, strengthening voxel-level semantic\nlearning and spatial consistency. Experiments on the SemanticKITTI benchmark\ndemonstrate that CurriFlow achieves state-of-the-art performance with a mean\nIoU of 16.9, validating the effectiveness of our motion-guided and\ncurriculum-aware design for camera-based 3D semantic scene completion.",
        "url": "http://arxiv.org/abs/2510.12362v1",
        "published_date": "2025-10-14T10:25:26+00:00",
        "updated_date": "2025-10-14T10:25:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinzhou Lin",
            "Jie Zhou",
            "Wenhao Xu",
            "Rongtao Xu",
            "Changwei Wang",
            "Shunpeng Chen",
            "Kexue Fu",
            "Yihua Shao",
            "Li Guo",
            "Shibiao Xu"
        ],
        "tldr": "The paper introduces CurriFlow, a novel framework for 3D Semantic Scene Completion that uses optical flow for temporal alignment and curriculum learning for depth fusion, achieving state-of-the-art performance on SemanticKITTI.",
        "tldr_zh": "该论文介绍了一种名为 CurriFlow 的新型 3D 语义场景补全框架，该框架使用光流进行时间对齐，并使用课程学习进行深度融合，在 SemanticKITTI 上实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Hybrid Gaussian Splatting for Novel Urban View Synthesis",
        "summary": "This paper describes the Qualcomm AI Research solution to the RealADSim-NVS\nchallenge, hosted at the RealADSim Workshop at ICCV 2025. The challenge\nconcerns novel view synthesis in street scenes, and participants are required\nto generate, starting from car-centric frames captured during some training\ntraversals, renders of the same urban environment as viewed from a different\ntraversal (e.g. different street lane or car direction). Our solution is\ninspired by hybrid methods in scene generation and generative simulators\nmerging gaussian splatting and diffusion models, and it is composed of two\nstages: First, we fit a 3D reconstruction of the scene and render novel views\nas seen from the target cameras. Then, we enhance the resulting frames with a\ndedicated single-step diffusion model. We discuss specific choices made in the\ninitialization of gaussian primitives as well as the finetuning of the enhancer\nmodel and its training data curation. We report the performance of our model\ndesign and we ablate its components in terms of novel view quality as measured\nby PSNR, SSIM and LPIPS. On the public leaderboard reporting test results, our\nproposal reaches an aggregated score of 0.432, achieving the second place\noverall.",
        "url": "http://arxiv.org/abs/2510.12308v1",
        "published_date": "2025-10-14T09:09:13+00:00",
        "updated_date": "2025-10-14T09:09:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mohamed Omran",
            "Farhad Zanjani",
            "Davide Abati",
            "Jens Petersen",
            "Amirhossein Habibian"
        ],
        "tldr": "This paper presents a hybrid Gaussian splatting and diffusion model approach for novel view synthesis in urban street scenes, achieving second place in the RealADSim-NVS challenge at ICCV 2025. The method uses 3D reconstruction and a single-step diffusion model for image enhancement.",
        "tldr_zh": "该论文提出了一种混合高斯溅射和扩散模型方法，用于城市街道场景中的新视角合成，并在 ICCV 2025 的 RealADSim-NVS 挑战赛中获得第二名。该方法使用 3D 重建和一个单步扩散模型进行图像增强。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "BEEP3D: Box-Supervised End-to-End Pseudo-Mask Generation for 3D Instance Segmentation",
        "summary": "3D instance segmentation is crucial for understanding complex 3D\nenvironments, yet fully supervised methods require dense point-level\nannotations, resulting in substantial annotation costs and labor overhead. To\nmitigate this, box-level annotations have been explored as a weaker but more\nscalable form of supervision. However, box annotations inherently introduce\nambiguity in overlapping regions, making accurate point-to-instance assignment\nchallenging. Recent methods address this ambiguity by generating pseudo-masks\nthrough training a dedicated pseudo-labeler in an additional training stage.\nHowever, such two-stage pipelines often increase overall training time and\ncomplexity, hinder end-to-end optimization. To overcome these challenges, we\npropose BEEP3D-Box-supervised End-to-End Pseudo-mask generation for 3D instance\nsegmentation. BEEP3D adopts a student-teacher framework, where the teacher\nmodel serves as a pseudo-labeler and is updated by the student model via an\nExponential Moving Average. To better guide the teacher model to generate\nprecise pseudo-masks, we introduce an instance center-based query refinement\nthat enhances position query localization and leverages features near instance\ncenters. Additionally, we design two novel losses-query consistency loss and\nmasked feature consistency loss-to align semantic and geometric signals between\npredictions and pseudo-masks. Extensive experiments on ScanNetV2 and S3DIS\ndatasets demonstrate that BEEP3D achieves competitive or superior performance\ncompared to state-of-the-art weakly supervised methods while remaining\ncomputationally efficient.",
        "url": "http://arxiv.org/abs/2510.12182v1",
        "published_date": "2025-10-14T06:23:18+00:00",
        "updated_date": "2025-10-14T06:23:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Youngju Yoo",
            "Seho Kim",
            "Changick Kim"
        ],
        "tldr": "The paper introduces BEEP3D, a box-supervised end-to-end pseudo-mask generation method for 3D instance segmentation using a student-teacher framework and novel losses, achieving competitive performance on ScanNetV2 and S3DIS.",
        "tldr_zh": "该论文介绍了BEEP3D，一种基于盒监督的端到端伪掩码生成方法，用于三维实例分割，采用学生-教师框架和新颖的损失函数，在ScanNetV2和S3DIS数据集上取得了有竞争力的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Gaussian Semantic Field for One-shot LiDAR Global Localization",
        "summary": "We present a one-shot LiDAR global localization algorithm featuring semantic\ndisambiguation ability based on a lightweight tri-layered scene graph. While\nlandmark semantic registration-based methods have shown promising performance\nimprovements in global localization compared with geometric-only methods,\nlandmarks can be repetitive and misleading for correspondence establishment. We\npropose to mitigate this problem by modeling semantic distributions with\ncontinuous functions learned from a population of Gaussian processes. Compared\nwith discrete semantic labels, the continuous functions capture finer-grained\ngeo-semantic information and also provide more detailed metric information for\ncorrespondence establishment. We insert this continuous function as the middle\nlayer between the object layer and the metric-semantic layer, forming a\ntri-layered 3D scene graph, serving as a light-weight yet performant backend\nfor one-shot localization. We term our global localization pipeline Outram-GSF\n(Gaussian semantic field) and conduct a wide range of experiments on publicly\navailable data sets, validating the superior performance against the current\nstate-of-the-art.",
        "url": "http://arxiv.org/abs/2510.12101v1",
        "published_date": "2025-10-14T03:08:02+00:00",
        "updated_date": "2025-10-14T03:08:02+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Pengyu Yin",
            "Shenghai Yuan",
            "Haozhi Cao",
            "Xingyu Ji",
            "Ruofei Bai",
            "Siyu Chen",
            "Lihua Xie"
        ],
        "tldr": "This paper introduces a novel one-shot LiDAR global localization algorithm, Outram-GSF, which uses Gaussian processes to model semantic distributions for improved accuracy and robustness compared to existing methods.",
        "tldr_zh": "本文提出了一种新的单次激光雷达全局定位算法 Outram-GSF，该算法使用高斯过程来建模语义分布，与现有方法相比，提高了准确性和鲁棒性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Prompt-Guided Spatial Understanding with RGB-D Transformers for Fine-Grained Object Relation Reasoning",
        "summary": "Spatial reasoning in large-scale 3D environments such as warehouses remains a\nsignificant challenge for vision-language systems due to scene clutter,\nocclusions, and the need for precise spatial understanding. Existing models\noften struggle with generalization in such settings, as they rely heavily on\nlocal appearance and lack explicit spatial grounding. In this work, we\nintroduce a dedicated spatial reasoning framework for the Physical AI Spatial\nIntelligence Warehouse dataset introduced in the Track 3 2025 AI City\nChallenge. Our approach enhances spatial comprehension by embedding mask\ndimensions in the form of bounding box coordinates directly into the input\nprompts, enabling the model to reason over object geometry and layout. We\nfine-tune the framework across four question categories namely: Distance\nEstimation, Object Counting, Multi-choice Grounding, and Spatial Relation\nInference using task-specific supervision. To further improve consistency with\nthe evaluation system, normalized answers are appended to the GPT response\nwithin the training set. Our comprehensive pipeline achieves a final score of\n73.0606, placing 4th overall on the public leaderboard. These results\ndemonstrate the effectiveness of structured prompt enrichment and targeted\noptimization in advancing spatial reasoning for real-world industrial\nenvironments.",
        "url": "http://arxiv.org/abs/2510.11996v1",
        "published_date": "2025-10-13T22:51:20+00:00",
        "updated_date": "2025-10-13T22:51:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tanner Muturi",
            "Blessing Agyei Kyem",
            "Joshua Kofi Asamoah",
            "Neema Jakisa Owor",
            "Richard Dyzinela",
            "Andrews Danyo",
            "Yaw Adu-Gyamfi",
            "Armstrong Aboah"
        ],
        "tldr": "This paper introduces a prompt-guided RGB-D Transformer framework for spatial reasoning in 3D warehouse environments, achieving 4th place in the AI City Challenge by embedding bounding box coordinates into prompts and fine-tuning on specific spatial reasoning tasks.",
        "tldr_zh": "本文介绍了一种prompt引导的RGB-D Transformer框架，用于3D仓库环境中的空间推理。该方法通过将边界框坐标嵌入到prompt中，并针对特定空间推理任务进行微调，在AI City Challenge中获得了第4名。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "AngularFuse: A Closer Look at Angle-based Perception for Spatial-Sensitive Multi-Modality Image Fusion",
        "summary": "Visible-infrared image fusion is crucial in key applications such as\nautonomous driving and nighttime surveillance. Its main goal is to integrate\nmultimodal information to produce enhanced images that are better suited for\ndownstream tasks. Although deep learning based fusion methods have made\nsignificant progress, mainstream unsupervised approaches still face serious\nchallenges in practical applications. Existing methods mostly rely on manually\ndesigned loss functions to guide the fusion process. However, these loss\nfunctions have obvious limitations. On one hand, the reference images\nconstructed by existing methods often lack details and have uneven brightness.\nOn the other hand, the widely used gradient losses focus only on gradient\nmagnitude. To address these challenges, this paper proposes an angle-based\nperception framework for spatial-sensitive image fusion (AngularFuse). At\nfirst, we design a cross-modal complementary mask module to force the network\nto learn complementary information between modalities. Then, a fine-grained\nreference image synthesis strategy is introduced. By combining Laplacian edge\nenhancement with adaptive histogram equalization, reference images with richer\ndetails and more balanced brightness are generated. Last but not least, we\nintroduce an angle-aware loss, which for the first time constrains both\ngradient magnitude and direction simultaneously in the gradient domain.\nAngularFuse ensures that the fused images preserve both texture intensity and\ncorrect edge orientation. Comprehensive experiments on the MSRS, RoadScene, and\nM3FD public datasets show that AngularFuse outperforms existing mainstream\nmethods with clear margin. Visual comparisons further confirm that our method\nproduces sharper and more detailed results in challenging scenes, demonstrating\nsuperior fusion capability.",
        "url": "http://arxiv.org/abs/2510.12260v1",
        "published_date": "2025-10-14T08:13:15+00:00",
        "updated_date": "2025-10-14T08:13:15+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "eess.IV"
        ],
        "authors": [
            "Xiaopeng Liu",
            "Yupei Lin",
            "Sen Zhang",
            "Xiao Wang",
            "Yukai Shi",
            "Liang Lin"
        ],
        "tldr": "This paper introduces AngularFuse, a novel angle-based perception framework for visible-infrared image fusion, employing a cross-modal complementary mask module, a fine-grained reference image synthesis strategy, and an angle-aware loss function. It outperforms existing methods on multiple datasets.",
        "tldr_zh": "本文介绍了AngularFuse，一种新颖的基于角度感知的可见光-红外图像融合框架，它采用了跨模态互补掩码模块、细粒度的参考图像合成策略和角度感知损失函数。在多个数据集上，该方法优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]