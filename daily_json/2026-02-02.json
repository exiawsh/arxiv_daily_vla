[
    {
        "title": "TF-Lane: Traffic Flow Module for Robust Lane Perception",
        "summary": "Autonomous driving systems require robust lane perception capabilities, yet existing vision-based detection methods suffer significant performance degradation when visual sensors provide insufficient cues, such as in occluded or lane-missing scenarios. While some approaches incorporate high-definition maps as supplementary information, these solutions face challenges of high subscription costs and limited real-time performance. To address these limitations, we explore an innovative information source: traffic flow, which offers real-time capabilities without additional costs. This paper proposes a TrafficFlow-aware Lane perception Module (TFM) that effectively extracts real-time traffic flow features and seamlessly integrates them with existing lane perception algorithms. This solution originated from real-world autonomous driving conditions and was subsequently validated on open-source algorithms and datasets. Extensive experiments on four mainstream models and two public datasets (Nuscenes and OpenLaneV2) using standard evaluation metrics show that TFM consistently improves performance, achieving up to +4.1% mAP gain on the Nuscenes dataset.",
        "url": "http://arxiv.org/abs/2602.01277v1",
        "published_date": "2026-02-01T15:18:48+00:00",
        "updated_date": "2026-02-01T15:18:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yihan Xie",
            "Han Xia",
            "Zhen Yang"
        ],
        "tldr": "This paper introduces a TrafficFlow-aware Lane perception Module (TFM) that leverages real-time traffic flow information to enhance lane perception in autonomous driving, improving performance in challenging scenarios without relying on expensive HD maps.",
        "tldr_zh": "本文提出了一种交通流感知的车道感知模块(TFM)，该模块利用实时交通流信息来增强自动驾驶中的车道感知，从而在具有挑战性的场景中提高性能，而无需依赖昂贵的高清地图。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OASIS-DC: Generalizable Depth Completion via Output-level Alignment of Sparse-Integrated Monocular Pseudo Depth",
        "summary": "Recent monocular foundation models excel at zero-shot depth estimation, yet their outputs are inherently relative rather than metric, limiting direct use in robotics and autonomous driving. We leverage the fact that relative depth preserves global layout and boundaries: by calibrating it with sparse range measurements, we transform it into a pseudo metric depth prior. Building on this prior, we design a refinement network that follows the prior where reliable and deviates where necessary, enabling accurate metric predictions from very few labeled samples. The resulting system is particularly effective when curated validation data are unavailable, sustaining stable scale and sharp edges across few-shot regimes. These findings suggest that coupling foundation priors with sparse anchors is a practical route to robust, deployment-ready depth completion under real-world label scarcity.",
        "url": "http://arxiv.org/abs/2602.01268v1",
        "published_date": "2026-02-01T14:57:33+00:00",
        "updated_date": "2026-02-01T14:57:33+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Jaehyeon Cho",
            "Jhonghyun An"
        ],
        "tldr": "This paper introduces OASIS-DC, a depth completion method that combines monocular foundation model priors with sparse range measurements to generate accurate metric depth maps, especially effective in label-scarce environments.",
        "tldr_zh": "本文介绍了一种深度补全方法OASIS-DC，该方法结合了单目基础模型的先验知识和稀疏范围测量，以生成准确的度量深度图，尤其是在标签稀缺的环境中非常有效。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs",
        "summary": "Vision-Language-Action (VLA) models have emerged as a dominant paradigm for generalist robotic manipulation, unifying perception and control within a single end-to-end architecture. However, despite their success in controlled environments, reliable real-world deployment is severely hindered by their fragility to visual disturbances. While existing literature extensively addresses physical occlusions caused by scene geometry, a critical mode remains largely unexplored: image corruptions. These sensor-level artifacts, ranging from electronic noise and dead pixels to lens contaminants, directly compromise the integrity of the visual signal prior to interpretation. In this work, we quantify this vulnerability, demonstrating that state-of-the-art VLAs such as $π_{0.5}$ and SmolVLA, suffer catastrophic performance degradation, dropping from 90\\% success rates to as low as 2\\%, under common signal artifacts. To mitigate this, we introduce the Corruption Restoration Transformer (CRT), a plug-and-play and model-agnostic vision transformer designed to immunize VLA models against sensor disturbances. Leveraging an adversarial training objective, CRT restores clean observations from corrupted inputs without requiring computationally expensive fine-tuning of the underlying model. Extensive experiments across the LIBERO and Meta-World benchmarks demonstrate that CRT effectively recovers lost performance, enabling VLAs to maintain near-baseline success rates, even under severe visual corruption.",
        "url": "http://arxiv.org/abs/2602.01158v1",
        "published_date": "2026-02-01T11:09:08+00:00",
        "updated_date": "2026-02-01T11:09:08+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Daniel Yezid Guarnizo Orjuela",
            "Leonardo Scappatura",
            "Veronica Di Gennaro",
            "Riccardo Andrea Izzo",
            "Gianluca Bardaro",
            "Matteo Matteucci"
        ],
        "tldr": "This paper introduces a Corruption Restoration Transformer (CRT) to improve the robustness of Vision-Language-Action (VLA) models against image corruptions, a significant problem impacting their real-world deployment.",
        "tldr_zh": "该论文介绍了一种 Corruption Restoration Transformer (CRT)，旨在提高视觉-语言-动作 (VLA) 模型对图像损坏的鲁棒性，这是一个影响其真实世界部署的重要问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LightCity: An Urban Dataset for Outdoor Inverse Rendering and Reconstruction under Multi-illumination Conditions",
        "summary": "Inverse rendering in urban scenes is pivotal for applications like autonomous driving and digital twins. Yet, it faces significant challenges due to complex illumination conditions, including multi-illumination and indirect light and shadow effects. However, the effects of these challenges on intrinsic decomposition and 3D reconstruction have not been explored due to the lack of appropriate datasets. In this paper, we present LightCity, a novel high-quality synthetic urban dataset featuring diverse illumination conditions with realistic indirect light and shadow effects. LightCity encompasses over 300 sky maps with highly controllable illumination, varying scales with street-level and aerial perspectives over 50K images, and rich properties such as depth, normal, material components, light and indirect light, etc. Besides, we leverage LightCity to benchmark three fundamental tasks in the urban environments and conduct a comprehensive analysis of these benchmarks, laying a robust foundation for advancing related research.",
        "url": "http://arxiv.org/abs/2602.01118v1",
        "published_date": "2026-02-01T09:37:00+00:00",
        "updated_date": "2026-02-01T09:37:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jingjing Wang",
            "Qirui Hu",
            "Chong Bao",
            "Yuke Zhu",
            "Hujun Bao",
            "Zhaopeng Cui",
            "Guofeng Zhang"
        ],
        "tldr": "The paper introduces LightCity, a new synthetic urban dataset with diverse and realistic illumination conditions, designed to benchmark and advance research in inverse rendering and 3D reconstruction for urban environments.",
        "tldr_zh": "该论文介绍了一个新的合成城市数据集LightCity，该数据集具有多样且逼真的照明条件，旨在为城市环境中的逆向渲染和3D重建研究提供基准并推动其发展。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "KAN We Flow? Advancing Robotic Manipulation with 3D Flow Matching via KAN & RWKV",
        "summary": "Diffusion-based visuomotor policies excel at modeling action distributions but are inference-inefficient, since recursively denoising from noise to policy requires many steps and heavy UNet backbones, which hinders deployment on resource-constrained robots. Flow matching alleviates the sampling burden by learning a one-step vector field, yet prior implementations still inherit large UNet-style architectures. In this work, we present KAN-We-Flow, a flow-matching policy that draws on recent advances in Receptance Weighted Key Value (RWKV) and Kolmogorov-Arnold Networks (KAN) from vision to build a lightweight and highly expressive backbone for 3D manipulation. Concretely, we introduce an RWKV-KAN block: an RWKV first performs efficient time/channel mixing to propagate task context, and a subsequent GroupKAN layer applies learnable spline-based, groupwise functional mappings to perform feature-wise nonlinear calibration of the action mapping on RWKV outputs. Moreover, we introduce an Action Consistency Regularization (ACR), a lightweight auxiliary loss that enforces alignment between predicted action trajectories and expert demonstrations via Euler extrapolation, providing additional supervision to stabilize training and improve policy precision. Without resorting to large UNets, our design reduces parameters by 86.8\\%, maintains fast runtime, and achieves state-of-the-art success rates on Adroit, Meta-World, and DexArt benchmarks. Our project page can be viewed in \\href{https://zhihaochen-2003.github.io/KAN-We-Flow.github.io/}{\\textcolor{red}{link}}",
        "url": "http://arxiv.org/abs/2602.01115v1",
        "published_date": "2026-02-01T09:27:56+00:00",
        "updated_date": "2026-02-01T09:27:56+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Zhihao Chen",
            "Yiyuan Ge",
            "Ziyang Wang"
        ],
        "tldr": "This paper introduces KAN-We-Flow, a lightweight flow-matching policy for robotic manipulation that utilizes RWKV and KAN architectures and action consistency regularization to achieve state-of-the-art performance with significantly reduced parameters.",
        "tldr_zh": "本文介绍了 KAN-We-Flow，一种轻量级的流匹配策略，用于机器人操作，它利用 RWKV 和 KAN 架构以及动作一致性正则化，以显著减少的参数实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CLAMP: Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining",
        "summary": "Leveraging pre-trained 2D image representations in behavior cloning policies has achieved great success and has become a standard approach for robotic manipulation. However, such representations fail to capture the 3D spatial information about objects and scenes that is essential for precise manipulation. In this work, we introduce Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining (CLAMP), a novel 3D pre-training framework that utilizes point clouds and robot actions. From the merged point cloud computed from RGB-D images and camera extrinsics, we re-render multi-view four-channel image observations with depth and 3D coordinates, including dynamic wrist views, to provide clearer views of target objects for high-precision manipulation tasks. The pre-trained encoders learn to associate the 3D geometric and positional information of objects with robot action patterns via contrastive learning on large-scale simulated robot trajectories. During encoder pre-training, we pre-train a Diffusion Policy to initialize the policy weights for fine-tuning, which is essential for improving fine-tuning sample efficiency and performance. After pre-training, we fine-tune the policy on a limited amount of task demonstrations using the learned image and action representations. We demonstrate that this pre-training and fine-tuning design substantially improves learning efficiency and policy performance on unseen tasks. Furthermore, we show that CLAMP outperforms state-of-the-art baselines across six simulated tasks and five real-world tasks.",
        "url": "http://arxiv.org/abs/2602.00937v1",
        "published_date": "2026-01-31T23:32:54+00:00",
        "updated_date": "2026-01-31T23:32:54+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "I-Chun Arthur Liu",
            "Krzysztof Choromanski",
            "Sandy Huang",
            "Connor Schenck"
        ],
        "tldr": "CLAMP introduces a 3D pre-training framework using contrastive learning to associate 3D scene information with robot actions, significantly improving fine-tuning efficiency and performance on robotic manipulation tasks in simulation and the real world.",
        "tldr_zh": "CLAMP 引入了一个 3D 预训练框架，使用对比学习将 3D 场景信息与机器人动作关联起来，显著提高了在模拟和真实世界中机器人操作任务的微调效率和性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]