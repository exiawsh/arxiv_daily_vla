[
    {
        "title": "Generating Adversarial Events: A Motion-Aware Point Cloud Framework",
        "summary": "Event cameras have been widely adopted in safety-critical domains such as autonomous driving, robotics, and human-computer interaction. A pressing challenge arises from the vulnerability of deep neural networks to adversarial examples, which poses a significant threat to the reliability of event-based systems. Nevertheless, research into adversarial attacks on events is scarce. This is primarily due to the non-differentiable nature of mainstream event representations, which hinders the extension of gradient-based attack methods. In this paper, we propose MA-ADV, a novel \\textbf{M}otion-\\textbf{A}ware \\textbf{Adv}ersarial framework. To the best of our knowledge, this is the first work to generate adversarial events by leveraging point cloud representations. MA-ADV accounts for high-frequency noise in events and employs a diffusion-based approach to smooth perturbations, while fully leveraging the spatial and temporal relationships among events. Finally, MA-ADV identifies the minimal-cost perturbation through a combination of sample-wise Adam optimization, iterative refinement, and binary search. Extensive experimental results validate that MA-ADV ensures a 100\\% attack success rate with minimal perturbation cost, and also demonstrate enhanced robustness against defenses, underscoring the critical security challenges facing future event-based perception systems.",
        "url": "http://arxiv.org/abs/2602.08230v1",
        "published_date": "2026-02-09T03:06:07+00:00",
        "updated_date": "2026-02-09T03:06:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Hongwei Ren",
            "Youxin Jiang",
            "Qifei Gu",
            "Xiangqian Wu"
        ],
        "tldr": "This paper introduces MA-ADV, a novel framework for generating adversarial events using point cloud representations, demonstrating a 100% attack success rate and robustness against defenses, thus highlighting security vulnerabilities in event-based perception systems.",
        "tldr_zh": "本文介绍了一种新的框架MA-ADV，该框架使用点云表示生成对抗性事件，展示了100％的攻击成功率和针对防御的鲁棒性，从而突出了基于事件的感知系统中的安全漏洞。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "MambaFusion: Adaptive State-Space Fusion for Multimodal 3D Object Detection",
        "summary": "Reliable 3D object detection is fundamental to autonomous driving, and multimodal fusion algorithms using cameras and LiDAR remain a persistent challenge. Cameras provide dense visual cues but ill posed depth; LiDAR provides a precise 3D structure but sparse coverage. Existing BEV-based fusion frameworks have made good progress, but they have difficulties including inefficient context modeling, spatially invariant fusion, and reasoning under uncertainty. We introduce MambaFusion, a unified multi-modal detection framework that achieves efficient, adaptive, and physically grounded 3D perception. MambaFusion interleaves selective state-space models (SSMs) with windowed transformers to propagate the global context in linear time while preserving local geometric fidelity. A multi-modal token alignment (MTA) module and reliability-aware fusion gates dynamically re-weight camera-LiDAR features based on spatial confidence and calibration consistency. Finally, a structure-conditioned diffusion head integrates graph-based reasoning with uncertainty-aware denoising, enforcing physical plausibility, and calibrated confidence. MambaFusion establishes new state-of-the-art performance on nuScenes benchmarks while operating with linear-time complexity. The framework demonstrates that coupling SSM-based efficiency with reliability-driven fusion yields robust, temporally stable, and interpretable 3D perception for real-world autonomous driving systems.",
        "url": "http://arxiv.org/abs/2602.08126v1",
        "published_date": "2026-02-08T21:10:46+00:00",
        "updated_date": "2026-02-08T21:10:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Venkatraman Narayanan",
            "Bala Sai",
            "Rahul Ahuja",
            "Pratik Likhar",
            "Varun Ravi Kumar",
            "Senthil Yogamani"
        ],
        "tldr": "MambaFusion introduces a novel multimodal 3D object detection framework using selective state-space models and reliability-aware fusion to achieve state-of-the-art performance on nuScenes with linear-time complexity, improving robustness and interpretability.",
        "tldr_zh": "MambaFusion 引入了一种新颖的多模态 3D 对象检测框架，该框架使用选择性状态空间模型和可靠性感知融合，在 nuScenes 上以线性时间复杂度实现了最先进的性能，提高了鲁棒性和可解释性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Modeling 3D Pedestrian-Vehicle Interactions for Vehicle-Conditioned Pose Forecasting",
        "summary": "Accurately predicting pedestrian motion is crucial for safe and reliable autonomous driving in complex urban environments. In this work, we present a 3D vehicle-conditioned pedestrian pose forecasting framework that explicitly incorporates surrounding vehicle information. To support this, we enhance the Waymo-3DSkelMo dataset with aligned 3D vehicle bounding boxes, enabling realistic modeling of multi-agent pedestrian-vehicle interactions. We introduce a sampling scheme to categorize scenes by pedestrian and vehicle count, facilitating training across varying interaction complexities. Our proposed network adapts the TBIFormer architecture with a dedicated vehicle encoder and pedestrian-vehicle interaction cross-attention module to fuse pedestrian and vehicle features, allowing predictions to be conditioned on both historical pedestrian motion and surrounding vehicles. Extensive experiments demonstrate substantial improvements in forecasting accuracy and validate different approaches for modeling pedestrian-vehicle interactions, highlighting the importance of vehicle-aware 3D pose prediction for autonomous driving. Code is available at: https://github.com/GuangxunZhu/VehCondPose3D",
        "url": "http://arxiv.org/abs/2602.08962v1",
        "published_date": "2026-02-09T17:58:53+00:00",
        "updated_date": "2026-02-09T17:58:53+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Guangxun Zhu",
            "Xuan Liu",
            "Nicolas Pugeault",
            "Chongfeng Wei",
            "Edmond S. L. Ho"
        ],
        "tldr": "This paper introduces a 3D vehicle-conditioned pedestrian pose forecasting framework, using an enhanced Waymo-3DSkelMo dataset and a modified TBIFormer architecture with cross-attention, to improve pedestrian motion prediction in autonomous driving scenarios.",
        "tldr_zh": "本文提出了一个3D车辆条件下的行人姿态预测框架，使用增强的Waymo-3DSkelMo数据集和一个带有交叉注意机制的改进TBIFormer架构，以提高自动驾驶场景中的行人运动预测。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reliability-aware Execution Gating for Near-field and Off-axis Vision-guided Robotic Alignment",
        "summary": "Vision-guided robotic systems are increasingly deployed in precision alignment tasks that require reliable execution under near-field and off-axis configurations. While recent advances in pose estimation have significantly improved numerical accuracy, practical robotic systems still suffer from frequent execution failures even when pose estimates appear accurate. This gap suggests that pose accuracy alone is insufficient to guarantee execution-level reliability. In this paper, we reveal that such failures arise from a deterministic geometric error amplification mechanism, in which small pose estimation errors are magnified through system structure and motion execution, leading to unstable or failed alignment. Rather than modifying pose estimation algorithms, we propose a Reliability-aware Execution Gating mechanism that operates at the execution level. The proposed approach evaluates geometric consistency and configuration risk before execution, and selectively rejects or scales high-risk pose updates. We validate the proposed method on a real UR5 robotic platform performing single-step visual alignment tasks under varying camera-target distances and off-axis configurations. Experimental results demonstrate that the proposed execution gating significantly improves task success rates, reduces execution variance, and suppresses tail-risk behavior, while leaving average pose accuracy largely unchanged. Importantly, the proposed mechanism is estimator-agnostic and can be readily integrated with both classical geometry-based and learning-based pose estimation pipelines. These results highlight the importance of execution-level reliability modeling and provide a practical solution for improving robustness in near-field vision-guided robotic systems.",
        "url": "http://arxiv.org/abs/2602.08466v1",
        "published_date": "2026-02-09T10:14:39+00:00",
        "updated_date": "2026-02-09T10:14:39+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Ning Hu",
            "Senhao Cao",
            "Maochen Li"
        ],
        "tldr": "This paper proposes a reliability-aware execution gating mechanism for vision-guided robots that selectively rejects or scales high-risk pose updates based on geometric consistency and configuration risk, improving alignment success rates without modifying pose estimation algorithms.",
        "tldr_zh": "本文提出了一种可靠性感知执行门控机制，用于视觉引导机器人。该机制基于几何一致性和配置风险，选择性地拒绝或缩放高风险的姿态更新，从而在不修改姿态估计算法的情况下，提高了对准成功率。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Informative Object-centric Next Best View for Object-aware 3D Gaussian Splatting in Cluttered Scenes",
        "summary": "In cluttered scenes with inevitable occlusions and incomplete observations, selecting informative viewpoints is essential for building a reliable representation. In this context, 3D Gaussian Splatting (3DGS) offers a distinct advantage, as it can explicitly guide the selection of subsequent viewpoints and then refine the representation with new observations. However, existing approaches rely solely on geometric cues, neglect manipulation-relevant semantics, and tend to prioritize exploitation over exploration. To tackle these limitations, we introduce an instance-aware Next Best View (NBV) policy that prioritizes underexplored regions by leveraging object features. Specifically, our object-aware 3DGS distills instancelevel information into one-hot object vectors, which are used to compute confidence-weighted information gain that guides the identification of regions associated with erroneous and uncertain Gaussians. Furthermore, our method can be easily adapted to an object-centric NBV, which focuses view selection on a target object, thereby improving reconstruction robustness to object placement. Experiments demonstrate that our NBV policy reduces depth error by up to 77.14% on the synthetic dataset and 34.10% on the real-world GraspNet dataset compared to baselines. Moreover, compared to targeting the entire scene, performing NBV on a specific object yields an additional reduction of 25.60% in depth error for that object. We further validate the effectiveness of our approach through real-world robotic manipulation tasks.",
        "url": "http://arxiv.org/abs/2602.08266v1",
        "published_date": "2026-02-09T04:50:36+00:00",
        "updated_date": "2026-02-09T04:50:36+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Seunghoon Jeong",
            "Eunho Lee",
            "Jeongyun Kim",
            "Ayoung Kim"
        ],
        "tldr": "This paper introduces an object-aware Next Best View (NBV) policy for 3D Gaussian Splatting (3DGS) that leverages object features to prioritize underexplored regions and improve reconstruction accuracy, particularly in cluttered scenes. It also demonstrates improved performance in robotic manipulation tasks.",
        "tldr_zh": "本文提出了一种面向3D高斯溅射（3DGS）的物体感知最佳观察点（NBV）策略，该策略利用物体特征来优先考虑未充分探索的区域，并提高重建精度，尤其是在杂乱的场景中。它还展示了在机器人操作任务中改进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Self-Supervised Bootstrapping of Action-Predictive Embodied Reasoning",
        "summary": "Embodied Chain-of-Thought (CoT) reasoning has significantly enhanced Vision-Language-Action (VLA) models, yet current methods rely on rigid templates to specify reasoning primitives (e.g., objects in the scene, high-level plans, structural affordances). These templates can force policies to process irrelevant information that distracts from critical action-prediction signals. This creates a bottleneck: without successful policies, we cannot verify reasoning quality; without quality reasoning, we cannot build robust policies. We introduce R&B-EnCoRe, which enables models to bootstrap embodied reasoning from internet-scale knowledge through self-supervised refinement. By treating reasoning as a latent variable within importance-weighted variational inference, models can generate and distill a refined reasoning training dataset of embodiment-specific strategies without external rewards, verifiers, or human annotation. We validate R&B-EnCoRe across manipulation (Franka Panda in simulation, WidowX in hardware), legged navigation (bipedal, wheeled, bicycle, quadruped), and autonomous driving embodiments using various VLA architectures with 1B, 4B, 7B, and 30B parameters. Our approach achieves 28% gains in manipulation success, 101% improvement in navigation scores, and 21% reduction in collision-rate metric over models that indiscriminately reason about all available primitives. R&B-EnCoRe enables models to distill reasoning that is predictive of successful control, bypassing manual annotation engineering while grounding internet-scale knowledge in physical execution.",
        "url": "http://arxiv.org/abs/2602.08167v1",
        "published_date": "2026-02-09T00:10:17+00:00",
        "updated_date": "2026-02-09T00:10:17+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Milan Ganai",
            "Katie Luo",
            "Jonas Frey",
            "Clark Barrett",
            "Marco Pavone"
        ],
        "tldr": "The paper introduces R&B-EnCoRe, a self-supervised method that bootstraps embodied reasoning for Vision-Language-Action models by refining reasoning strategies without external rewards or human annotation, leading to significant performance gains across various robotic embodiments and tasks.",
        "tldr_zh": "该论文介绍了 R&B-EnCoRe，一种自监督方法，通过在没有外部奖励或人工标注的情况下完善推理策略，来引导视觉-语言-动作模型的具身推理，从而在各种机器人化身和任务中实现显着的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Gesture Matters: Pedestrian Gesture Recognition for AVs Through Skeleton Pose Evaluation",
        "summary": "Gestures are a key component of non-verbal communication in traffic, often helping pedestrian-to-driver interactions when formal traffic rules may be insufficient. This problem becomes more apparent when autonomous vehicles (AVs) struggle to interpret such gestures. In this study, we present a gesture classification framework using 2D pose estimation applied to real-world video sequences from the WIVW dataset. We categorise gestures into four primary classes (Stop, Go, Thank & Greet, and No Gesture) and extract 76 static and dynamic features from normalised keypoints. Our analysis demonstrates that hand position and movement velocity are especially discriminative in distinguishing between gesture classes, achieving a classification accuracy score of 87%. These findings not only improve the perceptual capabilities of AV systems but also contribute to the broader understanding of pedestrian behaviour in traffic contexts.",
        "url": "http://arxiv.org/abs/2602.08479v1",
        "published_date": "2026-02-09T10:28:21+00:00",
        "updated_date": "2026-02-09T10:28:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.ET",
            "cs.HC",
            "cs.LG"
        ],
        "authors": [
            "Alif Rizqullah Mahdi",
            "Mahdi Rezaei",
            "Natasha Merat"
        ],
        "tldr": "This paper presents a gesture classification framework for autonomous vehicles using 2D pose estimation on real-world video, achieving 87% accuracy in classifying four gesture classes.",
        "tldr_zh": "本文提出了一种用于自动驾驶汽车的手势分类框架，该框架使用2D姿势估计技术处理真实世界的视频，在四个手势类别中的分类准确率达到87%。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "BiManiBench: A Hierarchical Benchmark for Evaluating Bimanual Coordination of Multimodal Large Language Models",
        "summary": "Multimodal Large Language Models (MLLMs) have significantly advanced embodied AI, and using them to benchmark robotic intelligence has become a pivotal trend. However, existing frameworks remain predominantly confined to single-arm manipulation, failing to capture the spatio-temporal coordination required for bimanual tasks like lifting a heavy pot. To address this, we introduce BiManiBench, a hierarchical benchmark evaluating MLLMs across three tiers: fundamental spatial reasoning, high-level action planning, and low-level end-effector control. Our framework isolates unique bimanual challenges, such as arm reachability and kinematic constraints, thereby distinguishing perceptual hallucinations from planning failures. Analysis of over 30 state-of-the-art models reveals that despite high-level reasoning proficiency, MLLMs struggle with dual-arm spatial grounding and control, frequently resulting in mutual interference and sequencing errors. These findings suggest the current paradigm lacks a deep understanding of mutual kinematic constraints, highlighting the need for future research to focus on inter-arm collision-avoidance and fine-grained temporal sequencing.",
        "url": "http://arxiv.org/abs/2602.08392v1",
        "published_date": "2026-02-09T08:47:14+00:00",
        "updated_date": "2026-02-09T08:47:14+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Xin Wu",
            "Zhixuan Liang",
            "Yue Ma",
            "Mengkang Hu",
            "Zhiyuan Qin",
            "Xiu Li"
        ],
        "tldr": "The paper introduces BiManiBench, a hierarchical benchmark for evaluating the bimanual coordination capabilities of MLLMs in robotics, revealing limitations in spatial grounding and control despite high-level reasoning proficiency.",
        "tldr_zh": "该论文介绍了BiManiBench，一个分层基准，用于评估MLLM在机器人技术中的双臂协调能力，揭示了尽管具有高级推理能力，但在空间定位和控制方面的局限性。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs",
        "summary": "While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.",
        "url": "http://arxiv.org/abs/2602.08241v1",
        "published_date": "2026-02-09T03:33:23+00:00",
        "updated_date": "2026-02-09T03:33:23+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Siqu Ou",
            "Tianrui Wan",
            "Zhiyuan Zhao",
            "Junyu Gao",
            "Xuelong Li"
        ],
        "tldr": "This paper introduces SAYO, a reinforcement learning-based approach to improve visual attention in MLLMs, addressing the issue of weak visual focus and error propagation in reasoning tasks. Experiments show performance improvements on multimodal benchmarks.",
        "tldr_zh": "该论文介绍了SAYO，一种基于强化学习的方法，旨在提高MLLM中的视觉注意力，解决推理任务中视觉焦点较弱和误差传播的问题。实验表明，在多模态基准测试中，性能有所提高。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]