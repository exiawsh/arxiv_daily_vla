[
    {
        "title": "AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving",
        "summary": "Autonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present AutoDriDM, a decision-centric, progressive benchmark with 6,650 questions across three dimensions - Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models' reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.",
        "url": "http://arxiv.org/abs/2601.14702v1",
        "published_date": "2026-01-21T06:29:09+00:00",
        "updated_date": "2026-01-21T06:29:09+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Zecong Tang",
            "Zixu Wang",
            "Yifei Wang",
            "Weitong Lian",
            "Tianjian Gao",
            "Haoran Li",
            "Tengju Ru",
            "Lingyi Meng",
            "Zhejun Cui",
            "Yichen Zhu",
            "Qi Kang",
            "Kaixuan Wang",
            "Yu Zhang"
        ],
        "tldr": "The paper introduces AutoDriDM, a new benchmark specifically designed to evaluate the decision-making capabilities of vision-language models in autonomous driving, highlighting the gap between perception and decision-making performance in current VLMs.",
        "tldr_zh": "该论文介绍了AutoDriDM，一个专门用于评估视觉-语言模型在自动驾驶中决策能力的新基准，强调了当前视觉-语言模型在感知和决策性能之间的差距。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
        "summary": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \\mid v)$ and a language-conditioned posterior $π(a \\mid v, \\ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.",
        "url": "http://arxiv.org/abs/2601.15197v1",
        "published_date": "2026-01-21T17:15:22+00:00",
        "updated_date": "2026-01-21T17:15:22+00:00",
        "categories": [
            "cs.AI",
            "cs.CL",
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Shijie Lian",
            "Bin Yu",
            "Xiaopeng Lin",
            "Laurence T. Yang",
            "Zhaolong Shen",
            "Changti Wu",
            "Yuzhuo Miao",
            "Cong Huang",
            "Kai Chen"
        ],
        "tldr": "The paper proposes BayesianVLA, a framework to mitigate information collapse in Vision-Language-Action models by enforcing instruction following via Bayesian decomposition and maximizing pointwise mutual information between actions and instructions, leading to improved generalization.",
        "tldr_zh": "该论文提出了 BayesianVLA，一种通过贝叶斯分解来强制执行指令遵循，并通过最大化动作和指令之间的逐点互信息来缓解视觉-语言-动作模型中的信息崩溃的框架，从而提高泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ExPrIS: Knowledge-Level Expectations as Priors for Object Interpretation from Sensor Data",
        "summary": "While deep learning has significantly advanced robotic object recognition, purely data-driven approaches often lack semantic consistency and fail to leverage valuable, pre-existing knowledge about the environment. This report presents the ExPrIS project, which addresses this challenge by investigating how knowledge-level expectations can serve as to improve object interpretation from sensor data. Our approach is based on the incremental construction of a 3D Semantic Scene Graph (3DSSG). We integrate expectations from two sources: contextual priors from past observations and semantic knowledge from external graphs like ConceptNet. These are embedded into a heterogeneous Graph Neural Network (GNN) to create an expectation-biased inference process. This method moves beyond static, frame-by-frame analysis to enhance the robustness and consistency of scene understanding over time. The report details this architecture, its evaluation, and outlines its planned integration on a mobile robotic platform.",
        "url": "http://arxiv.org/abs/2601.15025v1",
        "published_date": "2026-01-21T14:27:38+00:00",
        "updated_date": "2026-01-21T14:27:38+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Marian Renz",
            "Martin Günther",
            "Felix Igelbrink",
            "Oscar Lima",
            "Martin Atzmueller"
        ],
        "tldr": "The paper introduces ExPrIS, a system that leverages knowledge-level expectations from past observations and external knowledge graphs within a GNN to improve object interpretation from sensor data for robotic scene understanding.",
        "tldr_zh": "该论文介绍了 ExPrIS，一个利用来自过去观察和外部知识图谱的知识水平预期在 GNN 中改进传感器数据中的对象解释的系统，用于机器人场景理解。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SpatialMem: Unified 3D Memory with Metric Anchoring and Fast Retrieval",
        "summary": "We present SpatialMem, a memory-centric system that unifies 3D geometry, semantics, and language into a single, queryable representation. Starting from casually captured egocentric RGB video, SpatialMem reconstructs metrically scaled indoor environments, detects structural 3D anchors (walls, doors, windows) as the first-layer scaffold, and populates a hierarchical memory with open-vocabulary object nodes -- linking evidence patches, visual embeddings, and two-layer textual descriptions to 3D coordinates -- for compact storage and fast retrieval. This design enables interpretable reasoning over spatial relations (e.g., distance, direction, visibility) and supports downstream tasks such as language-guided navigation and object retrieval without specialized sensors. Experiments across three real-life indoor scenes demonstrate that SpatialMem maintains strong anchor-description-level navigation completion and hierarchical retrieval accuracy under increasing clutter and occlusion, offering an efficient and extensible framework for embodied spatial intelligence.",
        "url": "http://arxiv.org/abs/2601.14895v1",
        "published_date": "2026-01-21T11:32:24+00:00",
        "updated_date": "2026-01-21T11:32:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xinyi Zheng",
            "Yunze Liu",
            "Chi-Hao Wu",
            "Fan Zhang",
            "Hao Zheng",
            "Wenqi Zhou",
            "Walterio W. Mayol-Cuevas",
            "Junxiao Shen"
        ],
        "tldr": "SpatialMem is a system that unifies 3D scene understanding with language through a hierarchical 3D memory structure, enabling spatial reasoning and language-guided navigation from egocentric video.",
        "tldr_zh": "SpatialMem是一个将3D场景理解与语言融合的系统，它通过分层的3D记忆结构，从以自我为中心的视频中实现空间推理和语言引导的导航。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "XD-MAP: Cross-Modal Domain Adaptation using Semantic Parametric Mapping",
        "summary": "Until open-world foundation models match the performance of specialized approaches, the effectiveness of deep learning models remains heavily dependent on dataset availability. Training data must align not only with the target object categories but also with the sensor characteristics and modalities. To bridge the gap between available datasets and deployment domains, domain adaptation strategies are widely used. In this work, we propose a novel approach to transferring sensor-specific knowledge from an image dataset to LiDAR, an entirely different sensing domain. Our method XD-MAP leverages detections from a neural network on camera images to create a semantic parametric map. The map elements are modeled to produce pseudo labels in the target domain without any manual annotation effort. Unlike previous domain transfer approaches, our method does not require direct overlap between sensors and enables extending the angular perception range from a front-view camera to a full 360 view. On our large-scale road feature dataset, XD-MAP outperforms single shot baseline approaches by +19.5 mIoU for 2D semantic segmentation, +19.5 PQth for 2D panoptic segmentation, and +32.3 mIoU in 3D semantic segmentation. The results demonstrate the effectiveness of our approach achieving strong performance on LiDAR data without any manual labeling.",
        "url": "http://arxiv.org/abs/2601.14477v1",
        "published_date": "2026-01-20T21:00:26+00:00",
        "updated_date": "2026-01-20T21:00:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "eess.IV"
        ],
        "authors": [
            "Frank Bieder",
            "Hendrik Königshof",
            "Haohao Hu",
            "Fabian Immel",
            "Yinzhe Shen",
            "Jan-Hendrik Pauls",
            "Christoph Stiller"
        ],
        "tldr": "The paper introduces XD-MAP, a novel cross-modal domain adaptation method that transfers knowledge from camera images to LiDAR data by creating semantic parametric maps for pseudo-label generation, achieving significant performance gains in semantic segmentation tasks on a large-scale road feature dataset.",
        "tldr_zh": "该论文介绍了一种名为XD-MAP的新型跨模态领域自适应方法，通过创建语义参数化地图来生成伪标签，将知识从相机图像转移到激光雷达数据，从而在一个大规模道路特征数据集上的语义分割任务中实现了显著的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Gaussian Based Adaptive Multi-Modal 3D Semantic Occupancy Prediction",
        "summary": "The sparse object detection paradigm shift towards dense 3D semantic occupancy prediction is necessary for dealing with long-tail safety challenges for autonomous vehicles. Nonetheless, the current voxelization methods commonly suffer from excessive computation complexity demands, where the fusion process is brittle, static, and breaks down under dynamic environmental settings. To this end, this research work enhances a novel Gaussian-based adaptive camera-LiDAR multimodal 3D occupancy prediction model that seamlessly bridges the semantic strengths of camera modality with the geometric strengths of LiDAR modality through a memory-efficient 3D Gaussian model. The proposed solution has four key components: (1) LiDAR Depth Feature Aggregation (LDFA), where depth-wise deformable sampling is employed for dealing with geometric sparsity, (2) Entropy-Based Feature Smoothing, where cross-entropy is employed for handling domain-specific noise, (3) Adaptive Camera-LiDAR Fusion, where dynamic recalibration of sensor outputs is performed based on model outputs, and (4) Gauss-Mamba Head that uses Selective State Space Models for global context decoding that enjoys linear computation complexity.",
        "url": "http://arxiv.org/abs/2601.14448v1",
        "published_date": "2026-01-20T20:11:09+00:00",
        "updated_date": "2026-01-20T20:11:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "A. Enes Doruk"
        ],
        "tldr": "This paper introduces a Gaussian-based adaptive multimodal 3D occupancy prediction model for autonomous vehicles, addressing computational complexity and brittleness in dynamic environments using a combination of LiDAR and camera data with an efficient Gaussian representation and selective state space models.",
        "tldr_zh": "本文提出了一种基于高斯的自适应多模态3D占据预测模型，用于自动驾驶车辆，通过结合激光雷达和相机数据，并使用高效的高斯表示和选择性状态空间模型，解决了动态环境中计算复杂性和脆弱性的问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Vision-Based Natural Language Scene Understanding for Autonomous Driving: An Extended Dataset and a New Model for Traffic Scene Description Generation",
        "summary": "Traffic scene understanding is essential for enabling autonomous vehicles to accurately perceive and interpret their environment, thereby ensuring safe navigation. This paper presents a novel framework that transforms a single frontal-view camera image into a concise natural language description, effectively capturing spatial layouts, semantic relationships, and driving-relevant cues. The proposed model leverages a hybrid attention mechanism to enhance spatial and semantic feature extraction and integrates these features to generate contextually rich and detailed scene descriptions. To address the limited availability of specialized datasets in this domain, a new dataset derived from the BDD100K dataset has been developed, with comprehensive guidelines provided for its construction. Furthermore, the study offers an in-depth discussion of relevant evaluation metrics, identifying the most appropriate measures for this task. Extensive quantitative evaluations using metrics such as CIDEr and SPICE, complemented by human judgment assessments, demonstrate that the proposed model achieves strong performance and effectively fulfills its intended objectives on the newly developed dataset.",
        "url": "http://arxiv.org/abs/2601.14438v1",
        "published_date": "2026-01-20T19:50:42+00:00",
        "updated_date": "2026-01-20T19:50:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.LG"
        ],
        "authors": [
            "Danial Sadrian Zadeh",
            "Otman A. Basir",
            "Behzad Moshiri"
        ],
        "tldr": "This paper introduces a novel vision-based framework for generating natural language descriptions of traffic scenes from a single camera image, along with a new dataset derived from BDD100K for this task, demonstrating strong performance through quantitative and qualitative evaluations.",
        "tldr_zh": "该论文介绍了一个新的基于视觉的框架，用于从单个摄像头图像生成交通场景的自然语言描述，并提出了一个从 BDD100K 数据集派生的新数据集。通过定量和定性评估，该方法表现出强大的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Copy-Trasform-Paste: Zero-Shot Object-Object Alignment Guided by Vision-Language and Geometric Constraints",
        "summary": "We study zero-shot 3D alignment of two given meshes, using a text prompt describing their spatial relation -- an essential capability for content creation and scene assembly. Earlier approaches primarily rely on geometric alignment procedures, while recent work leverages pretrained 2D diffusion models to model language-conditioned object-object spatial relationships. In contrast, we directly optimize the relative pose at test time, updating translation, rotation, and isotropic scale with CLIP-driven gradients via a differentiable renderer, without training a new model. Our framework augments language supervision with geometry-aware objectives: a variant of soft-Iterative Closest Point (ICP) term to encourage surface attachment and a penetration loss to discourage interpenetration. A phased schedule strengthens contact constraints over time, and camera control concentrates the optimization on the interaction region. To enable evaluation, we curate a benchmark containing diverse categories and relations, and compare against baselines. Our method outperforms all alternatives, yielding semantically faithful and physically plausible alignments.",
        "url": "http://arxiv.org/abs/2601.14207v1",
        "published_date": "2026-01-20T18:12:55+00:00",
        "updated_date": "2026-01-20T18:12:55+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Rotem Gatenyo",
            "Ohad Fried"
        ],
        "tldr": "The paper presents a zero-shot method for 3D object alignment based on language descriptions of spatial relationships, using CLIP-driven optimization and geometric constraints like soft-ICP and penetration loss, outperforming existing baselines.",
        "tldr_zh": "该论文提出了一种基于空间关系语言描述的零样本3D对象对齐方法，使用CLIP驱动的优化和几何约束（如soft-ICP和穿透损失），优于现有基线。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]