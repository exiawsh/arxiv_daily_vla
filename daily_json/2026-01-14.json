[
    {
        "title": "SPARK: Scalable Real-Time Point Cloud Aggregation with Multi-View Self-Calibration",
        "summary": "Real-time multi-camera 3D reconstruction is crucial for 3D perception, immersive interaction, and robotics. Existing methods struggle with multi-view fusion, camera extrinsic uncertainty, and scalability for large camera setups. We propose SPARK, a self-calibrating real-time multi-camera point cloud reconstruction framework that jointly handles point cloud fusion and extrinsic uncertainty. SPARK consists of: (1) a geometry-aware online extrinsic estimation module leveraging multi-view priors and enforcing cross-view and temporal consistency for stable self-calibration, and (2) a confidence-driven point cloud fusion strategy modeling depth reliability and visibility at pixel and point levels to suppress noise and view-dependent inconsistencies. By performing frame-wise fusion without accumulation, SPARK produces stable point clouds in dynamic scenes while scaling linearly with the number of cameras. Extensive experiments on real-world multi-camera systems show that SPARK outperforms existing approaches in extrinsic accuracy, geometric consistency, temporal stability, and real-time performance, demonstrating its effectiveness and scalability for large-scale multi-camera 3D reconstruction.",
        "url": "http://arxiv.org/abs/2601.08414v1",
        "published_date": "2026-01-13T10:32:22+00:00",
        "updated_date": "2026-01-13T10:32:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chentian Sun"
        ],
        "tldr": "SPARK is a real-time, self-calibrating multi-camera 3D reconstruction framework that addresses extrinsic uncertainty and scales linearly with the number of cameras, outperforming existing methods in accuracy, stability, and performance.",
        "tldr_zh": "SPARK是一个实时的、自校准的多相机3D重建框架，解决了外参不确定性问题，并随相机数量线性扩展。在精度、稳定性和性能方面优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory",
        "summary": "VLA models have shown promising potential in embodied navigation by unifying perception and planning while inheriting the strong generalization abilities of large VLMs. However, most existing VLA models rely on reactive mappings directly from observations to actions, lacking the explicit reasoning capabilities and persistent memory required for complex, long-horizon navigation tasks. To address these challenges, we propose VLingNav, a VLA model for embodied navigation grounded in linguistic-driven cognition. First, inspired by the dual-process theory of human cognition, we introduce an adaptive chain-of-thought mechanism, which dynamically triggers explicit reasoning only when necessary, enabling the agent to fluidly switch between fast, intuitive execution and slow, deliberate planning. Second, to handle long-horizon spatial dependencies, we develop a visual-assisted linguistic memory module that constructs a persistent, cross-modal semantic memory, enabling the agent to recall past observations to prevent repetitive exploration and infer movement trends for dynamic environments. For the training recipe, we construct Nav-AdaCoT-2.9M, the largest embodied navigation dataset with reasoning annotations to date, enriched with adaptive CoT annotations that induce a reasoning paradigm capable of adjusting both when to think and what to think about. Moreover, we incorporate an online expert-guided reinforcement learning stage, enabling the model to surpass pure imitation learning and to acquire more robust, self-explored navigation behaviors. Extensive experiments demonstrate that VLingNav achieves state-of-the-art performance across a wide range of embodied navigation benchmarks. Notably, VLingNav transfers to real-world robotic platforms in a zero-shot manner, executing various navigation tasks and demonstrating strong cross-domain and cross-task generalization.",
        "url": "http://arxiv.org/abs/2601.08665v1",
        "published_date": "2026-01-13T15:43:43+00:00",
        "updated_date": "2026-01-13T15:43:43+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Shaoan Wang",
            "Yuanfei Luo",
            "Xingyu Chen",
            "Aocheng Luo",
            "Dongyue Li",
            "Chang Liu",
            "Sheng Chen",
            "Yangang Zhang",
            "Junzhi Yu"
        ],
        "tldr": "The paper introduces VLingNav, a VLA model for embodied navigation that incorporates adaptive chain-of-thought reasoning and a visual-assisted linguistic memory to improve performance on long-horizon and complex navigation tasks, demonstrating strong generalization to real-world robots.",
        "tldr_zh": "该论文介绍了VLingNav，一种用于具身导航的VLA模型，它结合了自适应思维链推理和视觉辅助的语言记忆，以提高在长程和复杂导航任务中的性能，并展示了对现实世界机器人的强大泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SoC: Semantic Orthogonal Calibration for Test-Time Prompt Tuning",
        "summary": "With the increasing adoption of vision-language models (VLMs) in critical decision-making systems such as healthcare or autonomous driving, the calibration of their uncertainty estimates becomes paramount. Yet, this dimension has been largely underexplored in the VLM test-time prompt-tuning (TPT) literature, which has predominantly focused on improving their discriminative performance. Recent state-of-the-art advocates for enforcing full orthogonality over pairs of text prompt embeddings to enhance separability, and therefore calibration. Nevertheless, as we theoretically show in this work, the inherent gradients from fully orthogonal constraints will strongly push semantically related classes away, ultimately making the model overconfident. Based on our findings, we propose Semantic Orthogonal Calibration (SoC), a Huber-based regularizer that enforces smooth prototype separation while preserving semantic proximity, thereby improving calibration compared to prior orthogonality-based approaches. Across a comprehensive empirical validation, we demonstrate that SoC consistently improves calibration performance, while also maintaining competitive discriminative capabilities.",
        "url": "http://arxiv.org/abs/2601.08617v1",
        "published_date": "2026-01-13T15:00:03+00:00",
        "updated_date": "2026-01-13T15:00:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Leo Fillioux",
            "Omprakash Chakraborty",
            "Ismail Ben Ayed",
            "Paul-Henry Cournède",
            "Stergios Christodoulidis",
            "Maria Vakalopoulou",
            "Jose Dolz"
        ],
        "tldr": "This paper introduces Semantic Orthogonal Calibration (SoC), a novel regularizer for test-time prompt tuning of VLMs that improves calibration by balancing prototype separation and semantic proximity, leading to better uncertainty estimates crucial for applications like autonomous driving.",
        "tldr_zh": "本文介绍了语义正交校准（SoC），一种新的VLM测试时prompt tuning的正则化方法，通过平衡原型分离和语义邻近性来提高校准，从而改善不确定性估计，这对于自动驾驶等应用至关重要。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Safer Mobile Agents: Scalable Generation and Evaluation of Diverse Scenarios for VLMs",
        "summary": "Vision Language Models (VLMs) are increasingly deployed in autonomous vehicles and mobile systems, making it crucial to evaluate their ability to support safer decision-making in complex environments. However, existing benchmarks inadequately cover diverse hazardous situations, especially anomalous scenarios with spatio-temporal dynamics. While image editing models are a promising means to synthesize such hazards, it remains challenging to generate well-formulated scenarios that include moving, intrusive, and distant objects frequently observed in the real world. To address this gap, we introduce \\textbf{HazardForge}, a scalable pipeline that leverages image editing models to generate these scenarios with layout decision algorithms, and validation modules. Using HazardForge, we construct \\textbf{MovSafeBench}, a multiple-choice question (MCQ) benchmark comprising 7,254 images and corresponding QA pairs across 13 object categories, covering both normal and anomalous objects. Experiments using MovSafeBench show that VLM performance degrades notably under conditions including anomalous objects, with the largest drop in scenarios requiring nuanced motion understanding.",
        "url": "http://arxiv.org/abs/2601.08470v1",
        "published_date": "2026-01-13T11:55:31+00:00",
        "updated_date": "2026-01-13T11:55:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Takara Taniguchi",
            "Kuniaki Saito",
            "Atsushi Hashimoto"
        ],
        "tldr": "This paper introduces HazardForge, a pipeline for generating diverse and anomalous driving scenarios to evaluate VLM safety, and MovSafeBench, a new benchmark showing VLM performance degradation in such scenarios.",
        "tldr_zh": "本文介绍了HazardForge，一个用于生成多样化和异常驾驶场景的pipeline，以评估VLM的安全性。同时提出了MovSafeBench，一个新的基准测试，表明VLM在这些场景下的性能下降。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Semantic Misalignment in Vision-Language Models under Perceptual Degradation",
        "summary": "Vision-Language Models (VLMs) are increasingly deployed in autonomous driving and embodied AI systems, where reliable perception is critical for safe semantic reasoning and decision-making. While recent VLMs demonstrate strong performance on multimodal benchmarks, their robustness to realistic perception degradation remains poorly understood. In this work, we systematically study semantic misalignment in VLMs under controlled degradation of upstream visual perception, using semantic segmentation on the Cityscapes dataset as a representative perception module. We introduce perception-realistic corruptions that induce only moderate drops in conventional segmentation metrics, yet observe severe failures in downstream VLM behavior, including hallucinated object mentions, omission of safety-critical entities, and inconsistent safety judgments. To quantify these effects, we propose a set of language-level misalignment metrics that capture hallucination, critical omission, and safety misinterpretation, and analyze their relationship with segmentation quality across multiple contrastive and generative VLMs. Our results reveal a clear disconnect between pixel-level robustness and multimodal semantic reliability, highlighting a critical limitation of current VLM-based systems and motivating the need for evaluation frameworks that explicitly account for perception uncertainty in safety-critical applications.",
        "url": "http://arxiv.org/abs/2601.08355v1",
        "published_date": "2026-01-13T09:13:05+00:00",
        "updated_date": "2026-01-13T09:13:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guo Cheng"
        ],
        "tldr": "This paper investigates the robustness of Vision-Language Models (VLMs) in autonomous driving scenarios by analyzing their performance under realistic perceptual degradation, revealing a significant semantic misalignment issue. They show that VLMs can fail in safety-critical reasoning even when pixel-level segmentation metrics are still reasonable, highlighting the need for better evaluation frameworks.",
        "tldr_zh": "本文研究了视觉-语言模型（VLM）在自动驾驶场景中的鲁棒性，通过分析其在真实感知退化下的性能，揭示了一个重要的语义不对齐问题。 他们表明，即使像素级分割指标仍然合理，VLM也可能在安全关键的推理中失败，突出了对更好的评估框架的需求。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CogniMap3D: Cognitive 3D Mapping and Rapid Retrieval",
        "summary": "We present CogniMap3D, a bioinspired framework for dynamic 3D scene understanding and reconstruction that emulates human cognitive processes. Our approach maintains a persistent memory bank of static scenes, enabling efficient spatial knowledge storage and rapid retrieval. CogniMap3D integrates three core capabilities: a multi-stage motion cue framework for identifying dynamic objects, a cognitive mapping system for storing, recalling, and updating static scenes across multiple visits, and a factor graph optimization strategy for refining camera poses. Given an image stream, our model identifies dynamic regions through motion cues with depth and camera pose priors, then matches static elements against its memory bank. When revisiting familiar locations, CogniMap3D retrieves stored scenes, relocates cameras, and updates memory with new observations. Evaluations on video depth estimation, camera pose reconstruction, and 3D mapping tasks demonstrate its state-of-the-art performance, while effectively supporting continuous scene understanding across extended sequences and multiple visits.",
        "url": "http://arxiv.org/abs/2601.08175v1",
        "published_date": "2026-01-13T03:09:35+00:00",
        "updated_date": "2026-01-13T03:09:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Feiran Wang",
            "Junyi Wu",
            "Dawen Cai",
            "Yuan Hong",
            "Yan Yan"
        ],
        "tldr": "CogniMap3D is a bio-inspired framework for dynamic 3D scene understanding, reconstruction, and persistent storage, enabling efficient spatial knowledge management and rapid retrieval across multiple visits.",
        "tldr_zh": "CogniMap3D是一个受生物学启发的框架，用于动态3D场景理解、重建和持久存储，实现了高效的空间知识管理和跨多次访问的快速检索。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Cross-Platform Generalization: Domain Adaptive 3D Detection with Augmentation and Pseudo-Labeling",
        "summary": "This technical report represents the award-winning solution to the Cross-platform 3D Object Detection task in the RoboSense2025 Challenge. Our approach is built upon PVRCNN++, an efficient 3D object detection framework that effectively integrates point-based and voxel-based features. On top of this foundation, we improve cross-platform generalization by narrowing domain gaps through tailored data augmentation and a self-training strategy with pseudo-labels. These enhancements enabled our approach to secure the 3rd place in the challenge, achieving a 3D AP of 62.67% for the Car category on the phase-1 target domain, and 58.76% and 49.81% for Car and Pedestrian categories respectively on the phase-2 target domain.",
        "url": "http://arxiv.org/abs/2601.08174v1",
        "published_date": "2026-01-13T03:09:20+00:00",
        "updated_date": "2026-01-13T03:09:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiyan Feng",
            "Wenbo Zhang",
            "Lu Zhang",
            "Yunzhi Zhuge",
            "Huchuan Lu",
            "You He"
        ],
        "tldr": "This paper presents a solution for cross-platform 3D object detection using PVRCNN++ with tailored data augmentation and pseudo-labeling to improve generalization, achieving top results in the RoboSense2025 Challenge.",
        "tldr_zh": "本文提出了一种用于跨平台三维目标检测的解决方案，该方案使用 PVRCNN++，并通过定制的数据增强和伪标签来提高泛化能力，在 RoboSense2025 挑战赛中取得了优异的成绩。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Keyframe-based Dense Mapping with the Graph of View-Dependent Local Maps",
        "summary": "In this article, we propose a new keyframe-based mapping system. The proposed method updates local Normal Distribution Transform maps (NDT) using data from an RGB-D sensor. The cells of the NDT are stored in 2D view-dependent structures to better utilize the properties and uncertainty model of RGB-D cameras. This method naturally represents an object closer to the camera origin with higher precision. The local maps are stored in the pose graph which allows correcting global map after loop closure detection. We also propose a procedure that allows merging and filtering local maps to obtain a global map of the environment. Finally, we compare our method with Octomap and NDT-OM and provide example applications of the proposed mapping method.",
        "url": "http://arxiv.org/abs/2601.08520v1",
        "published_date": "2026-01-13T13:02:22+00:00",
        "updated_date": "2026-01-13T13:02:22+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Krzysztof Zielinski",
            "Dominik Belter"
        ],
        "tldr": "This paper introduces a keyframe-based dense mapping system using RGB-D data and view-dependent NDT maps stored in a pose graph, enabling loop closure and global map creation. It compares favorably to Octomap and NDT-OM.",
        "tldr_zh": "本文提出了一种基于关键帧的密集地图构建系统，该系统使用RGB-D数据和存储在姿态图中的视角相关的NDT地图，从而实现闭环检测和全局地图创建。它与Octomap和NDT-OM相比具有优势。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Real-Time Localization Framework for Autonomous Basketball Robots",
        "summary": "Localization is a fundamental capability for autonomous robots, enabling them to operate effectively in dynamic environments. In Robocon 2025, accurate and reliable localization is crucial for improving shooting precision, avoiding collisions with other robots, and navigating the competition field efficiently. In this paper, we propose a hybrid localization algorithm that integrates classical techniques with learning based methods that rely solely on visual data from the court's floor to achieve self-localization on the basketball field.",
        "url": "http://arxiv.org/abs/2601.08713v1",
        "published_date": "2026-01-13T16:40:18+00:00",
        "updated_date": "2026-01-13T16:40:18+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Naren Medarametla",
            "Sreejon Mondal"
        ],
        "tldr": "This paper introduces a hybrid localization algorithm for autonomous basketball robots, combining classical and learning-based methods using visual data from the court floor to achieve self-localization.",
        "tldr_zh": "本文提出了一种用于自主篮球机器人的混合定位算法，该算法结合了经典方法和基于学习的方法，利用球场地板的视觉数据来实现自我定位。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]