[
    {
        "title": "ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting",
        "summary": "We introduce ForeSight, a novel joint detection and forecasting framework for\nvision-based 3D perception in autonomous vehicles. Traditional approaches treat\ndetection and forecasting as separate sequential tasks, limiting their ability\nto leverage temporal cues. ForeSight addresses this limitation with a\nmulti-task streaming and bidirectional learning approach, allowing detection\nand forecasting to share query memory and propagate information seamlessly. The\nforecast-aware detection transformer enhances spatial reasoning by integrating\ntrajectory predictions from a multiple hypothesis forecast memory queue, while\nthe streaming forecast transformer improves temporal consistency using past\nforecasts and refined detections. Unlike tracking-based methods, ForeSight\neliminates the need for explicit object association, reducing error propagation\nwith a tracking-free model that efficiently scales across multi-frame\nsequences. Experiments on the nuScenes dataset show that ForeSight achieves\nstate-of-the-art performance, achieving an EPA of 54.9%, surpassing previous\nmethods by 9.3%, while also attaining the best mAP and minADE among multi-view\ndetection and forecasting models.",
        "url": "http://arxiv.org/abs/2508.07089v1",
        "published_date": "2025-08-09T20:18:10+00:00",
        "updated_date": "2025-08-09T20:18:10+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Sandro Papais",
            "Letian Wang",
            "Brian Cheong",
            "Steven L. Waslander"
        ],
        "tldr": "ForeSight is a novel joint detection and trajectory forecasting framework for autonomous vehicles that achieves state-of-the-art performance on the nuScenes dataset by using a multi-task streaming and bidirectional learning approach.",
        "tldr_zh": "ForeSight是一个用于自动驾驶车辆的新型联合检测和轨迹预测框架，通过使用多任务流式和双向学习方法，在nuScenes数据集上实现了最先进的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "AgriVLN: Vision-and-Language Navigation for Agricultural Robots",
        "summary": "Agricultural robots have emerged as powerful members in agricultural tasks,\nnevertheless, still heavily rely on manual operation or untransportable railway\nfor movement, resulting in limited mobility and poor adaptability.\nVision-and-Language Navigation (VLN) enables robots to navigate to the target\ndestinations following natural language instructions, demonstrating strong\nperformance on several domains. However, none of the existing benchmarks or\nmethods is specifically designed for agricultural scenes. To bridge this gap,\nwe propose Agriculture to Agriculture (A2A) benchmark, containing 1,560\nepisodes across six diverse agricultural scenes, in which all realistic RGB\nvideos are captured by front-facing camera on a quadruped robot at a height of\n0.38 meters, aligning with the practical deployment conditions. Meanwhile, we\npropose Vision-and-Language Navigation for Agricultural Robots (AgriVLN)\nbaseline based on Vision-Language Model (VLM) prompted with carefully crafted\ntemplates, which can understand both given instructions and agricultural\nenvironments to generate appropriate low-level actions for robot control. When\nevaluated on A2A, AgriVLN performs well on short instructions but struggles\nwith long instructions, because it often fails to track which part of the\ninstruction is currently being executed. To address this, we further propose\nSubtask List (STL) instruction decomposition module and integrate it into\nAgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare\nAgriVLN with several existing VLN methods, demonstrating the state-of-the-art\nperformance in the agricultural domain.",
        "url": "http://arxiv.org/abs/2508.07406v1",
        "published_date": "2025-08-10T16:07:23+00:00",
        "updated_date": "2025-08-10T16:07:23+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Xiaobei Zhao",
            "Xingqi Lyu",
            "Xiang Li"
        ],
        "tldr": "The paper introduces AgriVLN, a new vision-and-language navigation benchmark (A2A) and baseline model for agricultural robots, addressing the gap in existing VLN research for agricultural environments. They also propose a Subtask List module to improve performance on longer instructions.",
        "tldr_zh": "该论文介绍了 AgriVLN，一个新的用于农业机器人的视觉语言导航基准测试 (A2A) 和基线模型，旨在弥补现有 VLN 研究在农业环境中的空白。他们还提出了一个子任务列表模块，以提高在较长指令下的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Understanding Dynamic Scenes in Ego Centric 4D Point Clouds",
        "summary": "Understanding dynamic 4D scenes from an egocentric perspective-modeling\nchanges in 3D spatial structure over time-is crucial for human-machine\ninteraction, autonomous navigation, and embodied intelligence. While existing\negocentric datasets contain dynamic scenes, they lack unified 4D annotations\nand task-driven evaluation protocols for fine-grained spatio-temporal\nreasoning, especially on motion of objects and human, together with their\ninteractions. To address this gap, we introduce EgoDynamic4D, a novel QA\nbenchmark on highly dynamic scenes, comprising RGB-D video, camera poses,\nglobally unique instance masks, and 4D bounding boxes. We construct 927K QA\npairs accompanied by explicit Chain-of-Thought (CoT), enabling verifiable,\nstep-by-step spatio-temporal reasoning. We design 12 dynamic QA tasks covering\nagent motion, human-object interaction, trajectory prediction, relation\nunderstanding, and temporal-causal reasoning, with fine-grained,\nmultidimensional metrics. To tackle these tasks, we propose an end-to-end\nspatio-temporal reasoning framework that unifies dynamic and static scene\ninformation, using instance-aware feature encoding, time and camera encoding,\nand spatially adaptive down-sampling to compress large 4D scenes into token\nsequences manageable by LLMs. Experiments on EgoDynamic4D show that our method\nconsistently outperforms baselines, validating the effectiveness of multimodal\ntemporal modeling for egocentric dynamic scene understanding.",
        "url": "http://arxiv.org/abs/2508.07251v1",
        "published_date": "2025-08-10T09:08:04+00:00",
        "updated_date": "2025-08-10T09:08:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junsheng Huang",
            "Shengyu Hao",
            "Bocheng Hu",
            "Gaoang Wang"
        ],
        "tldr": "The paper introduces EgoDynamic4D, a new QA benchmark for understanding dynamic 4D scenes from an egocentric perspective, and proposes a spatio-temporal reasoning framework leveraging LLMs to address the benchmark's tasks.",
        "tldr_zh": "本文介绍了EgoDynamic4D，这是一个新的QA基准，用于理解以自我为中心的动态4D场景。同时，提出了一种利用LLM的时空推理框架来解决该基准测试中的任务。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction",
        "summary": "Predicting pedestrian motion trajectories is critical for the path planning\nand motion control of autonomous vehicles. Recent diffusion-based models have\nshown promising results in capturing the inherent stochasticity of pedestrian\nbehavior for trajectory prediction. However, the absence of explicit semantic\nmodelling of pedestrian intent in many diffusion-based methods may result in\nmisinterpreted behaviors and reduced prediction accuracy. To address the above\nchallenges, we propose a diffusion-based pedestrian trajectory prediction\nframework that incorporates both short-term and long-term motion intentions.\nShort-term intent is modelled using a residual polar representation, which\ndecouples direction and magnitude to capture fine-grained local motion\npatterns. Long-term intent is estimated through a learnable, token-based\nendpoint predictor that generates multiple candidate goals with associated\nprobabilities, enabling multimodal and context-aware intention modelling.\nFurthermore, we enhance the diffusion process by incorporating adaptive\nguidance and a residual noise predictor that dynamically refines denoising\naccuracy. The proposed framework is evaluated on the widely used ETH, UCY, and\nSDD benchmarks, demonstrating competitive results against state-of-the-art\nmethods.",
        "url": "http://arxiv.org/abs/2508.07146v1",
        "published_date": "2025-08-10T02:36:33+00:00",
        "updated_date": "2025-08-10T02:36:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yu Liu",
            "Zhijie Liu",
            "Xiao Ren",
            "You-Fu Li",
            "He Kong"
        ],
        "tldr": "This paper proposes an intention-aware diffusion model for pedestrian trajectory prediction, incorporating short-term and long-term intent modelling with adaptive guidance for improved accuracy on standard datasets.",
        "tldr_zh": "该论文提出了一个意图感知的扩散模型，用于行人轨迹预测。该模型结合了短期和长期意图建模，并采用自适应引导，从而在标准数据集上实现了更高的预测精度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Communication-Efficient Multi-Agent 3D Detection via Hybrid Collaboration",
        "summary": "Collaborative 3D detection can substantially boost detection performance by\nallowing agents to exchange complementary information. It inherently results in\na fundamental trade-off between detection performance and communication\nbandwidth. To tackle this bottleneck issue, we propose a novel hybrid\ncollaboration that adaptively integrates two types of communication messages:\nperceptual outputs, which are compact, and raw observations, which offer richer\ninformation. This approach focuses on two key aspects: i) integrating\ncomplementary information from two message types and ii) prioritizing the most\ncritical data within each type. By adaptively selecting the most critical set\nof messages, it ensures optimal perceptual information and adaptability,\neffectively meeting the demands of diverse communication scenarios.Building on\nthis hybrid collaboration, we present \\texttt{HyComm}, a\ncommunication-efficient LiDAR-based collaborative 3D detection system.\n\\texttt{HyComm} boasts two main benefits: i) it facilitates adaptable\ncompression rates for messages, addressing various communication requirements,\nand ii) it uses standardized data formats for messages. This ensures they are\nindependent of specific detection models, fostering adaptability across\ndifferent agent configurations. To evaluate HyComm, we conduct experiments on\nboth real-world and simulation datasets: DAIR-V2X and OPV2V. HyComm\nconsistently outperforms previous methods and achieves a superior\nperformance-bandwidth trade-off regardless of whether agents use the same or\nvaried detection models. It achieves a lower communication volume of more than\n2,006$\\times$ and still outperforms Where2comm on DAIR-V2X in terms of AP50.\nThe related code will be released.",
        "url": "http://arxiv.org/abs/2508.07092v1",
        "published_date": "2025-08-09T20:33:37+00:00",
        "updated_date": "2025-08-09T20:33:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yue Hu",
            "Juntong Peng",
            "Yunqiao Yang",
            "Siheng Chen"
        ],
        "tldr": "The paper introduces HyComm, a communication-efficient LiDAR-based collaborative 3D detection system that adaptively integrates perceptual outputs and raw observations, achieving a superior performance-bandwidth trade-off in multi-agent 3D detection.",
        "tldr_zh": "该论文介绍了一种名为HyComm的通信高效的基于激光雷达的协同3D检测系统，该系统自适应地整合了感知输出和原始观测，在多智能体3D检测中实现了卓越的性能-带宽权衡。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]