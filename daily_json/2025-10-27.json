[
    {
        "title": "DAMap: Distance-aware MapNet for High Quality HD Map Construction",
        "summary": "Predicting High-definition (HD) map elements with high quality (high\nclassification and localization scores) is crucial to the safety of autonomous\ndriving vehicles. However, current methods perform poorly in high quality\npredictions due to inherent task misalignment. Two main factors are responsible\nfor misalignment: 1) inappropriate task labels due to one-to-many matching\nqueries sharing the same labels, and 2) sub-optimal task features due to\ntask-shared sampling mechanism. In this paper, we reveal two inherent defects\nin current methods and develop a novel HD map construction method named DAMap\nto address these problems. Specifically, DAMap consists of three components:\nDistance-aware Focal Loss (DAFL), Hybrid Loss Scheme (HLS), and Task Modulated\nDeformable Attention (TMDA). The DAFL is introduced to assign appropriate\nclassification labels for one-to-many matching samples. The TMDA is proposed to\nobtain discriminative task-specific features. Furthermore, the HLS is proposed\nto better utilize the advantages of the DAFL. We perform extensive experiments\nand consistently achieve performance improvement on the NuScenes and Argoverse2\nbenchmarks under different metrics, baselines, splits, backbones, and\nschedules. Code will be available at https://github.com/jpdong-xjtu/DAMap.",
        "url": "http://arxiv.org/abs/2510.22675v1",
        "published_date": "2025-10-26T13:29:26+00:00",
        "updated_date": "2025-10-26T13:29:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jinpeng Dong",
            "Chen Li",
            "Yutong Lin",
            "Jingwen Fu",
            "Sanping Zhou",
            "Nanning Zheng"
        ],
        "tldr": "The paper introduces DAMap, a novel HD map construction method addressing task misalignment issues in current methods by using Distance-aware Focal Loss, Hybrid Loss Scheme, and Task Modulated Deformable Attention. It achieves consistent performance improvements on NuScenes and Argoverse2 benchmarks.",
        "tldr_zh": "该论文介绍了一种名为DAMap的新型高清地图构建方法，通过使用距离感知焦点损失、混合损失方案和任务调制可变形注意力来解决当前方法中存在的任务不对齐问题。在NuScenes和Argoverse2基准测试中，该方法取得了持续的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LVD-GS: Gaussian Splatting SLAM for Dynamic Scenes via Hierarchical Explicit-Implicit Representation Collaboration Rendering",
        "summary": "3D Gaussian Splatting SLAM has emerged as a widely used technique for\nhigh-fidelity mapping in spatial intelligence. However, existing methods often\nrely on a single representation scheme, which limits their performance in\nlarge-scale dynamic outdoor scenes and leads to cumulative pose errors and\nscale ambiguity. To address these challenges, we propose \\textbf{LVD-GS}, a\nnovel LiDAR-Visual 3D Gaussian Splatting SLAM system. Motivated by the human\nchain-of-thought process for information seeking, we introduce a hierarchical\ncollaborative representation module that facilitates mutual reinforcement for\nmapping optimization, effectively mitigating scale drift and enhancing\nreconstruction robustness. Furthermore, to effectively eliminate the influence\nof dynamic objects, we propose a joint dynamic modeling module that generates\nfine-grained dynamic masks by fusing open-world segmentation with implicit\nresidual constraints, guided by uncertainty estimates from DINO-Depth features.\nExtensive evaluations on KITTI, nuScenes, and self-collected datasets\ndemonstrate that our approach achieves state-of-the-art performance compared to\nexisting methods.",
        "url": "http://arxiv.org/abs/2510.22669v1",
        "published_date": "2025-10-26T13:16:39+00:00",
        "updated_date": "2025-10-26T13:16:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Wenkai Zhu",
            "Xu Li",
            "Qimin Xu",
            "Benwu Wang",
            "Kun Wei",
            "Yiming Peng",
            "Zihang Wang"
        ],
        "tldr": "This paper presents LVD-GS, a novel LiDAR-Visual 3D Gaussian Splatting SLAM system for dynamic scenes, utilizing hierarchical representation and dynamic object removal to improve mapping accuracy and robustness. It outperforms existing methods on KITTI, nuScenes, and self-collected datasets.",
        "tldr_zh": "该论文提出了LVD-GS，一种用于动态场景的新型LiDAR-Visual 3D高斯溅射SLAM系统，利用分层表示和动态物体移除来提高地图构建的精度和鲁棒性。在KITTI、nuScenes和自采集数据集上的表现优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Cross-View UAV Geo-Localization with Precision-Focused Efficient Design: A Hierarchical Distillation Approach with Multi-view Refinement",
        "summary": "Cross-view geo-localization (CVGL) enables UAV localization by matching\naerial images to geo-tagged satellite databases, which is critical for\nautonomous navigation in GNSS-denied environments. However, existing methods\nrely on resource-intensive fine-grained feature extraction and alignment, where\nmultiple branches and modules significantly increase inference costs, limiting\ntheir deployment on edge devices. We propose Precision-Focused Efficient Design\n(PFED), a resource-efficient framework combining hierarchical knowledge\ntransfer and multi-view representation refinement. This innovative method\ncomprises two key components: 1) During training, Hierarchical Distillation\nparadigm for fast and accurate CVGL (HD-CVGL), coupled with Uncertainty-Aware\nPrediction Alignment (UAPA) to distill essential information and mitigate the\ndata imbalance without incurring additional inference overhead. 2) During\ninference, an efficient Multi-view Refinement Module (MRM) leverages mutual\ninformation to filter redundant samples and effectively utilize the multi-view\ndata. Extensive experiments show that PFED achieves state-of-the-art\nperformance in both accuracy and efficiency, reaching 97.15\\% Recall@1 on\nUniversity-1652 while being over $5 \\times$ more efficient in FLOPs and $3\n\\times$ faster than previous top methods. Furthermore, PFED runs at 251.5 FPS\non the AGX Orin edge device, demonstrating its practical viability for\nreal-time UAV applications. The project is available at\nhttps://github.com/SkyEyeLoc/PFED",
        "url": "http://arxiv.org/abs/2510.22582v1",
        "published_date": "2025-10-26T08:47:20+00:00",
        "updated_date": "2025-10-26T08:47:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jian Sun",
            "Kangdao Liu",
            "Chi Zhang",
            "Chuangquan Chen",
            "Junge Shen",
            "Chi-Man Vong"
        ],
        "tldr": "This paper introduces a Precision-Focused Efficient Design (PFED) framework for cross-view UAV geo-localization, achieving state-of-the-art accuracy and efficiency by hierarchical knowledge transfer and multi-view representation refinement, suitable for edge device deployment.",
        "tldr_zh": "本文介绍了一种面向精度的高效设计（PFED）框架，用于跨视角无人机地理定位，通过分层知识迁移和多视角表示细化，实现了最先进的精度和效率，适合边缘设备部署。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "3D Roadway Scene Object Detection with LIDARs in Snowfall Conditions",
        "summary": "Because 3D structure of a roadway environment can be characterized directly\nby a Light Detection and Ranging (LiDAR) sensors, they can be used to obtain\nexceptional situational awareness for assitive and autonomous driving systems.\nAlthough LiDARs demonstrate good performance in clean and clear weather\nconditions, their performance significantly deteriorates in adverse weather\nconditions such as those involving atmospheric precipitation. This may render\nperception capabilities of autonomous systems that use LiDAR data in learning\nbased models to perform object detection and ranging ineffective. While efforts\nhave been made to enhance the accuracy of these models, the extent of signal\ndegradation under various weather conditions remains largely not quantified. In\nthis study, we focus on the performance of an automotive grade LiDAR in snowy\nconditions in order to develop a physics-based model that examines failure\nmodes of a LiDAR sensor. Specifically, we investigated how the LiDAR signal\nattenuates with different snowfall rates and how snow particles near the source\nserve as small but efficient reflectors. Utilizing our model, we transform data\nfrom clear conditions to simulate snowy scenarios, enabling a comparison of our\nsynthetic data with actual snowy conditions. Furthermore, we employ this\nsynthetic data, representative of different snowfall rates, to explore the\nimpact on a pre-trained object detection model, assessing its performance under\nvarying levels of snowfall",
        "url": "http://arxiv.org/abs/2510.22436v1",
        "published_date": "2025-10-25T21:14:45+00:00",
        "updated_date": "2025-10-25T21:14:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ghazal Farhani",
            "Taufiq Rahman",
            "Syed Mostaquim Ali",
            "Andrew Liu",
            "Mohamed Zaki",
            "Dominique Charlebois",
            "Benoit Anctil"
        ],
        "tldr": "This paper investigates the impact of snowfall on LiDAR performance for 3D object detection in autonomous driving, developing a physics-based model to simulate snowy conditions and evaluate a pre-trained object detection model.",
        "tldr_zh": "本文研究了降雪对激光雷达性能的影响，旨在提升自动驾驶中3D物体检测的准确性。研究开发了一种基于物理的模型来模拟降雪条件，并评估预训练的物体检测模型在不同降雪强度下的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Fully Interpretable Statistical Approach for Roadside LiDAR Background Subtraction",
        "summary": "We present a fully interpretable and flexible statistical method for\nbackground subtraction in roadside LiDAR data, aimed at enhancing\ninfrastructure-based perception in automated driving. Our approach introduces\nboth a Gaussian distribution grid (GDG), which models the spatial statistics of\nthe background using background-only scans, and a filtering algorithm that uses\nthis representation to classify LiDAR points as foreground or background. The\nmethod supports diverse LiDAR types, including multiline 360 degree and\nmicro-electro-mechanical systems (MEMS) sensors, and adapts to various\nconfigurations. Evaluated on the publicly available RCooper dataset, it\noutperforms state-of-the-art techniques in accuracy and flexibility, even with\nminimal background data. Its efficient implementation ensures reliable\nperformance on low-resource hardware, enabling scalable real-world deployment.",
        "url": "http://arxiv.org/abs/2510.22390v1",
        "published_date": "2025-10-25T18:18:10+00:00",
        "updated_date": "2025-10-25T18:18:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Aitor Iglesias",
            "Nerea Aranjuelo",
            "Patricia Javierre",
            "Ainhoa Menendez",
            "Ignacio Arganda-Carreras",
            "Marcos Nieto"
        ],
        "tldr": "The paper presents a fully interpretable Gaussian distribution grid (GDG) method for background subtraction in roadside LiDAR data, outperforming state-of-the-art techniques on the RCooper dataset and suitable for low-resource hardware.",
        "tldr_zh": "该论文提出了一种完全可解释的高斯分布网格（GDG）方法，用于路侧激光雷达数据中的背景去除，在RCooper数据集上优于现有技术，并适用于低资源硬件。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Cross-view Localization and Synthesis - Datasets, Challenges and Opportunities",
        "summary": "Cross-view localization and synthesis are two fundamental tasks in cross-view\nvisual understanding, which deals with cross-view datasets: overhead (satellite\nor aerial) and ground-level imagery. These tasks have gained increasing\nattention due to their broad applications in autonomous navigation, urban\nplanning, and augmented reality. Cross-view localization aims to estimate the\ngeographic position of ground-level images based on information provided by\noverhead imagery while cross-view synthesis seeks to generate ground-level\nimages based on information from the overhead imagery. Both tasks remain\nchallenging due to significant differences in viewing perspective, resolution,\nand occlusion, which are widely embedded in cross-view datasets. Recent years\nhave witnessed rapid progress driven by the availability of large-scale\ndatasets and novel approaches. Typically, cross-view localization is formulated\nas an image retrieval problem where ground-level features are matched with\ntiled overhead images feature, extracted by convolutional neural networks\n(CNNs) or vision transformers (ViTs) for cross-view feature embedding.\nCross-view synthesis, on the other hand, seeks to generate ground-level views\nbased on information from overhead imagery, generally using generative\nadversarial networks (GANs) or diffusion models. This paper presents a\ncomprehensive survey of advances in cross-view localization and synthesis,\nreviewing widely used datasets, highlighting key challenges, and providing an\norganized overview of state-of-the-art techniques. Furthermore, it discusses\ncurrent limitations, offers comparative analyses, and outlines promising\ndirections for future research. We also include the project page via\nhttps://github.com/GDAOSU/Awesome-Cross-View-Methods.",
        "url": "http://arxiv.org/abs/2510.22736v1",
        "published_date": "2025-10-26T16:09:53+00:00",
        "updated_date": "2025-10-26T16:09:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ningli Xu",
            "Rongjun Qin"
        ],
        "tldr": "This paper surveys cross-view localization and synthesis, which are essential tasks using overhead and ground-level imagery, reviewing datasets, challenges, state-of-the-art techniques, and future research directions.",
        "tldr_zh": "本文综述了跨视角定位和合成技术，这两者都是利用俯视和地面图像的重要任务。论文回顾了数据集、挑战、最先进的技术和未来的研究方向。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]