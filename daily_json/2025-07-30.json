[
    {
        "title": "From Seeing to Experiencing: Scaling Navigation Foundation Models with Reinforcement Learning",
        "summary": "Navigation foundation models trained on massive webscale data enable agents\nto generalize across diverse environments and embodiments. However, these\nmodels trained solely on offline data, often lack the capacity to reason about\nthe consequences of their actions or adapt through counterfactual\nunderstanding. They thus face significant limitations in the real-world urban\nnavigation where interactive and safe behaviors, such as avoiding obstacles and\nmoving pedestrians, are critical. To tackle these challenges, we introduce the\nSeeing-to-Experiencing framework to scale the capability of navigation\nfoundation models with reinforcement learning. S2E combines the strengths of\npre-training on videos and post-training through RL. It maintains the\ngeneralizability acquired from large-scale real-world videos while enhancing\nits interactivity through RL in simulation environments. Specifically, we\nintroduce two innovations: an Anchor-Guided Distribution Matching strategy,\nwhich stabilizes learning and models diverse motion patterns through\nanchor-based supervision; and a Residual-Attention Module, which obtains\nreactive behaviors from simulation environments without erasing the model's\npretrained knowledge. Moreover, we establish a comprehensive end-to-end\nevaluation benchmark, NavBench-GS, built on photorealistic 3DGS reconstructions\nof real-world scenes that incorporate physical interactions. It can\nsystematically assess the generalizability and safety of navigation foundation\nmodels. Extensive experiments show that S2E mitigates the diminishing returns\noften seen when scaling with offline data alone. We perform a thorough analysis\nof the benefits of Reinforcement Learning compared to Supervised Fine-Tuning in\nthe context of post-training for robot learning. Our findings emphasize the\ncrucial role of integrating interactive online experiences to effectively scale\nfoundation models in Robotics.",
        "url": "http://arxiv.org/abs/2507.22028v1",
        "published_date": "2025-07-29T17:26:10+00:00",
        "updated_date": "2025-07-29T17:26:10+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Honglin He",
            "Yukai Ma",
            "Wayne Wu",
            "Bolei Zhou"
        ],
        "tldr": "This paper introduces a Seeing-to-Experiencing (S2E) framework that combines pre-training on webscale videos with reinforcement learning to improve the interactive and safe navigation capabilities of foundation models in real-world environments. They also provide a new benchmark, NavBench-GS, for evaluating navigation models.",
        "tldr_zh": "该论文介绍了一个名为 Seeing-to-Experiencing (S2E) 的框架，它结合了基于大规模网络视频的预训练和强化学习，以提高基础模型在真实环境中进行交互式和安全导航的能力。他们还提供了一个新的基准测试 NavBench-GS，用于评估导航模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Research Challenges and Progress in the End-to-End V2X Cooperative Autonomous Driving Competition",
        "summary": "With the rapid advancement of autonomous driving technology,\nvehicle-to-everything (V2X) communication has emerged as a key enabler for\nextending perception range and enhancing driving safety by providing visibility\nbeyond the line of sight. However, integrating multi-source sensor data from\nboth ego-vehicles and infrastructure under real-world constraints, such as\nlimited communication bandwidth and dynamic environments, presents significant\ntechnical challenges. To facilitate research in this area, we organized the\nEnd-to-End Autonomous Driving through V2X Cooperation Challenge, which features\ntwo tracks: cooperative temporal perception and cooperative end-to-end\nplanning. Built on the UniV2X framework and the V2X-Seq-SPD dataset, the\nchallenge attracted participation from over 30 teams worldwide and established\na unified benchmark for evaluating cooperative driving systems. This paper\ndescribes the design and outcomes of the challenge, highlights key research\nproblems including bandwidth-aware fusion, robust multi-agent planning, and\nheterogeneous sensor integration, and analyzes emerging technical trends among\ntop-performing solutions. By addressing practical constraints in communication\nand data fusion, the challenge contributes to the development of scalable and\nreliable V2X-cooperative autonomous driving systems.",
        "url": "http://arxiv.org/abs/2507.21610v1",
        "published_date": "2025-07-29T09:06:40+00:00",
        "updated_date": "2025-07-29T09:06:40+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "I.4.9"
        ],
        "authors": [
            "Ruiyang Hao",
            "Haibao Yu",
            "Jiaru Zhong",
            "Chuanye Wang",
            "Jiahao Wang",
            "Yiming Kan",
            "Wenxian Yang",
            "Siqi Fan",
            "Huilin Yin",
            "Jianing Qiu",
            "Yao Mu",
            "Jiankai Sun",
            "Li Chen",
            "Walter Zimmer",
            "Dandan Zhang",
            "Shanghang Zhang",
            "Mac Schwager",
            "Wei Huang",
            "Xiaobo Zhang",
            "Ping Luo",
            "Zaiqing Nie"
        ],
        "tldr": "This paper introduces the End-to-End Autonomous Driving through V2X Cooperation Challenge, highlighting research problems and trends in cooperative perception and planning for autonomous vehicles under real-world constraints.",
        "tldr_zh": "本文介绍了端到端V2X协同自动驾驶挑战赛，重点介绍了在实际约束下自动驾驶车辆的协同感知和规划中的研究问题和趋势。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RelMap: Enhancing Online Map Construction with Class-Aware Spatial Relation and Semantic Priors",
        "summary": "Online high-definition (HD) map construction plays an increasingly important\nrole in scaling autonomous driving systems. Transformer-based methods have\nbecome prevalent in online HD map construction; however, existing approaches\noften neglect the inherent spatial and semantic relationships among map\nelements, which limits their accuracy and generalization. To address this, we\npropose RelMap, an end-to-end framework that enhances online map construction\nby incorporating spatial relations and semantic priors. We introduce a\nClass-aware Spatial Relation Prior, which explicitly encodes relative\npositional dependencies between map elements using a learnable class-aware\nrelation encoder. Additionally, we propose a Mixture-of-Experts (MoE)-based\nSemantic Prior, which routes features to class-specific experts based on\npredicted class probabilities, refining instance feature decoding. Our method\nis compatible with both single-frame and temporal perception backbones,\nachieving state-of-the-art performance on both the nuScenes and Argoverse 2\ndatasets.",
        "url": "http://arxiv.org/abs/2507.21567v1",
        "published_date": "2025-07-29T07:58:52+00:00",
        "updated_date": "2025-07-29T07:58:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianhui Cai",
            "Yun Zhang",
            "Zewei Zhou",
            "Zhiyu Huang",
            "Jiaqi Ma"
        ],
        "tldr": "RelMap enhances online HD map construction by incorporating class-aware spatial relations and semantic priors using a learnable relation encoder and a Mixture-of-Experts-based Semantic Prior, achieving state-of-the-art performance on nuScenes and Argoverse 2 datasets.",
        "tldr_zh": "RelMap通过结合类别感知的空间关系和语义先验，利用可学习的关系编码器和基于混合专家模型的语义先验，增强了在线高清地图构建，并在nuScenes和Argoverse 2数据集上实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Recursive Visual Imagination and Adaptive Linguistic Grounding for Vision Language Navigation",
        "summary": "Vision Language Navigation (VLN) typically requires agents to navigate to\nspecified objects or remote regions in unknown scenes by obeying linguistic\ncommands. Such tasks require organizing historical visual observations for\nlinguistic grounding, which is critical for long-sequence navigational\ndecisions. However, current agents suffer from overly detailed scene\nrepresentation and ambiguous vision-language alignment, which weaken their\ncomprehension of navigation-friendly high-level scene priors and easily lead to\nbehaviors that violate linguistic commands. To tackle these issues, we propose\na navigation policy by recursively summarizing along-the-way visual\nperceptions, which are adaptively aligned with commands to enhance linguistic\ngrounding. In particular, by structurally modeling historical trajectories as\ncompact neural grids, several Recursive Visual Imagination (RVI) techniques are\nproposed to motivate agents to focus on the regularity of visual transitions\nand semantic scene layouts, instead of dealing with misleading geometric\ndetails. Then, an Adaptive Linguistic Grounding (ALG) technique is proposed to\nalign the learned situational memories with different linguistic components\npurposefully. Such fine-grained semantic matching facilitates the accurate\nanticipation of navigation actions and progress. Our navigation policy\noutperforms the state-of-the-art methods on the challenging VLN-CE and\nObjectNav tasks, showing the superiority of our RVI and ALG techniques for VLN.",
        "url": "http://arxiv.org/abs/2507.21450v1",
        "published_date": "2025-07-29T02:40:07+00:00",
        "updated_date": "2025-07-29T02:40:07+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Bolei Chen",
            "Jiaxu Kang",
            "Yifei Wang",
            "Ping Zhong",
            "Qi Wu",
            "Jianxin Wang"
        ],
        "tldr": "This paper introduces Recursive Visual Imagination (RVI) and Adaptive Linguistic Grounding (ALG) to improve Vision Language Navigation (VLN) by focusing on high-level scene priors and fine-grained semantic matching, achieving state-of-the-art results on VLN-CE and ObjectNav.",
        "tldr_zh": "本文提出了递归视觉想象（RVI）和自适应语言基础（ALG）来改进视觉语言导航（VLN），通过关注高层场景先验和细粒度语义匹配，在VLN-CE和ObjectNav任务上取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MapDiffusion: Generative Diffusion for Vectorized Online HD Map Construction and Uncertainty Estimation in Autonomous Driving",
        "summary": "Autonomous driving requires an understanding of the static environment from\nsensor data. Learned Bird's-Eye View (BEV) encoders are commonly used to fuse\nmultiple inputs, and a vector decoder predicts a vectorized map representation\nfrom the latent BEV grid. However, traditional map construction models provide\ndeterministic point estimates, failing to capture uncertainty and the inherent\nambiguities of real-world environments, such as occlusions and missing lane\nmarkings. We propose MapDiffusion, a novel generative approach that leverages\nthe diffusion paradigm to learn the full distribution of possible vectorized\nmaps. Instead of predicting a single deterministic output from learned queries,\nMapDiffusion iteratively refines randomly initialized queries, conditioned on a\nBEV latent grid, to generate multiple plausible map samples. This allows\naggregating samples to improve prediction accuracy and deriving uncertainty\nestimates that directly correlate with scene ambiguity. Extensive experiments\non the nuScenes dataset demonstrate that MapDiffusion achieves state-of-the-art\nperformance in online map construction, surpassing the baseline by 5% in\nsingle-sample performance. We further show that aggregating multiple samples\nconsistently improves performance along the ROC curve, validating the benefit\nof distribution modeling. Additionally, our uncertainty estimates are\nsignificantly higher in occluded areas, reinforcing their value in identifying\nregions with ambiguous sensor input. By modeling the full map distribution,\nMapDiffusion enhances the robustness and reliability of online vectorized HD\nmap construction, enabling uncertainty-aware decision-making for autonomous\nvehicles in complex environments.",
        "url": "http://arxiv.org/abs/2507.21423v1",
        "published_date": "2025-07-29T01:16:40+00:00",
        "updated_date": "2025-07-29T01:16:40+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Thomas Monninger",
            "Zihan Zhang",
            "Zhipeng Mo",
            "Md Zafar Anwar",
            "Steffen Staab",
            "Sihao Ding"
        ],
        "tldr": "MapDiffusion uses a diffusion model to generate multiple plausible vectorized HD maps from BEV latent grids, improving accuracy and providing uncertainty estimates for autonomous driving. It achieves state-of-the-art performance on the nuScenes dataset.",
        "tldr_zh": "MapDiffusion使用扩散模型从BEV潜在网格生成多个可能的矢量化高清地图，提高了准确性并为自动驾驶提供不确定性估计。 它在nuScenes数据集上实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Collaborative Perceiver: Elevating Vision-based 3D Object Detection via Local Density-Aware Spatial Occupancy",
        "summary": "Vision-based bird's-eye-view (BEV) 3D object detection has advanced\nsignificantly in autonomous driving by offering cost-effectiveness and rich\ncontextual information. However, existing methods often construct BEV\nrepresentations by collapsing extracted object features, neglecting intrinsic\nenvironmental contexts, such as roads and pavements. This hinders detectors\nfrom comprehensively perceiving the characteristics of the physical world. To\nalleviate this, we introduce a multi-task learning framework, Collaborative\nPerceiver (CoP), that leverages spatial occupancy as auxiliary information to\nmine consistent structural and conceptual similarities shared between 3D object\ndetection and occupancy prediction tasks, bridging gaps in spatial\nrepresentations and feature refinement. To this end, we first propose a\npipeline to generate dense occupancy ground truths incorporating local density\ninformation (LDO) for reconstructing detailed environmental information. Next,\nwe employ a voxel-height-guided sampling (VHS) strategy to distill fine-grained\nlocal features according to distinct object properties. Furthermore, we develop\na global-local collaborative feature fusion (CFF) module that seamlessly\nintegrates complementary knowledge between both tasks, thus composing more\nrobust BEV representations. Extensive experiments on the nuScenes benchmark\ndemonstrate that CoP outperforms existing vision-based frameworks, achieving\n49.5\\% mAP and 59.2\\% NDS on the test set. Code and supplementary materials are\navailable at this link https://github.com/jichengyuan/Collaborative-Perceiver.",
        "url": "http://arxiv.org/abs/2507.21358v3",
        "published_date": "2025-07-28T21:56:43+00:00",
        "updated_date": "2025-07-31T17:07:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jicheng Yuan",
            "Manh Nguyen Duc",
            "Qian Liu",
            "Manfred Hauswirth",
            "Danh Le Phuoc"
        ],
        "tldr": "The paper introduces Collaborative Perceiver (CoP), a multi-task learning framework that leverages spatial occupancy as auxiliary information for vision-based 3D object detection in autonomous driving, achieving state-of-the-art results on the nuScenes benchmark.",
        "tldr_zh": "该论文介绍了协作感知器 (CoP)，一种多任务学习框架，它利用空间占用作为辅助信息，用于自动驾驶中基于视觉的 3D 对象检测，并在 nuScenes 基准测试中取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SAMITE: Position Prompted SAM2 with Calibrated Memory for Visual Object Tracking",
        "summary": "Visual Object Tracking (VOT) is widely used in applications like autonomous\ndriving to continuously track targets in videos. Existing methods can be\nroughly categorized into template matching and autoregressive methods, where\nthe former usually neglects the temporal dependencies across frames and the\nlatter tends to get biased towards the object categories during training,\nshowing weak generalizability to unseen classes. To address these issues, some\nmethods propose to adapt the video foundation model SAM2 for VOT, where the\ntracking results of each frame would be encoded as memory for conditioning the\nrest of frames in an autoregressive manner. Nevertheless, existing methods fail\nto overcome the challenges of object occlusions and distractions, and do not\nhave any measures to intercept the propagation of tracking errors. To tackle\nthem, we present a SAMITE model, built upon SAM2 with additional modules,\nincluding: (1) Prototypical Memory Bank: We propose to quantify the\nfeature-wise and position-wise correctness of each frame's tracking results,\nand select the best frames to condition subsequent frames. As the features of\noccluded and distracting objects are feature-wise and position-wise inaccurate,\ntheir scores would naturally be lower and thus can be filtered to intercept\nerror propagation; (2) Positional Prompt Generator: To further reduce the\nimpacts of distractors, we propose to generate positional mask prompts to\nprovide explicit positional clues for the target, leading to more accurate\ntracking. Extensive experiments have been conducted on six benchmarks, showing\nthe superiority of SAMITE. The code is available at\nhttps://github.com/Sam1224/SAMITE.",
        "url": "http://arxiv.org/abs/2507.21732v1",
        "published_date": "2025-07-29T12:11:56+00:00",
        "updated_date": "2025-07-29T12:11:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qianxiong Xu",
            "Lanyun Zhu",
            "Chenxi Liu",
            "Guosheng Lin",
            "Cheng Long",
            "Ziyue Li",
            "Rui Zhao"
        ],
        "tldr": "The paper introduces SAMITE, an improved visual object tracking method building upon SAM2, which uses a prototypical memory bank and positional prompt generator to address occlusion, distractions, and error propagation in video tracking.",
        "tldr_zh": "该论文介绍了SAMITE，一种改进的视觉目标跟踪方法，它建立在SAM2的基础上，使用原型记忆库和位置提示生成器来解决视频跟踪中的遮挡、干扰和误差传播问题。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]