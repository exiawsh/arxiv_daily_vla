[
    {
        "title": "WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering",
        "summary": "Forward and inverse rendering have emerged as key techniques for enabling\nunderstanding and reconstruction in the context of autonomous driving (AD).\nHowever, complex weather and illumination pose great challenges to this task.\nThe emergence of large diffusion models has shown promise in achieving\nreasonable results through learning from 2D priors, but these models are\ndifficult to control and lack robustness. In this paper, we introduce\nWeatherDiffusion, a diffusion-based framework for forward and inverse rendering\non AD scenes with various weather and lighting conditions. Our method enables\nauthentic estimation of material properties, scene geometry, and lighting, and\nfurther supports controllable weather and illumination editing through the use\nof predicted intrinsic maps guided by text descriptions. We observe that\ndifferent intrinsic maps should correspond to different regions of the original\nimage. Based on this observation, we propose Intrinsic map-aware attention\n(MAA) to enable high-quality inverse rendering. Additionally, we introduce a\nsynthetic dataset (\\ie WeatherSynthetic) and a real-world dataset (\\ie\nWeatherReal) for forward and inverse rendering on AD scenes with diverse\nweather and lighting. Extensive experiments show that our WeatherDiffusion\noutperforms state-of-the-art methods on several benchmarks. Moreover, our\nmethod demonstrates significant value in downstream tasks for AD, enhancing the\nrobustness of object detection and image segmentation in challenging weather\nscenarios.",
        "url": "http://arxiv.org/abs/2508.06982v1",
        "published_date": "2025-08-09T13:29:39+00:00",
        "updated_date": "2025-08-09T13:29:39+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yixin Zhu",
            "Zuoliang Zhu",
            "Miloš Hašan",
            "Jian Yang",
            "Jin Xie",
            "Beibei Wang"
        ],
        "tldr": "WeatherDiffusion is a diffusion-based framework for forward and inverse rendering in autonomous driving scenes, improving robustness in challenging weather through intrinsic map-aware attention and new datasets. It also enhances object detection and image segmentation in adverse weather conditions.",
        "tldr_zh": "WeatherDiffusion是一个基于扩散模型的框架，用于自动驾驶场景中的正向和逆向渲染，通过内在地图感知注意力机制和新数据集提高在恶劣天气下的鲁棒性。它还能增强物体检测和图像分割在不利天气条件下的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding",
        "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress\nacross a range of vision-language tasks and demonstrate strong potential for\ntraffic accident understanding. However, existing MLLMs in this domain\nprimarily focus on coarse-grained image-level or video-level comprehension and\noften struggle to handle fine-grained visual details or localized scene\ncomponents, limiting their applicability in complex accident scenarios. To\naddress these limitations, we propose SafePLUG, a novel framework that empowers\nMLLMs with both Pixel-Level Understanding and temporal Grounding for\ncomprehensive traffic accident analysis. SafePLUG supports both\narbitrary-shaped visual prompts for region-aware question answering and\npixel-level segmentation based on language instructions, while also enabling\nthe recognition of temporally anchored events in traffic accident scenarios. To\nadvance the development of MLLMs for traffic accident understanding, we curate\na new dataset containing multimodal question-answer pairs centered on diverse\naccident scenarios, with detailed pixel-level annotations and temporal event\nboundaries. Experimental results show that SafePLUG achieves strong performance\non multiple tasks, including region-based question answering, pixel-level\nsegmentation, temporal event localization, and accident event understanding.\nThese capabilities lay a foundation for fine-grained understanding of complex\ntraffic scenes, with the potential to improve driving safety and enhance\nsituational awareness in smart transportation systems. The code, dataset, and\nmodel checkpoints will be made publicly available at:\nhttps://zihaosheng.github.io/SafePLUG",
        "url": "http://arxiv.org/abs/2508.06763v1",
        "published_date": "2025-08-09T00:25:24+00:00",
        "updated_date": "2025-08-09T00:25:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zihao Sheng",
            "Zilin Huang",
            "Yen-Jung Chen",
            "Yansong Qu",
            "Yuhao Luo",
            "Yue Leng",
            "Sikai Chen"
        ],
        "tldr": "The paper introduces SafePLUG, a framework that enhances multimodal large language models (MLLMs) for traffic accident understanding by incorporating pixel-level understanding and temporal grounding. They also provide a new dataset with pixel-level annotations and temporal event boundaries.",
        "tldr_zh": "该论文介绍了SafePLUG，一个通过结合像素级理解和时间定位来增强多模态大型语言模型 (MLLM) 在交通事故理解方面的框架。他们还提供了一个新的数据集，其中包含像素级注释和时间事件边界。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]