[
    {
        "title": "ReWorld: Multi-Dimensional Reward Modeling for Embodied World Models",
        "summary": "Recently, video-based world models that learn to simulate the dynamics have gained increasing attention in robot learning. However, current approaches primarily emphasize visual generative quality while overlooking physical fidelity, dynamic consistency, and task logic, especially for contact-rich manipulation tasks, which limits their applicability to downstream tasks. To this end, we introduce ReWorld, a framework aimed to employ reinforcement learning to align the video-based embodied world models with physical realism, task completion capability, embodiment plausibility and visual quality. Specifically, we first construct a large-scale (~235K) video preference dataset and employ it to train a hierarchical reward model designed to capture multi-dimensional reward consistent with human preferences. We further propose a practical alignment algorithm that post-trains flow-based world models using this reward through a computationally efficient PPO-style algorithm. Comprehensive experiments and theoretical analysis demonstrate that ReWorld significantly improves the physical fidelity, logical coherence, embodiment and visual quality of generated rollouts, outperforming previous methods.",
        "url": "http://arxiv.org/abs/2601.12428v1",
        "published_date": "2026-01-18T14:27:10+00:00",
        "updated_date": "2026-01-18T14:27:10+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Baorui Peng",
            "Wenyao Zhang",
            "Liang Xu",
            "Zekun Qi",
            "Jiazhao Zhang",
            "Hongsi Liu",
            "Wenjun Zeng",
            "Xin Jin"
        ],
        "tldr": "ReWorld uses reinforcement learning and a multi-dimensional reward model trained on a large video preference dataset to improve the physical realism, task completion, embodiment, and visual quality of video-based embodied world models.",
        "tldr_zh": "ReWorld使用强化学习和一个基于大型视频偏好数据集训练的多维奖励模型来提高基于视频的具身世界模型的物理真实性、任务完成能力、具身性和视觉质量。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CD-TWINSAFE: A ROS-enabled Digital Twin for Scene Understanding and Safety Emerging V2I Technology",
        "summary": "In this paper, the CD-TWINSAFE is introduced, a V2I-based digital twin for Autonomous Vehicles. The proposed architecture is composed of two stacks running simultaneously, an on-board driving stack that includes a stereo camera for scene understanding, and a digital twin stack that runs an Unreal Engine 5 replica of the scene viewed by the camera as well as returning safety alerts to the cockpit. The on-board stack is implemented on the vehicle side including 2 main autonomous modules; localization and perception. The position and orientation of the ego vehicle are obtained using on-board sensors. Furthermore, the perception module is responsible for processing 20-fps images from stereo camera and understands the scene through two complementary pipelines. The pipeline are working on object detection and feature extraction including object velocity, yaw and the safety metrics time-to-collision and time-headway. The collected data form the driving stack are sent to the infrastructure side through the ROS-enabled architecture in the form of custom ROS2 messages and sent over UDP links that ride a 4G modem for V2I communication. The environment is monitored via the digital twin through the shared messages which update the information of the spawned ego vehicle and detected objects based on the real-time localization and perception data. Several tests with different driving scenarios to confirm the validity and real-time response of the proposed architecture.",
        "url": "http://arxiv.org/abs/2601.12373v1",
        "published_date": "2026-01-18T12:07:26+00:00",
        "updated_date": "2026-01-18T12:07:26+00:00",
        "categories": [
            "cs.CV",
            "cs.HC",
            "cs.RO"
        ],
        "authors": [
            "Amro Khaled",
            "Farah Khaled",
            "Omar Riad",
            "Catherine M. Elias"
        ],
        "tldr": "The paper introduces CD-TWINSAFE, a ROS-enabled digital twin architecture for autonomous vehicles using V2I communication. It utilizes a stereo camera for scene understanding and provides safety alerts via a digital twin running in Unreal Engine 5.",
        "tldr_zh": "本文介绍了一种名为CD-TWINSAFE的基于V2I的自动驾驶车辆数字孪生架构，该架构通过ROS实现，使用立体相机进行场景理解，并通过在Unreal Engine 5中运行的数字孪生提供安全警报。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Prompts to Pavement: LMMs-based Agentic Behavior-Tree Generation Framework for Autonomous Vehicles",
        "summary": "Autonomous vehicles (AVs) require adaptive behavior planners to navigate unpredictable, real-world environments safely. Traditional behavior trees (BTs) offer structured decision logic but are inherently static and demand labor-intensive manual tuning, limiting their applicability at SAE Level 5 autonomy. This paper presents an agentic framework that leverages large language models (LLMs) and multi-modal vision models (LVMs) to generate and adapt BTs on the fly. A specialized Descriptor agent applies chain-of-symbols prompting to assess scene criticality, a Planner agent constructs high-level sub-goals via in-context learning, and a Generator agent synthesizes executable BT sub-trees in XML format. Integrated into a CARLA+Nav2 simulation, our system triggers only upon baseline BT failure, demonstrating successful navigation around unexpected obstacles (e.g., street blockage) with no human intervention. Compared to a static BT baseline, this approach is a proof-of-concept that extends to diverse driving scenarios.",
        "url": "http://arxiv.org/abs/2601.12358v1",
        "published_date": "2026-01-18T11:32:29+00:00",
        "updated_date": "2026-01-18T11:32:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Omar Y. Goba",
            "Ahmed Y. Gado",
            "Catherine M. Elias",
            "Ahmed Hussein"
        ],
        "tldr": "This paper introduces an LLM/LVM-based agentic framework for generating and adapting behavior trees for autonomous vehicles in real-time, enabling navigation in unexpected scenarios where static BTs fail.",
        "tldr_zh": "本文介绍了一种基于LLM/LVM的代理框架，用于实时生成和调整自动驾驶车辆的行为树，从而在静态BT失效的意外情况下实现导航。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OpenNavMap: Structure-Free Topometric Mapping via Large-Scale Collaborative Localization",
        "summary": "Scalable and maintainable map representations are fundamental to enabling large-scale visual navigation and facilitating the deployment of robots in real-world environments. While collaborative localization across multi-session mapping enhances efficiency, traditional structure-based methods struggle with high maintenance costs and fail in feature-less environments or under significant viewpoint changes typical of crowd-sourced data. To address this, we propose OPENNAVMAP, a lightweight, structure-free topometric system leveraging 3D geometric foundation models for on-demand reconstruction. Our method unifies dynamic programming-based sequence matching, geometric verification, and confidence-calibrated optimization to robust, coarse-to-fine submap alignment without requiring pre-built 3D models. Evaluations on the Map-Free benchmark demonstrate superior accuracy over structure-from-motion and regression baselines, achieving an average translation error of 0.62m. Furthermore, the system maintains global consistency across 15km of multi-session data with an absolute trajectory error below 3m for map merging. Finally, we validate practical utility through 12 successful autonomous image-goal navigation tasks on simulated and physical robots. Code and datasets will be publicly available in https://rpl-cs-ucl.github.io/OpenNavMap_page.",
        "url": "http://arxiv.org/abs/2601.12291v1",
        "published_date": "2026-01-18T07:24:46+00:00",
        "updated_date": "2026-01-18T07:24:46+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Jianhao Jiao",
            "Changkun Liu",
            "Jingwen Yu",
            "Boyi Liu",
            "Qianyi Zhang",
            "Yue Wang",
            "Dimitrios Kanoulas"
        ],
        "tldr": "The paper introduces OPENNAVMAP, a structure-free topometric mapping system for large-scale collaborative localization, using 3D geometric foundation models for on-demand reconstruction and achieving high accuracy and global consistency in map merging and autonomous navigation.",
        "tldr_zh": "该论文介绍了一种名为OPENNAVMAP的无结构拓扑地图系统，用于大规模协作定位，利用3D几何基础模型进行按需重建，并在地图合并和自主导航中实现了高精度和全局一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]