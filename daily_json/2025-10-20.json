[
    {
        "title": "Vision-Centric 4D Occupancy Forecasting and Planning via Implicit Residual World Models",
        "summary": "End-to-end autonomous driving systems increasingly rely on vision-centric\nworld models to understand and predict their environment. However, a common\nineffectiveness in these models is the full reconstruction of future scenes,\nwhich expends significant capacity on redundantly modeling static backgrounds.\nTo address this, we propose IR-WM, an Implicit Residual World Model that\nfocuses on modeling the current state and evolution of the world. IR-WM first\nestablishes a robust bird's-eye-view representation of the current state from\nthe visual observation. It then leverages the BEV features from the previous\ntimestep as a strong temporal prior and predicts only the \"residual\", i.e., the\nchanges conditioned on the ego-vehicle's actions and scene context. To\nalleviate error accumulation over time, we further apply an alignment module to\ncalibrate semantic and dynamic misalignments. Moreover, we investigate\ndifferent forecasting-planning coupling schemes and demonstrate that the\nimplicit future state generated by world models substantially improves planning\naccuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D\noccupancy forecasting and trajectory planning.",
        "url": "http://arxiv.org/abs/2510.16729v1",
        "published_date": "2025-10-19T06:45:37+00:00",
        "updated_date": "2025-10-19T06:45:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jianbiao Mei",
            "Yu Yang",
            "Xuemeng Yang",
            "Licheng Wen",
            "Jiajun Lv",
            "Botian Shi",
            "Yong Liu"
        ],
        "tldr": "The paper introduces IR-WM, an Implicit Residual World Model for vision-centric autonomous driving that predicts changes in the environment rather than reconstructing the entire scene, achieving state-of-the-art performance in 4D occupancy forecasting and trajectory planning on nuScenes.",
        "tldr_zh": "该论文介绍了IR-WM，一种用于视觉中心自动驾驶的隐式残差世界模型，它预测环境中的变化，而不是重建整个场景，在nuScenes上实现了4D占用预测和轨迹规划的最先进性能。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Unsupervised Monocular Road Segmentation for Autonomous Driving via Scene Geometry",
        "summary": "This paper presents a fully unsupervised approach for binary road\nsegmentation (road vs. non-road), eliminating the reliance on costly manually\nlabeled datasets. The method leverages scene geometry and temporal cues to\ndistinguish road from non-road regions. Weak labels are first generated from\ngeometric priors, marking pixels above the horizon as non-road and a predefined\nquadrilateral in front of the vehicle as road. In a refinement stage, temporal\nconsistency is enforced by tracking local feature points across frames and\npenalizing inconsistent label assignments using mutual information\nmaximization. This enhances both precision and temporal stability. On the\nCityscapes dataset, the model achieves an Intersection-over-Union (IoU) of\n0.82, demonstrating high accuracy with a simple design. These findings\ndemonstrate the potential of combining geometric constraints and temporal\nconsistency for scalable unsupervised road segmentation in autonomous driving.",
        "url": "http://arxiv.org/abs/2510.16790v1",
        "published_date": "2025-10-19T10:59:43+00:00",
        "updated_date": "2025-10-19T10:59:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sara Hatami Rostami",
            "Behrooz Nasihatkon"
        ],
        "tldr": "This paper presents an unsupervised monocular road segmentation method using geometric priors and temporal consistency, achieving a high IoU on the Cityscapes dataset.",
        "tldr_zh": "本文提出了一种基于几何先验和时间一致性的无监督单目道路分割方法，并在Cityscapes数据集上实现了较高的IoU。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Comprehensive Survey on World Models for Embodied AI",
        "summary": "Embodied AI requires agents that perceive, act, and anticipate how actions\nreshape future world states. World models serve as internal simulators that\ncapture environment dynamics, enabling forward and counterfactual rollouts to\nsupport perception, prediction, and decision making. This survey presents a\nunified framework for world models in embodied AI. Specifically, we formalize\nthe problem setting and learning objectives, and propose a three-axis taxonomy\nencompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2)\nTemporal Modeling, Sequential Simulation and Inference vs. Global Difference\nPrediction; (3) Spatial Representation, Global Latent Vector, Token Feature\nSequence, Spatial Latent Grid, and Decomposed Rendering Representation. We\nsystematize data resources and metrics across robotics, autonomous driving, and\ngeneral video settings, covering pixel prediction quality, state-level\nunderstanding, and task performance. Furthermore, we offer a quantitative\ncomparison of state-of-the-art models and distill key open challenges,\nincluding the scarcity of unified datasets and the need for evaluation metrics\nthat assess physical consistency over pixel fidelity, the trade-off between\nmodel performance and the computational efficiency required for real-time\ncontrol, and the core modeling difficulty of achieving long-horizon temporal\nconsistency while mitigating error accumulation. Finally, we maintain a curated\nbibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.",
        "url": "http://arxiv.org/abs/2510.16732v1",
        "published_date": "2025-10-19T07:12:32+00:00",
        "updated_date": "2025-10-19T07:12:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinqing Li",
            "Xin He",
            "Le Zhang",
            "Yun Liu"
        ],
        "tldr": "This paper presents a comprehensive survey of world models for embodied AI, formalizing the problem, proposing a taxonomy, systematizing data and metrics, comparing state-of-the-art models, and outlining open challenges.",
        "tldr_zh": "本文对具身人工智能的世界模型进行了全面的综述，形式化了问题，提出了分类法，系统化了数据和指标，比较了最先进的模型，并概述了未解决的挑战。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Pursuing Minimal Sufficiency in Spatial Reasoning",
        "summary": "Spatial reasoning, the ability to ground language in 3D understanding,\nremains a persistent challenge for Vision-Language Models (VLMs). We identify\ntwo fundamental bottlenecks: inadequate 3D understanding capabilities stemming\nfrom 2D-centric pre-training, and reasoning failures induced by redundant 3D\ninformation. To address these, we first construct a Minimal Sufficient Set\n(MSS) of information before answering a given question: a compact selection of\n3D perception results from \\textit{expert models}. We introduce MSSR (Minimal\nSufficient Spatial Reasoner), a dual-agent framework that implements this\nprinciple. A Perception Agent programmatically queries 3D scenes using a\nversatile perception toolbox to extract sufficient information, including a\nnovel SOG (Situated Orientation Grounding) module that robustly extracts\nlanguage-grounded directions. A Reasoning Agent then iteratively refines this\ninformation to pursue minimality, pruning redundant details and requesting\nmissing ones in a closed loop until the MSS is curated. Extensive experiments\ndemonstrate that our method, by explicitly pursuing both sufficiency and\nminimality, significantly improves accuracy and achieves state-of-the-art\nperformance across two challenging benchmarks. Furthermore, our framework\nproduces interpretable reasoning paths, offering a promising source of\nhigh-quality training data for future models. Source code is available at\nhttps://github.com/gyj155/mssr.",
        "url": "http://arxiv.org/abs/2510.16688v1",
        "published_date": "2025-10-19T02:29:09+00:00",
        "updated_date": "2025-10-19T02:29:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yejie Guo",
            "Yunzhong Hou",
            "Wufei Ma",
            "Meng Tang",
            "Ming-Hsuan Yang"
        ],
        "tldr": "The paper introduces MSSR, a dual-agent framework for spatial reasoning that improves accuracy and interpretability by constructing a Minimal Sufficient Set (MSS) of 3D information, achieving SOTA performance on challenging benchmarks.",
        "tldr_zh": "该论文介绍了一种用于空间推理的双代理框架MSSR，通过构建3D信息的最小充分集（MSS）来提高准确性和可解释性，并在具有挑战性的基准测试中实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]