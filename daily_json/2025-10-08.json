[
    {
        "title": "ALISE: Annotation-Free LiDAR Instance Segmentation for Autonomous Driving",
        "summary": "The manual annotation of outdoor LiDAR point clouds for instance segmentation\nis extremely costly and time-consuming. Current methods attempt to reduce this\nburden but still rely on some form of human labeling. To completely eliminate\nthis dependency, we introduce ALISE, a novel framework that performs LiDAR\ninstance segmentation without any annotations. The central challenge is to\ngenerate high-quality pseudo-labels in a fully unsupervised manner. Our\napproach starts by employing Vision Foundation Models (VFMs), guided by text\nand images, to produce initial pseudo-labels. We then refine these labels\nthrough a dedicated spatio-temporal voting module, which combines 2D and 3D\nsemantics for both offline and online optimization. To achieve superior feature\nlearning, we further introduce two forms of semantic supervision: a set of 2D\nprior-based losses that inject visual knowledge into the 3D network, and a\nnovel prototype-based contrastive loss that builds a discriminative feature\nspace by exploiting 3D semantic consistency. This comprehensive design results\nin significant performance gains, establishing a new state-of-the-art for\nunsupervised 3D instance segmentation. Remarkably, our approach even\noutperforms MWSIS, a method that operates with supervision from ground-truth\n(GT) 2D bounding boxes by a margin of 2.53% in mAP (50.95% vs. 48.42%).",
        "url": "http://arxiv.org/abs/2510.05752v1",
        "published_date": "2025-10-07T10:15:18+00:00",
        "updated_date": "2025-10-07T10:15:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yongxuan Lyu",
            "Guangfeng Jiang",
            "Hongsi Liu",
            "Jun Liu"
        ],
        "tldr": "The paper introduces ALISE, a novel annotation-free framework for LiDAR instance segmentation in autonomous driving, utilizing Vision Foundation Models and spatio-temporal voting to generate high-quality pseudo-labels and achieve state-of-the-art unsupervised performance.",
        "tldr_zh": "该论文介绍了一种名为ALISE的全新无标注LiDAR点云实例分割框架，用于自动驾驶。该框架利用视觉基础模型和时空投票生成高质量的伪标签，实现了最先进的无监督性能。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Dropping the D: RGB-D SLAM Without the Depth Sensor",
        "summary": "We present DropD-SLAM, a real-time monocular SLAM system that achieves\nRGB-D-level accuracy without relying on depth sensors. The system replaces\nactive depth input with three pretrained vision modules: a monocular metric\ndepth estimator, a learned keypoint detector, and an instance segmentation\nnetwork. Dynamic objects are suppressed using dilated instance masks, while\nstatic keypoints are assigned predicted depth values and backprojected into 3D\nto form metrically scaled features. These are processed by an unmodified RGB-D\nSLAM back end for tracking and mapping. On the TUM RGB-D benchmark, DropD-SLAM\nattains 7.4 cm mean ATE on static sequences and 1.8 cm on dynamic sequences,\nmatching or surpassing state-of-the-art RGB-D methods while operating at 22 FPS\non a single GPU. These results suggest that modern pretrained vision models can\nreplace active depth sensors as reliable, real-time sources of metric scale,\nmarking a step toward simpler and more cost-effective SLAM systems.",
        "url": "http://arxiv.org/abs/2510.06216v1",
        "published_date": "2025-10-07T17:59:30+00:00",
        "updated_date": "2025-10-07T17:59:30+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Mert Kiray",
            "Alican Karaomer",
            "Benjamin Busam"
        ],
        "tldr": "This paper presents DropD-SLAM, a monocular SLAM system achieving RGB-D accuracy without a depth sensor by leveraging pretrained vision models for depth estimation, keypoint detection, and instance segmentation. It demonstrates comparable or superior performance to RGB-D methods on the TUM RGB-D benchmark.",
        "tldr_zh": "该论文介绍了DropD-SLAM，一种单目SLAM系统，它利用预训练的视觉模型进行深度估计、关键点检测和实例分割，在没有深度传感器的情况下实现了RGB-D的精度。该系统在TUM RGB-D基准测试中表现出与RGB-D方法相当甚至更好的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models",
        "summary": "Recent advances in generative models have sparked exciting new possibilities\nin the field of autonomous vehicles. Specifically, video generation models are\nnow being explored as controllable virtual testing environments.\nSimultaneously, end-to-end (E2E) driving models have emerged as a streamlined\nalternative to conventional modular autonomous driving systems, gaining\npopularity for their simplicity and scalability. However, the application of\nthese techniques to simulation and planning raises important questions. First,\nwhile video generation models can generate increasingly realistic videos, can\nthese videos faithfully adhere to the specified conditions and be realistic\nenough for E2E autonomous planner evaluation? Second, given that data is\ncrucial for understanding and controlling E2E planners, how can we gain deeper\ninsights into their biases and improve their ability to generalize to\nout-of-distribution scenarios? In this work, we bridge the gap between the\ndriving models and generative world models (Drive&Gen) to address these\nquestions. We propose novel statistical measures leveraging E2E drivers to\nevaluate the realism of generated videos. By exploiting the controllability of\nthe video generation model, we conduct targeted experiments to investigate\ndistribution gaps affecting E2E planner performance. Finally, we show that\nsynthetic data produced by the video generation model offers a cost-effective\nalternative to real-world data collection. This synthetic data effectively\nimproves E2E model generalization beyond existing Operational Design Domains,\nfacilitating the expansion of autonomous vehicle services into new operational\ncontexts.",
        "url": "http://arxiv.org/abs/2510.06209v1",
        "published_date": "2025-10-07T17:58:32+00:00",
        "updated_date": "2025-10-07T17:58:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahao Wang",
            "Zhenpei Yang",
            "Yijing Bai",
            "Yingwei Li",
            "Yuliang Zou",
            "Bo Sun",
            "Abhijit Kundu",
            "Jose Lezama",
            "Luna Yue Huang",
            "Zehao Zhu",
            "Jyh-Jing Hwang",
            "Dragomir Anguelov",
            "Mingxing Tan",
            "Chiyu Max Jiang"
        ],
        "tldr": "This paper introduces a framework called Drive&Gen that uses end-to-end driving models to evaluate the realism and controllability of generated videos for autonomous driving simulation, and demonstrates the use of synthetic data from video generation to improve E2E planner generalization.",
        "tldr_zh": "本文介绍了一个名为Drive&Gen的框架，该框架使用端到端驾驶模型来评估生成视频的真实性和可控性，用于自动驾驶仿真，并展示了使用视频生成产生的合成数据来提高E2E规划器的泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "The Safety Challenge of World Models for Embodied AI Agents: A Review",
        "summary": "The rapid progress in embodied artificial intelligence has highlighted the\nnecessity for more advanced and integrated models that can perceive, interpret,\nand predict environmental dynamics. In this context, World Models (WMs) have\nbeen introduced to provide embodied agents with the abilities to anticipate\nfuture environmental states and fill in knowledge gaps, thereby enhancing\nagents' ability to plan and execute actions. However, when dealing with\nembodied agents it is fundamental to ensure that predictions are safe for both\nthe agent and the environment. In this article, we conduct a comprehensive\nliterature review of World Models in the domains of autonomous driving and\nrobotics, with a specific focus on the safety implications of scene and control\ngeneration tasks. Our review is complemented by an empirical analysis, wherein\nwe collect and examine predictions from state-of-the-art models, identify and\ncategorize common faults (herein referred to as pathologies), and provide a\nquantitative evaluation of the results.",
        "url": "http://arxiv.org/abs/2510.05865v1",
        "published_date": "2025-10-07T12:35:09+00:00",
        "updated_date": "2025-10-07T12:35:09+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Lorenzo Baraldi",
            "Zifan Zeng",
            "Chongzhe Zhang",
            "Aradhana Nayak",
            "Hongbo Zhu",
            "Feng Liu",
            "Qunli Zhang",
            "Peng Wang",
            "Shiming Liu",
            "Zheng Hu",
            "Angelo Cangelosi",
            "Lorenzo Baraldi"
        ],
        "tldr": "This paper reviews the safety implications of using World Models in autonomous driving and robotics, empirically analyzing and categorizing prediction faults from state-of-the-art models.",
        "tldr_zh": "本文回顾了在自动驾驶和机器人领域使用世界模型（World Models）的安全性影响，并对现有模型的预测错误进行了实证分析和分类。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI",
        "summary": "Large language models leverage internet-scale text data, yet embodied AI\nremains constrained by the prohibitive costs of physical trajectory collection.\nDesktop environments -- particularly gaming -- offer a compelling alternative:\nthey provide rich sensorimotor interactions at scale while maintaining the\nstructured observation-action coupling essential for embodied learning. We\npresent D2E (Desktop to Embodied AI), a framework that demonstrates desktop\ninteractions can serve as an effective pretraining substrate for robotics\nembodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT\nfor Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a\ncomplete pipeline from scalable desktop data collection to verified transfer in\nembodied domains. Our framework comprises three components: (1) the OWA Toolkit\nthat unifies diverse desktop interactions into a standardized format with 152x\ncompression, (2) the Generalist-IDM that achieves strong zero-shot\ngeneralization across unseen games through timestamp-based event prediction,\nenabling internet-scale pseudo-labeling, and (3) VAPT that transfers\ndesktop-pretrained representations to physical manipulation and navigation.\nUsing 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of\npseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO\nmanipulation and 83.3% on CANVAS navigation benchmarks. This validates that\nsensorimotor primitives in digital interactions exhibit sufficient invariance\nto transfer meaningfully to physical embodied tasks, establishing desktop\npretraining as a practical paradigm for robotics. We will make all our work\npublic, including the OWA toolkit, datasets of human-collected and\npseudo-labeled, and VAPT-trained models available at\nhttps://worv-ai.github.io/d2e/",
        "url": "http://arxiv.org/abs/2510.05684v1",
        "published_date": "2025-10-07T08:40:33+00:00",
        "updated_date": "2025-10-07T08:40:33+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Suwhan Choi",
            "Jaeyoon Jung",
            "Haebin Seong",
            "Minchan Kim",
            "Minyeong Kim",
            "Yongjun Cho",
            "Yoonshik Kim",
            "Yubeen Park",
            "Youngjae Yu",
            "Yunsung Lee"
        ],
        "tldr": "The paper introduces D2E, a framework for pretraining embodied AI agents using large-scale desktop interaction data and demonstrates its effectiveness by achieving high success rates on robotic manipulation and navigation benchmarks, proving that desktop interactions can transfer to physical embodied tasks.",
        "tldr_zh": "该论文介绍了D2E，一个利用大规模桌面交互数据预训练具身智能代理的框架。通过在机器人操作和导航基准测试中取得较高的成功率，证明了桌面交互可以迁移到物理具身任务。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DeLTa: Demonstration and Language-Guided Novel Transparent Object Manipulation",
        "summary": "Despite the prevalence of transparent object interactions in human everyday\nlife, transparent robotic manipulation research remains limited to\nshort-horizon tasks and basic grasping capabilities.Although some methods have\npartially addressed these issues, most of them have limitations in\ngeneralizability to novel objects and are insufficient for precise long-horizon\nrobot manipulation. To address this limitation, we propose DeLTa (Demonstration\nand Language-Guided Novel Transparent Object Manipulation), a novel framework\nthat integrates depth estimation, 6D pose estimation, and vision-language\nplanning for precise long-horizon manipulation of transparent objects guided by\nnatural task instructions. A key advantage of our method is its\nsingle-demonstration approach, which generalizes 6D trajectories to novel\ntransparent objects without requiring category-level priors or additional\ntraining. Additionally, we present a task planner that refines the\nVLM-generated plan to account for the constraints of a single-arm, eye-in-hand\nrobot for long-horizon object manipulation tasks. Through comprehensive\nevaluation, we demonstrate that our method significantly outperforms existing\ntransparent object manipulation approaches, particularly in long-horizon\nscenarios requiring precise manipulation capabilities. Project page:\nhttps://sites.google.com/view/DeLTa25/",
        "url": "http://arxiv.org/abs/2510.05662v1",
        "published_date": "2025-10-07T08:18:29+00:00",
        "updated_date": "2025-10-07T08:18:29+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Taeyeop Lee",
            "Gyuree Kang",
            "Bowen Wen",
            "Youngho Kim",
            "Seunghyeok Back",
            "In So Kweon",
            "David Hyunchul Shim",
            "Kuk-Jin Yoon"
        ],
        "tldr": "This paper introduces DeLTa, a framework for long-horizon transparent object manipulation using single-demonstration learning and vision-language planning, showing improved performance over existing methods.",
        "tldr_zh": "该论文介绍了DeLTa，一个用于长程透明物体操作的框架，它采用单次演示学习和视觉语言规划，并展示了优于现有方法的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]