[
    {
        "title": "GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation",
        "summary": "Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.",
        "url": "http://arxiv.org/abs/2512.12751v1",
        "published_date": "2025-12-14T16:23:51+00:00",
        "updated_date": "2025-12-14T16:23:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenya Yang",
            "Zhe Liu",
            "Yuxiang Lu",
            "Liping Hou",
            "Chenxuan Miao",
            "Siyi Peng",
            "Bailan Feng",
            "Xiang Bai",
            "Hengshuang Zhao"
        ],
        "tldr": "GenieDrive is a physics-aware driving video generation framework that uses 4D occupancy as a foundation and introduces a VAE with tri-plane representation and mutual control attention for improved forecasting and video quality.",
        "tldr_zh": "GenieDrive是一个具有物理感知能力的驾驶视频生成框架，它使用4D occupancy作为基础，并引入了具有三平面表示的VAE和互控制注意力机制，以提高预测和视频质量。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation",
        "summary": "Embodied agents face a critical dilemma that end-to-end models lack interpretability and explicit 3D reasoning, while modular systems ignore cross-component interdependencies and synergies. To bridge this gap, we propose the Dynamic 3D Vision-Language-Planning Model (D3D-VLP). Our model introduces two key innovations: 1) A Dynamic 3D Chain-of-Thought (3D CoT) that unifies planning, grounding, navigation, and question answering within a single 3D-VLM and CoT pipeline; 2) A Synergistic Learning from Fragmented Supervision (SLFS) strategy, which uses a masked autoregressive loss to learn from massive and partially-annotated hybrid data. This allows different CoT components to mutually reinforce and implicitly supervise each other. To this end, we construct a large-scale dataset with 10M hybrid samples from 5K real scans and 20K synthetic scenes that are compatible with online learning methods such as RL and DAgger. Our D3D-VLP achieves state-of-the-art results on multiple benchmarks, including Vision-and-Language Navigation (R2R-CE, REVERIE-CE, NavRAG-CE), Object-goal Navigation (HM3D-OVON), and Task-oriented Sequential Grounding and Navigation (SG3D). Real-world mobile manipulation experiments further validate the effectiveness.",
        "url": "http://arxiv.org/abs/2512.12622v1",
        "published_date": "2025-12-14T09:53:15+00:00",
        "updated_date": "2025-12-14T09:53:15+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Zihan Wang",
            "Seungjun Lee",
            "Guangzhao Dai",
            "Gim Hee Lee"
        ],
        "tldr": "The paper introduces D3D-VLP, a dynamic 3D vision-language-planning model that unifies planning, grounding, navigation, and question answering using a 3D Chain-of-Thought and a synergistic learning strategy, achieving state-of-the-art results on several benchmarks related to embodied AI.",
        "tldr_zh": "该论文介绍了D3D-VLP，一个动态的3D视觉-语言-规划模型，它通过使用3D思维链和协同学习策略统一了规划、定位、导航和问答，并在多个与具身人工智能相关的基准测试中取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]