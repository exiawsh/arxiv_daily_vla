[
    {
        "title": "Robustness Is a Function, Not a Number: A Factorized Comprehensive Study of OOD Robustness in Vision-Based Driving",
        "summary": "Out of distribution (OOD) robustness in autonomous driving is often reduced to a single number, hiding what breaks a policy. We decompose environments along five axes: scene (rural/urban), season, weather, time (day/night), and agent mix; and measure performance under controlled $k$-factor perturbations ($k \\in \\{0,1,2,3\\}$). Using closed loop control in VISTA, we benchmark FC, CNN, and ViT policies, train compact ViT heads on frozen foundation-model (FM) features, and vary ID support in scale, diversity, and temporal context. (1) ViT policies are markedly more OOD-robust than comparably sized CNN/FC, and FM features yield state-of-the-art success at a latency cost. (2) Naive temporal inputs (multi-frame) do not beat the best single-frame baseline. (3) The largest single factor drops are rural $\\rightarrow$ urban and day $\\rightarrow$ night ($\\sim 31\\%$ each); actor swaps $\\sim 10\\%$, moderate rain $\\sim 7\\%$; season shifts can be drastic, and combining a time flip with other changes further degrades performance. (4) FM-feature policies stay above $85\\%$ under three simultaneous changes; non-FM single-frame policies take a large first-shift hit, and all no-FM models fall below $50\\%$ by three changes. (5) Interactions are non-additive: some pairings partially offset, whereas season-time combinations are especially harmful. (6) Training on winter/snow is most robust to single-factor shifts, while a rural+summer baseline gives the best overall OOD performance. (7) Scaling traces/views improves robustness ($+11.8$ points from $5$ to $14$ traces), yet targeted exposure to hard conditions can substitute for scale. (8) Using multiple ID environments broadens coverage and strengthens weak cases (urban OOD $60.6\\% \\rightarrow 70.1\\%$) with a small ID drop; single-ID preserves peak performance but in a narrow domain. These results yield actionable design rules for OOD-robust driving policies.",
        "url": "http://arxiv.org/abs/2602.09018v1",
        "published_date": "2026-02-09T18:59:03+00:00",
        "updated_date": "2026-02-09T18:59:03+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Amir Mallak",
            "Alaa Maalouf"
        ],
        "tldr": "This paper systematically analyzes the OOD robustness of vision-based driving policies across multiple environmental factors, revealing insights into the strengths and weaknesses of different architectures and training strategies.",
        "tldr_zh": "本文系统地分析了基于视觉的驾驶策略在多种环境因素下的OOD鲁棒性，揭示了不同架构和训练策略的优势和劣势。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Online Monitoring Framework for Automotive Time Series Data using JEPA Embeddings",
        "summary": "As autonomous vehicles are rolled out, measures must be taken to ensure their safe operation. In order to supervise a system that is already in operation, monitoring frameworks are frequently employed. These run continuously online in the background, supervising the system status and recording anomalies. This work proposes an online monitoring framework to detect anomalies in object state representations. Thereby, a key challenge is creating a framework for anomaly detection without anomaly labels, which are usually unavailable for unknown anomalies. To address this issue, this work applies a self-supervised embedding method to translate object data into a latent representation space. For this, a JEPA-based self-supervised prediction task is constructed, allowing training without anomaly labels and the creation of rich object embeddings. The resulting expressive JEPA embeddings serve as input for established anomaly detection methods, in order to identify anomalies within object state representations. This framework is particularly useful for applications in real-world environments, where new or unknown anomalies may occur during operation for which there are no labels available. Experiments performed on the publicly available, real-world nuScenes dataset illustrate the framework's capabilities.",
        "url": "http://arxiv.org/abs/2602.09985v1",
        "published_date": "2026-02-10T17:10:29+00:00",
        "updated_date": "2026-02-10T17:10:29+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Alexander Fertig",
            "Karthikeyan Chandra Sekaran",
            "Lakshman Balasubramanian",
            "Michael Botsch"
        ],
        "tldr": "This paper proposes an online anomaly detection framework for autonomous vehicles using JEPA embeddings and unsupervised learning, which is trained on object state representations from the nuScenes dataset.",
        "tldr_zh": "本文提出了一种基于JEPA嵌入和无监督学习的自动驾驶车辆在线异常检测框架，该框架在来自nuScenes数据集的对象状态表示上进行训练。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MVISTA-4D: View-Consistent 4D World Model with Test-Time Action Inference for Robotic Manipulation",
        "summary": "World-model-based imagine-then-act becomes a promising paradigm for robotic manipulation, yet existing approaches typically support either purely image-based forecasting or reasoning over partial 3D geometry, limiting their ability to predict complete 4D scene dynamics. This work proposes a novel embodied 4D world model that enables geometrically consistent, arbitrary-view RGBD generation: given only a single-view RGBD observation as input, the model imagines the remaining viewpoints, which can then be back-projected and fused to assemble a more complete 3D structure across time. To efficiently learn the multi-view, cross-modality generation, we explicitly design cross-view and cross-modality feature fusion that jointly encourage consistency between RGB and depth and enforce geometric alignment across views. Beyond prediction, converting generated futures into actions is often handled by inverse dynamics, which is ill-posed because multiple actions can explain the same transition. We address this with a test-time action optimization strategy that backpropagates through the generative model to infer a trajectory-level latent best matching the predicted future, and a residual inverse dynamics model that turns this trajectory prior into accurate executable actions. Experiments on three datasets demonstrate strong performance on both 4D scene generation and downstream manipulation, and ablations provide practical insights into the key design choices.",
        "url": "http://arxiv.org/abs/2602.09878v1",
        "published_date": "2026-02-10T15:19:17+00:00",
        "updated_date": "2026-02-10T15:19:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaxu Wang",
            "Yicheng Jiang",
            "Tianlun He",
            "Jingkai Sun",
            "Qiang Zhang",
            "Junhao He",
            "Jiahang Cao",
            "Zesen Gan",
            "Mingyuan Sun",
            "Qiming Shao",
            "Xiangyu Yue"
        ],
        "tldr": "The paper proposes a 4D world model (MVISTA-4D) that can generate geometrically consistent, arbitrary-view RGBD data from a single-view RGBD observation and uses a test-time optimization strategy to infer actions for robotic manipulation.",
        "tldr_zh": "该论文提出了一种4D世界模型（MVISTA-4D），它可以从单视角RGBD观测生成几何一致的任意视角RGBD数据，并使用测试时优化策略来推断机器人操作的动作。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Time2General: Learning Spatiotemporal Invariant Representations for Domain-Generalization Video Semantic Segmentation",
        "summary": "Domain Generalized Video Semantic Segmentation (DGVSS) is trained on a single labeled driving domain and is directly deployed on unseen domains without target labels and test-time adaptation while maintaining temporally consistent predictions over video streams. In practice, both domain shift and temporal-sampling shift break correspondence-based propagation and fixed-stride temporal aggregation, causing severe frame-to-frame flicker even in label-stable regions. We propose Time2General, a DGVSS framework built on Stability Queries. Time2General introduces a Spatio-Temporal Memory Decoder that aggregates multi-frame context into a clip-level spatio-temporal memory and decodes temporally consistent per-frame masks without explicit correspondence propagation. To further suppress flicker and improve robustness to varying sampling rates, the Masked Temporal Consistency Loss is proposed to regularize temporal prediction discrepancies across different strides, and randomize training strides to expose the model to diverse temporal gaps. Extensive experiments on multiple driving benchmarks show that Time2General achieves a substantial improvement in cross-domain accuracy and temporal stability over prior DGSS and VSS baselines while running at up to 18 FPS. Code will be released after the review process.",
        "url": "http://arxiv.org/abs/2602.09648v1",
        "published_date": "2026-02-10T10:55:25+00:00",
        "updated_date": "2026-02-10T10:55:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Siyu Chen",
            "Ting Han",
            "Haoling Huang",
            "Chaolei Wang",
            "Chengzheng Fu",
            "Duxin Zhu",
            "Guorong Cai",
            "Jinhe Su"
        ],
        "tldr": "The paper proposes Time2General, a domain-generalization video semantic segmentation framework that uses a Spatio-Temporal Memory Decoder and Masked Temporal Consistency Loss to achieve temporally consistent predictions across different domains and sampling rates, showing improvements in accuracy and stability.",
        "tldr_zh": "该论文提出了Time2General，一种领域泛化视频语义分割框架，它使用时空记忆解码器和掩蔽时序一致性损失，以在不同领域和采样率下实现时间上一致的预测，并显示出在准确性和稳定性方面的改进。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model",
        "summary": "3D affordance grounding aims to highlight the actionable regions on 3D objects, which is crucial for robotic manipulation. Previous research primarily focused on learning affordance knowledge from static cues such as language and images, which struggle to provide sufficient dynamic interaction context that can reveal temporal and causal cues. To alleviate this predicament, we collect a comprehensive video-based 3D affordance dataset, \\textit{VIDA}, which contains 38K human-object-interaction videos covering 16 affordance types, 38 object categories, and 22K point clouds. Based on \\textit{VIDA}, we propose a strong baseline: VideoAfford, which activates multimodal large language models with additional affordance segmentation capabilities, enabling both world knowledge reasoning and fine-grained affordance grounding within a unified framework. To enhance action understanding capability, we leverage a latent action encoder to extract dynamic interaction priors from HOI videos. Moreover, we introduce a \\textit{spatial-aware} loss function to enable VideoAfford to obtain comprehensive 3D spatial knowledge. Extensive experimental evaluations demonstrate that our model significantly outperforms well-established methods and exhibits strong open-world generalization with affordance reasoning abilities. All datasets and code will be publicly released to advance research in this area.",
        "url": "http://arxiv.org/abs/2602.09638v1",
        "published_date": "2026-02-10T10:36:57+00:00",
        "updated_date": "2026-02-10T10:36:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hanqing Wang",
            "Mingyu Liu",
            "Xiaoyu Chen",
            "Chengwei MA",
            "Yiming Zhong",
            "Wenti Yin",
            "Yuhao Liu",
            "Zhiqing Cui",
            "Jiahao Yuan",
            "Lu Dai",
            "Zhiyuan Ma",
            "Hui Xiong"
        ],
        "tldr": "The paper introduces VideoAfford, a multimodal large language model-based approach for 3D affordance grounding using a newly collected human-object interaction video dataset (VIDA), demonstrating improved performance and generalization in affordance reasoning.",
        "tldr_zh": "该论文介绍了VideoAfford，一种基于多模态大型语言模型的3D可供性推理方法，利用新收集的人-物交互视频数据集(VIDA)，展示了在可供性推理方面改进的性能和泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RAD: Retrieval-Augmented Monocular Metric Depth Estimation for Underrepresented Classes",
        "summary": "Monocular Metric Depth Estimation (MMDE) is essential for physically intelligent systems, yet accurate depth estimation for underrepresented classes in complex scenes remains a persistent challenge. To address this, we propose RAD, a retrieval-augmented framework that approximates the benefits of multi-view stereo by utilizing retrieved neighbors as structural geometric proxies. Our method first employs an uncertainty-aware retrieval mechanism to identify low-confidence regions in the input and retrieve RGB-D context samples containing semantically similar content. We then process both the input and retrieved context via a dual-stream network and fuse them using a matched cross-attention module, which transfers geometric information only at reliable point correspondences. Evaluations on NYU Depth v2, KITTI, and Cityscapes demonstrate that RAD significantly outperforms state-of-the-art baselines on underrepresented classes, reducing relative absolute error by 29.2% on NYU Depth v2, 13.3% on KITTI, and 7.2% on Cityscapes, while maintaining competitive performance on standard in-domain benchmarks.",
        "url": "http://arxiv.org/abs/2602.09532v1",
        "published_date": "2026-02-10T08:44:31+00:00",
        "updated_date": "2026-02-10T08:44:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Michael Baltaxe",
            "Dan Levi",
            "Sagie Benaim"
        ],
        "tldr": "The paper introduces RAD, a retrieval-augmented monocular depth estimation framework that leverages retrieved RGB-D context samples to improve depth estimation for underrepresented classes in complex scenes, demonstrating significant improvements over state-of-the-art methods on various datasets.",
        "tldr_zh": "该论文介绍了一种检索增强的单目深度估计框架RAD，它利用检索到的RGB-D上下文样本来提高复杂场景中代表性不足的类的深度估计，并在各种数据集上展示了相对于最先进方法的显着改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]