[
    {
        "title": "RiemanLine: Riemannian Manifold Representation of 3D Lines for Factor Graph Optimization",
        "summary": "Minimal parametrization of 3D lines plays a critical role in camera\nlocalization and structural mapping. Existing representations in robotics and\ncomputer vision predominantly handle independent lines, overlooking structural\nregularities such as sets of parallel lines that are pervasive in man-made\nenvironments. This paper introduces \\textbf{RiemanLine}, a unified minimal\nrepresentation for 3D lines formulated on Riemannian manifolds that jointly\naccommodates both individual lines and parallel-line groups. Our key idea is to\ndecouple each line landmark into global and local components: a shared\nvanishing direction optimized on the unit sphere $\\mathcal{S}^2$, and scaled\nnormal vectors constrained on orthogonal subspaces, enabling compact encoding\nof structural regularities. For $n$ parallel lines, the proposed representation\nreduces the parameter space from $4n$ (orthonormal form) to $2n+2$, naturally\nembedding parallelism without explicit constraints. We further integrate this\nparameterization into a factor graph framework, allowing global direction\nalignment and local reprojection optimization within a unified manifold-based\nbundle adjustment. Extensive experiments on ICL-NUIM, TartanAir, and synthetic\nbenchmarks demonstrate that our method achieves significantly more accurate\npose estimation and line reconstruction, while reducing parameter\ndimensionality and improving convergence stability.",
        "url": "http://arxiv.org/abs/2508.04335v1",
        "published_date": "2025-08-06T11:27:38+00:00",
        "updated_date": "2025-08-06T11:27:38+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Yanyan Li",
            "Ze Yang",
            "Keisuke Tateno",
            "Federico Tombari Liang Zhao",
            "Gim Hee Lee"
        ],
        "tldr": "The paper introduces RiemanLine, a novel Riemannian manifold representation for 3D lines that efficiently encodes both individual and parallel line groups, improving pose estimation and line reconstruction in factor graph optimization.",
        "tldr_zh": "该论文介绍了RiemanLine，一种新的黎曼流形3D线表示方法，可以有效地编码单个和平行线组，从而提高因子图优化中的姿态估计和线重建。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Segment Any Vehicle: Semantic and Visual Context Driven SAM and A Benchmark",
        "summary": "With the rapid advancement of autonomous driving, vehicle perception,\nparticularly detection and segmentation, has placed increasingly higher demands\non algorithmic performance. Pre-trained large segmentation models, especially\nSegment Anything Model (SAM), have sparked significant interest and inspired\nnew research directions in artificial intelligence. However, SAM cannot be\ndirectly applied to the fine-grained task of vehicle part segmentation, as its\ntext-prompted segmentation functionality is not publicly accessible, and the\nmask regions generated by its default mode lack semantic labels, limiting its\nutility in structured, category-specific segmentation tasks. To address these\nlimitations, we propose SAV, a novel framework comprising three core\ncomponents: a SAM-based encoder-decoder, a vehicle part knowledge graph, and a\ncontext sample retrieval encoding module. The knowledge graph explicitly models\nthe spatial and geometric relationships among vehicle parts through a\nstructured ontology, effectively encoding prior structural knowledge.\nMeanwhile, the context retrieval module enhances segmentation by identifying\nand leveraging visually similar vehicle instances from training data, providing\nrich contextual priors for improved generalization. Furthermore, we introduce a\nnew large-scale benchmark dataset for vehicle part segmentation, named\nVehicleSeg10K, which contains 11,665 high-quality pixel-level annotations\nacross diverse scenes and viewpoints. We conduct comprehensive experiments on\nthis dataset and two other datasets, benchmarking multiple representative\nbaselines to establish a solid foundation for future research and comparison. %\nBoth the dataset and source code of this paper will be released upon\nacceptance. Both the dataset and source code of this paper will be released on\nhttps://github.com/Event-AHU/SAV",
        "url": "http://arxiv.org/abs/2508.04260v1",
        "published_date": "2025-08-06T09:46:49+00:00",
        "updated_date": "2025-08-06T09:46:49+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Xiao Wang",
            "Ziwen Wang",
            "Wentao Wu",
            "Anjie Wang",
            "Jiashu Wu",
            "Yantao Pan",
            "Chenglong Li"
        ],
        "tldr": "The paper introduces SAV, a framework leveraging SAM, a vehicle part knowledge graph, and context retrieval for fine-grained vehicle part segmentation, along with a new large-scale benchmark dataset (VehicleSeg10K). This addresses SAM's limitations for structured vehicle part segmentation.",
        "tldr_zh": "该论文提出了SAV框架，利用SAM、车辆部件知识图谱和上下文检索进行细粒度的车辆部件分割，并发布了一个新的大型基准数据集（VehicleSeg10K）。这解决了SAM在结构化车辆部件分割中的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Occupancy Learning with Spatiotemporal Memory",
        "summary": "3D occupancy becomes a promising perception representation for autonomous\ndriving to model the surrounding environment at a fine-grained scale. However,\nit remains challenging to efficiently aggregate 3D occupancy over time across\nmultiple input frames due to the high processing cost and the uncertainty and\ndynamics of voxels. To address this issue, we propose ST-Occ, a scene-level\noccupancy representation learning framework that effectively learns the\nspatiotemporal feature with temporal consistency. ST-Occ consists of two core\ndesigns: a spatiotemporal memory that captures comprehensive historical\ninformation and stores it efficiently through a scene-level representation and\na memory attention that conditions the current occupancy representation on the\nspatiotemporal memory with a model of uncertainty and dynamic awareness. Our\nmethod significantly enhances the spatiotemporal representation learned for 3D\noccupancy prediction tasks by exploiting the temporal dependency between\nmulti-frame inputs. Experiments show that our approach outperforms the\nstate-of-the-art methods by a margin of 3 mIoU and reduces the temporal\ninconsistency by 29%.",
        "url": "http://arxiv.org/abs/2508.04705v1",
        "published_date": "2025-08-06T17:59:52+00:00",
        "updated_date": "2025-08-06T17:59:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziyang Leng",
            "Jiawei Yang",
            "Wenlong Yi",
            "Bolei Zhou"
        ],
        "tldr": "The paper introduces ST-Occ, a framework for 3D occupancy prediction in autonomous driving that uses a spatiotemporal memory and attention mechanism to improve temporal consistency and accuracy compared to state-of-the-art methods.",
        "tldr_zh": "该论文介绍了ST-Occ，一个用于自动驾驶中3D occupancy预测的框架。该框架利用时空记忆和注意力机制来提高时间一致性和准确性，优于现有最佳方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BEVCon: Advancing Bird's Eye View Perception with Contrastive Learning",
        "summary": "We present BEVCon, a simple yet effective contrastive learning framework\ndesigned to improve Bird's Eye View (BEV) perception in autonomous driving. BEV\nperception offers a top-down-view representation of the surrounding\nenvironment, making it crucial for 3D object detection, segmentation, and\ntrajectory prediction tasks. While prior work has primarily focused on\nenhancing BEV encoders and task-specific heads, we address the underexplored\npotential of representation learning in BEV models. BEVCon introduces two\ncontrastive learning modules: an instance feature contrast module for refining\nBEV features and a perspective view contrast module that enhances the image\nbackbone. The dense contrastive learning designed on top of detection losses\nleads to improved feature representations across both the BEV encoder and the\nbackbone. Extensive experiments on the nuScenes dataset demonstrate that BEVCon\nachieves consistent performance gains, achieving up to +2.4% mAP improvement\nover state-of-the-art baselines. Our results highlight the critical role of\nrepresentation learning in BEV perception and offer a complementary avenue to\nconventional task-specific optimizations.",
        "url": "http://arxiv.org/abs/2508.04702v1",
        "published_date": "2025-08-06T17:59:37+00:00",
        "updated_date": "2025-08-06T17:59:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziyang Leng",
            "Jiawei Yang",
            "Zhicheng Ren",
            "Bolei Zhou"
        ],
        "tldr": "BEVCon introduces a contrastive learning framework for improving Bird's Eye View perception in autonomous driving, achieving significant mAP improvements on the nuScenes dataset by enhancing both BEV features and image backbone representations.",
        "tldr_zh": "BEVCon 提出了一个对比学习框架，用于提升自动驾驶中的鸟瞰图视角感知能力。通过增强鸟瞰图特征和图像骨干网络的表示，在 nuScenes 数据集上实现了显著的 mAP 提升。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TurboTrain: Towards Efficient and Balanced Multi-Task Learning for Multi-Agent Perception and Prediction",
        "summary": "End-to-end training of multi-agent systems offers significant advantages in\nimproving multi-task performance. However, training such models remains\nchallenging and requires extensive manual design and monitoring. In this work,\nwe introduce TurboTrain, a novel and efficient training framework for\nmulti-agent perception and prediction. TurboTrain comprises two key components:\na multi-agent spatiotemporal pretraining scheme based on masked reconstruction\nlearning and a balanced multi-task learning strategy based on gradient conflict\nsuppression. By streamlining the training process, our framework eliminates the\nneed for manually designing and tuning complex multi-stage training pipelines,\nsubstantially reducing training time and improving performance. We evaluate\nTurboTrain on a real-world cooperative driving dataset, V2XPnP-Seq, and\ndemonstrate that it further improves the performance of state-of-the-art\nmulti-agent perception and prediction models. Our results highlight that\npretraining effectively captures spatiotemporal multi-agent features and\nsignificantly benefits downstream tasks. Moreover, the proposed balanced\nmulti-task learning strategy enhances detection and prediction.",
        "url": "http://arxiv.org/abs/2508.04682v1",
        "published_date": "2025-08-06T17:46:40+00:00",
        "updated_date": "2025-08-06T17:46:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zewei Zhou",
            "Seth Z. Zhao",
            "Tianhui Cai",
            "Zhiyu Huang",
            "Bolei Zhou",
            "Jiaqi Ma"
        ],
        "tldr": "TurboTrain is a new training framework for multi-agent perception and prediction that uses spatiotemporal pretraining and gradient conflict suppression for balanced multi-task learning, improving performance and reducing training time.",
        "tldr_zh": "TurboTrain是一个新的多智能体感知和预测训练框架，它使用时空预训练和梯度冲突抑制来实现平衡的多任务学习，从而提高性能并减少训练时间。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RoboTron-Sim: Improving Real-World Driving via Simulated Hard-Case",
        "summary": "Collecting real-world data for rare high-risk scenarios, long-tailed driving\nevents, and complex interactions remains challenging, leading to poor\nperformance of existing autonomous driving systems in these critical\nsituations. In this paper, we propose RoboTron-Sim that improves real-world\ndriving in critical situations by utilizing simulated hard cases. First, we\ndevelop a simulated dataset called Hard-case Augmented Synthetic Scenarios\n(HASS), which covers 13 high-risk edge-case categories, as well as balanced\nenvironmental conditions such as day/night and sunny/rainy. Second, we\nintroduce Scenario-aware Prompt Engineering (SPE) and an Image-to-Ego Encoder\n(I2E Encoder) to enable multimodal large language models to effectively learn\nreal-world challenging driving skills from HASS, via adapting to environmental\ndeviations and hardware differences between real-world and simulated scenarios.\nExtensive experiments on nuScenes show that RoboTron-Sim improves driving\nperformance in challenging scenarios by around 50%, achieving state-of-the-art\nresults in real-world open-loop planning. Qualitative results further\ndemonstrate the effectiveness of RoboTron-Sim in better managing rare high-risk\ndriving scenarios. Project page: https://stars79689.github.io/RoboTron-Sim/",
        "url": "http://arxiv.org/abs/2508.04642v1",
        "published_date": "2025-08-06T17:07:25+00:00",
        "updated_date": "2025-08-06T17:07:25+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Baihui Xiao",
            "Chengjian Feng",
            "Zhijian Huang",
            "Feng yan",
            "Yujie Zhong",
            "Lin Ma"
        ],
        "tldr": "RoboTron-Sim improves real-world autonomous driving in challenging situations by using a simulated dataset (HASS) and techniques like Scenario-aware Prompt Engineering and an Image-to-Ego Encoder to bridge the sim-to-real gap, achieving a 50% performance improvement on nuScenes.",
        "tldr_zh": "RoboTron-Sim通过使用模拟数据集(HASS)以及情景感知提示工程和图像到自我编码器等技术来弥合模拟到现实的差距，从而改善了现实世界中具有挑战性场景下的自动驾驶性能，在nuScenes上实现了50%的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline",
        "summary": "Incrementally recovering real-sized 3D geometry from a pose-free RGB stream\nis a challenging task in 3D reconstruction, requiring minimal assumptions on\ninput data. Existing methods can be broadly categorized into end-to-end and\nvisual SLAM-based approaches, both of which either struggle with long sequences\nor depend on slow test-time optimization and depth sensors. To address this, we\nfirst integrate a depth estimator into an RGB-D SLAM system, but this approach\nis hindered by inaccurate geometric details in predicted depth. Through further\ninvestigation, we find that 3D Gaussian mapping can effectively solve this\nproblem. Building on this, we propose an online 3D reconstruction method using\n3D Gaussian-based SLAM, combined with a feed-forward recurrent prediction\nmodule to directly infer camera pose from optical flow. This approach replaces\nslow test-time optimization with fast network inference, significantly\nimproving tracking speed. Additionally, we introduce a local graph rendering\ntechnique to enhance robustness in feed-forward pose prediction. Experimental\nresults on the Replica and TUM-RGBD datasets, along with a real-world\ndeployment demonstration, show that our method achieves performance on par with\nthe state-of-the-art SplaTAM, while reducing tracking time by more than 90\\%.",
        "url": "http://arxiv.org/abs/2508.04597v1",
        "published_date": "2025-08-06T16:16:58+00:00",
        "updated_date": "2025-08-06T16:16:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Linqing Zhao",
            "Xiuwei Xu",
            "Yirui Wang",
            "Hao Wang",
            "Wenzhao Zheng",
            "Yansong Tang",
            "Haibin Yan",
            "Jiwen Lu"
        ],
        "tldr": "This paper introduces a feed-forward RGB SLAM baseline using 3D Gaussian mapping and a recurrent prediction module for fast and robust pose estimation, achieving comparable performance to SplaTAM with significantly reduced tracking time.",
        "tldr_zh": "本文提出了一种前馈RGB SLAM基线，它使用3D高斯映射和一个循环预测模块来实现快速而鲁棒的姿态估计。该方法在性能上与SplaTAM相当，但显著降低了跟踪时间。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SplitGaussian: Reconstructing Dynamic Scenes via Visual Geometry Decomposition",
        "summary": "Reconstructing dynamic 3D scenes from monocular video remains fundamentally\nchallenging due to the need to jointly infer motion, structure, and appearance\nfrom limited observations. Existing dynamic scene reconstruction methods based\non Gaussian Splatting often entangle static and dynamic elements in a shared\nrepresentation, leading to motion leakage, geometric distortions, and temporal\nflickering. We identify that the root cause lies in the coupled modeling of\ngeometry and appearance across time, which hampers both stability and\ninterpretability. To address this, we propose \\textbf{SplitGaussian}, a novel\nframework that explicitly decomposes scene representations into static and\ndynamic components. By decoupling motion modeling from background geometry and\nallowing only the dynamic branch to deform over time, our method prevents\nmotion artifacts in static regions while supporting view- and time-dependent\nappearance refinement. This disentangled design not only enhances temporal\nconsistency and reconstruction fidelity but also accelerates convergence.\nExtensive experiments demonstrate that SplitGaussian outperforms prior\nstate-of-the-art methods in rendering quality, geometric stability, and motion\nseparation.",
        "url": "http://arxiv.org/abs/2508.04224v1",
        "published_date": "2025-08-06T09:00:13+00:00",
        "updated_date": "2025-08-06T09:00:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiahui Li",
            "Shengeng Tang",
            "Jingxuan He",
            "Gang Huang",
            "Zhangye Wang",
            "Yantao Pan",
            "Lechao Cheng"
        ],
        "tldr": "The paper introduces SplitGaussian, a novel method for dynamic scene reconstruction from monocular video that decomposes scene representations into static and dynamic components, improving rendering quality and geometric stability.",
        "tldr_zh": "该论文介绍了SplitGaussian，一种从单目视频重建动态场景的新方法，它将场景表示分解为静态和动态组件，从而提高渲染质量和几何稳定性。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "IDCNet: Guided Video Diffusion for Metric-Consistent RGBD Scene Generation with Precise Camera Control",
        "summary": "We present IDC-Net (Image-Depth Consistency Network), a novel framework\ndesigned to generate RGB-D video sequences under explicit camera trajectory\ncontrol. Unlike approaches that treat RGB and depth generation separately,\nIDC-Net jointly synthesizes both RGB images and corresponding depth maps within\na unified geometry-aware diffusion model. The joint learning framework\nstrengthens spatial and geometric alignment across frames, enabling more\nprecise camera control in the generated sequences. To support the training of\nthis camera-conditioned model and ensure high geometric fidelity, we construct\na camera-image-depth consistent dataset with metric-aligned RGB videos, depth\nmaps, and accurate camera poses, which provides precise geometric supervision\nwith notably improved inter-frame geometric consistency. Moreover, we introduce\na geometry-aware transformer block that enables fine-grained camera control,\nenhancing control over the generated sequences. Extensive experiments show that\nIDC-Net achieves improvements over state-of-the-art approaches in both visual\nquality and geometric consistency of generated scene sequences. Notably, the\ngenerated RGB-D sequences can be directly feed for downstream 3D Scene\nreconstruction tasks without extra post-processing steps, showcasing the\npractical benefits of our joint learning framework. See more at\nhttps://idcnet-scene.github.io.",
        "url": "http://arxiv.org/abs/2508.04147v1",
        "published_date": "2025-08-06T07:19:16+00:00",
        "updated_date": "2025-08-06T07:19:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lijuan Liu",
            "Wenfa Li",
            "Dongbo Zhang",
            "Shuo Wang",
            "Shaohui Jiao"
        ],
        "tldr": "IDC-Net is a novel diffusion model that jointly generates metric-consistent RGB-D video sequences with precise camera trajectory control, demonstrating improvements in visual quality, geometric consistency, and downstream 3D reconstruction tasks.",
        "tldr_zh": "IDC-Net是一种新型扩散模型，可以联合生成具有精确相机轨迹控制的、度量一致的RGB-D视频序列，在视觉质量、几何一致性和下游3D重建任务方面均有改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Intention Enhanced Diffusion Model for Multimodal Pedestrian Trajectory Prediction",
        "summary": "Predicting pedestrian motion trajectories is critical for path planning and\nmotion control of autonomous vehicles. However, accurately forecasting crowd\ntrajectories remains a challenging task due to the inherently multimodal and\nuncertain nature of human motion. Recent diffusion-based models have shown\npromising results in capturing the stochasticity of pedestrian behavior for\ntrajectory prediction. However, few diffusion-based approaches explicitly\nincorporate the underlying motion intentions of pedestrians, which can limit\nthe interpretability and precision of prediction models. In this work, we\npropose a diffusion-based multimodal trajectory prediction model that\nincorporates pedestrians' motion intentions into the prediction framework. The\nmotion intentions are decomposed into lateral and longitudinal components, and\na pedestrian intention recognition module is introduced to enable the model to\neffectively capture these intentions. Furthermore, we adopt an efficient\nguidance mechanism that facilitates the generation of interpretable\ntrajectories. The proposed framework is evaluated on two widely used human\ntrajectory prediction benchmarks, ETH and UCY, on which it is compared against\nstate-of-the-art methods. The experimental results demonstrate that our method\nachieves competitive performance.",
        "url": "http://arxiv.org/abs/2508.04229v1",
        "published_date": "2025-08-06T09:04:54+00:00",
        "updated_date": "2025-08-06T09:04:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yu Liu",
            "Zhijie Liu",
            "Xiao Ren",
            "You-Fu Li",
            "He Kong"
        ],
        "tldr": "This paper introduces a diffusion-based multimodal trajectory prediction model that incorporates pedestrian motion intentions, decomposed into lateral and longitudinal components, achieving competitive performance on ETH and UCY datasets.",
        "tldr_zh": "本文介绍了一种基于扩散的多模态轨迹预测模型，该模型结合了行人运动意图（分解为横向和纵向分量），并在 ETH 和 UCY 数据集上取得了有竞争力的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]