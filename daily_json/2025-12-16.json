[
    {
        "title": "MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion",
        "summary": "Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.",
        "url": "http://arxiv.org/abs/2512.13177v1",
        "published_date": "2025-12-15T10:37:59+00:00",
        "updated_date": "2025-12-15T10:37:59+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Minghui Hou",
            "Wei-Hsing Huang",
            "Shaofeng Liang",
            "Daizong Liu",
            "Tai-Hao Wen",
            "Gang Wang",
            "Runwei Guan",
            "Weiping Ding"
        ],
        "tldr": "The paper introduces MMDrive, a multimodal vision-language model that extends 2D image understanding to 3D scene understanding by incorporating occupancy maps, LiDAR point clouds, and textual descriptions, achieving improved performance on autonomous driving benchmarks.",
        "tldr_zh": "该论文介绍了MMDrive，一种多模态视觉语言模型，通过结合 occupancy maps、激光雷达点云和文本描述，将2D图像理解扩展到3D场景理解，并在自动驾驶基准测试上实现了性能提升。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Motus: A Unified Latent Action World Model",
        "summary": "While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level \"delta action\" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.",
        "url": "http://arxiv.org/abs/2512.13030v1",
        "published_date": "2025-12-15T06:58:40+00:00",
        "updated_date": "2025-12-15T06:58:40+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Hongzhe Bi",
            "Hengkai Tan",
            "Shenghao Xie",
            "Zeyuan Wang",
            "Shuhe Huang",
            "Haitian Liu",
            "Ruowen Zhao",
            "Yao Feng",
            "Chendong Xiang",
            "Yinze Rong",
            "Hongyan Zhao",
            "Hanyu Liu",
            "Zhizhong Su",
            "Lei Ma",
            "Hang Su",
            "Jun Zhu"
        ],
        "tldr": "Motus proposes a unified latent action world model for embodied agents using a Mixture-of-Transformer architecture and UniDiffuser-style scheduler, achieving SOTA performance in simulation and real-world robotic tasks.",
        "tldr_zh": "Motus提出了一种统一的潜在动作世界模型，用于具身智能体，它使用混合Transformer架构和UniDiffuser风格的调度器，在模拟和真实世界机器人任务中实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding",
        "summary": "Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.",
        "url": "http://arxiv.org/abs/2512.12822v1",
        "published_date": "2025-12-14T20:02:43+00:00",
        "updated_date": "2025-12-14T20:02:43+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yongyuan Liang",
            "Xiyao Wang",
            "Yuanchen Ju",
            "Jianwei Yang",
            "Furong Huang"
        ],
        "tldr": "Lemon introduces a unified transformer architecture for 3D multimodal understanding, achieving state-of-the-art performance across various tasks by jointly processing point clouds and language, addressing limitations of prior fragmented approaches.",
        "tldr_zh": "Lemon 提出了一种用于 3D 多模态理解的统一 Transformer 架构，通过联合处理点云和语言，在各种任务中实现了最先进的性能，解决了之前碎片化方法的局限性。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning",
        "summary": "Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI",
        "url": "http://arxiv.org/abs/2512.12799v1",
        "published_date": "2025-12-14T18:45:54+00:00",
        "updated_date": "2025-12-14T18:45:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhe Liu",
            "Runhui Huang",
            "Rui Yang",
            "Siming Yan",
            "Zining Wang",
            "Lu Hou",
            "Di Lin",
            "Xiang Bai",
            "Hengshuang Zhao"
        ],
        "tldr": "DrivePI introduces a spatial-aware 4D MLLM for unified autonomous driving tasks (understanding, perception, prediction, and planning), outperforming existing VLA and VA models with a relatively small 0.5B backbone.",
        "tldr_zh": "DrivePI 提出了一种空间感知 4D MLLM，用于统一的自动驾驶任务（理解、感知、预测和规划），以相对较小的 0.5B 主干网络超越了现有的 VLA 和 VA 模型。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Computer vision training dataset generation for robotic environments using Gaussian splatting",
        "summary": "This paper introduces a novel pipeline for generating large-scale, highly realistic, and automatically labeled datasets for computer vision tasks in robotic environments. Our approach addresses the critical challenges of the domain gap between synthetic and real-world imagery and the time-consuming bottleneck of manual annotation. We leverage 3D Gaussian Splatting (3DGS) to create photorealistic representations of the operational environment and objects. These assets are then used in a game engine where physics simulations create natural arrangements. A novel, two-pass rendering technique combines the realism of splats with a shadow map generated from proxy meshes. This map is then algorithmically composited with the image to add both physically plausible shadows and subtle highlights, significantly enhancing realism. Pixel-perfect segmentation masks are generated automatically and formatted for direct use with object detection models like YOLO. Our experiments show that a hybrid training strategy, combining a small set of real images with a large volume of our synthetic data, yields the best detection and segmentation performance, confirming this as an optimal strategy for efficiently achieving robust and accurate models.",
        "url": "http://arxiv.org/abs/2512.13411v1",
        "published_date": "2025-12-15T15:00:17+00:00",
        "updated_date": "2025-12-15T15:00:17+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Patryk Niżeniec",
            "Marcin Iwanowski"
        ],
        "tldr": "This paper presents a pipeline using Gaussian Splatting and game engines to generate photorealistic, automatically labeled datasets for training computer vision models in robotic environments, showing improved performance with hybrid real/synthetic data training.",
        "tldr_zh": "本文提出了一种利用高斯溅射和游戏引擎生成逼真且自动标注的数据集的流程，用于训练机器人环境中的计算机视觉模型，并表明混合真实/合成数据训练可提高性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving",
        "summary": "Learning interactive motion behaviors among multiple agents is a core challenge in autonomous driving. While imitation learning models generate realistic trajectories, they often inherit biases from datasets dominated by safe demonstrations, limiting robustness in safety-critical cases. Moreover, most studies rely on open-loop evaluation, overlooking compounding errors in closed-loop execution. We address these limitations with two complementary strategies. First, we propose Group Relative Behavior Optimization (GRBO), a reinforcement learning post-training method that fine-tunes pretrained behavior models via group relative advantage maximization with human regularization. Using only 10% of the training dataset, GRBO improves safety performance by over 40% while preserving behavioral realism. Second, we introduce Warm-K, a warm-started Top-K sampling strategy that balances consistency and diversity in motion selection. Our Warm-K method-based test-time scaling enhances behavioral consistency and reactivity at test time without retraining, mitigating covariate shift and reducing performance discrepancies. Demo videos are available in the supplementary material.",
        "url": "http://arxiv.org/abs/2512.13262v1",
        "published_date": "2025-12-15T12:18:50+00:00",
        "updated_date": "2025-12-15T12:18:50+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Hyunki Seong",
            "Jeong-Kyun Lee",
            "Heesoo Myeong",
            "Yongho Shin",
            "Hyun-Mook Cho",
            "Duck Hoon Kim",
            "Pranav Desai",
            "Monu Surana"
        ],
        "tldr": "This paper introduces GRBO, a reinforcement learning post-training method for improving safety in autonomous driving, and Warm-K, a test-time sampling strategy for enhancing behavioral consistency and reactivity, demonstrating improved safety and performance with limited data.",
        "tldr_zh": "本文介绍了GRBO，一种用于提高自动驾驶安全性的强化学习后训练方法，以及Warm-K，一种用于增强行为一致性和反应性的测试时采样策略，证明了在有限数据下改进的安全性和性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection",
        "summary": "Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.",
        "url": "http://arxiv.org/abs/2512.13250v1",
        "published_date": "2025-12-15T12:04:26+00:00",
        "updated_date": "2025-12-15T12:04:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Juil Koo",
            "Daehyeon Choi",
            "Sangwoo Youn",
            "Phillip Y. Lee",
            "Minhyuk Sung"
        ],
        "tldr": "The paper introduces Visually Grounded Active View Selection (VG-AVS), a method for selecting the most informative viewpoint for visual question answering using only visual information, and demonstrates its effectiveness on synthetic and real scenes.",
        "tldr_zh": "该论文介绍了视觉引导的主动视角选择(VG-AVS)，这是一种仅使用视觉信息选择视觉问题解答的最佳视角的方法，并在合成和真实场景中证明了其有效性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather",
        "summary": "Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.",
        "url": "http://arxiv.org/abs/2512.13107v1",
        "published_date": "2025-12-15T09:03:46+00:00",
        "updated_date": "2025-12-15T09:03:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zhijian He",
            "Feifei Liu",
            "Yuwei Li",
            "Zhanpeng Liu",
            "Jintao Cheng",
            "Xieyuanli Chen",
            "Xiaoyu Tang"
        ],
        "tldr": "This paper introduces DiffFusion, a diffusion-based framework for robust multi-modal 3D object detection in adverse weather, using diffusion models for denoising and a bidirectional adaptive fusion module for modality alignment.",
        "tldr_zh": "该论文介绍了DiffFusion，一个基于扩散的框架，用于在恶劣天气下实现鲁棒的多模态3D目标检测，利用扩散模型进行去噪，并使用双向自适应融合模块进行模态对齐。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training",
        "summary": "Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a \"free\" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the \"entropy collapse\" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.",
        "url": "http://arxiv.org/abs/2512.13043v1",
        "published_date": "2025-12-15T07:11:56+00:00",
        "updated_date": "2025-12-15T07:11:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Tong Wei",
            "Yijun Yang",
            "Changhao Zhang",
            "Junliang Xing",
            "Yuanchun Shi",
            "Zongqing Lu",
            "Deheng Ye"
        ],
        "tldr": "GTR-Turbo improves multi-turn reinforcement learning for VLMs by using a merged checkpoint of the training model as a free teacher, achieving better performance and efficiency compared to previous methods relying on expensive, privileged teacher models.",
        "tldr_zh": "GTR-Turbo通过使用训练模型的合并检查点作为免费教师，改进了视觉语言模型的多轮强化学习，与之前依赖昂贵、特权教师模型的方法相比，实现了更好的性能和效率。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Light Field Based 6DoF Tracking of Previously Unobserved Objects",
        "summary": "Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.",
        "url": "http://arxiv.org/abs/2512.13007v1",
        "published_date": "2025-12-15T06:04:49+00:00",
        "updated_date": "2025-12-15T06:04:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nikolai Goncharov",
            "James L. Gray",
            "Donald G. Dansereau"
        ],
        "tldr": "The paper introduces a novel, model-free 6DoF object tracking method based on light fields and vision foundation models, demonstrating robustness to complex visual behavior like reflections and providing a new dataset for evaluation.",
        "tldr_zh": "该论文提出了一种基于光场和视觉基础模型的全新、无模型的6自由度物体跟踪方法，展示了对反射等复杂视觉行为的鲁棒性，并提供了一个新的评估数据集。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning",
        "summary": "This paper proposes two new algorithms for the lane keeping system (LKS) in autonomous vehicles (AVs) operating under snowy road conditions. These algorithms use deep reinforcement learning (DRL) to handle uncertainties and slippage. They include Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) and end-to-end Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG), two action-robust approaches for decision-making. In the AR-RDPG method, within the perception layer, camera images are first denoised using multi-scale neural networks. Then, the centerline coefficients are extracted by a pre-trained deep convolutional neural network (DCNN). These coefficients, concatenated with the driving characteristics, are used as input to the control layer. The AR-CADPG method presents an end-to-end approach in which a convolutional neural network (CNN) and an attention mechanism are integrated within a DRL framework. Both methods are first trained in the CARLA simulator and validated under various snowy scenarios. Real-world experiments on a Jetson Nano-based autonomous vehicle confirm the feasibility and stability of the learned policies. Among the two models, the AR-CADPG approach demonstrates superior path-tracking accuracy and robustness, highlighting the effectiveness of combining temporal memory, adversarial resilience, and attention mechanisms in AVs.",
        "url": "http://arxiv.org/abs/2512.12987v1",
        "published_date": "2025-12-15T05:23:23+00:00",
        "updated_date": "2025-12-15T05:23:23+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Amin Jalal Aghdasian",
            "Farzaneh Abdollahi",
            "Ali Kamali Iglie"
        ],
        "tldr": "This paper introduces two action-robust deep reinforcement learning algorithms, AR-RDPG and AR-CADPG, for lane keeping in autonomous vehicles under snowy conditions, demonstrating their feasibility and stability in both simulation and real-world experiments.",
        "tldr_zh": "本文介绍了两种用于雪地环境下自动驾驶车辆车道保持的鲁棒性深度强化学习算法，AR-RDPG 和 AR-CADPG，并在仿真和真实实验中展示了它们的可行性和稳定性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SLIM-VDB: A Real-Time 3D Probabilistic Semantic Mapping Framework",
        "summary": "This paper introduces SLIM-VDB, a new lightweight semantic mapping system with probabilistic semantic fusion for closed-set or open-set dictionaries. Advances in data structures from the computer graphics community, such as OpenVDB, have demonstrated significantly improved computational and memory efficiency in volumetric scene representation. Although OpenVDB has been used for geometric mapping in robotics applications, semantic mapping for scene understanding with OpenVDB remains unexplored. In addition, existing semantic mapping systems lack support for integrating both fixed-category and open-language label predictions within a single framework. In this paper, we propose a novel 3D semantic mapping system that leverages the OpenVDB data structure and integrates a unified Bayesian update framework for both closed- and open-set semantic fusion. Our proposed framework, SLIM-VDB, achieves significant reduction in both memory and integration times compared to current state-of-the-art semantic mapping approaches, while maintaining comparable mapping accuracy. An open-source C++ codebase with a Python interface is available at https://github.com/umfieldrobotics/slim-vdb.",
        "url": "http://arxiv.org/abs/2512.12945v1",
        "published_date": "2025-12-15T03:16:04+00:00",
        "updated_date": "2025-12-15T03:16:04+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Anja Sheppard",
            "Parker Ewen",
            "Joey Wilson",
            "Advaith V. Sethuraman",
            "Benard Adewole",
            "Anran Li",
            "Yuzhen Chen",
            "Ram Vasudevan",
            "Katherine A. Skinner"
        ],
        "tldr": "SLIM-VDB is a new semantic mapping system using OpenVDB for efficient 3D scene representation, integrating a Bayesian framework for both closed- and open-set semantic fusion, achieving memory and speed improvements.",
        "tldr_zh": "SLIM-VDB是一个新的语义地图构建系统，它使用OpenVDB来实现高效的3D场景表示，并集成了一个贝叶斯框架，用于封闭和开放集合语义融合，实现了内存和速度的提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition",
        "summary": "Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.",
        "url": "http://arxiv.org/abs/2512.12885v1",
        "published_date": "2025-12-14T23:56:34+00:00",
        "updated_date": "2025-12-14T23:56:34+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.IR",
            "cs.RO"
        ],
        "authors": [
            "Minghao Zhu",
            "Zhihao Zhang",
            "Anmol Sidhu",
            "Keith Redmill"
        ],
        "tldr": "The paper introduces SignRAG, a zero-shot road sign recognition system using a Vision Language Model (VLM) and Large Language Model (LLM) based Retrieval-Augmented Generation (RAG) approach, achieving promising accuracy on a dataset of regulatory signs.",
        "tldr_zh": "该论文介绍了SignRAG，一个零样本道路标志识别系统，使用基于视觉语言模型（VLM）和大型语言模型（LLM）的检索增强生成（RAG）方法，在法规标志数据集上取得了良好的精度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection",
        "summary": "In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.",
        "url": "http://arxiv.org/abs/2512.12884v1",
        "published_date": "2025-12-14T23:56:16+00:00",
        "updated_date": "2025-12-14T23:56:16+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Xiangzhong Liu",
            "Jiajie Zhang",
            "Hao Shen"
        ],
        "tldr": "This paper proposes a novel cross-level sensor fusion method using a Transformer to integrate object lists (from smart sensors/V2X) with raw camera images for 3D object detection, demonstrating performance improvements on nuScenes.",
        "tldr_zh": "本文提出了一种新颖的跨层传感器融合方法，该方法使用Transformer将目标列表（来自智能传感器/V2X）与原始相机图像集成，用于3D目标检测，并在nuScenes数据集上展示了性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GradID: Adversarial Detection via Intrinsic Dimensionality of Gradients",
        "summary": "Despite their remarkable performance, deep neural networks exhibit a critical vulnerability: small, often imperceptible, adversarial perturbations can lead to drastically altered model predictions. Given the stringent reliability demands of applications such as medical diagnosis and autonomous driving, robust detection of such adversarial attacks is paramount. In this paper, we investigate the geometric properties of a model's input loss landscape. We analyze the Intrinsic Dimensionality (ID) of the model's gradient parameters, which quantifies the minimal number of coordinates required to describe the data points on their underlying manifold. We reveal a distinct and consistent difference in the ID for natural and adversarial data, which forms the basis of our proposed detection method. We validate our approach across two distinct operational scenarios. First, in a batch-wise context for identifying malicious data groups, our method demonstrates high efficacy on datasets like MNIST and SVHN. Second, in the critical individual-sample setting, we establish new state-of-the-art results on challenging benchmarks such as CIFAR-10 and MS COCO. Our detector significantly surpasses existing methods against a wide array of attacks, including CW and AutoAttack, achieving detection rates consistently above 92\\% on CIFAR-10. The results underscore the robustness of our geometric approach, highlighting that intrinsic dimensionality is a powerful fingerprint for adversarial detection across diverse datasets and attack strategies.",
        "url": "http://arxiv.org/abs/2512.12827v1",
        "published_date": "2025-12-14T20:16:03+00:00",
        "updated_date": "2025-12-14T20:16:03+00:00",
        "categories": [
            "cs.LG",
            "cs.CR",
            "cs.CV"
        ],
        "authors": [
            "Mohammad Mahdi Razmjoo",
            "Mohammad Mahdi Sharifian",
            "Saeed Bagheri Shouraki"
        ],
        "tldr": "The paper introduces GradID, a novel adversarial attack detection method based on the intrinsic dimensionality of a model's gradient parameters, achieving state-of-the-art detection rates across various datasets and attacks.",
        "tldr_zh": "该论文介绍了一种名为GradID的新型对抗攻击检测方法，该方法基于模型梯度参数的内在维度，在各种数据集和攻击中实现了最先进的检测率。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]