[
    {
        "title": "Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution",
        "summary": "We introduce Orion, a visual agent framework that can take in any modality and generate any modality. Using an agentic framework with multiple tool-calling capabilities, Orion is designed for visual AI tasks and achieves state-of-the-art results. Unlike traditional vision-language models that produce descriptive outputs, Orion orchestrates a suite of specialized computer vision tools, including object detection, keypoint localization, panoptic segmentation, Optical Character Recognition, and geometric analysis, to execute complex multi-step visual workflows. The system achieves competitive performance on MMMU, MMBench, DocVQA, and MMLongBench while extending monolithic vision-language models to production-grade visual intelligence. By combining neural perception with symbolic execution, Orion enables autonomous visual reasoning, marking a transition from passive visual understanding to active, tool-driven visual intelligence.",
        "url": "http://arxiv.org/abs/2511.14210v1",
        "published_date": "2025-11-18T07:41:02+00:00",
        "updated_date": "2025-11-18T07:41:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "N Dinesh Reddy",
            "Sudeep Pillai"
        ],
        "tldr": "Orion is a new visual agent framework using tool-calling capabilities for multimodal perception and execution, achieving state-of-the-art results on visual AI tasks by combining neural perception and symbolic execution.",
        "tldr_zh": "Orion是一个新的视觉代理框架，利用工具调用能力进行多模态感知和执行，通过结合神经感知和符号执行，在视觉AI任务上实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "PAVE: An End-to-End Dataset for Production Autonomous Vehicle Evaluation",
        "summary": "Most existing autonomous-driving datasets (e.g., KITTI, nuScenes, and the Waymo Perception Dataset), collected by human-driving mode or unidentified driving mode, can only serve as early training for the perception and prediction of autonomous vehicles (AVs). To evaluate the real behavioral safety of AVs controlled in the black box, we present the first end-to-end benchmark dataset collected entirely by autonomous-driving mode in the real world. This dataset contains over 100 hours of naturalistic data from multiple production autonomous-driving vehicle models in the market. We segment the original data into 32,727 key frames, each consisting of four synchronized camera images and high-precision GNSS/IMU data (0.8 cm localization accuracy). For each key frame, 20 Hz vehicle trajectories spanning the past 6 s and future 5 s are provided, along with detailed 2D annotations of surrounding vehicles, pedestrians, traffic lights, and traffic signs. These key frames have rich scenario-level attributes, including driver intent, area type (covering highways, urban roads, and residential areas), lighting (day, night, or dusk), weather (clear or rain), road surface (paved or unpaved), traffic and vulnerable road users (VRU) density, traffic lights, and traffic signs (warning, prohibition, and indication). To evaluate the safety of AVs, we employ an end-to-end motion planning model that predicts vehicle trajectories with an Average Displacement Error (ADE) of 1.4 m on autonomous-driving frames. The dataset continues to expand by over 10 hours of new data weekly, thereby providing a sustainable foundation for research on AV driving behavior analysis and safety evaluation.",
        "url": "http://arxiv.org/abs/2511.14185v1",
        "published_date": "2025-11-18T06:41:34+00:00",
        "updated_date": "2025-11-18T06:41:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiangyu Li",
            "Chen Wang",
            "Yumao Liu",
            "Dengbo He",
            "Jiahao Zhang",
            "Ke Ma"
        ],
        "tldr": "The paper introduces PAVE, a new large-scale dataset collected entirely from production autonomous vehicles, designed for end-to-end AV safety evaluation, featuring detailed annotations and scenario attributes.",
        "tldr_zh": "该论文介绍了PAVE，一个完全由量产自动驾驶车辆收集的大规模数据集，专为端到端自动驾驶安全评估而设计，具有详细的注释和场景属性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action",
        "summary": "Household tidying is an important application area, yet current benchmarks neither model user preferences nor support mobility, and they generalize poorly, making it hard to comprehensively assess integrated language-to-action capabilities. To address this, we propose RoboTidy, a unified benchmark for language-guided household tidying that supports Vision-Language-Action (VLA) and Vision-Language-Navigation (VLN) training and evaluation. RoboTidy provides 500 photorealistic 3D Gaussian Splatting (3DGS) household scenes (covering 500 objects and containers) with collisions, formulates tidying as an \"Action (Object, Container)\" list, and supplies 6.4k high-quality manipulation demonstration trajectories and 1.5k naviagtion trajectories to support both few-shot and large-scale training. We also deploy RoboTidy in the real world for object tidying, establishing an end-to-end benchmark for household tidying. RoboTidy offers a scalable platform and bridges a key gap in embodied AI by enabling holistic and realistic evaluation of language-guided robots.",
        "url": "http://arxiv.org/abs/2511.14161v1",
        "published_date": "2025-11-18T05:54:05+00:00",
        "updated_date": "2025-11-18T05:54:05+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Xiaoquan Sun",
            "Ruijian Zhang",
            "Kang Pang",
            "Bingchen Miao",
            "Yuxiang Tan",
            "Zhen Yang",
            "Ming Li",
            "Jiayu Chen"
        ],
        "tldr": "The paper introduces RoboTidy, a new benchmark using 3D Gaussian Splatting for language-guided household tidying, featuring realistic scenes, manipulation and navigation trajectories, and real-world validation.",
        "tldr_zh": "该论文介绍了RoboTidy，一个新的基准测试，使用3D高斯溅射进行语言引导的家庭整理，具有逼真的场景、操作和导航轨迹，以及真实世界的验证。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Enhancing End-to-End Autonomous Driving with Risk Semantic Distillaion from VLM",
        "summary": "The autonomous driving (AD) system has exhibited remarkable performance in complex driving scenarios. However, generalization is still a key limitation for the current system, which refers to the ability to handle unseen scenarios or unfamiliar sensor configurations.Related works have explored the use of Vision-Language Models (VLMs) to address few-shot or zero-shot tasks. While promising, these methods introduce a new challenge: the emergence of a hybrid AD system, where two distinct systems are used to plan a trajectory, leading to potential inconsistencies. Alternative research directions have explored Vision-Language-Action (VLA) frameworks that generate control actions from VLM directly. However, these end-to-end solutions demonstrate prohibitive computational demands. To overcome these challenges, we introduce Risk Semantic Distillation (RSD), a novel framework that leverages VLMs to enhance the training of End-to-End (E2E) AD backbones. By providing risk attention for key objects, RSD addresses the issue of generalization. Specifically, we introduce RiskHead, a plug-in module that distills causal risk estimates from Vision-Language Models into Bird's-Eye-View (BEV) features, yielding interpretable risk-attention maps.This approach allows BEV features to learn richer and more nuanced risk attention representations, which directly enhance the model's ability to handle spatial boundaries and risky objects.By focusing on risk attention, RSD aligns better with human-like driving behavior, which is essential to navigate in complex and dynamic environments. Our experiments on the Bench2Drive benchmark demonstrate the effectiveness of RSD in managing complex and unpredictable driving conditions. Due to the enhanced BEV representations enabled by RSD, we observed a significant improvement in both perception and planning capabilities.",
        "url": "http://arxiv.org/abs/2511.14499v1",
        "published_date": "2025-11-18T13:46:18+00:00",
        "updated_date": "2025-11-18T13:46:18+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Jack Qin",
            "Zhitao Wang",
            "Yinan Zheng",
            "Keyu Chen",
            "Yang Zhou",
            "Yuanxin Zhong",
            "Siyuan Cheng"
        ],
        "tldr": "This paper introduces Risk Semantic Distillation (RSD), a novel framework leveraging VLMs to enhance end-to-end autonomous driving by distilling risk attention into BEV features, improving generalization and handling of risky objects.",
        "tldr_zh": "该论文介绍了一种名为风险语义蒸馏 (RSD) 的新框架，该框架利用 VLM 通过将风险注意力提炼到 BEV 特征中来增强端到端自动驾驶，从而提高泛化能力并处理危险对象。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning",
        "summary": "Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.",
        "url": "http://arxiv.org/abs/2511.14396v1",
        "published_date": "2025-11-18T12:01:06+00:00",
        "updated_date": "2025-11-18T12:01:06+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Xiuxiu Qi",
            "Yu Yang",
            "Jiannong Cao",
            "Luyao Bai",
            "Chongshan Fan",
            "Chengtai Cao",
            "Hongpeng Wang"
        ],
        "tldr": "This paper introduces CCoL, a novel behavioral cloning framework that addresses compounding errors and semantic-physical misalignment in vision-language-action tasks by using continuous co-learning and bidirectional cross-attention, demonstrating improved performance in simulation and real-world robotic manipulation.",
        "tldr_zh": "本文介绍了一种新的行为克隆框架CCoL，通过连续协同学习和双向交叉注意力解决了视觉-语言-动作任务中的复合误差和语义-物理不对齐问题，并在模拟和现实世界机器人操作中展示了改进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing LLM-based Autonomous Driving with Modular Traffic Light and Sign Recognition",
        "summary": "Large Language Models (LLMs) are increasingly used for decision-making and planning in autonomous driving, showing promising reasoning capabilities and potential to generalize across diverse traffic situations. However, current LLM-based driving agents lack explicit mechanisms to enforce traffic rules and often struggle to reliably detect small, safety-critical objects such as traffic lights and signs. To address this limitation, we introduce TLS-Assist, a modular redundancy layer that augments LLM-based autonomous driving agents with explicit traffic light and sign recognition. TLS-Assist converts detections into structured natural language messages that are injected into the LLM input, enforcing explicit attention to safety-critical cues. The framework is plug-and-play, model-agnostic, and supports both single-view and multi-view camera setups. We evaluate TLS-Assist in a closed-loop setup on the LangAuto benchmark in CARLA. The results demonstrate relative driving performance improvements of up to 14% over LMDrive and 7% over BEVDriver, while consistently reducing traffic light and sign infractions. We publicly release the code and models on https://github.com/iis-esslingen/TLS-Assist.",
        "url": "http://arxiv.org/abs/2511.14391v1",
        "published_date": "2025-11-18T11:52:52+00:00",
        "updated_date": "2025-11-18T11:52:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fabian Schmidt",
            "Noushiq Mohammed Kayilan Abdul Nazar",
            "Markus Enzweiler",
            "Abhinav Valada"
        ],
        "tldr": "The paper introduces TLS-Assist, a modular plug-and-play component that enhances LLM-based autonomous driving by explicitly incorporating traffic light and sign recognition, improving performance and safety in CARLA simulations.",
        "tldr_zh": "该论文介绍了TLS-Assist，一个模块化的即插即用组件，通过显式地整合交通灯和标志识别来增强基于LLM的自动驾驶，从而提高CARLA模拟中的性能和安全性。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Cheating Stereo Matching in Full-scale: Physical Adversarial Attack against Binocular Depth Estimation in Autonomous Driving",
        "summary": "Though deep neural models adopted to realize the perception of autonomous driving have proven vulnerable to adversarial examples, known attacks often leverage 2D patches and target mostly monocular perception. Therefore, the effectiveness of Physical Adversarial Examples (PAEs) on stereo-based binocular depth estimation remains largely unexplored. To this end, we propose the first texture-enabled physical adversarial attack against stereo matching models in the context of autonomous driving. Our method employs a 3D PAE with global camouflage texture rather than a local 2D patch-based one, ensuring both visual consistency and attack effectiveness across different viewpoints of stereo cameras. To cope with the disparity effect of these cameras, we also propose a new 3D stereo matching rendering module that allows the PAE to be aligned with real-world positions and headings in binocular vision. We further propose a novel merging attack that seamlessly blends the target into the environment through fine-grained PAE optimization. It has significantly enhanced stealth and lethality upon existing hiding attacks that fail to get seamlessly merged into the background. Extensive evaluations show that our PAEs can successfully fool the stereo models into producing erroneous depth information.",
        "url": "http://arxiv.org/abs/2511.14386v1",
        "published_date": "2025-11-18T11:45:46+00:00",
        "updated_date": "2025-11-18T11:45:46+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kangqiao Zhao",
            "Shuo Huai",
            "Xurui Song",
            "Jun Luo"
        ],
        "tldr": "This paper presents a novel physical adversarial attack (PAE) against stereo matching models used in autonomous driving, employing 3D textured objects that are seamlessly blended into the environment to fool depth estimation.",
        "tldr_zh": "本文提出了一种新的物理对抗攻击 (PAE)，针对自动驾驶中使用的立体匹配模型，采用无缝融合到环境中的 3D 纹理对象来欺骗深度估计。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "V2VLoc: Robust GNSS-Free Collaborative Perception via LiDAR Localization",
        "summary": "Multi-agents rely on accurate poses to share and align observations, enabling a collaborative perception of the environment. However, traditional GNSS-based localization often fails in GNSS-denied environments, making consistent feature alignment difficult in collaboration. To tackle this challenge, we propose a robust GNSS-free collaborative perception framework based on LiDAR localization. Specifically, we propose a lightweight Pose Generator with Confidence (PGC) to estimate compact pose and confidence representations. To alleviate the effects of localization errors, we further develop the Pose-Aware Spatio-Temporal Alignment Transformer (PASTAT), which performs confidence-aware spatial alignment while capturing essential temporal context. Additionally, we present a new simulation dataset, V2VLoc, which can be adapted for both LiDAR localization and collaborative detection tasks. V2VLoc comprises three subsets: Town1Loc, Town4Loc, and V2VDet. Town1Loc and Town4Loc offer multi-traversal sequences for training in localization tasks, whereas V2VDet is specifically intended for the collaborative detection task. Extensive experiments conducted on the V2VLoc dataset demonstrate that our approach achieves state-of-the-art performance under GNSS-denied conditions. We further conduct extended experiments on the real-world V2V4Real dataset to validate the effectiveness and generalizability of PASTAT.",
        "url": "http://arxiv.org/abs/2511.14247v1",
        "published_date": "2025-11-18T08:34:41+00:00",
        "updated_date": "2025-11-18T08:34:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenkai Lin",
            "Qiming Xia",
            "Wen Li",
            "Xun Huang",
            "Chenglu Wen"
        ],
        "tldr": "This paper introduces a GNSS-free collaborative perception framework, V2VLoc, using LiDAR localization with a novel pose generator and alignment transformer, achieving state-of-the-art performance in GNSS-denied environments and validated on both simulated and real-world datasets.",
        "tldr_zh": "该论文介绍了一个基于激光雷达定位的GNSS-free协同感知框架V2VLoc，该框架采用了一种新的姿态生成器和对齐Transformer，在GNSS受限环境中实现了最先进的性能，并在模拟和真实世界数据集中进行了验证。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "iGaussian: Real-Time Camera Pose Estimation via Feed-Forward 3D Gaussian Splatting Inversion",
        "summary": "Recent trends in SLAM and visual navigation have embraced 3D Gaussians as the preferred scene representation, highlighting the importance of estimating camera poses from a single image using a pre-built Gaussian model. However, existing approaches typically rely on an iterative \\textit{render-compare-refine} loop, where candidate views are first rendered using NeRF or Gaussian Splatting, then compared against the target image, and finally, discrepancies are used to update the pose. This multi-round process incurs significant computational overhead, hindering real-time performance in robotics. In this paper, we propose iGaussian, a two-stage feed-forward framework that achieves real-time camera pose estimation through direct 3D Gaussian inversion. Our method first regresses a coarse 6DoF pose using a Gaussian Scene Prior-based Pose Regression Network with spatial uniform sampling and guided attention mechanisms, then refines it through feature matching and multi-model fusion. The key contribution lies in our cross-correlation module that aligns image embeddings with 3D Gaussian attributes without differentiable rendering, coupled with a Weighted Multiview Predictor that fuses features from Multiple strategically sampled viewpoints. Experimental results on the NeRF Synthetic, Mip-NeRF 360, and T\\&T+DB datasets demonstrate a significant performance improvement over previous methods, reducing median rotation errors to 0.2° while achieving 2.87 FPS tracking on mobile robots, which is an impressive 10 times speedup compared to optimization-based approaches. Code: https://github.com/pythongod-exe/iGaussian",
        "url": "http://arxiv.org/abs/2511.14149v1",
        "published_date": "2025-11-18T05:22:22+00:00",
        "updated_date": "2025-11-18T05:22:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hao Wang",
            "Linqing Zhao",
            "Xiuwei Xu",
            "Jiwen Lu",
            "Haibin Yan"
        ],
        "tldr": "The paper presents iGaussian, a feed-forward framework for real-time camera pose estimation from a single image using 3D Gaussian Splatting, achieving a significant speedup compared to iterative optimization-based methods.",
        "tldr_zh": "该论文提出了iGaussian，一个基于3D高斯溅射的实时相机位姿估计前馈框架，与迭代优化方法相比，实现了显著的速度提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RTS-Mono: A Real-Time Self-Supervised Monocular Depth Estimation Method for Real-World Deployment",
        "summary": "Depth information is crucial for autonomous driving and intelligent robot navigation. The simplicity and flexibility of self-supervised monocular depth estimation are conducive to its role in these fields. However, most existing monocular depth estimation models consume many computing resources. Although some methods have reduced the model's size and improved computing efficiency, the performance deteriorates, seriously hindering the real-world deployment of self-supervised monocular depth estimation models in the real world. To address this problem, we proposed a real-time self-supervised monocular depth estimation method and implemented it in the real world. It is called RTS-Mono, which is a lightweight and efficient encoder-decoder architecture. The encoder is based on Lite-Encoder, and the decoder is designed with a multi-scale sparse fusion framework to minimize redundancy, ensure performance, and improve inference speed. RTS-Mono achieved state-of-the-art (SoTA) performance in high and low resolutions with extremely low parameter counts (3 M) in experiments based on the KITTI dataset. Compared with lightweight methods, RTS-Mono improved Abs Rel and Sq Rel by 5.6% and 9.8% at low resolution and improved Sq Rel and RMSE by 6.1% and 1.9% at high resolution. In real-world deployment experiments, RTS-Mono has extremely high accuracy and can perform real-time inference on Nvidia Jetson Orin at a speed of 49 FPS. Source code is available at https://github.com/ZYCheng777/RTS-Mono.",
        "url": "http://arxiv.org/abs/2511.14107v1",
        "published_date": "2025-11-18T03:47:04+00:00",
        "updated_date": "2025-11-18T03:47:04+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeyu Cheng",
            "Tongfei Liu",
            "Tao Lei",
            "Xiang Hua",
            "Yi Zhang",
            "Chengkai Tang"
        ],
        "tldr": "The paper introduces RTS-Mono, a real-time, lightweight self-supervised monocular depth estimation method achieving state-of-the-art performance with low parameter counts and real-world deployment on Nvidia Jetson Orin.",
        "tldr_zh": "该论文介绍了一种名为RTS-Mono的实时、轻量级的自监督单目深度估计方法，该方法以低参数量实现了最先进的性能，并在Nvidia Jetson Orin上进行了实际部署。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Error-Driven Scene Editing for 3D Grounding in Large Language Models",
        "summary": "Despite recent progress in 3D-LLMs, they remain limited in accurately grounding language to visual and spatial elements in 3D environments. This limitation stems in part from training data that focuses on language reasoning rather than spatial understanding due to scarce 3D resources, leaving inherent grounding biases unresolved. To address this, we propose 3D scene editing as a key mechanism to generate precise visual counterfactuals that mitigate these biases through fine-grained spatial manipulation, without requiring costly scene reconstruction or large-scale 3D data collection. Furthermore, to make these edits targeted and directly address the specific weaknesses of the model, we introduce DEER-3D, an error-driven framework following a structured \"Decompose, Diagnostic Evaluation, Edit, and Re-train\" workflow, rather than broadly or randomly augmenting data as in conventional approaches. Specifically, upon identifying a grounding failure of the 3D-LLM, our framework first diagnoses the exact predicate-level error (e.g., attribute or spatial relation). It then executes minimal, predicate-aligned 3D scene edits, such as recoloring or repositioning, to produce targeted counterfactual supervision for iterative model fine-tuning, significantly enhancing grounding accuracy. We evaluate our editing pipeline across multiple benchmarks for 3D grounding and scene understanding tasks, consistently demonstrating improvements across all evaluated datasets through iterative refinement. DEER-3D underscores the effectiveness of targeted, error-driven scene editing in bridging linguistic reasoning capabilities with spatial grounding in 3D LLMs.",
        "url": "http://arxiv.org/abs/2511.14086v1",
        "published_date": "2025-11-18T03:13:29+00:00",
        "updated_date": "2025-11-18T03:13:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Yue Zhang",
            "Zun Wang",
            "Han Lin",
            "Jialu Li",
            "Jianing Yang",
            "Yonatan Bitton",
            "Idan Szpektor",
            "Mohit Bansal"
        ],
        "tldr": "The paper introduces DEER-3D, an error-driven framework that uses targeted 3D scene editing to improve the spatial grounding capabilities of 3D Large Language Models by generating visual counterfactuals and iteratively fine-tuning the model based on identified predicate-level errors.",
        "tldr_zh": "该论文提出了DEER-3D，一个误差驱动的框架，通过有针对性的3D场景编辑，生成视觉反事实，并基于识别出的谓词级误差迭代地微调模型，从而提高3D大语言模型的空间定位能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VLMs Guided Interpretable Decision Making for Autonomous Driving",
        "summary": "Recent advancements in autonomous driving (AD) have explored the use of vision-language models (VLMs) within visual question answering (VQA) frameworks for direct driving decision-making. However, these approaches often depend on handcrafted prompts and suffer from inconsistent performance, limiting their robustness and generalization in real-world scenarios. In this work, we evaluate state-of-the-art open-source VLMs on high-level decision-making tasks using ego-view visual inputs and identify critical limitations in their ability to deliver reliable, context-aware decisions. Motivated by these observations, we propose a new approach that shifts the role of VLMs from direct decision generators to semantic enhancers. Specifically, we leverage their strong general scene understanding to enrich existing vision-based benchmarks with structured, linguistically rich scene descriptions. Building on this enriched representation, we introduce a multi-modal interactive architecture that fuses visual and linguistic features for more accurate decision-making and interpretable textual explanations. Furthermore, we design a post-hoc refinement module that utilizes VLMs to enhance prediction reliability. Extensive experiments on two autonomous driving benchmarks demonstrate that our approach achieves state-of-the-art performance, offering a promising direction for integrating VLMs into reliable and interpretable AD systems.",
        "url": "http://arxiv.org/abs/2511.13881v1",
        "published_date": "2025-11-17T19:57:51+00:00",
        "updated_date": "2025-11-17T19:57:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xin Hu",
            "Taotao Jing",
            "Renran Tian",
            "Zhengming Ding"
        ],
        "tldr": "This paper uses VLMs to enhance scene understanding in autonomous driving, improving decision-making accuracy and interpretability by enriching visual inputs with linguistic descriptions and refining predictions post-hoc.",
        "tldr_zh": "本文利用视觉语言模型（VLMs）增强自动驾驶中的场景理解，通过用语言描述丰富视觉输入并进行事后预测优化，从而提高决策的准确性和可解释性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GRLoc: Geometric Representation Regression for Visual Localization",
        "summary": "Absolute Pose Regression (APR) has emerged as a compelling paradigm for visual localization. However, APR models typically operate as black boxes, directly regressing a 6-DoF pose from a query image, which can lead to memorizing training views rather than understanding 3D scene geometry. In this work, we propose a geometrically-grounded alternative. Inspired by novel view synthesis, which renders images from intermediate geometric representations, we reformulate APR as its inverse that regresses the underlying 3D representations directly from the image, and we name this paradigm Geometric Representation Regression (GRR). Our model explicitly predicts two disentangled geometric representations in the world coordinate system: (1) a ray bundle's directions to estimate camera rotation, and (2) a corresponding pointmap to estimate camera translation. The final 6-DoF camera pose is then recovered from these geometric components using a differentiable deterministic solver. This disentangled approach, which separates the learned visual-to-geometry mapping from the final pose calculation, introduces a strong geometric prior into the network. We find that the explicit decoupling of rotation and translation predictions measurably boosts performance. We demonstrate state-of-the-art performance on 7-Scenes and Cambridge Landmarks datasets, validating that modeling the inverse rendering process is a more robust path toward generalizable absolute pose estimation.",
        "url": "http://arxiv.org/abs/2511.13864v1",
        "published_date": "2025-11-17T19:30:22+00:00",
        "updated_date": "2025-11-17T19:30:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Changyang Li",
            "Xuejian Ma",
            "Lixiang Liu",
            "Zhan Li",
            "Qingan Yan",
            "Yi Xu"
        ],
        "tldr": "The paper introduces Geometric Representation Regression (GRR), a novel approach for visual localization that regresses disentangled 3D geometric representations (ray directions and pointmap) from an image to estimate camera pose, outperforming existing Absolute Pose Regression (APR) methods.",
        "tldr_zh": "该论文介绍了几何表示回归（GRR），一种用于视觉定位的新方法，它从图像中回归解耦的3D几何表示（射线方向和点图），以估计相机姿态，优于现有的绝对姿态回归（APR）方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mind the Gap: Evaluating LLM Understanding of Human-Taught Road Safety Principles",
        "summary": "Following road safety norms is non-negotiable not only for humans but also for the AI systems that govern autonomous vehicles. In this work, we evaluate how well multi-modal large language models (LLMs) understand road safety concepts, specifically through schematic and illustrative representations. We curate a pilot dataset of images depicting traffic signs and road-safety norms sourced from school text books and use it to evaluate models capabilities in a zero-shot setting. Our preliminary results show that these models struggle with safety reasoning and reveal gaps between human learning and model interpretation. We further provide an analysis of these performance gaps for future research.",
        "url": "http://arxiv.org/abs/2511.13909v1",
        "published_date": "2025-11-17T21:01:48+00:00",
        "updated_date": "2025-11-17T21:01:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chalamalasetti Kranti"
        ],
        "tldr": "This paper evaluates the understanding of road safety principles by multi-modal LLMs using a newly curated image dataset from school textbooks, revealing significant gaps between human and model interpretation.",
        "tldr_zh": "本文使用来自学校教科书的新策划图像数据集，评估了多模态LLM对道路安全原则的理解，揭示了人类和模型解释之间的显著差距。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]