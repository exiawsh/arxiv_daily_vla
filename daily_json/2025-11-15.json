[
    {
        "title": "Depth Anything 3: Recovering the Visual Space from Any Views",
        "summary": "We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.",
        "url": "http://arxiv.org/abs/2511.10647v1",
        "published_date": "2025-11-13T18:59:53+00:00",
        "updated_date": "2025-11-13T18:59:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haotong Lin",
            "Sili Chen",
            "Junhao Liew",
            "Donny Y. Chen",
            "Zhenyu Li",
            "Guang Shi",
            "Jiashi Feng",
            "Bingyi Kang"
        ],
        "tldr": "Depth Anything 3 (DA3) introduces a simplified architecture for predicting spatially consistent geometry from multiple views, achieving state-of-the-art results on a new visual geometry benchmark.",
        "tldr_zh": "Depth Anything 3 (DA3) 提出了一种简化的架构，用于从多个视图预测空间一致的几何体，并在新的视觉几何基准上取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]