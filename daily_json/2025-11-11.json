[
    {
        "title": "MVU-Eval: Towards Multi-Video Understanding Evaluation for Multimodal LLMs",
        "summary": "The advent of Multimodal Large Language Models (MLLMs) has expanded AI\ncapabilities to visual modalities, yet existing evaluation benchmarks remain\nlimited to single-video understanding, overlooking the critical need for\nmulti-video understanding in real-world scenarios (e.g., sports analytics and\nautonomous driving). To address this significant gap, we introduce MVU-Eval,\nthe first comprehensive benchmark for evaluating Multi-Video Understanding for\nMLLMs. Specifically, our MVU-Eval mainly assesses eight core competencies\nthrough 1,824 meticulously curated question-answer pairs spanning 4,959 videos\nfrom diverse domains, addressing both fundamental perception tasks and\nhigh-order reasoning tasks. These capabilities are rigorously aligned with\nreal-world applications such as multi-sensor synthesis in autonomous systems\nand cross-angle sports analytics. Through extensive evaluation of\nstate-of-the-art open-source and closed-source models, we reveal significant\nperformance discrepancies and limitations in current MLLMs' ability to perform\nunderstanding across multiple videos. The benchmark will be made publicly\navailable to foster future research.",
        "url": "http://arxiv.org/abs/2511.07250v1",
        "published_date": "2025-11-10T16:02:33+00:00",
        "updated_date": "2025-11-10T16:02:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Tianhao Peng",
            "Haochen Wang",
            "Yuanxing Zhang",
            "Zekun Wang",
            "Zili Wang",
            "Ge Zhang",
            "Jian Yang",
            "Shihao Li",
            "Yanghai Wang",
            "Xintao Wang",
            "Houyi Li",
            "Wei Ji",
            "Pengfei Wan",
            "Wenhao Huang",
            "Zhaoxiang Zhang",
            "Jiaheng Liu"
        ],
        "tldr": "This paper introduces MVU-Eval, a new benchmark for evaluating the multi-video understanding capabilities of Multimodal LLMs, revealing limitations in current models and providing a resource for future research.",
        "tldr_zh": "该论文介绍了MVU-Eval，一个新的基准，用于评估多模态LLM的多视频理解能力，揭示了当前模型的局限性，并为未来的研究提供了资源。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 10,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "TrueCity: Real and Simulated Urban Data for Cross-Domain 3D Scene Understanding",
        "summary": "3D semantic scene understanding remains a long-standing challenge in the 3D\ncomputer vision community. One of the key issues pertains to limited real-world\nannotated data to facilitate generalizable models. The common practice to\ntackle this issue is to simulate new data. Although synthetic datasets offer\nscalability and perfect labels, their designer-crafted scenes fail to capture\nreal-world complexity and sensor noise, resulting in a synthetic-to-real domain\ngap. Moreover, no benchmark provides synchronized real and simulated point\nclouds for segmentation-oriented domain shift analysis. We introduce TrueCity,\nthe first urban semantic segmentation benchmark with cm-accurate annotated\nreal-world point clouds, semantic 3D city models, and annotated simulated point\nclouds representing the same city. TrueCity proposes segmentation classes\naligned with international 3D city modeling standards, enabling consistent\nevaluation of synthetic-to-real gap. Our extensive experiments on common\nbaselines quantify domain shift and highlight strategies for exploiting\nsynthetic data to enhance real-world 3D scene understanding. We are convinced\nthat the TrueCity dataset will foster further development of sim-to-real gap\nquantification and enable generalizable data-driven models. The data, code, and\n3D models are available online: https://tum-gis.github.io/TrueCity/",
        "url": "http://arxiv.org/abs/2511.07007v1",
        "published_date": "2025-11-10T11:57:50+00:00",
        "updated_date": "2025-11-10T11:57:50+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Duc Nguyen",
            "Yan-Ling Lai",
            "Qilin Zhang",
            "Prabin Gyawali",
            "Benedikt Schwab",
            "Olaf Wysocki",
            "Thomas H. Kolbe"
        ],
        "tldr": "TrueCity is a new urban semantic segmentation benchmark providing synchronized real and simulated point clouds for sim-to-real domain shift analysis, enabling more generalizable 3D scene understanding models.",
        "tldr_zh": "TrueCity是一个新的城市语义分割基准，提供同步的真实和模拟点云，用于sim-to-real的域迁移分析，从而能够实现更通用的3D场景理解模型。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "PlanT 2.0: Exposing Biases and Structural Flaws in Closed-Loop Driving",
        "summary": "Most recent work in autonomous driving has prioritized benchmark performance\nand methodological innovation over in-depth analysis of model failures, biases,\nand shortcut learning. This has led to incremental improvements without a deep\nunderstanding of the current failures. While it is straightforward to look at\nsituations where the model fails, it is hard to understand the underlying\nreason. This motivates us to conduct a systematic study, where inputs to the\nmodel are perturbed and the predictions observed. We introduce PlanT 2.0, a\nlightweight, object-centric planning transformer designed for autonomous\ndriving research in CARLA. The object-level representation enables controlled\nanalysis, as the input can be easily perturbed (e.g., by changing the location\nor adding or removing certain objects), in contrast to sensor-based models. To\ntackle the scenarios newly introduced by the challenging CARLA Leaderboard 2.0,\nwe introduce multiple upgrades to PlanT, achieving state-of-the-art performance\non Longest6 v2, Bench2Drive, and the CARLA validation routes. Our analysis\nexposes insightful failures, such as a lack of scene understanding caused by\nlow obstacle diversity, rigid expert behaviors leading to exploitable\nshortcuts, and overfitting to a fixed set of expert trajectories. Based on\nthese findings, we argue for a shift toward data-centric development, with a\nfocus on richer, more robust, and less biased datasets. We open-source our code\nand model at https://github.com/autonomousvision/plant2.",
        "url": "http://arxiv.org/abs/2511.07292v1",
        "published_date": "2025-11-10T16:41:47+00:00",
        "updated_date": "2025-11-10T16:41:47+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Simon Gerstenecker",
            "Andreas Geiger",
            "Katrin Renz"
        ],
        "tldr": "The paper introduces PlanT 2.0, an improved object-centric planning transformer for autonomous driving in CARLA, and analyzes its failure modes, exposing biases and structural flaws, advocating for a data-centric approach.",
        "tldr_zh": "该论文介绍了PlanT 2.0，一种改进的面向对象的规划Transformer，用于CARLA中的自动驾驶，并分析了其失效模式，揭示了偏差和结构缺陷，从而提倡以数据为中心的方法。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Leveraging Text-Driven Semantic Variation for Robust OOD Segmentation",
        "summary": "In autonomous driving and robotics, ensuring road safety and reliable\ndecision-making critically depends on out-of-distribution (OOD) segmentation.\nWhile numerous methods have been proposed to detect anomalous objects on the\nroad, leveraging the vision-language space-which provides rich linguistic\nknowledge-remains an underexplored field. We hypothesize that incorporating\nthese linguistic cues can be especially beneficial in the complex contexts\nfound in real-world autonomous driving scenarios.\n  To this end, we present a novel approach that trains a Text-Driven OOD\nSegmentation model to learn a semantically diverse set of objects in the\nvision-language space. Concretely, our approach combines a vision-language\nmodel's encoder with a transformer decoder, employs Distance-Based OOD prompts\nlocated at varying semantic distances from in-distribution (ID) classes, and\nutilizes OOD Semantic Augmentation for OOD representations. By aligning visual\nand textual information, our approach effectively generalizes to unseen objects\nand provides robust OOD segmentation in diverse driving environments.\n  We conduct extensive experiments on publicly available OOD segmentation\ndatasets such as Fishyscapes, Segment-Me-If-You-Can, and Road Anomaly datasets,\ndemonstrating that our approach achieves state-of-the-art performance across\nboth pixel-level and object-level evaluations. This result underscores the\npotential of vision-language-based OOD segmentation to bolster the safety and\nreliability of future autonomous driving systems.",
        "url": "http://arxiv.org/abs/2511.07238v1",
        "published_date": "2025-11-10T15:54:23+00:00",
        "updated_date": "2025-11-10T15:54:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Seungheon Song",
            "Jaekoo Lee"
        ],
        "tldr": "This paper introduces a novel text-driven approach for out-of-distribution (OOD) segmentation in autonomous driving, leveraging vision-language models to improve the detection of anomalous objects, achieving state-of-the-art results on benchmark datasets.",
        "tldr_zh": "本文提出了一种新颖的文本驱动方法，用于自动驾驶中的域外（OOD）分割，利用视觉-语言模型来提高异常物体的检测，并在基准数据集上取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Omni-View: Unlocking How Generation Facilitates Understanding in Unified 3D Model based on Multiview images",
        "summary": "This paper presents Omni-View, which extends the unified multimodal\nunderstanding and generation to 3D scenes based on multiview images, exploring\nthe principle that \"generation facilitates understanding\". Consisting of\nunderstanding model, texture module, and geometry module, Omni-View jointly\nmodels scene understanding, novel view synthesis, and geometry estimation,\nenabling synergistic interaction between 3D scene understanding and generation\ntasks. By design, it leverages the spatiotemporal modeling capabilities of its\ntexture module responsible for appearance synthesis, alongside the explicit\ngeometric constraints provided by its dedicated geometry module, thereby\nenriching the model's holistic understanding of 3D scenes. Trained with a\ntwo-stage strategy, Omni-View achieves a state-of-the-art score of 55.4 on the\nVSI-Bench benchmark, outperforming existing specialized 3D understanding\nmodels, while simultaneously delivering strong performance in both novel view\nsynthesis and 3D scene generation.",
        "url": "http://arxiv.org/abs/2511.07222v1",
        "published_date": "2025-11-10T15:44:48+00:00",
        "updated_date": "2025-11-10T15:44:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "JiaKui Hu",
            "Shanshan Zhao",
            "Qing-Guo Chen",
            "Xuerui Qiu",
            "Jialun Liu",
            "Zhao Xu",
            "Weihua Luo",
            "Kaifu Zhang",
            "Yanye Lu"
        ],
        "tldr": "The paper introduces Omni-View, a unified model for 3D scene understanding, novel view synthesis, and geometry estimation from multiview images, claiming SOTA performance on VSI-Bench by leveraging a \"generation facilitates understanding\" principle.",
        "tldr_zh": "该论文介绍了Omni-View，一个统一的模型，用于从多视角图像中进行3D场景理解、新视角合成和几何估计。该模型声称在VSI-Bench上达到了SOTA性能，通过利用“生成促进理解”的原则。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HENet++: Hybrid Encoding and Multi-task Learning for 3D Perception and End-to-end Autonomous Driving",
        "summary": "Three-dimensional feature extraction is a critical component of autonomous\ndriving systems, where perception tasks such as 3D object detection,\nbird's-eye-view (BEV) semantic segmentation, and occupancy prediction serve as\nimportant constraints on 3D features. While large image encoders,\nhigh-resolution images, and long-term temporal inputs can significantly enhance\nfeature quality and deliver remarkable performance gains, these techniques are\noften incompatible in both training and inference due to computational resource\nconstraints. Moreover, different tasks favor distinct feature representations,\nmaking it difficult for a single model to perform end-to-end inference across\nmultiple tasks while maintaining accuracy comparable to that of single-task\nmodels. To alleviate these issues, we present the HENet and HENet++ framework\nfor multi-task 3D perception and end-to-end autonomous driving. Specifically,\nwe propose a hybrid image encoding network that uses a large image encoder for\nshort-term frames and a small one for long-term frames. Furthermore, our\nframework simultaneously extracts both dense and sparse features, providing\nmore suitable representations for different tasks, reducing cumulative errors,\nand delivering more comprehensive information to the planning module. The\nproposed architecture maintains compatibility with various existing 3D feature\nextraction methods and supports multimodal inputs. HENet++ achieves\nstate-of-the-art end-to-end multi-task 3D perception results on the nuScenes\nbenchmark, while also attaining the lowest collision rate on the nuScenes\nend-to-end autonomous driving benchmark.",
        "url": "http://arxiv.org/abs/2511.07106v1",
        "published_date": "2025-11-10T13:49:59+00:00",
        "updated_date": "2025-11-10T13:49:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhongyu Xia",
            "Zhiwei Lin",
            "Yongtao Wang",
            "Ming-Hsuan Yang"
        ],
        "tldr": "HENet++ proposes a hybrid image encoding network and multi-task learning framework for 3D perception and end-to-end autonomous driving, achieving state-of-the-art performance on nuScenes benchmark.",
        "tldr_zh": "HENet++提出了一种混合图像编码网络和多任务学习框架，用于3D感知和端到端自动驾驶，并在nuScenes基准测试中取得了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RaLD: Generating High-Resolution 3D Radar Point Clouds with Latent Diffusion",
        "summary": "Millimeter-wave radar offers a promising sensing modality for autonomous\nsystems thanks to its robustness in adverse conditions and low cost. However,\nits utility is significantly limited by the sparsity and low resolution of\nradar point clouds, which poses challenges for tasks requiring dense and\naccurate 3D perception. Despite that recent efforts have shown great potential\nby exploring generative approaches to address this issue, they often rely on\ndense voxel representations that are inefficient and struggle to preserve\nstructural detail. To fill this gap, we make the key observation that latent\ndiffusion models (LDMs), though successful in other modalities, have not been\neffectively leveraged for radar-based 3D generation due to a lack of compatible\nrepresentations and conditioning strategies. We introduce RaLD, a framework\nthat bridges this gap by integrating scene-level frustum-based LiDAR\nautoencoding, order-invariant latent representations, and direct radar spectrum\nconditioning. These insights lead to a more compact and expressive generation\nprocess. Experiments show that RaLD produces dense and accurate 3D point clouds\nfrom raw radar spectrums, offering a promising solution for robust perception\nin challenging environments.",
        "url": "http://arxiv.org/abs/2511.07067v1",
        "published_date": "2025-11-10T13:03:58+00:00",
        "updated_date": "2025-11-10T13:03:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruijie Zhang",
            "Bixin Zeng",
            "Shengpeng Wang",
            "Fuhui Zhou",
            "Wei Wang"
        ],
        "tldr": "The paper introduces RaLD, a latent diffusion model-based framework for generating high-resolution 3D radar point clouds from raw radar spectrums, addressing the limitations of sparse radar data for autonomous systems by using frustum-based LiDAR autoencoding and order-invariant latent representations.",
        "tldr_zh": "本文介绍了RaLD，一个基于潜在扩散模型的框架，用于从原始雷达频谱生成高分辨率3D雷达点云。它通过使用基于视锥的激光雷达自动编码和顺序不变的潜在表示，解决了自动驾驶系统中稀疏雷达数据的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Certified L2-Norm Robustness of 3D Point Cloud Recognition in the Frequency Domain",
        "summary": "3D point cloud classification is a fundamental task in safety-critical\napplications such as autonomous driving, robotics, and augmented reality.\nHowever, recent studies reveal that point cloud classifiers are vulnerable to\nstructured adversarial perturbations and geometric corruptions, posing risks to\ntheir deployment in safety-critical scenarios. Existing certified defenses\nlimit point-wise perturbations but overlook subtle geometric distortions that\npreserve individual points yet alter the overall structure, potentially leading\nto misclassification. In this work, we propose FreqCert, a novel certification\nframework that departs from conventional spatial domain defenses by shifting\nrobustness analysis to the frequency domain, enabling structured certification\nagainst global L2-bounded perturbations. FreqCert first transforms the input\npoint cloud via the graph Fourier transform (GFT), then applies structured\nfrequency-aware subsampling to generate multiple sub-point clouds. Each\nsub-cloud is independently classified by a standard model, and the final\nprediction is obtained through majority voting, where sub-clouds are\nconstructed based on spectral similarity rather than spatial proximity, making\nthe partitioning more stable under L2 perturbations and better aligned with the\nobject's intrinsic structure. We derive a closed-form lower bound on the\ncertified L2 robustness radius and prove its tightness under minimal and\ninterpretable assumptions, establishing a theoretical foundation for frequency\ndomain certification. Extensive experiments on the ModelNet40 and ScanObjectNN\ndatasets demonstrate that FreqCert consistently achieves higher certified\naccuracy and empirical accuracy under strong perturbations. Our results suggest\nthat spectral representations provide an effective pathway toward certifiable\nrobustness in 3D point cloud recognition.",
        "url": "http://arxiv.org/abs/2511.07029v1",
        "published_date": "2025-11-10T12:24:35+00:00",
        "updated_date": "2025-11-10T12:24:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liang Zhou",
            "Qiming Wang",
            "Tianze Chen"
        ],
        "tldr": "The paper proposes FreqCert, a novel certified defense framework for 3D point cloud recognition that operates in the frequency domain, achieving higher certified and empirical accuracy against L2-bounded perturbations by leveraging spectral representations and frequency-aware subsampling.",
        "tldr_zh": "该论文提出了FreqCert，一种新颖的3D点云识别认证防御框架，它在频域中运行，通过利用频谱表示和频率感知子采样，实现了针对L2有界扰动的更高的认证精度和经验精度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mono3DVG-EnSD: Enhanced Spatial-aware and Dimension-decoupled Text Encoding for Monocular 3D Visual Grounding",
        "summary": "Monocular 3D Visual Grounding (Mono3DVG) is an emerging task that locates 3D\nobjects in RGB images using text descriptions with geometric cues. However,\nexisting methods face two key limitations. Firstly, they often over-rely on\nhigh-certainty keywords that explicitly identify the target object while\nneglecting critical spatial descriptions. Secondly, generalized textual\nfeatures contain both 2D and 3D descriptive information, thereby capturing an\nadditional dimension of details compared to singular 2D or 3D visual features.\nThis characteristic leads to cross-dimensional interference when refining\nvisual features under text guidance. To overcome these challenges, we propose\nMono3DVG-EnSD, a novel framework that integrates two key components: the\nCLIP-Guided Lexical Certainty Adapter (CLIP-LCA) and the Dimension-Decoupled\nModule (D2M). The CLIP-LCA dynamically masks high-certainty keywords while\nretaining low-certainty implicit spatial descriptions, thereby forcing the\nmodel to develop a deeper understanding of spatial relationships in captions\nfor object localization. Meanwhile, the D2M decouples dimension-specific\n(2D/3D) textual features from generalized textual features to guide\ncorresponding visual features at same dimension, which mitigates\ncross-dimensional interference by ensuring dimensionally-consistent cross-modal\ninteractions. Through comprehensive comparisons and ablation studies on the\nMono3DRefer dataset, our method achieves state-of-the-art (SOTA) performance\nacross all metrics. Notably, it improves the challenging Far(Acc@0.5) scenario\nby a significant +13.54%.",
        "url": "http://arxiv.org/abs/2511.06908v1",
        "published_date": "2025-11-10T10:02:30+00:00",
        "updated_date": "2025-11-10T10:02:30+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Yuzhen Li",
            "Min Liu",
            "Zhaoyang Li",
            "Yuan Bian",
            "Xueping Wang",
            "Erbo Zhai",
            "Yaonan Wang"
        ],
        "tldr": "This paper introduces Mono3DVG-EnSD, a novel framework for Monocular 3D Visual Grounding that enhances spatial-aware text encoding and decouples dimensional features to improve performance, achieving SOTA results on the Mono3DRefer dataset.",
        "tldr_zh": "该论文提出了Mono3DVG-EnSD，一种用于单目3D视觉定位的新框架，它增强了空间感知文本编码并解耦了维度特征，从而提高了性能，并在Mono3DRefer数据集上实现了SOTA结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "PanoNav: Mapless Zero-Shot Object Navigation with Panoramic Scene Parsing and Dynamic Memory",
        "summary": "Zero-shot object navigation (ZSON) in unseen environments remains a\nchallenging problem for household robots, requiring strong perceptual\nunderstanding and decision-making capabilities. While recent methods leverage\nmetric maps and Large Language Models (LLMs), they often depend on depth\nsensors or prebuilt maps, limiting the spatial reasoning ability of Multimodal\nLarge Language Models (MLLMs). Mapless ZSON approaches have emerged to address\nthis, but they typically make short-sighted decisions, leading to local\ndeadlocks due to a lack of historical context. We propose PanoNav, a fully\nRGB-only, mapless ZSON framework that integrates a Panoramic Scene Parsing\nmodule to unlock the spatial parsing potential of MLLMs from panoramic RGB\ninputs, and a Memory-guided Decision-Making mechanism enhanced by a Dynamic\nBounded Memory Queue to incorporate exploration history and avoid local\ndeadlocks. Experiments on the public navigation benchmark show that PanoNav\nsignificantly outperforms representative baselines in both SR and SPL metrics.",
        "url": "http://arxiv.org/abs/2511.06840v1",
        "published_date": "2025-11-10T08:31:32+00:00",
        "updated_date": "2025-11-10T08:31:32+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Qunchao Jin",
            "Yilin Wu",
            "Changhao Chen"
        ],
        "tldr": "PanoNav is a mapless, RGB-only ZSON framework using panoramic scene parsing and a dynamic memory queue to improve spatial reasoning and avoid deadlocks, outperforming baselines on navigation benchmarks.",
        "tldr_zh": "PanoNav是一个无需地图、仅使用RGB的ZSON框架，它利用全景场景解析和动态记忆队列来提高空间推理能力并避免死锁，在导航基准测试中优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Robust and High-Fidelity 3D Gaussian Splatting: Fusing Pose Priors and Geometry Constraints for Texture-Deficient Outdoor Scenes",
        "summary": "3D Gaussian Splatting (3DGS) has emerged as a key rendering pipeline for\ndigital asset creation due to its balance between efficiency and visual\nquality. To address the issues of unstable pose estimation and scene\nrepresentation distortion caused by geometric texture inconsistency in large\noutdoor scenes with weak or repetitive textures, we approach the problem from\ntwo aspects: pose estimation and scene representation. For pose estimation, we\nleverage LiDAR-IMU Odometry to provide prior poses for cameras in large-scale\nenvironments. These prior pose constraints are incorporated into COLMAP's\ntriangulation process, with pose optimization performed via bundle adjustment.\nEnsuring consistency between pixel data association and prior poses helps\nmaintain both robustness and accuracy. For scene representation, we introduce\nnormal vector constraints and effective rank regularization to enforce\nconsistency in the direction and shape of Gaussian primitives. These\nconstraints are jointly optimized with the existing photometric loss to enhance\nthe map quality. We evaluate our approach using both public and self-collected\ndatasets. In terms of pose optimization, our method requires only one-third of\nthe time while maintaining accuracy and robustness across both datasets. In\nterms of scene representation, the results show that our method significantly\noutperforms conventional 3DGS pipelines. Notably, on self-collected datasets\ncharacterized by weak or repetitive textures, our approach demonstrates\nenhanced visualization capabilities and achieves superior overall performance.\nCodes and data will be publicly available at\nhttps://github.com/justinyeah/normal_shape.git.",
        "url": "http://arxiv.org/abs/2511.06765v1",
        "published_date": "2025-11-10T06:45:08+00:00",
        "updated_date": "2025-11-10T06:45:08+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Meijun Guo",
            "Yongliang Shi",
            "Caiyun Liu",
            "Yixiao Feng",
            "Ming Ma",
            "Tinghai Yan",
            "Weining Lu",
            "Bin Liang"
        ],
        "tldr": "This paper enhances 3D Gaussian Splatting for outdoor scenes with weak textures by incorporating pose priors from LiDAR-IMU Odometry and geometric constraints on Gaussian primitives, improving both pose estimation and scene representation.",
        "tldr_zh": "本文通过融合LiDAR-IMU里程计的姿态先验和高斯基元的几何约束，增强了3D高斯溅射在弱纹理室外场景中的性能，从而改善了姿态估计和场景表示。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SlotVLA: Towards Modeling of Object-Relation Representations in Robotic Manipulation",
        "summary": "Inspired by how humans reason over discrete objects and their relationships,\nwe explore whether compact object-centric and object-relation representations\ncan form a foundation for multitask robotic manipulation. Most existing robotic\nmultitask models rely on dense embeddings that entangle both object and\nbackground cues, raising concerns about both efficiency and interpretability.\nIn contrast, we study object-relation-centric representations as a pathway to\nmore structured, efficient, and explainable visuomotor control. Our\ncontributions are two-fold. First, we introduce LIBERO+, a fine-grained\nbenchmark dataset designed to enable and evaluate object-relation reasoning in\nrobotic manipulation. Unlike prior datasets, LIBERO+ provides object-centric\nannotations that enrich demonstrations with box- and mask-level labels as well\nas instance-level temporal tracking, supporting compact and interpretable\nvisuomotor representations. Second, we propose SlotVLA, a slot-attention-based\nframework that captures both objects and their relations for action decoding.\nIt uses a slot-based visual tokenizer to maintain consistent temporal object\nrepresentations, a relation-centric decoder to produce task-relevant\nembeddings, and an LLM-driven module that translates these embeddings into\nexecutable actions. Experiments on LIBERO+ demonstrate that object-centric slot\nand object-relation slot representations drastically reduce the number of\nrequired visual tokens, while providing competitive generalization. Together,\nLIBERO+ and SlotVLA provide a compact, interpretable, and effective foundation\nfor advancing object-relation-centric robotic manipulation.",
        "url": "http://arxiv.org/abs/2511.06754v1",
        "published_date": "2025-11-10T06:33:44+00:00",
        "updated_date": "2025-11-10T06:33:44+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Taisei Hanyu",
            "Nhat Chung",
            "Huy Le",
            "Toan Nguyen",
            "Yuki Ikebe",
            "Anthony Gunderman",
            "Duy Nguyen Ho Minh",
            "Khoa Vo",
            "Tung Kieu",
            "Kashu Yamazaki",
            "Chase Rainwater",
            "Anh Nguyen",
            "Ngan Le"
        ],
        "tldr": "The paper introduces LIBERO+, a new benchmark dataset for object-relation reasoning in robotic manipulation, and SlotVLA, a slot-attention-based framework that captures object-centric representations for improved visuomotor control.",
        "tldr_zh": "该论文介绍了LIBERO+，一个新的用于机器人操作中对象关系推理的基准数据集；以及SlotVLA，一个基于槽注意力的框架，它捕获以对象为中心的表示，以改进视觉运动控制。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Semi-distributed Cross-modal Air-Ground Relative Localization",
        "summary": "Efficient, accurate, and flexible relative localization is crucial in\nair-ground collaborative tasks. However, current approaches for robot relative\nlocalization are primarily realized in the form of distributed multi-robot SLAM\nsystems with the same sensor configuration, which are tightly coupled with the\nstate estimation of all robots, limiting both flexibility and accuracy. To this\nend, we fully leverage the high capacity of Unmanned Ground Vehicle (UGV) to\nintegrate multiple sensors, enabling a semi-distributed cross-modal air-ground\nrelative localization framework. In this work, both the UGV and the Unmanned\nAerial Vehicle (UAV) independently perform SLAM while extracting deep\nlearning-based keypoints and global descriptors, which decouples the relative\nlocalization from the state estimation of all agents. The UGV employs a local\nBundle Adjustment (BA) with LiDAR, camera, and an IMU to rapidly obtain\naccurate relative pose estimates. The BA process adopts sparse keypoint\noptimization and is divided into two stages: First, optimizing camera poses\ninterpolated from LiDAR-Inertial Odometry (LIO), followed by estimating the\nrelative camera poses between the UGV and UAV. Additionally, we implement an\nincremental loop closure detection algorithm using deep learning-based\ndescriptors to maintain and retrieve keyframes efficiently. Experimental\nresults demonstrate that our method achieves outstanding performance in both\naccuracy and efficiency. Unlike traditional multi-robot SLAM approaches that\ntransmit images or point clouds, our method only transmits keypoint pixels and\ntheir descriptors, effectively constraining the communication bandwidth under\n0.3 Mbps. Codes and data will be publicly available on\nhttps://github.com/Ascbpiac/cross-model-relative-localization.git.",
        "url": "http://arxiv.org/abs/2511.06749v1",
        "published_date": "2025-11-10T06:28:31+00:00",
        "updated_date": "2025-11-10T06:28:31+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Weining Lu",
            "Deer Bin",
            "Lian Ma",
            "Ming Ma",
            "Zhihao Ma",
            "Xiangyang Chen",
            "Longfei Wang",
            "Yixiao Feng",
            "Zhouxian Jiang",
            "Yongliang Shi",
            "Bin Liang"
        ],
        "tldr": "This paper presents a semi-distributed cross-modal air-ground relative localization framework using a UGV and UAV, decoupling relative localization from state estimation by transmitting only keypoint pixels and descriptors, achieving high accuracy and efficiency with low communication bandwidth.",
        "tldr_zh": "本文提出了一种半分布式跨模态空地相对定位框架，使用无人地面车和无人机，通过仅传输关键点像素和描述符将相对定位与状态估计分离，实现了高精度和高效率，同时降低了通信带宽。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Relative Energy Learning for LiDAR Out-of-Distribution Detection",
        "summary": "Out-of-distribution (OOD) detection is a critical requirement for reliable\nautonomous driving, where safety depends on recognizing road obstacles and\nunexpected objects beyond the training distribution. Despite extensive research\non OOD detection in 2D images, direct transfer to 3D LiDAR point clouds has\nbeen proven ineffective. Current LiDAR OOD methods struggle to distinguish rare\nanomalies from common classes, leading to high false-positive rates and\noverconfident errors in safety-critical settings. We propose Relative Energy\nLearning (REL), a simple yet effective framework for OOD detection in LiDAR\npoint clouds. REL leverages the energy gap between positive (in-distribution)\nand negative logits as a relative scoring function, mitigating calibration\nissues in raw energy values and improving robustness across various scenes. To\naddress the absence of OOD samples during training, we propose a lightweight\ndata synthesis strategy called Point Raise, which perturbs existing point\nclouds to generate auxiliary anomalies without altering the inlier semantics.\nEvaluated on SemanticKITTI and the Spotting the Unexpected (STU) benchmark, REL\nconsistently outperforms existing methods by a large margin. Our results\nhighlight that modeling relative energy, combined with simple synthetic\noutliers, provides a principled and scalable solution for reliable OOD\ndetection in open-world autonomous driving.",
        "url": "http://arxiv.org/abs/2511.06720v1",
        "published_date": "2025-11-10T05:29:18+00:00",
        "updated_date": "2025-11-10T05:29:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zizhao Li",
            "Zhengkang Xiang",
            "Jiayang Ao",
            "Joseph West",
            "Kourosh Khoshelham"
        ],
        "tldr": "This paper introduces Relative Energy Learning (REL), a novel framework for Out-of-Distribution (OOD) detection in LiDAR point clouds using relative energy scoring and a point cloud perturbation data synthesis strategy, achieving superior performance on SemanticKITTI and STU datasets.",
        "tldr_zh": "本文介绍了一种名为相对能量学习 (REL) 的新框架，用于激光雷达点云中的异常检测 (OOD)，该框架使用相对能量评分和点云扰动数据合成策略，在 SemanticKITTI 和 STU 数据集上实现了卓越的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SPAN: Spatial-Projection Alignment for Monocular 3D Object Detection",
        "summary": "Existing monocular 3D detectors typically tame the pronounced nonlinear\nregression of 3D bounding box through decoupled prediction paradigm, which\nemploys multiple branches to estimate geometric center, depth, dimensions, and\nrotation angle separately. Although this decoupling strategy simplifies the\nlearning process, it inherently ignores the geometric collaborative constraints\nbetween different attributes, resulting in the lack of geometric consistency\nprior, thereby leading to suboptimal performance. To address this issue, we\npropose novel Spatial-Projection Alignment (SPAN) with two pivotal components:\n(i). Spatial Point Alignment enforces an explicit global spatial constraint\nbetween the predicted and ground-truth 3D bounding boxes, thereby rectifying\nspatial drift caused by decoupled attribute regression. (ii). 3D-2D Projection\nAlignment ensures that the projected 3D box is aligned tightly within its\ncorresponding 2D detection bounding box on the image plane, mitigating\nprojection misalignment overlooked in previous works. To ensure training\nstability, we further introduce a Hierarchical Task Learning strategy that\nprogressively incorporates spatial-projection alignment as 3D attribute\npredictions refine, preventing early stage error propagation across attributes.\nExtensive experiments demonstrate that the proposed method can be easily\nintegrated into any established monocular 3D detector and delivers significant\nperformance improvements.",
        "url": "http://arxiv.org/abs/2511.06702v1",
        "published_date": "2025-11-10T04:48:48+00:00",
        "updated_date": "2025-11-10T04:48:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yifan Wang",
            "Yian Zhao",
            "Fanqi Pu",
            "Xiaochen Yang",
            "Yang Tang",
            "Xi Chen",
            "Wenming Yang"
        ],
        "tldr": "This paper proposes a Spatial-Projection Alignment (SPAN) method for monocular 3D object detection that enforces geometric consistency between 3D attributes and 2D projections, leading to improved performance.",
        "tldr_zh": "本文提出了一种用于单目3D物体检测的空间-投影对齐（SPAN）方法，该方法强制3D属性和2D投影之间的几何一致性，从而提高性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "REOcc: Camera-Radar Fusion with Radar Feature Enrichment for 3D Occupancy Prediction",
        "summary": "Vision-based 3D occupancy prediction has made significant advancements, but\nits reliance on cameras alone struggles in challenging environments. This\nlimitation has driven the adoption of sensor fusion, among which camera-radar\nfusion stands out as a promising solution due to their complementary strengths.\nHowever, the sparsity and noise of the radar data limits its effectiveness,\nleading to suboptimal fusion performance. In this paper, we propose REOcc, a\nnovel camera-radar fusion network designed to enrich radar feature\nrepresentations for 3D occupancy prediction. Our approach introduces two main\ncomponents, a Radar Densifier and a Radar Amplifier, which refine radar\nfeatures by integrating spatial and contextual information, effectively\nenhancing spatial density and quality. Extensive experiments on the\nOcc3D-nuScenes benchmark demonstrate that REOcc achieves significant\nperformance gains over the camera-only baseline model, particularly in dynamic\nobject classes. These results underscore REOcc's capability to mitigate the\nsparsity and noise of the radar data. Consequently, radar complements camera\ndata more effectively, unlocking the full potential of camera-radar fusion for\nrobust and reliable 3D occupancy prediction.",
        "url": "http://arxiv.org/abs/2511.06666v1",
        "published_date": "2025-11-10T03:23:52+00:00",
        "updated_date": "2025-11-10T03:23:52+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chaehee Song",
            "Sanmin Kim",
            "Hyeonjun Jeong",
            "Juyeb Shin",
            "Joonhee Lim",
            "Dongsuk Kum"
        ],
        "tldr": "The paper introduces REOcc, a camera-radar fusion network with Radar Densifier and Amplifier modules, to improve 3D occupancy prediction by enriching sparse and noisy radar features, showing performance gains on Occ3D-nuScenes, especially for dynamic objects.",
        "tldr_zh": "本文提出了REOcc，一个相机-雷达融合网络，包含雷达密度器和放大器模块，通过丰富稀疏和噪声大的雷达特征来提高3D occupancy预测性能。实验结果在Occ3D-nuScenes数据集上显示，尤其是在动态物体类别上，性能有所提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DIAL-GS: Dynamic Instance Aware Reconstruction for Label-free Street Scenes with 4D Gaussian Splatting",
        "summary": "Urban scene reconstruction is critical for autonomous driving, enabling\nstructured 3D representations for data synthesis and closed-loop testing.\nSupervised approaches rely on costly human annotations and lack scalability,\nwhile current self-supervised methods often confuse static and dynamic elements\nand fail to distinguish individual dynamic objects, limiting fine-grained\nediting. We propose DIAL-GS, a novel dynamic instance-aware reconstruction\nmethod for label-free street scenes with 4D Gaussian Splatting. We first\naccurately identify dynamic instances by exploiting appearance-position\ninconsistency between warped rendering and actual observation. Guided by\ninstance-level dynamic perception, we employ instance-aware 4D Gaussians as the\nunified volumetric representation, realizing dynamic-adaptive and\ninstance-aware reconstruction. Furthermore, we introduce a reciprocal mechanism\nthrough which identity and dynamics reinforce each other, enhancing both\nintegrity and consistency. Experiments on urban driving scenarios show that\nDIAL-GS surpasses existing self-supervised baselines in reconstruction quality\nand instance-level editing, offering a concise yet powerful solution for urban\nscene modeling.",
        "url": "http://arxiv.org/abs/2511.06632v1",
        "published_date": "2025-11-10T02:18:40+00:00",
        "updated_date": "2025-11-10T02:18:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chenpeng Su",
            "Wenhua Wu",
            "Chensheng Peng",
            "Tianchen Deng",
            "Zhe Liu",
            "Hesheng Wang"
        ],
        "tldr": "DIAL-GS uses 4D Gaussian Splatting and appearance-position inconsistency detection to reconstruct dynamic street scenes in a self-supervised manner, enabling instance-aware editing.",
        "tldr_zh": "DIAL-GS利用4D高斯溅射和外观-位置不一致性检测，以自监督方式重建动态街景，从而实现实例感知的编辑。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Low-Rank Method for Vision Language Model Hallucination Mitigation in Autonomous Driving",
        "summary": "Vision Language Models (VLMs) are increasingly used in autonomous driving to\nhelp understand traffic scenes, but they sometimes produce hallucinations,\nwhich are false details not grounded in the visual input. Detecting and\nmitigating hallucinations is challenging when ground-truth references are\nunavailable and model internals are inaccessible. This paper proposes a novel\nself-contained low-rank approach to automatically rank multiple candidate\ncaptions generated by multiple VLMs based on their hallucination levels, using\nonly the captions themselves without requiring external references or model\naccess. By constructing a sentence-embedding matrix and decomposing it into a\nlow-rank consensus component and a sparse residual, we use the residual\nmagnitude to rank captions: selecting the one with the smallest residual as the\nmost hallucination-free. Experiments on the NuScenes dataset demonstrate that\nour approach achieves 87% selection accuracy in identifying hallucination-free\ncaptions, representing a 19% improvement over the unfiltered baseline and a\n6-10% improvement over multi-agent debate method. The sorting produced by\nsparse error magnitudes shows strong correlation with human judgments of\nhallucinations, validating our scoring mechanism. Additionally, our method,\nwhich can be easily parallelized, reduces inference time by 51-67% compared to\ndebate approaches, making it practical for real-time autonomous driving\napplications.",
        "url": "http://arxiv.org/abs/2511.06496v1",
        "published_date": "2025-11-09T18:50:30+00:00",
        "updated_date": "2025-11-09T18:50:30+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Keke Long",
            "Jiacheng Guo",
            "Tianyun Zhang",
            "Hongkai Yu",
            "Xiaopeng Li"
        ],
        "tldr": "This paper introduces a low-rank method to mitigate hallucinations in Vision Language Models for autonomous driving by ranking candidate captions based on their residual magnitude after low-rank decomposition of sentence embeddings, achieving improved accuracy and inference time compared to baselines.",
        "tldr_zh": "本文提出了一种低秩方法，通过基于句子嵌入的低秩分解后的残差幅度对候选字幕进行排序，以减轻自动驾驶中视觉语言模型的幻觉，与基线相比，提高了准确性和推理时间。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]