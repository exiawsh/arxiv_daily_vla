[
    {
        "title": "EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes",
        "summary": "Robust 3D geometry estimation from videos is critical for applications such as autonomous navigation, SLAM, and 3D scene reconstruction. Recent methods like DUSt3R demonstrate that regressing dense pointmaps from image pairs enables accurate and efficient pose-free reconstruction. However, existing RGB-only approaches struggle under real-world conditions involving dynamic objects and extreme illumination, due to the inherent limitations of conventional cameras. In this paper, we propose EAG3R, a novel geometry estimation framework that augments pointmap-based reconstruction with asynchronous event streams. Built upon the MonST3R backbone, EAG3R introduces two key innovations: (1) a retinex-inspired image enhancement module and a lightweight event adapter with SNR-aware fusion mechanism that adaptively combines RGB and event features based on local reliability; and (2) a novel event-based photometric consistency loss that reinforces spatiotemporal coherence during global optimization. Our method enables robust geometry estimation in challenging dynamic low-light scenes without requiring retraining on night-time data. Extensive experiments demonstrate that EAG3R significantly outperforms state-of-the-art RGB-only baselines across monocular depth estimation, camera pose tracking, and dynamic reconstruction tasks.",
        "url": "http://arxiv.org/abs/2512.00771v1",
        "published_date": "2025-11-30T08:05:28+00:00",
        "updated_date": "2025-11-30T08:05:28+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xiaoshan Wu",
            "Yifei Yu",
            "Xiaoyang Lyu",
            "Yihua Huang",
            "Bo Wang",
            "Baoheng Zhang",
            "Zhongrui Wang",
            "Xiaojuan Qi"
        ],
        "tldr": "EAG3R is a new 3D geometry estimation framework leveraging event streams to enhance pointmap-based reconstruction, particularly in dynamic and low-light environments, outperforming RGB-only methods.",
        "tldr_zh": "EAG3R是一个新的三维几何估计框架，利用事件流来增强基于点图的重建，尤其是在动态和低光照环境中，性能优于仅使用RGB的方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "REM: Evaluating LLM Embodied Spatial Reasoning through Multi-Frame Trajectories",
        "summary": "Humans build viewpoint-independent cognitive maps through navigation, enabling intuitive reasoning about object permanence and spatial relations. We argue that multimodal large language models (MLLMs), despite extensive video training, lack this fundamental spatial reasoning capability, a critical limitation for embodied applications. To demonstrate these limitations and drive research, we introduce REM (Reasoning over Embodied Multi-Frame Trajectories), a benchmark using controllable 3D environments for long-horizon embodied spatial reasoning. REM systematically evaluates key aspects like object permanence/distinction, spatial relationships, and numerical tracking across dynamic embodied viewpoints. Our evaluation shows that the best-performing current models exhibit promising overall performance, but become increasingly unreliable at even moderate complexity levels easily handled by humans. These findings highlight challenges MLLMs face in developing robust spatial representations from sequential visual input. Consequently, REM provides targeted metrics and diagnostics to foster improved spatial understanding in future models.",
        "url": "http://arxiv.org/abs/2512.00736v1",
        "published_date": "2025-11-30T05:20:22+00:00",
        "updated_date": "2025-11-30T05:20:22+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Jacob Thompson",
            "Emiliano Garcia-Lopez",
            "Yonatan Bisk"
        ],
        "tldr": "The paper introduces REM, a benchmark to evaluate spatial reasoning capabilities of MLLMs in embodied environments, revealing their limitations in object permanence, spatial relationships, and tracking when faced with increasing complexity.",
        "tldr_zh": "该论文介绍了REM，一个用于评估MLLM在具身环境中空间推理能力的基准，揭示了它们在物体永恒性、空间关系和跟踪方面面临复杂性时的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "MM-ACT: Learn from Multimodal Parallel Generation to Act",
        "summary": "A generalist robotic policy needs both semantic understanding for task planning and the ability to interact with the environment through predictive capabilities. To tackle this, we present MM-ACT, a unified Vision-Language-Action (VLA) model that integrates text, image, and action in shared token space and performs generation across all three modalities. MM-ACT adopts a re-mask parallel decoding strategy for text and image generation, and employs a one-step parallel decoding strategy for action generation to improve efficiency. We introduce Context-Shared Multimodal Learning, a unified training paradigm that supervises generation in all three modalities from a shared context, enhancing action generation through cross-modal learning. Experiments were conducted on the LIBERO simulation and Franka real-robot setups as well as RoboTwin2.0 to assess in-domain and out-of-domain performances respectively. Our approach achieves a success rate of 96.3% on LIBERO, 72.0% across three tasks of real Franka, and 52.38% across eight bimanual tasks of RoboTwin2.0 with an additional gain of 9.25% from cross-modal learning. We release our codes, models and data at https://github.com/HHYHRHY/MM-ACT.",
        "url": "http://arxiv.org/abs/2512.00975v1",
        "published_date": "2025-11-30T16:46:35+00:00",
        "updated_date": "2025-11-30T16:46:35+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Haotian Liang",
            "Xinyi Chen",
            "Bin Wang",
            "Mingkang Chen",
            "Yitian Liu",
            "Yuhao Zhang",
            "Zanxin Chen",
            "Tianshuo Yang",
            "Yilun Chen",
            "Jiangmiao Pang",
            "Dong Liu",
            "Xiaokang Yang",
            "Yao Mu",
            "Wenqi Shao",
            "Ping Luo"
        ],
        "tldr": "The paper introduces MM-ACT, a unified Vision-Language-Action model that integrates text, image, and action in a shared token space, achieving strong performance in robotic manipulation tasks across simulated and real-world environments.",
        "tldr_zh": "该论文介绍了MM-ACT，一个统一的视觉-语言-动作模型，它在共享的token空间中集成了文本、图像和动作，并在模拟和真实世界的机器人操作任务中取得了出色的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient and Scalable Monocular Human-Object Interaction Motion Reconstruction",
        "summary": "Generalized robots must learn from diverse, large-scale human-object interactions (HOI) to operate robustly in the real world. Monocular internet videos offer a nearly limitless and readily available source of data, capturing an unparalleled diversity of human activities, objects, and environments. However, accurately and scalably extracting 4D interaction data from these in-the-wild videos remains a significant and unsolved challenge. Thus, in this work, we introduce 4DHOISolver, a novel and efficient optimization framework that constrains the ill-posed 4D HOI reconstruction problem by leveraging sparse, human-in-the-loop contact point annotations, while maintaining high spatio-temporal coherence and physical plausibility. Leveraging this framework, we introduce Open4DHOI, a new large-scale 4D HOI dataset featuring a diverse catalog of 144 object types and 103 actions. Furthermore, we demonstrate the effectiveness of our reconstructions by enabling an RL-based agent to imitate the recovered motions. However, a comprehensive benchmark of existing 3D foundation models indicates that automatically predicting precise human-object contact correspondences remains an unsolved problem, underscoring the immediate necessity of our human-in-the-loop strategy while posing an open challenge to the community. Data and code will be publicly available at https://wenboran2002.github.io/open4dhoi/",
        "url": "http://arxiv.org/abs/2512.00960v1",
        "published_date": "2025-11-30T16:21:47+00:00",
        "updated_date": "2025-11-30T16:21:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Boran Wen",
            "Ye Lu",
            "Keyan Wan",
            "Sirui Wang",
            "Jiahong Zhou",
            "Junxuan Liang",
            "Xinpeng Liu",
            "Bang Xiao",
            "Dingbang Huang",
            "Ruiyang Liu",
            "Yong-Lu Li"
        ],
        "tldr": "The paper introduces 4DHOISolver, a framework for reconstructing human-object interaction (HOI) motion from monocular videos using sparse human-in-the-loop annotations, and presents Open4DHOI, a large-scale 4D HOI dataset. The authors further demonstrate the application of their framework for RL-based imitation learning.",
        "tldr_zh": "本文介绍了一个名为4DHOISolver的框架，该框架利用稀疏的人工标注从单目视频中重建人与物体交互（HOI）的运动，并提出了一个大型4D HOI数据集Open4DHOI。作者进一步展示了他们的框架在基于强化学习的模仿学习中的应用。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dual-Projection Fusion for Accurate Upright Panorama Generation in Robotic Vision",
        "summary": "Panoramic cameras, capable of capturing a 360-degree field of view, are crucial in robotic vision, particularly in environments with sparse features. However, non-upright panoramas due to unstable robot postures hinder downstream tasks. Traditional IMU-based correction methods suffer from drift and external disturbances, while vision-based approaches offer a promising alternative. This study presents a dual-stream angle-aware generation network that jointly estimates camera inclination angles and reconstructs upright panoramic images. The network comprises a CNN branch that extracts local geometric structures from equirectangular projections and a ViT branch that captures global contextual cues from cubemap projections. These are integrated through a dual-projection adaptive fusion module that aligns spatial features across both domains. To further enhance performance, we introduce a high-frequency enhancement block, circular padding, and channel attention mechanisms to preserve 360° continuity and improve geometric sensitivity. Experiments on the SUN360 and M3D datasets demonstrate that our method outperforms existing approaches in both inclination estimation and upright panorama generation. Ablation studies further validate the contribution of each module and highlight the synergy between the two tasks. The code and related datasets can be found at: https://github.com/YuhaoShine/DualProjectionFusion.",
        "url": "http://arxiv.org/abs/2512.00911v1",
        "published_date": "2025-11-30T14:28:21+00:00",
        "updated_date": "2025-11-30T14:28:21+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuhao Shan",
            "Qianyi Yuan",
            "Jingguo Liu",
            "Shigang Li",
            "Jianfeng Li",
            "Tong Chen"
        ],
        "tldr": "This paper introduces a dual-projection fusion network using CNN and ViT branches to generate accurate upright panoramas for robotic vision, addressing the issue of non-upright panoramas caused by unstable robot postures. The method shows improved performance on standard datasets.",
        "tldr_zh": "本文提出了一种双投影融合网络，使用 CNN 和 ViT 分支来生成精确的机器人视觉正立全景图，解决了由机器人姿势不稳定引起的非正立全景图的问题。该方法在标准数据集上表现出改进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead",
        "summary": "Vision-Language-Action (VLA) models built on pretrained Vision-Language Models (VLMs) show strong potential but are limited in practicality due to their large parameter counts. To mitigate this issue, using a lightweight VLM has been explored, but it compromises spatiotemporal reasoning. Although some methods suggest that incorporating additional 3D inputs can help, they usually rely on large VLMs to fuse 3D and 2D inputs and still lack temporal understanding. Therefore, we propose SwiftVLA, an architecture that enhances a compact model with 4D understanding while preserving design efficiency. Specifically, our approach features a pretrained 4D visual geometry transformer with a temporal cache that extracts 4D features from 2D images. Then, to enhance the VLM's ability to exploit both 2D images and 4D features, we introduce Fusion Tokens, a set of learnable tokens trained with a future prediction objective to generate unified representations for action generation. Finally, we introduce a mask-and-reconstruct strategy that masks 4D inputs to the VLM and trains the VLA to reconstruct them, enabling the VLM to learn effective 4D representations and allowing the 4D branch to be dropped at inference with minimal performance loss. Experiments in real and simulated environments show that SwiftVLA outperforms lightweight baselines and rivals VLAs up to 7 times larger, achieving comparable performance on edge devices while being 18 times faster and reducing memory footprint by 12 times.",
        "url": "http://arxiv.org/abs/2512.00903v1",
        "published_date": "2025-11-30T14:10:28+00:00",
        "updated_date": "2025-11-30T14:10:28+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Chaojun Ni",
            "Cheng Chen",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Wenzhao Zheng",
            "Boyuan Wang",
            "Tianrun Chen",
            "Guosheng Zhao",
            "Haoyun Li",
            "Zhehao Dong",
            "Qiang Zhang",
            "Yun Ye",
            "Yang Wang",
            "Guan Huang",
            "Wenjun Mei"
        ],
        "tldr": "SwiftVLA proposes a lightweight VLA architecture that uses a 4D visual geometry transformer with temporal caching and Fusion Tokens to enhance spatiotemporal reasoning while maintaining efficiency, achieving comparable performance to larger models on edge devices.",
        "tldr_zh": "SwiftVLA 提出了一种轻量级的 VLA 架构，该架构使用具有时间缓存的 4D 视觉几何 Transformer 和 Fusion Tokens 来增强时空推理，同时保持效率，在边缘设备上实现了与更大模型相当的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TrajDiff: End-to-end Autonomous Driving without Perception Annotation",
        "summary": "End-to-end autonomous driving systems directly generate driving policies from raw sensor inputs. While these systems can extract effective environmental features for planning, relying on auxiliary perception tasks, developing perception annotation-free planning paradigms has become increasingly critical due to the high cost of manual perception annotation. In this work, we propose TrajDiff, a Trajectory-oriented BEV Conditioned Diffusion framework that establishes a fully perception annotation-free generative method for end-to-end autonomous driving. TrajDiff requires only raw sensor inputs and future trajectory, constructing Gaussian BEV heatmap targets that inherently capture driving modalities. We design a simple yet effective trajectory-oriented BEV encoder to extract the TrajBEV feature without perceptual supervision. Furthermore, we introduce Trajectory-oriented BEV Diffusion Transformer (TB-DiT), which leverages ego-state information and the predicted TrajBEV features to directly generate diverse yet plausible trajectories, eliminating the need for handcrafted motion priors. Beyond architectural innovations, TrajDiff enables exploration of data scaling benefits in the annotation-free setting. Evaluated on the NAVSIM benchmark, TrajDiff achieves 87.5 PDMS, establishing state-of-the-art performance among all annotation-free methods. With data scaling, it further improves to 88.5 PDMS, which is comparable to advanced perception-based approaches. Our code and model will be made publicly available.",
        "url": "http://arxiv.org/abs/2512.00723v1",
        "published_date": "2025-11-30T04:34:20+00:00",
        "updated_date": "2025-11-30T04:34:20+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Xingtai Gui",
            "Jianbo Zhao",
            "Wencheng Han",
            "Jikai Wang",
            "Jiahao Gong",
            "Feiyang Tan",
            "Cheng-zhong Xu",
            "Jianbing Shen"
        ],
        "tldr": "The paper introduces TrajDiff, a novel end-to-end autonomous driving framework that eliminates the need for perception annotations by using a trajectory-oriented BEV diffusion model. It achieves state-of-the-art performance among annotation-free methods and approaches perception-based methods with data scaling.",
        "tldr_zh": "该论文介绍了TrajDiff，一种无需感知标注的端到端自动驾驶框架，它使用面向轨迹的BEV扩散模型。该方法在所有无标注方法中实现了最先进的性能，并通过数据扩展接近了基于感知的算法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]