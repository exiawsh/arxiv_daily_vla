[
    {
        "title": "Seeing Space and Motion: Enhancing Latent Actions with Spatial and Dynamic Awareness for VLA",
        "summary": "Latent Action Models (LAMs) enable Vision- Language-Action (VLA) systems to\nlearn semantic action rep- resentations from large-scale unannotated data. Yet,\nwe identify two bottlenecks of LAMs: 1) the commonly adopted end-to-end trained\nimage encoder suffers from poor spatial understanding; 2) LAMs can be fragile\nwhen input frames are distant, leading to limited temporal perception. Such\nfactors inevitably hinder stable and clear action modeling. To this end, we\npropose Farsighted-LAM, a latent action framework with geometry- aware spatial\nencoding and multi-scale temporal modeling, capturing structural priors and\ndynamic motion patterns from consecutive frames. We further propose SSM-VLA, an\nend- to-end VLA framework built upon Farsighted-LAM, which integrates\nstructured perception with a visual Chain-of-Thought module to explicitly\nreason about environmental dynamics, enhancing decision consistency and\ninterpretability. We validate SSM-VLA on multiple VLA tasks in both simulation\nand real- world settings, and achieve state-of-the-art performance. Our results\ndemonstrate that our strategy of combining geometry- aware modeling, temporal\ncoherence, and explicit reasoning is effective in enhancing the robustness and\ngeneralizability of embodied intelligence.",
        "url": "http://arxiv.org/abs/2509.26251v1",
        "published_date": "2025-09-30T13:41:43+00:00",
        "updated_date": "2025-09-30T13:41:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhejia Cai",
            "Yandan Yang",
            "Xinyuan Chang",
            "Shiyi Liang",
            "Ronghan Chen",
            "Feng Xiong",
            "Mu Xu",
            "Ruqi Huang"
        ],
        "tldr": "The paper introduces Farsighted-LAM and SSM-VLA, enhancing Vision-Language-Action systems by incorporating geometry-aware spatial encoding and multi-scale temporal modeling for improved spatial understanding and temporal perception, achieving state-of-the-art performance on VLA tasks.",
        "tldr_zh": "该论文介绍了Farsighted-LAM和SSM-VLA，通过结合几何感知的空间编码和多尺度时间建模来增强视觉-语言-动作系统，从而提高空间理解和时间感知能力，并在VLA任务上取得了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond Overall Accuracy: Pose- and Occlusion-driven Fairness Analysis in Pedestrian Detection for Autonomous Driving",
        "summary": "Pedestrian detection plays a critical role in autonomous driving (AD), where\nensuring safety and reliability is important. While many detection models aim\nto reduce miss-rates and handle challenges such as occlusion and long-range\nrecognition, fairness remains an underexplored yet equally important concern.\nIn this work, we systematically investigate how variations in the pedestrian\npose--including leg status, elbow status, and body orientation--as well as\nindividual joint occlusions, affect detection performance. We evaluate five\npedestrian-specific detectors (F2DNet, MGAN, ALFNet, CSP, and Cascade R-CNN)\nalongside three general-purpose models (YOLOv12 variants) on the EuroCity\nPersons Dense Pose (ECP-DP) dataset. Fairness is quantified using the Equal\nOpportunity Difference (EOD) metric across various confidence thresholds. To\nassess statistical significance and robustness, we apply the Z-test. Our\nfindings highlight biases against pedestrians with parallel legs, straight\nelbows, and lateral views. Occlusion of lower body joints has a more negative\nimpact on the detection rate compared to the upper body and head. Cascade R-CNN\nachieves the lowest overall miss-rate and exhibits the smallest bias across all\nattributes. To the best of our knowledge, this is the first comprehensive pose-\nand occlusion-aware fairness evaluation in pedestrian detection for AD.",
        "url": "http://arxiv.org/abs/2509.26166v1",
        "published_date": "2025-09-30T12:21:01+00:00",
        "updated_date": "2025-09-30T12:21:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mohammad Khoshkdahan",
            "Arman Akbari",
            "Arash Akbari",
            "Xuan Zhang"
        ],
        "tldr": "This paper investigates fairness in pedestrian detection for autonomous driving by analyzing the impact of pose variations and occlusions on detection performance across different models, finding biases related to pose and occlusion.",
        "tldr_zh": "该论文通过分析姿势变化和遮挡对不同模型检测性能的影响，研究了自动驾驶中行人检测的公平性，发现了与姿势和遮挡相关的偏差。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "EasyOcc: 3D Pseudo-Label Supervision for Fully Self-Supervised Semantic Occupancy Prediction Models",
        "summary": "Self-supervised models have recently achieved notable advancements,\nparticularly in the domain of semantic occupancy prediction. These models\nutilize sophisticated loss computation strategies to compensate for the absence\nof ground-truth labels. For instance, techniques such as novel view synthesis,\ncross-view rendering, and depth estimation have been explored to address the\nissue of semantic and depth ambiguity. However, such techniques typically incur\nhigh computational costs and memory usage during the training stage, especially\nin the case of novel view synthesis. To mitigate these issues, we propose 3D\npseudo-ground-truth labels generated by the foundation models Grounded-SAM and\nMetric3Dv2, and harness temporal information for label densification. Our 3D\npseudo-labels can be easily integrated into existing models, which yields\nsubstantial performance improvements, with mIoU increasing by 45\\%, from 9.73\nto 14.09, when implemented into the OccNeRF model. This stands in contrast to\nearlier advancements in the field, which are often not readily transferable to\nother architectures. Additionally, we propose a streamlined model, EasyOcc,\nachieving 13.86 mIoU. This model conducts learning solely from our labels,\navoiding complex rendering strategies mentioned previously. Furthermore, our\nmethod enables models to attain state-of-the-art performance when evaluated on\nthe full scene without applying the camera mask, with EasyOcc achieving 7.71\nmIoU, outperforming the previous best model by 31\\%. These findings highlight\nthe critical importance of foundation models, temporal context, and the choice\nof loss computation space in self-supervised learning for comprehensive scene\nunderstanding.",
        "url": "http://arxiv.org/abs/2509.26087v1",
        "published_date": "2025-09-30T11:01:32+00:00",
        "updated_date": "2025-09-30T11:01:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Seamie Hayes",
            "Ganesh Sistu",
            "Ciarán Eising"
        ],
        "tldr": "The paper proposes a method to generate 3D pseudo-labels using foundation models (Grounded-SAM and Metric3Dv2) and temporal information for self-supervised semantic occupancy prediction, achieving significant performance improvements with reduced computational cost.",
        "tldr_zh": "该论文提出了一种利用基础模型（Grounded-SAM和Metric3Dv2）和时间信息生成3D伪标签的方法，用于自监督语义占用预测，从而在降低计算成本的同时显著提高了性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PinPoint3D: Fine-Grained 3D Part Segmentation from a Few Clicks",
        "summary": "Fine-grained 3D part segmentation is crucial for enabling embodied AI systems\nto perform complex manipulation tasks, such as interacting with specific\nfunctional components of an object. However, existing interactive segmentation\nmethods are largely confined to coarse, instance-level targets, while\nnon-interactive approaches struggle with sparse, real-world scans and suffer\nfrom a severe lack of annotated data. To address these limitations, we\nintroduce PinPoint3D, a novel interactive framework for fine-grained,\nmulti-granularity 3D segmentation, capable of generating precise part-level\nmasks from only a few user point clicks. A key component of our work is a new\n3D data synthesis pipeline that we developed to create a large-scale,\nscene-level dataset with dense part annotations, overcoming a critical\nbottleneck that has hindered progress in this field. Through comprehensive\nexperiments and user studies, we demonstrate that our method significantly\noutperforms existing approaches, achieving an average IoU of around 55.8% on\neach object part under first-click settings and surpassing 71.3% IoU with only\na few additional clicks. Compared to current state-of-the-art baselines,\nPinPoint3D yields up to a 16% improvement in IoU and precision, highlighting\nits effectiveness on challenging, sparse point clouds with high efficiency. Our\nwork represents a significant step towards more nuanced and precise machine\nperception and interaction in complex 3D environments.",
        "url": "http://arxiv.org/abs/2509.25970v1",
        "published_date": "2025-09-30T09:05:29+00:00",
        "updated_date": "2025-09-30T09:05:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bojun Zhang",
            "Hangjian Ye",
            "Hao Zheng",
            "Jianzheng Huang",
            "Zhengyu Lin",
            "Zhenhong Guo",
            "Feng Zheng"
        ],
        "tldr": "PinPoint3D is a novel interactive 3D segmentation framework that uses a data synthesis pipeline to generate fine-grained, multi-granularity part segmentations from a few user clicks, achieving significant IoU improvements over existing methods.",
        "tldr_zh": "PinPoint3D是一种新型的交互式3D分割框架，它使用数据合成管道从几次用户点击中生成细粒度、多粒度的部件分割，与现有方法相比，实现了显著的IoU改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MuSLR: Multimodal Symbolic Logical Reasoning",
        "summary": "Multimodal symbolic logical reasoning, which aims to deduce new facts from\nmultimodal input via formal logic, is critical in high-stakes applications such\nas autonomous driving and medical diagnosis, as its rigorous, deterministic\nreasoning helps prevent serious consequences. To evaluate such capabilities of\ncurrent state-of-the-art vision language models (VLMs), we introduce the first\nbenchmark MuSLR for multimodal symbolic logical reasoning grounded in formal\nlogical rules. MuSLR comprises 1,093 instances across 7 domains, including 35\natomic symbolic logic and 976 logical combinations, with reasoning depths\nranging from 2 to 9. We evaluate 7 state-of-the-art VLMs on MuSLR and find that\nthey all struggle with multimodal symbolic reasoning, with the best model,\nGPT-4.1, achieving only 46.8%. Thus, we propose LogiCAM, a modular framework\nthat applies formal logical rules to multimodal inputs, boosting GPT-4.1's\nChain-of-Thought performance by 14.13%, and delivering even larger gains on\ncomplex logics such as first-order logic. We also conduct a comprehensive error\nanalysis, showing that around 70% of failures stem from logical misalignment\nbetween modalities, offering key insights to guide future improvements. All\ndata and code are publicly available at https://llm-symbol.github.io/MuSLR.",
        "url": "http://arxiv.org/abs/2509.25851v1",
        "published_date": "2025-09-30T06:42:20+00:00",
        "updated_date": "2025-09-30T06:42:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jundong Xu",
            "Hao Fei",
            "Yuhui Zhang",
            "Liangming Pan",
            "Qijun Huang",
            "Qian Liu",
            "Preslav Nakov",
            "Min-Yen Kan",
            "William Yang Wang",
            "Mong-Li Lee",
            "Wynne Hsu"
        ],
        "tldr": "The paper introduces MuSLR, a new benchmark for multimodal symbolic logical reasoning, finds that existing VLMs perform poorly on it, and proposes LogiCAM, a framework to improve their performance.",
        "tldr_zh": "该论文介绍了一个新的多模态符号逻辑推理基准测试 MuSLR，发现现有的 VLM 在该基准测试上表现不佳，并提出了 LogiCAM 框架来提高它们的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Online Mapping for Autonomous Driving: Addressing Sensor Generalization and Dynamic Map Updates in Campus Environments",
        "summary": "High-definition (HD) maps are essential for autonomous driving, providing\nprecise information such as road boundaries, lane dividers, and crosswalks to\nenable safe and accurate navigation. However, traditional HD map generation is\nlabor-intensive, expensive, and difficult to maintain in dynamic environments.\nTo overcome these challenges, we present a real-world deployment of an online\nmapping system on a campus golf cart platform equipped with dual front cameras\nand a LiDAR sensor. Our work tackles three core challenges: (1) labeling a 3D\nHD map for campus environment; (2) integrating and generalizing the SemVecMap\nmodel onboard; and (3) incrementally generating and updating the predicted HD\nmap to capture environmental changes. By fine-tuning with campus-specific data,\nour pipeline produces accurate map predictions and supports continual updates,\ndemonstrating its practical value in real-world autonomous driving scenarios.",
        "url": "http://arxiv.org/abs/2509.25542v1",
        "published_date": "2025-09-29T21:56:29+00:00",
        "updated_date": "2025-09-29T21:56:29+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Zihan Zhang",
            "Abhijit Ravichandran",
            "Pragnya Korti",
            "Luobin Wang",
            "Henrik I. Christensen"
        ],
        "tldr": "This paper presents a real-world online mapping system for autonomous driving on a campus environment, addressing challenges in labeling, generalization, and incremental updating of HD maps using cameras and LiDAR.",
        "tldr_zh": "本文提出了一个在校园环境中用于自动驾驶的实时在线地图构建系统，使用相机和激光雷达解决高清地图的标注、泛化和增量更新等挑战。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LLM-RG: Referential Grounding in Outdoor Scenarios using Large Language Models",
        "summary": "Referential grounding in outdoor driving scenes is challenging due to large\nscene variability, many visually similar objects, and dynamic elements that\ncomplicate resolving natural-language references (e.g., \"the black car on the\nright\"). We propose LLM-RG, a hybrid pipeline that combines off-the-shelf\nvision-language models for fine-grained attribute extraction with large\nlanguage models for symbolic reasoning. LLM-RG processes an image and a\nfree-form referring expression by using an LLM to extract relevant object types\nand attributes, detecting candidate regions, generating rich visual descriptors\nwith a VLM, and then combining these descriptors with spatial metadata into\nnatural-language prompts that are input to an LLM for chain-of-thought\nreasoning to identify the referent's bounding box. Evaluated on the Talk2Car\nbenchmark, LLM-RG yields substantial gains over both LLM and VLM-based\nbaselines. Additionally, our ablations show that adding 3D spatial cues further\nimproves grounding. Our results demonstrate the complementary strengths of VLMs\nand LLMs, applied in a zero-shot manner, for robust outdoor referential\ngrounding.",
        "url": "http://arxiv.org/abs/2509.25528v1",
        "published_date": "2025-09-29T21:32:54+00:00",
        "updated_date": "2025-09-29T21:32:54+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Pranav Saxena",
            "Avigyan Bhattacharya",
            "Ji Zhang",
            "Wenshan Wang"
        ],
        "tldr": "The paper introduces LLM-RG, a hybrid LLM/VLM pipeline for referential grounding in outdoor driving scenes, leveraging chain-of-thought reasoning to identify objects based on natural language descriptions and visual cues. It achieves significant gains on the Talk2Car benchmark.",
        "tldr_zh": "该论文介绍了LLM-RG，一个用于户外驾驶场景中指称对象定位的混合LLM/VLM流水线，利用思维链推理来根据自然语言描述和视觉线索识别对象。在Talk2Car基准测试中取得了显著的提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]