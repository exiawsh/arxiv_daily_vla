[
    {
        "title": "FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM",
        "summary": "We present FoundationSLAM, a learning-based monocular dense SLAM system that addresses the absence of geometric consistency in previous flow-based approaches for accurate and robust tracking and mapping. Our core idea is to bridge flow estimation with geometric reasoning by leveraging the guidance from foundation depth models. To this end, we first develop a Hybrid Flow Network that produces geometry-aware correspondences, enabling consistent depth and pose inference across diverse keyframes. To enforce global consistency, we propose a Bi-Consistent Bundle Adjustment Layer that jointly optimizes keyframe pose and depth under multi-view constraints. Furthermore, we introduce a Reliability-Aware Refinement mechanism that dynamically adapts the flow update process by distinguishing between reliable and uncertain regions, forming a closed feedback loop between matching and optimization. Extensive experiments demonstrate that FoundationSLAM achieves superior trajectory accuracy and dense reconstruction quality across multiple challenging datasets, while running in real-time at 18 FPS, demonstrating strong generalization to various scenarios and practical applicability of our method.",
        "url": "http://arxiv.org/abs/2512.25008v1",
        "published_date": "2025-12-31T17:57:45+00:00",
        "updated_date": "2025-12-31T17:57:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuchen Wu",
            "Jiahe Li",
            "Fabio Tosi",
            "Matteo Poggi",
            "Jin Zheng",
            "Xiao Bai"
        ],
        "tldr": "FoundationSLAM leverages foundation depth models to improve the geometric consistency of flow-based monocular dense SLAM, achieving real-time performance and superior accuracy in challenging datasets.",
        "tldr_zh": "FoundationSLAM利用基础深度模型来提高基于光流的单目稠密SLAM的几何一致性，在具有挑战性的数据集中实现了实时性能和卓越的准确性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Semi-Supervised Diversity-Aware Domain Adaptation for 3D Object detection",
        "summary": "3D object detectors are fundamental components of perception systems in autonomous vehicles. While these detectors achieve remarkable performance on standard autonomous driving benchmarks, they often struggle to generalize across different domains - for instance, a model trained in the U.S. may perform poorly in regions like Asia or Europe. This paper presents a novel lidar domain adaptation method based on neuron activation patterns, demonstrating that state-of-the-art performance can be achieved by annotating only a small, representative, and diverse subset of samples from the target domain if they are correctly selected. The proposed approach requires very small annotation budget and, when combined with post-training techniques inspired by continual learning prevent weight drift from the original model. Empirical evaluation shows that the proposed domain adaptation approach outperforms both linear probing and state-of-the-art domain adaptation techniques.",
        "url": "http://arxiv.org/abs/2512.24922v1",
        "published_date": "2025-12-31T15:26:09+00:00",
        "updated_date": "2025-12-31T15:26:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bartłomiej Olber",
            "Jakub Winter",
            "Paweł Wawrzyński",
            "Andrii Gamalii",
            "Daniel Górniak",
            "Marcin Łojek",
            "Robert Nowak",
            "Krystian Radlak"
        ],
        "tldr": "This paper introduces a semi-supervised domain adaptation method for 3D object detection in autonomous driving using neuron activation patterns and a small, diverse subset of labeled target domain data, achieving SOTA performance with a minimal annotation budget.",
        "tldr_zh": "本文提出了一种半监督域适应方法，用于自动驾驶中的3D物体检测，该方法利用神经元激活模式和来自目标域的一小部分多样化标记数据，以最小的标注预算实现SOTA性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents",
        "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across a wide range of vision-language tasks. However, their performance as embodied agents, which requires multi-round dialogue spatial reasoning and sequential action prediction, needs further exploration. Our work investigates this potential in the context of Vision-and-Language Navigation (VLN) by introducing a unified and extensible evaluation framework to probe MLLMs as zero-shot agents by bridging traditional navigation datasets into a standardized benchmark, named VLN-MME. We simplify the evaluation with a highly modular and accessible design. This flexibility streamlines experiments, enabling structured comparisons and component-level ablations across diverse MLLM architectures, agent designs, and navigation tasks. Crucially, enabled by our framework, we observe that enhancing our baseline agent with Chain-of-Thought (CoT) reasoning and self-reflection leads to an unexpected performance decrease. This suggests MLLMs exhibit poor context awareness in embodied navigation tasks; although they can follow instructions and structure their output, their 3D spatial reasoning fidelity is low. VLN-MME lays the groundwork for systematic evaluation of general-purpose MLLMs in embodied navigation settings and reveals limitations in their sequential decision-making capabilities. We believe these findings offer crucial guidance for MLLM post-training as embodied agents.",
        "url": "http://arxiv.org/abs/2512.24851v1",
        "published_date": "2025-12-31T13:21:21+00:00",
        "updated_date": "2025-12-31T13:21:21+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Xunyi Zhao",
            "Gengze Zhou",
            "Qi Wu"
        ],
        "tldr": "The paper introduces VLN-MME, a framework for evaluating MLLMs in vision-and-language navigation, revealing limitations in their spatial reasoning and sequential decision-making capabilities even with Chain-of-Thought prompting.",
        "tldr_zh": "该论文介绍了VLN-MME，一个用于评估MLLM在视觉语言导航中的框架，揭示了它们在空间推理和顺序决策能力方面的局限性，即使使用思维链提示也是如此。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow",
        "summary": "Generative video modeling has emerged as a compelling tool to zero-shot reason about plausible physical interactions for open-world manipulation. Yet, it remains a challenge to translate such human-led motions into the low-level actions demanded by robotic systems. We observe that given an initial image and task instruction, these models excel at synthesizing sensible object motions. Thus, we introduce Dream2Flow, a framework that bridges video generation and robotic control through 3D object flow as an intermediate representation. Our method reconstructs 3D object motions from generated videos and formulates manipulation as object trajectory tracking. By separating the state changes from the actuators that realize those changes, Dream2Flow overcomes the embodiment gap and enables zero-shot guidance from pre-trained video models to manipulate objects of diverse categories-including rigid, articulated, deformable, and granular. Through trajectory optimization or reinforcement learning, Dream2Flow converts reconstructed 3D object flow into executable low-level commands without task-specific demonstrations. Simulation and real-world experiments highlight 3D object flow as a general and scalable interface for adapting video generation models to open-world robotic manipulation. Videos and visualizations are available at https://dream2flow.github.io/.",
        "url": "http://arxiv.org/abs/2512.24766v1",
        "published_date": "2025-12-31T10:25:24+00:00",
        "updated_date": "2025-12-31T10:25:24+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Karthik Dharmarajan",
            "Wenlong Huang",
            "Jiajun Wu",
            "Li Fei-Fei",
            "Ruohan Zhang"
        ],
        "tldr": "Dream2Flow uses 3D object flow as an intermediate representation to bridge video generation models and robotic manipulation, enabling zero-shot guidance for manipulating diverse objects by converting generated motions into executable low-level commands.",
        "tldr_zh": "Dream2Flow 使用 3D 对象流作为中间表示，桥接视频生成模型和机器人操作，通过将生成的运动转换为可执行的底层命令，实现对各种对象进行零样本引导操作。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Lifting Vision: Ground to Aerial Localization with Reasoning Guided Planning",
        "summary": "Multimodal intelligence development recently show strong progress in visual understanding and high level reasoning. Though, most reasoning system still reply on textual information as the main medium for inference. This limit their effectiveness in spatial tasks such as visual navigation and geo-localization. This work discuss about the potential scope of this field and eventually propose an idea visual reasoning paradigm Geo-Consistent Visual Planning, our introduced framework called Visual Reasoning for Localization, or ViReLoc, which performs planning and localization using only visual representations. The proposed framework learns spatial dependencies and geometric relations that text based reasoning often suffer to understand. By encoding step by step inference in the visual domain and optimizing with reinforcement based objectives, ViReLoc plans routes between two given ground images. The system also integrates contrastive learning and adaptive feature interaction to align cross view perspectives and reduce viewpoint differences. Experiments across diverse navigation and localization scenarios show consistent improvements in spatial reasoning accuracy and cross view retrieval performance. These results establish visual reasoning as a strong complementary approach for navigation and localization, and show that such tasks can be performed without real time global positioning system data, leading to more secure navigation solutions.",
        "url": "http://arxiv.org/abs/2512.24404v1",
        "published_date": "2025-12-30T18:36:39+00:00",
        "updated_date": "2025-12-30T18:36:39+00:00",
        "categories": [
            "cs.LG",
            "cs.CV"
        ],
        "authors": [
            "Soham Pahari",
            "M. Srinivas"
        ],
        "tldr": "The paper introduces ViReLoc, a visual reasoning framework for ground-to-aerial localization and navigation using reinforcement learning and contrastive learning, showing improvements in spatial reasoning without GPS data.",
        "tldr_zh": "本文介绍了一个名为ViReLoc的视觉推理框架，用于地面到空中定位和导航，该框架使用强化学习和对比学习，并在没有GPS数据的情况下，在空间推理方面表现出改进。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 7,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Video and Language Alignment in 2D Systems for 3D Multi-object Scenes with Multi-Information Derivative-Free Control",
        "summary": "Cross-modal systems trained on 2D visual inputs are presented with a dimensional shift when processing 3D scenes. An in-scene camera bridges the dimensionality gap but requires learning a control module. We introduce a new method that improves multivariate mutual information estimates by regret minimisation with derivative-free optimisation. Our algorithm enables off-the-shelf cross-modal systems trained on 2D visual inputs to adapt online to object occlusions and differentiate features. The pairing of expressive measures and value-based optimisation assists control of an in-scene camera to learn directly from the noisy outputs of vision-language models. The resulting pipeline improves performance in cross-modal tasks on multi-object 3D scenes without resorting to pretraining or finetuning.",
        "url": "http://arxiv.org/abs/2512.24826v1",
        "published_date": "2025-12-31T12:39:03+00:00",
        "updated_date": "2025-12-31T12:39:03+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jason Armitage",
            "Rico Sennnrich"
        ],
        "tldr": "This paper introduces a derivative-free optimization method for bridging the gap between 2D-trained vision-language models and 3D multi-object scenes by controlling an in-scene camera, improving cross-modal task performance without pretraining or finetuning.",
        "tldr_zh": "该论文提出了一种无导数优化方法，通过控制场景内摄像头来弥合2D训练的视觉语言模型和3D多对象场景之间的差距，从而在无需预训练或微调的情况下提高跨模态任务的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]