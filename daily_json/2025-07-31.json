[
    {
        "title": "DepR: Depth Guided Single-view Scene Reconstruction with Instance-level Diffusion",
        "summary": "We propose DepR, a depth-guided single-view scene reconstruction framework\nthat integrates instance-level diffusion within a compositional paradigm.\nInstead of reconstructing the entire scene holistically, DepR generates\nindividual objects and subsequently composes them into a coherent 3D layout.\nUnlike previous methods that use depth solely for object layout estimation\nduring inference and therefore fail to fully exploit its rich geometric\ninformation, DepR leverages depth throughout both training and inference.\nSpecifically, we introduce depth-guided conditioning to effectively encode\nshape priors into diffusion models. During inference, depth further guides DDIM\nsampling and layout optimization, enhancing alignment between the\nreconstruction and the input image. Despite being trained on limited synthetic\ndata, DepR achieves state-of-the-art performance and demonstrates strong\ngeneralization in single-view scene reconstruction, as shown through\nevaluations on both synthetic and real-world datasets.",
        "url": "http://arxiv.org/abs/2507.22825v1",
        "published_date": "2025-07-30T16:40:46+00:00",
        "updated_date": "2025-07-30T16:40:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qingcheng Zhao",
            "Xiang Zhang",
            "Haiyang Xu",
            "Zeyuan Chen",
            "Jianwen Xie",
            "Yuan Gao",
            "Zhuowen Tu"
        ],
        "tldr": "DepR uses depth guidance and instance-level diffusion for single-view 3D scene reconstruction, achieving state-of-the-art performance even with limited synthetic training data and demonstrating strong generalization.",
        "tldr_zh": "DepR 利用深度引导和实例级别的扩散进行单视图3D场景重建，即使在有限的合成训练数据下也能实现最先进的性能，并表现出强大的泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Social-Pose: Enhancing Trajectory Prediction with Human Body Pose",
        "summary": "Accurate human trajectory prediction is one of the most crucial tasks for\nautonomous driving, ensuring its safety. Yet, existing models often fail to\nfully leverage the visual cues that humans subconsciously communicate when\nnavigating the space. In this work, we study the benefits of predicting human\ntrajectories using human body poses instead of solely their Cartesian space\nlocations in time. We propose `Social-pose', an attention-based pose encoder\nthat effectively captures the poses of all humans in a scene and their social\nrelations. Our method can be integrated into various trajectory prediction\narchitectures. We have conducted extensive experiments on state-of-the-art\nmodels (based on LSTM, GAN, MLP, and Transformer), and showed improvements over\nall of them on synthetic (Joint Track Auto) and real (Human3.6M, Pedestrians\nand Cyclists in Road Traffic, and JRDB) datasets. We also explored the\nadvantages of using 2D versus 3D poses, as well as the effect of noisy poses\nand the application of our pose-based predictor in robot navigation scenarios.",
        "url": "http://arxiv.org/abs/2507.22742v1",
        "published_date": "2025-07-30T14:58:48+00:00",
        "updated_date": "2025-07-30T14:58:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yang Gao",
            "Saeed Saadatnejad",
            "Alexandre Alahi"
        ],
        "tldr": "The paper introduces 'Social-pose,' a pose-based trajectory prediction method using an attention-based pose encoder, demonstrating improvements across various architectures and datasets.",
        "tldr_zh": "该论文介绍了'Social-pose'，一种基于姿势的轨迹预测方法，使用基于注意力的姿势编码器，并在各种架构和数据集上展示了改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generative Active Learning for Long-tail Trajectory Prediction via Controllable Diffusion Model",
        "summary": "While data-driven trajectory prediction has enhanced the reliability of\nautonomous driving systems, it still struggles with rarely observed long-tail\nscenarios. Prior works addressed this by modifying model architectures, such as\nusing hypernetworks. In contrast, we propose refining the training process to\nunlock each model's potential without altering its structure. We introduce\nGenerative Active Learning for Trajectory prediction (GALTraj), the first\nmethod to successfully deploy generative active learning into trajectory\nprediction. It actively identifies rare tail samples where the model fails and\naugments these samples with a controllable diffusion model during training. In\nour framework, generating scenarios that are diverse, realistic, and preserve\ntail-case characteristics is paramount. Accordingly, we design a tail-aware\ngeneration method that applies tailored diffusion guidance to generate\ntrajectories that both capture rare behaviors and respect traffic rules. Unlike\nprior simulation methods focused solely on scenario diversity, GALTraj is the\nfirst to show how simulator-driven augmentation benefits long-tail learning in\ntrajectory prediction. Experiments on multiple trajectory datasets (WOMD,\nArgoverse2) with popular backbones (QCNet, MTR) confirm that our method\nsignificantly boosts performance on tail samples and also enhances accuracy on\nhead samples.",
        "url": "http://arxiv.org/abs/2507.22615v1",
        "published_date": "2025-07-30T12:36:05+00:00",
        "updated_date": "2025-07-30T12:36:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Daehee Park",
            "Monu Surana",
            "Pranav Desai",
            "Ashish Mehta",
            "Reuben MV John",
            "Kuk-Jin Yoon"
        ],
        "tldr": "This paper introduces GALTraj, a generative active learning method using a controllable diffusion model to augment rare long-tail trajectory data, improving performance on both tail and head samples in trajectory prediction for autonomous driving.",
        "tldr_zh": "本文介绍了GALTraj，一种生成式主动学习方法，它使用可控扩散模型来扩充罕见的尾部轨迹数据，从而提高自动驾驶轨迹预测中头部和尾部样本的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Recognizing Actions from Robotic View for Natural Human-Robot Interaction",
        "summary": "Natural Human-Robot Interaction (N-HRI) requires robots to recognize human\nactions at varying distances and states, regardless of whether the robot itself\nis in motion or stationary. This setup is more flexible and practical than\nconventional human action recognition tasks. However, existing benchmarks\ndesigned for traditional action recognition fail to address the unique\ncomplexities in N-HRI due to limited data, modalities, task categories, and\ndiversity of subjects and environments. To address these challenges, we\nintroduce ACTIVE (Action from Robotic View), a large-scale dataset tailored\nspecifically for perception-centric robotic views prevalent in mobile service\nrobots. ACTIVE comprises 30 composite action categories, 80 participants, and\n46,868 annotated video instances, covering both RGB and point cloud modalities.\nParticipants performed various human actions in diverse environments at\ndistances ranging from 3m to 50m, while the camera platform was also mobile,\nsimulating real-world scenarios of robot perception with varying camera heights\ndue to uneven ground. This comprehensive and challenging benchmark aims to\nadvance action and attribute recognition research in N-HRI. Furthermore, we\npropose ACTIVE-PC, a method that accurately perceives human actions at long\ndistances using Multilevel Neighborhood Sampling, Layered Recognizers, Elastic\nEllipse Query, and precise decoupling of kinematic interference from human\nactions. Experimental results demonstrate the effectiveness of ACTIVE-PC. Our\ncode is available at:\nhttps://github.com/wangzy01/ACTIVE-Action-from-Robotic-View.",
        "url": "http://arxiv.org/abs/2507.22522v1",
        "published_date": "2025-07-30T09:48:34+00:00",
        "updated_date": "2025-07-30T09:48:34+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Ziyi Wang",
            "Peiming Li",
            "Hong Liu",
            "Zhichao Deng",
            "Can Wang",
            "Jun Liu",
            "Junsong Yuan",
            "Mengyuan Liu"
        ],
        "tldr": "The paper introduces ACTIVE, a large-scale RGB and point cloud dataset for human action recognition from a robotic perspective, addressing limitations of existing benchmarks in Natural Human-Robot Interaction (N-HRI). It also proposes ACTIVE-PC, a method for accurate long-distance human action recognition and demonstrates its effectiveness.",
        "tldr_zh": "该论文介绍了ACTIVE，一个用于从机器人视角进行人体动作识别的大规模RGB和点云数据集，解决了现有自然人机交互（N-HRI）基准测试的局限性。同时，提出了ACTIVE-PC，一种用于精确远距离人体动作识别的方法，并验证了其有效性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TopoLiDM: Topology-Aware LiDAR Diffusion Models for Interpretable and Realistic LiDAR Point Cloud Generation",
        "summary": "LiDAR scene generation is critical for mitigating real-world LiDAR data\ncollection costs and enhancing the robustness of downstream perception tasks in\nautonomous driving. However, existing methods commonly struggle to capture\ngeometric realism and global topological consistency. Recent LiDAR Diffusion\nModels (LiDMs) predominantly embed LiDAR points into the latent space for\nimproved generation efficiency, which limits their interpretable ability to\nmodel detailed geometric structures and preserve global topological\nconsistency. To address these challenges, we propose TopoLiDM, a novel\nframework that integrates graph neural networks (GNNs) with diffusion models\nunder topological regularization for high-fidelity LiDAR generation. Our\napproach first trains a topological-preserving VAE to extract latent graph\nrepresentations by graph construction and multiple graph convolutional layers.\nThen we freeze the VAE and generate novel latent topological graphs through the\nlatent diffusion models. We also introduce 0-dimensional persistent homology\n(PH) constraints, ensuring the generated LiDAR scenes adhere to real-world\nglobal topological structures. Extensive experiments on the KITTI-360 dataset\ndemonstrate TopoLiDM's superiority over state-of-the-art methods, achieving\nimprovements of 22.6% lower Frechet Range Image Distance (FRID) and 9.2% lower\nMinimum Matching Distance (MMD). Notably, our model also enables fast\ngeneration speed with an average inference time of 1.68 samples/s, showcasing\nits scalability for real-world applications. We will release the related codes\nat https://github.com/IRMVLab/TopoLiDM.",
        "url": "http://arxiv.org/abs/2507.22454v1",
        "published_date": "2025-07-30T08:02:42+00:00",
        "updated_date": "2025-07-30T08:02:42+00:00",
        "categories": [
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Jiuming Liu",
            "Zheng Huang",
            "Mengmeng Liu",
            "Tianchen Deng",
            "Francesco Nex",
            "Hao Cheng",
            "Hesheng Wang"
        ],
        "tldr": "TopoLiDM uses topological regularization and graph neural networks within a diffusion model framework to generate high-fidelity and interpretable LiDAR point clouds, demonstrating improved performance and speed over existing methods.",
        "tldr_zh": "TopoLiDM 在扩散模型框架内使用拓扑正则化和图神经网络来生成高保真和可解释的 LiDAR 点云，与现有方法相比，在性能和速度上都有所提高。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Temporally Consistent Unsupervised Segmentation for Mobile Robot Perception",
        "summary": "Rapid progress in terrain-aware autonomous ground navigation has been driven\nby advances in supervised semantic segmentation. However, these methods rely on\ncostly data collection and labor-intensive ground truth labeling to train deep\nmodels. Furthermore, autonomous systems are increasingly deployed in\nunrehearsed, unstructured environments where no labeled data exists and\nsemantic categories may be ambiguous or domain-specific. Recent zero-shot\napproaches to unsupervised segmentation have shown promise in such settings but\ntypically operate on individual frames, lacking temporal consistency-a critical\nproperty for robust perception in unstructured environments. To address this\ngap we introduce Frontier-Seg, a method for temporally consistent unsupervised\nsegmentation of terrain from mobile robot video streams. Frontier-Seg clusters\nsuperpixel-level features extracted from foundation model\nbackbones-specifically DINOv2-and enforces temporal consistency across frames\nto identify persistent terrain boundaries or frontiers without human\nsupervision. We evaluate Frontier-Seg on a diverse set of benchmark\ndatasets-including RUGD and RELLIS-3D-demonstrating its ability to perform\nunsupervised segmentation across unstructured off-road environments.",
        "url": "http://arxiv.org/abs/2507.22194v1",
        "published_date": "2025-07-29T19:41:37+00:00",
        "updated_date": "2025-07-29T19:41:37+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Christian Ellis",
            "Maggie Wigness",
            "Craig Lennon",
            "Lance Fiondella"
        ],
        "tldr": "The paper introduces Frontier-Seg, a method for temporally consistent unsupervised terrain segmentation from mobile robot video streams using DINOv2 features and temporal consistency to identify terrain boundaries without human supervision.",
        "tldr_zh": "该论文介绍了Frontier-Seg，一种利用DINOv2特征和时间一致性从移动机器人视频流中进行时间一致的无监督地形分割方法，无需人工监督即可识别地形边界。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]