[
    {
        "title": "Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection",
        "summary": "Recent advances in foundation models have opened up new possibilities for\nenhancing 3D perception. In particular, DepthAnything offers dense and reliable\ngeometric priors from monocular RGB images, which can complement sparse LiDAR\ndata in autonomous driving scenarios. However, such priors remain underutilized\nin LiDAR-based 3D object detection. In this paper, we address the limited\nexpressiveness of raw LiDAR point features, especially the weak discriminative\ncapability of the reflectance attribute, by introducing depth priors predicted\nby DepthAnything. These priors are fused with the original LiDAR attributes to\nenrich each point's representation. To leverage the enhanced point features, we\npropose a point-wise feature extraction module. Then, a Dual-Path RoI feature\nextraction framework is employed, comprising a voxel-based branch for global\nsemantic context and a point-based branch for fine-grained structural details.\nTo effectively integrate the complementary RoI features, we introduce a\nbidirectional gated RoI feature fusion module that balances global and local\ncues. Extensive experiments on the KITTI benchmark show that our method\nconsistently improves detection accuracy, demonstrating the value of\nincorporating visual foundation model priors into LiDAR-based 3D object\ndetection.",
        "url": "http://arxiv.org/abs/2507.13899v1",
        "published_date": "2025-07-18T13:24:32+00:00",
        "updated_date": "2025-07-18T13:24:32+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yujian Mo",
            "Yan Wu",
            "Junqiao Zhao",
            "Jijun Wang",
            "Yinghao Hu",
            "Jun Yan"
        ],
        "tldr": "This paper enhances LiDAR-based 3D object detection by fusing depth priors from DepthAnything with LiDAR point features and proposes a novel feature extraction and fusion framework, achieving improved detection accuracy on the KITTI benchmark.",
        "tldr_zh": "该论文通过融合DepthAnything的深度先验与激光雷达点特征来增强基于激光雷达的3D目标检测，并提出了一个新的特征提取和融合框架，在KITTI基准测试上实现了更高的检测精度。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised Monocular Depth Estimation",
        "summary": "Monocular 3D lane detection is essential for autonomous driving, but\nchallenging due to the inherent lack of explicit spatial information.\nMulti-modal approaches rely on expensive depth sensors, while methods\nincorporating fully-supervised depth networks rely on ground-truth depth data\nthat is impractical to collect at scale. Additionally, existing methods assume\nthat camera parameters are available, limiting their applicability in scenarios\nlike crowdsourced high-definition (HD) lane mapping. To address these\nlimitations, we propose Depth3DLane, a novel dual-pathway framework that\nintegrates self-supervised monocular depth estimation to provide explicit\nstructural information, without the need for expensive sensors or additional\nground-truth depth data. Leveraging a self-supervised depth network to obtain a\npoint cloud representation of the scene, our bird's-eye view pathway extracts\nexplicit spatial information, while our front view pathway simultaneously\nextracts rich semantic information. Depth3DLane then uses 3D lane anchors to\nsample features from both pathways and infer accurate 3D lane geometry.\nFurthermore, we extend the framework to predict camera parameters on a\nper-frame basis and introduce a theoretically motivated fitting procedure to\nenhance stability on a per-segment basis. Extensive experiments demonstrate\nthat Depth3DLane achieves competitive performance on the OpenLane benchmark\ndataset. Furthermore, experimental results show that using learned parameters\ninstead of ground-truth parameters allows Depth3DLane to be applied in\nscenarios where camera calibration is infeasible, unlike previous methods.",
        "url": "http://arxiv.org/abs/2507.13857v1",
        "published_date": "2025-07-18T12:23:47+00:00",
        "updated_date": "2025-07-18T12:23:47+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Max van den Hoven",
            "Kishaan Jeeveswaran",
            "Pieter Piscaer",
            "Thijs Wensveen",
            "Elahe Arani",
            "Bahram Zonooz"
        ],
        "tldr": "The paper introduces Depth3DLane, a framework for monocular 3D lane detection that fuses self-supervised depth estimation to overcome limitations of existing methods by eliminating the need for expensive sensors or ground truth depth data, while also addressing the challenge of unknown camera parameters.",
        "tldr_zh": "该论文介绍了Depth3DLane，一个用于单目3D车道线检测的框架，它融合了自监督深度估计，克服了现有方法的局限性，无需昂贵的传感器或真实深度数据，同时解决了未知相机参数的挑战。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "One Step Closer: Creating the Future to Boost Monocular Semantic Scene Completion",
        "summary": "In recent years, visual 3D Semantic Scene Completion (SSC) has emerged as a\ncritical perception task for autonomous driving due to its ability to infer\ncomplete 3D scene layouts and semantics from single 2D images. However, in\nreal-world traffic scenarios, a significant portion of the scene remains\noccluded or outside the camera's field of view -- a fundamental challenge that\nexisting monocular SSC methods fail to address adequately. To overcome these\nlimitations, we propose Creating the Future SSC (CF-SSC), a novel temporal SSC\nframework that leverages pseudo-future frame prediction to expand the model's\neffective perceptual range. Our approach combines poses and depths to establish\naccurate 3D correspondences, enabling geometrically-consistent fusion of past,\npresent, and predicted future frames in 3D space. Unlike conventional methods\nthat rely on simple feature stacking, our 3D-aware architecture achieves more\nrobust scene completion by explicitly modeling spatial-temporal relationships.\nComprehensive experiments on SemanticKITTI and SSCBench-KITTI-360 benchmarks\ndemonstrate state-of-the-art performance, validating the effectiveness of our\napproach, highlighting our method's ability to improve occlusion reasoning and\n3D scene completion accuracy.",
        "url": "http://arxiv.org/abs/2507.13801v1",
        "published_date": "2025-07-18T10:24:58+00:00",
        "updated_date": "2025-07-18T10:24:58+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Haoang Lu",
            "Yuanqi Su",
            "Xiaoning Zhang",
            "Hao Hu"
        ],
        "tldr": "The paper introduces CF-SSC, a novel temporal Semantic Scene Completion framework that uses pseudo-future frame prediction to enhance 3D scene understanding from monocular images, achieving state-of-the-art results on standard benchmarks.",
        "tldr_zh": "该论文介绍了CF-SSC，一种新颖的时序语义场景补全框架，它利用伪未来帧预测来增强从单目图像中对3D场景的理解，并在标准基准测试中取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HeCoFuse: Cross-Modal Complementary V2X Cooperative Perception with Heterogeneous Sensors",
        "summary": "Real-world Vehicle-to-Everything (V2X) cooperative perception systems often\noperate under heterogeneous sensor configurations due to cost constraints and\ndeployment variability across vehicles and infrastructure. This heterogeneity\nposes significant challenges for feature fusion and perception reliability. To\naddress these issues, we propose HeCoFuse, a unified framework designed for\ncooperative perception across mixed sensor setups where nodes may carry Cameras\n(C), LiDARs (L), or both. By introducing a hierarchical fusion mechanism that\nadaptively weights features through a combination of channel-wise and spatial\nattention, HeCoFuse can tackle critical challenges such as cross-modality\nfeature misalignment and imbalanced representation quality. In addition, an\nadaptive spatial resolution adjustment module is employed to balance\ncomputational cost and fusion effectiveness. To enhance robustness across\ndifferent configurations, we further implement a cooperative learning strategy\nthat dynamically adjusts fusion type based on available modalities. Experiments\non the real-world TUMTraf-V2X dataset demonstrate that HeCoFuse achieves 43.22%\n3D mAP under the full sensor configuration (LC+LC), outperforming the CoopDet3D\nbaseline by 1.17%, and reaches an even higher 43.38% 3D mAP in the L+LC\nscenario, while maintaining 3D mAP in the range of 21.74% to 43.38% across nine\nheterogeneous sensor configurations. These results, validated by our\nfirst-place finish in the CVPR 2025 DriveX challenge, establish HeCoFuse as the\ncurrent state-of-the-art on TUM-Traf V2X dataset while demonstrating robust\nperformance across diverse sensor deployments.",
        "url": "http://arxiv.org/abs/2507.13677v1",
        "published_date": "2025-07-18T06:02:22+00:00",
        "updated_date": "2025-07-18T06:02:22+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.MM"
        ],
        "authors": [
            "Chuheng Wei",
            "Ziye Qin",
            "Walter Zimmer",
            "Guoyuan Wu",
            "Matthew J. Barth"
        ],
        "tldr": "HeCoFuse is a new cooperative perception framework designed for heterogeneous sensor configurations (Camera, LiDAR) in V2X environments, achieving state-of-the-art performance on the TUM-Traf V2X dataset.",
        "tldr_zh": "HeCoFuse是一个新的协同感知框架，专为V2X环境中异构传感器配置（相机、激光雷达）而设计，并在TUM-Traf V2X数据集上实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Moving Object Detection from Moving Camera Using Focus of Expansion Likelihood and Segmentation",
        "summary": "Separating moving and static objects from a moving camera viewpoint is\nessential for 3D reconstruction, autonomous navigation, and scene understanding\nin robotics. Existing approaches often rely primarily on optical flow, which\nstruggles to detect moving objects in complex, structured scenes involving\ncamera motion. To address this limitation, we propose Focus of Expansion\nLikelihood and Segmentation (FoELS), a method based on the core idea of\nintegrating both optical flow and texture information. FoELS computes the focus\nof expansion (FoE) from optical flow and derives an initial motion likelihood\nfrom the outliers of the FoE computation. This likelihood is then fused with a\nsegmentation-based prior to estimate the final moving probability. The method\neffectively handles challenges including complex structured scenes, rotational\ncamera motion, and parallel motion. Comprehensive evaluations on the DAVIS 2016\ndataset and real-world traffic videos demonstrate its effectiveness and\nstate-of-the-art performance.",
        "url": "http://arxiv.org/abs/2507.13628v1",
        "published_date": "2025-07-18T03:40:44+00:00",
        "updated_date": "2025-07-18T03:40:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Masahiro Ogawa",
            "Qi An",
            "Atsushi Yamashita"
        ],
        "tldr": "The paper introduces Focus of Expansion Likelihood and Segmentation (FoELS), a novel method for moving object detection from a moving camera that integrates optical flow and texture information, demonstrating state-of-the-art performance on standard datasets.",
        "tldr_zh": "该论文介绍了一种名为焦点扩展可能性和分割(FoELS)的新方法，用于从移动的摄像机中检测移动物体。该方法集成了光流和纹理信息，并在标准数据集上表现出最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]