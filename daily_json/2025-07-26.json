[
    {
        "title": "EffiComm: Bandwidth Efficient Multi Agent Communication",
        "summary": "Collaborative perception allows connected vehicles to exchange sensor\ninformation and overcome each vehicle's blind spots. Yet transmitting raw point\nclouds or full feature maps overwhelms Vehicle-to-Vehicle (V2V) communications,\ncausing latency and scalability problems. We introduce EffiComm, an end-to-end\nframework that transmits less than 40% of the data required by prior art while\nmaintaining state-of-the-art 3D object detection accuracy. EffiComm operates on\nBird's-Eye-View (BEV) feature maps from any modality and applies a two-stage\nreduction pipeline: (1) Selective Transmission (ST) prunes low-utility regions\nwith a confidence mask; (2) Adaptive Grid Reduction (AGR) uses a Graph Neural\nNetwork (GNN) to assign vehicle-specific keep ratios according to role and\nnetwork load. The remaining features are fused with a soft-gated\nMixture-of-Experts (MoE) attention layer, offering greater capacity and\nspecialization for effective feature integration. On the OPV2V benchmark,\nEffiComm reaches 0.84 mAP@0.7 while sending only an average of approximately\n1.5 MB per frame, outperforming previous methods on the accuracy-per-bit curve.\nThese results highlight the value of adaptive, learned communication for\nscalable Vehicle-to-Everything (V2X) perception.",
        "url": "http://arxiv.org/abs/2507.19354v1",
        "published_date": "2025-07-25T15:03:26+00:00",
        "updated_date": "2025-07-25T15:03:26+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Melih Yazgan",
            "Allen Xavier Arasan",
            "J. Marius Zöllner"
        ],
        "tldr": "EffiComm is an end-to-end framework for bandwidth-efficient multi-agent communication in connected vehicles, achieving state-of-the-art 3D object detection accuracy with significantly reduced data transmission using selective transmission and adaptive grid reduction.",
        "tldr_zh": "EffiComm是一个用于互联车辆中带宽高效多智能体通信的端到端框架，通过选择性传输和自适应网格缩减，以显著减少的数据传输量实现最先进的3D目标检测精度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Seeing Beyond Frames: Zero-Shot Pedestrian Intention Prediction with Raw Temporal Video and Multimodal Cues",
        "summary": "Pedestrian intention prediction is essential for autonomous driving in\ncomplex urban environments. Conventional approaches depend on supervised\nlearning over frame sequences and require extensive retraining to adapt to new\nscenarios. Here, we introduce BF-PIP (Beyond Frames Pedestrian Intention\nPrediction), a zero-shot approach built upon Gemini 2.5 Pro. It infers crossing\nintentions directly from short, continuous video clips enriched with structured\nJAAD metadata. In contrast to GPT-4V based methods that operate on discrete\nframes, BF-PIP processes uninterrupted temporal clips. It also incorporates\nbounding-box annotations and ego-vehicle speed via specialized multimodal\nprompts. Without any additional training, BF-PIP achieves 73% prediction\naccuracy, outperforming a GPT-4V baseline by 18 %. These findings illustrate\nthat combining temporal video inputs with contextual cues enhances\nspatiotemporal perception and improves intent inference under ambiguous\nconditions. This approach paves the way for agile, retraining-free perception\nmodule in intelligent transportation system.",
        "url": "http://arxiv.org/abs/2507.21161v1",
        "published_date": "2025-07-25T07:23:11+00:00",
        "updated_date": "2025-07-25T07:23:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Pallavi Zambare",
            "Venkata Nikhil Thanikella",
            "Ying Liu"
        ],
        "tldr": "The paper introduces BF-PIP, a zero-shot pedestrian intention prediction model leveraging Gemini 2.5 Pro on raw temporal video with multimodal cues, achieving improved accuracy over frame-based GPT-4V methods without retraining.",
        "tldr_zh": "该论文介绍了BF-PIP，一个零样本行人意图预测模型，利用Gemini 2.5 Pro处理原始时序视频并结合多模态提示，在无需重新训练的情况下，实现了比基于帧的GPT-4V方法更高的精度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "SaLF: Sparse Local Fields for Multi-Sensor Rendering in Real-Time",
        "summary": "High-fidelity sensor simulation of light-based sensors such as cameras and\nLiDARs is critical for safe and accurate autonomy testing. Neural radiance\nfield (NeRF)-based methods that reconstruct sensor observations via ray-casting\nof implicit representations have demonstrated accurate simulation of driving\nscenes, but are slow to train and render, hampering scale. 3D Gaussian\nSplatting (3DGS) has demonstrated faster training and rendering times through\nrasterization, but is primarily restricted to pinhole camera sensors,\npreventing usage for realistic multi-sensor autonomy evaluation. Moreover, both\nNeRF and 3DGS couple the representation with the rendering procedure (implicit\nnetworks for ray-based evaluation, particles for rasterization), preventing\ninteroperability, which is key for general usage. In this work, we present\nSparse Local Fields (SaLF), a novel volumetric representation that supports\nrasterization and raytracing. SaLF represents volumes as a sparse set of 3D\nvoxel primitives, where each voxel is a local implicit field. SaLF has fast\ntraining (<30 min) and rendering capabilities (50+ FPS for camera and 600+ FPS\nLiDAR), has adaptive pruning and densification to easily handle large scenes,\nand can support non-pinhole cameras and spinning LiDARs. We demonstrate that\nSaLF has similar realism as existing self-driving sensor simulation methods\nwhile improving efficiency and enhancing capabilities, enabling more scalable\nsimulation. https://waabi.ai/salf/",
        "url": "http://arxiv.org/abs/2507.18713v1",
        "published_date": "2025-07-24T18:01:22+00:00",
        "updated_date": "2025-07-24T18:01:22+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Yun Chen",
            "Matthew Haines",
            "Jingkang Wang",
            "Krzysztof Baron-Lis",
            "Sivabalan Manivasagam",
            "Ze Yang",
            "Raquel Urtasun"
        ],
        "tldr": "SaLF introduces a novel volumetric representation for multi-sensor rendering that combines the speed of 3DGS with the versatility of NeRFs, enabling real-time, high-fidelity sensor simulation for autonomous driving applications.",
        "tldr_zh": "SaLF 提出了一种新的体渲染表示方法，结合了 3DGS 的速度和 NeRF 的多功能性，从而为自动驾驶应用实现了实时、高保真的传感器仿真。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "DINO-SLAM: DINO-informed RGB-D SLAM for Neural Implicit and Explicit Representations",
        "summary": "This paper presents DINO-SLAM, a DINO-informed design strategy to enhance\nneural implicit (Neural Radiance Field -- NeRF) and explicit representations\n(3D Gaussian Splatting -- 3DGS) in SLAM systems through more comprehensive\nscene representations. Purposely, we rely on a Scene Structure Encoder (SSE)\nthat enriches DINO features into Enhanced DINO ones (EDINO) to capture\nhierarchical scene elements and their structural relationships. Building upon\nit, we propose two foundational paradigms for NeRF and 3DGS SLAM systems\nintegrating EDINO features. Our DINO-informed pipelines achieve superior\nperformance on the Replica, ScanNet, and TUM compared to state-of-the-art\nmethods.",
        "url": "http://arxiv.org/abs/2507.19474v1",
        "published_date": "2025-07-25T17:57:37+00:00",
        "updated_date": "2025-07-25T17:57:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziren Gong",
            "Xiaohan Li",
            "Fabio Tosi",
            "Youmin Zhang",
            "Stefano Mattoccia",
            "Jun Wu",
            "Matteo Poggi"
        ],
        "tldr": "DINO-SLAM enhances NeRF and 3DGS SLAM by integrating DINO features to improve scene representation and achieves state-of-the-art performance on benchmark datasets.",
        "tldr_zh": "DINO-SLAM通过整合DINO特征来增强NeRF和3DGS SLAM，以改进场景表示，并在基准数据集上实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Event-Based De-Snowing for Autonomous Driving",
        "summary": "Adverse weather conditions, particularly heavy snowfall, pose significant\nchallenges to both human drivers and autonomous vehicles. Traditional\nimage-based de-snowing methods often introduce hallucination artifacts as they\nrely solely on spatial information, while video-based approaches require high\nframe rates and suffer from alignment artifacts at lower frame rates. Camera\nparameters, such as exposure time, also influence the appearance of snowflakes,\nmaking the problem difficult to solve and heavily dependent on network\ngeneralization. In this paper, we propose to address the challenge of desnowing\nby using event cameras, which offer compressed visual information with\nsubmillisecond latency, making them ideal for de-snowing images, even in the\npresence of ego-motion. Our method leverages the fact that snowflake occlusions\nappear with a very distinctive streak signature in the spatio-temporal\nrepresentation of event data. We design an attention-based module that focuses\non events along these streaks to determine when a background point was occluded\nand use this information to recover its original intensity. We benchmark our\nmethod on DSEC-Snow, a new dataset created using a green-screen technique that\noverlays pre-recorded snowfall data onto the existing DSEC driving dataset,\nresulting in precise ground truth and synchronized image and event streams. Our\napproach outperforms state-of-the-art de-snowing methods by 3 dB in PSNR for\nimage reconstruction. Moreover, we show that off-the-shelf computer vision\nalgorithms can be applied to our reconstructions for tasks such as depth\nestimation and optical flow, achieving a $20\\%$ performance improvement over\nother de-snowing methods. Our work represents a crucial step towards enhancing\nthe reliability and safety of vision systems in challenging winter conditions,\npaving the way for more robust, all-weather-capable applications.",
        "url": "http://arxiv.org/abs/2507.20901v1",
        "published_date": "2025-07-25T17:48:28+00:00",
        "updated_date": "2025-07-25T17:48:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Manasi Muglikar",
            "Nico Messikommer",
            "Marco Cannici",
            "Davide Scaramuzza"
        ],
        "tldr": "This paper introduces an event-based de-snowing method for autonomous driving using an attention module to detect and remove snowflake occlusions, achieving state-of-the-art performance and improving downstream tasks. They also introduce a new dataset, DSEC-Snow.",
        "tldr_zh": "本文提出了一种基于事件的自动驾驶除雪方法，该方法使用注意力模块来检测和去除雪花遮挡，实现了最先进的性能并改善了下游任务。 他们还介绍了一个新的数据集 DSEC-Snow。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GS-Occ3D: Scaling Vision-only Occupancy Reconstruction for Autonomous Driving with Gaussian Splatting",
        "summary": "Occupancy is crucial for autonomous driving, providing essential geometric\npriors for perception and planning. However, existing methods predominantly\nrely on LiDAR-based occupancy annotations, which limits scalability and\nprevents leveraging vast amounts of potential crowdsourced data for\nauto-labeling. To address this, we propose GS-Occ3D, a scalable vision-only\nframework that directly reconstructs occupancy. Vision-only occupancy\nreconstruction poses significant challenges due to sparse viewpoints, dynamic\nscene elements, severe occlusions, and long-horizon motion. Existing\nvision-based methods primarily rely on mesh representation, which suffer from\nincomplete geometry and additional post-processing, limiting scalability. To\novercome these issues, GS-Occ3D optimizes an explicit occupancy representation\nusing an Octree-based Gaussian Surfel formulation, ensuring efficiency and\nscalability. Additionally, we decompose scenes into static background, ground,\nand dynamic objects, enabling tailored modeling strategies: (1) Ground is\nexplicitly reconstructed as a dominant structural element, significantly\nimproving large-area consistency; (2) Dynamic vehicles are separately modeled\nto better capture motion-related occupancy patterns. Extensive experiments on\nthe Waymo dataset demonstrate that GS-Occ3D achieves state-of-the-art geometry\nreconstruction results. By curating vision-only binary occupancy labels from\ndiverse urban scenes, we show their effectiveness for downstream occupancy\nmodels on Occ3D-Waymo and superior zero-shot generalization on Occ3D-nuScenes.\nIt highlights the potential of large-scale vision-based occupancy\nreconstruction as a new paradigm for scalable auto-labeling. Project Page:\nhttps://gs-occ3d.github.io/",
        "url": "http://arxiv.org/abs/2507.19451v2",
        "published_date": "2025-07-25T17:33:23+00:00",
        "updated_date": "2025-07-30T16:27:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Baijun Ye",
            "Minghui Qin",
            "Saining Zhang",
            "Moonjun Gong",
            "Shaoting Zhu",
            "Zebang Shen",
            "Luan Zhang",
            "Lu Zhang",
            "Hao Zhao",
            "Hang Zhao"
        ],
        "tldr": "GS-Occ3D is a novel vision-only occupancy reconstruction framework using Gaussian Splatting and tailored scene decomposition, achieving state-of-the-art results on Waymo and demonstrating effectiveness for downstream occupancy models with zero-shot generalization.",
        "tldr_zh": "GS-Occ3D是一个新颖的纯视觉占据重建框架，它使用高斯溅射和定制的场景分解，在Waymo数据集上实现了最先进的结果，并展示了其对下游占据模型的有效性以及零样本泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "BEV-LLM: Leveraging Multimodal BEV Maps for Scene Captioning in Autonomous Driving",
        "summary": "Autonomous driving technology has the potential to transform transportation,\nbut its wide adoption depends on the development of interpretable and\ntransparent decision-making systems. Scene captioning, which generates natural\nlanguage descriptions of the driving environment, plays a crucial role in\nenhancing transparency, safety, and human-AI interaction. We introduce BEV-LLM,\na lightweight model for 3D captioning of autonomous driving scenes. BEV-LLM\nleverages BEVFusion to combine 3D LiDAR point clouds and multi-view images,\nincorporating a novel absolute positional encoding for view-specific scene\ndescriptions. Despite using a small 1B parameter base model, BEV-LLM achieves\ncompetitive performance on the nuCaption dataset, surpassing state-of-the-art\nby up to 5\\% in BLEU scores. Additionally, we release two new datasets - nuView\n(focused on environmental conditions and viewpoints) and GroundView (focused on\nobject grounding) - to better assess scene captioning across diverse driving\nscenarios and address gaps in current benchmarks, along with initial\nbenchmarking results demonstrating their effectiveness.",
        "url": "http://arxiv.org/abs/2507.19370v1",
        "published_date": "2025-07-25T15:22:56+00:00",
        "updated_date": "2025-07-25T15:22:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Felix Brandstaetter",
            "Erik Schuetz",
            "Katharina Winter",
            "Fabian Flohr"
        ],
        "tldr": "BEV-LLM, a lightweight model using BEVFusion and novel positional encoding, achieves competitive scene captioning for autonomous driving and introduces two new datasets, nuView and GroundView, to address benchmark gaps.",
        "tldr_zh": "BEV-LLM是一个轻量级模型，它使用BEVFusion和新的位置编码，实现了具有竞争力的自动驾驶场景描述，并引入了两个新的数据集nuView和GroundView，以解决基准测试的差距。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multistream Network for LiDAR and Camera-based 3D Object Detection in Outdoor Scenes",
        "summary": "Fusion of LiDAR and RGB data has the potential to enhance outdoor 3D object\ndetection accuracy. To address real-world challenges in outdoor 3D object\ndetection, fusion of LiDAR and RGB input has started gaining traction. However,\neffective integration of these modalities for precise object detection task\nstill remains a largely open problem. To address that, we propose a MultiStream\nDetection (MuStD) network, that meticulously extracts task-relevant information\nfrom both data modalities. The network follows a three-stream structure. Its\nLiDAR-PillarNet stream extracts sparse 2D pillar features from the LiDAR input\nwhile the LiDAR-Height Compression stream computes Bird's-Eye View features. An\nadditional 3D Multimodal stream combines RGB and LiDAR features using UV\nmapping and polar coordinate indexing. Eventually, the features containing\ncomprehensive spatial, textural and geometric information are carefully fused\nand fed to a detection head for 3D object detection. Our extensive evaluation\non the challenging KITTI Object Detection Benchmark using public testing server\nat\nhttps://www.cvlibs.net/datasets/kitti/eval_object_detail.php?&result=d162ec699d6992040e34314d19ab7f5c217075e0\nestablishes the efficacy of our method by achieving new state-of-the-art or\nhighly competitive results in different categories while remaining among the\nmost efficient methods. Our code will be released through MuStD GitHub\nrepository at https://github.com/IbrahimUWA/MuStD.git",
        "url": "http://arxiv.org/abs/2507.19304v1",
        "published_date": "2025-07-25T14:20:16+00:00",
        "updated_date": "2025-07-25T14:20:16+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Muhammad Ibrahim",
            "Naveed Akhtar",
            "Haitian Wang",
            "Saeed Anwar",
            "Ajmal Mian"
        ],
        "tldr": "The paper proposes a MultiStream Detection (MuStD) network that fuses LiDAR and RGB data for 3D object detection, achieving state-of-the-art or competitive results on the KITTI dataset with efficient performance.",
        "tldr_zh": "该论文提出了一种多流检测（MuStD）网络，该网络融合了LiDAR和RGB数据用于3D目标检测，并在KITTI数据集上实现了最先进或有竞争力的结果，且性能高效。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CoopTrack: Exploring End-to-End Learning for Efficient Cooperative Sequential Perception",
        "summary": "Cooperative perception aims to address the inherent limitations of\nsingle-vehicle autonomous driving systems through information exchange among\nmultiple agents. Previous research has primarily focused on single-frame\nperception tasks. However, the more challenging cooperative sequential\nperception tasks, such as cooperative 3D multi-object tracking, have not been\nthoroughly investigated. Therefore, we propose CoopTrack, a fully\ninstance-level end-to-end framework for cooperative tracking, featuring\nlearnable instance association, which fundamentally differs from existing\napproaches. CoopTrack transmits sparse instance-level features that\nsignificantly enhance perception capabilities while maintaining low\ntransmission costs. Furthermore, the framework comprises two key components:\nMulti-Dimensional Feature Extraction, and Cross-Agent Association and\nAggregation, which collectively enable comprehensive instance representation\nwith semantic and motion features, and adaptive cross-agent association and\nfusion based on a feature graph. Experiments on both the V2X-Seq and Griffin\ndatasets demonstrate that CoopTrack achieves excellent performance.\nSpecifically, it attains state-of-the-art results on V2X-Seq, with 39.0\\% mAP\nand 32.8\\% AMOTA. The project is available at\nhttps://github.com/zhongjiaru/CoopTrack.",
        "url": "http://arxiv.org/abs/2507.19239v1",
        "published_date": "2025-07-25T13:04:54+00:00",
        "updated_date": "2025-07-25T13:04:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiaru Zhong",
            "Jiahao Wang",
            "Jiahui Xu",
            "Xiaofan Li",
            "Zaiqing Nie",
            "Haibao Yu"
        ],
        "tldr": "The paper introduces CoopTrack, an end-to-end framework for cooperative 3D multi-object tracking in autonomous driving, using learnable instance association and achieving state-of-the-art results on the V2X-Seq dataset.",
        "tldr_zh": "本文介绍了 CoopTrack，一个用于自动驾驶中协同 3D 多目标跟踪的端到端框架，它使用可学习的实例关联并在 V2X-Seq 数据集上取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Querying Autonomous Vehicle Point Clouds: Enhanced by 3D Object Counting with CounterNet",
        "summary": "Autonomous vehicles generate massive volumes of point cloud data, yet only a\nsubset is relevant for specific tasks such as collision detection, traffic\nanalysis, or congestion monitoring. Effectively querying this data is essential\nto enable targeted analytics. In this work, we formalize point cloud querying\nby defining three core query types: RETRIEVAL, COUNT, and AGGREGATION, each\naligned with distinct analytical scenarios. All these queries rely heavily on\naccurate object counts to produce meaningful results, making precise object\ncounting a critical component of query execution. Prior work has focused on\nindexing techniques for 2D video data, assuming detection models provide\naccurate counting information. However, when applied to 3D point cloud data,\nstate-of-the-art detection models often fail to generate reliable object\ncounts, leading to substantial errors in query results. To address this\nlimitation, we propose CounterNet, a heatmap-based network designed for\naccurate object counting in large-scale point cloud data. Rather than focusing\non accurate object localization, CounterNet detects object presence by finding\nobject centers to improve counting accuracy. We further enhance its performance\nwith a feature map partitioning strategy using overlapping regions, enabling\nbetter handling of both small and large objects in complex traffic scenes. To\nadapt to varying frame characteristics, we introduce a per-frame dynamic model\nselection strategy that selects the most effective configuration for each\ninput. Evaluations on three real-world autonomous vehicle datasets show that\nCounterNet improves counting accuracy by 5% to 20% across object categories,\nresulting in more reliable query outcomes across all supported query types.",
        "url": "http://arxiv.org/abs/2507.19209v1",
        "published_date": "2025-07-25T12:29:21+00:00",
        "updated_date": "2025-07-25T12:29:21+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Xiaoyu Zhang",
            "Zhifeng Bao",
            "Hai Dong",
            "Ziwei Wang",
            "Jiajun Liu"
        ],
        "tldr": "This paper introduces CounterNet, a heatmap-based network for accurate object counting in autonomous vehicle point clouds, addressing the limitations of existing detection models in providing reliable counts for point cloud querying.",
        "tldr_zh": "本文介绍了CounterNet，一种基于热图的网络，用于在自动驾驶车辆点云中进行精确的对象计数，解决了现有检测模型在为点云查询提供可靠计数方面的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VisHall3D: Monocular Semantic Scene Completion from Reconstructing the Visible Regions to Hallucinating the Invisible Regions",
        "summary": "This paper introduces VisHall3D, a novel two-stage framework for monocular\nsemantic scene completion that aims to address the issues of feature\nentanglement and geometric inconsistency prevalent in existing methods.\nVisHall3D decomposes the scene completion task into two stages: reconstructing\nthe visible regions (vision) and inferring the invisible regions\n(hallucination). In the first stage, VisFrontierNet, a visibility-aware\nprojection module, is introduced to accurately trace the visual frontier while\npreserving fine-grained details. In the second stage, OcclusionMAE, a\nhallucination network, is employed to generate plausible geometries for the\ninvisible regions using a noise injection mechanism. By decoupling scene\ncompletion into these two distinct stages, VisHall3D effectively mitigates\nfeature entanglement and geometric inconsistency, leading to significantly\nimproved reconstruction quality.\n  The effectiveness of VisHall3D is validated through extensive experiments on\ntwo challenging benchmarks: SemanticKITTI and SSCBench-KITTI-360. VisHall3D\nachieves state-of-the-art performance, outperforming previous methods by a\nsignificant margin and paves the way for more accurate and reliable scene\nunderstanding in autonomous driving and other applications.",
        "url": "http://arxiv.org/abs/2507.19188v1",
        "published_date": "2025-07-25T11:57:18+00:00",
        "updated_date": "2025-07-25T11:57:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoang Lu",
            "Yuanqi Su",
            "Xiaoning Zhang",
            "Longjun Gao",
            "Yu Xue",
            "Le Wang"
        ],
        "tldr": "VisHall3D is a two-stage framework for monocular semantic scene completion that decomposes the task into visible region reconstruction and invisible region hallucination, achieving state-of-the-art performance on SemanticKITTI and SSCBench-KITTI-360.",
        "tldr_zh": "VisHall3D是一个单目语义场景补全的两阶段框架，它将任务分解为可见区域重建和不可见区域幻觉生成，并在SemanticKITTI和SSCBench-KITTI-360上实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PatchTraj: Unified Time-Frequency Representation Learning via Dynamic Patches for Trajectory Prediction",
        "summary": "Pedestrian trajectory prediction is crucial for autonomous driving and\nrobotics. While existing point-based and grid-based methods expose two main\nlimitations: insufficiently modeling human motion dynamics, as they fail to\nbalance local motion details with long-range spatiotemporal dependencies, and\nthe time representations lack interaction with their frequency components in\njointly modeling trajectory sequences. To address these challenges, we propose\nPatchTraj, a dynamic patch-based framework that integrates time-frequency joint\nmodeling for trajectory prediction. Specifically, we decompose the trajectory\ninto raw time sequences and frequency components, and employ dynamic patch\npartitioning to perform multi-scale segmentation, capturing hierarchical motion\npatterns. Each patch undergoes adaptive embedding with scale-aware feature\nextraction, followed by hierarchical feature aggregation to model both\nfine-grained and long-range dependencies. The outputs of the two branches are\nfurther enhanced via cross-modal attention, facilitating complementary fusion\nof temporal and spectral cues. The resulting enhanced embeddings exhibit strong\nexpressive power, enabling accurate predictions even when using a vanilla\nTransformer architecture. Extensive experiments on ETH-UCY, SDD, NBA, and JRDB\ndatasets demonstrate that our method achieves state-of-the-art performance.\nNotably, on the egocentric JRDB dataset, PatchTraj attains significant relative\nimprovements of 26.7% in ADE and 17.4% in FDE, underscoring its substantial\npotential in embodied intelligence.",
        "url": "http://arxiv.org/abs/2507.19119v3",
        "published_date": "2025-07-25T09:55:33+00:00",
        "updated_date": "2025-07-31T15:04:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yanghong Liu",
            "Xingping Dong",
            "Ming Li",
            "Weixing Zhang",
            "Yidong Lou"
        ],
        "tldr": "The paper introduces PatchTraj, a dynamic patch-based framework for pedestrian trajectory prediction that integrates time-frequency joint modeling, achieving state-of-the-art results, particularly on the JRDB dataset.",
        "tldr_zh": "该论文介绍了PatchTraj，一个基于动态patch的行人轨迹预测框架，集成了时频联合建模，取得了最先进的结果，尤其是在JRDB数据集上。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Fine-Grained Traffic Inference from Road to Lane via Spatio-Temporal Graph Node Generation",
        "summary": "Fine-grained traffic management and prediction are fundamental to key\napplications such as autonomous driving, lane change guidance, and traffic\nsignal control. However, obtaining lane-level traffic data has become a\ncritical bottleneck for data-driven models due to limitations in the types and\nnumber of sensors and issues with the accuracy of tracking algorithms. To\naddress this, we propose the Fine-grained Road Traffic Inference (FRTI) task,\nwhich aims to generate more detailed lane-level traffic information using\nlimited road data, providing a more energy-efficient and cost-effective\nsolution for precise traffic management. This task is abstracted as the first\nscene of the spatio-temporal graph node generation problem. We designed a\ntwo-stage framework--RoadDiff--to solve the FRTI task. solve the FRTI task.\nThis framework leverages the Road-Lane Correlation Autoencoder-Decoder and the\nLane Diffusion Module to fully utilize the limited spatio-temporal dependencies\nand distribution relationships of road data to accurately infer fine-grained\nlane traffic states. Based on existing research, we designed several baseline\nmodels with the potential to solve the FRTI task and conducted extensive\nexperiments on six datasets representing different road conditions to validate\nthe effectiveness of the RoadDiff model in addressing the FRTI task. The\nrelevant datasets and code are available at\nhttps://github.com/ShuhaoLii/RoadDiff.",
        "url": "http://arxiv.org/abs/2507.19089v1",
        "published_date": "2025-07-25T09:15:18+00:00",
        "updated_date": "2025-07-25T09:15:18+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Shuhao Li",
            "Weidong Yang",
            "Yue Cui",
            "Xiaoxing Liu",
            "Lingkai Meng",
            "Lipeng Ma",
            "Fan Zhang"
        ],
        "tldr": "The paper introduces the Fine-grained Road Traffic Inference (FRTI) task, which aims to infer lane-level traffic information from limited road data using a two-stage framework called RoadDiff, addressing a critical bottleneck in data-driven models for autonomous driving and traffic management.",
        "tldr_zh": "该论文介绍了细粒度道路交通推断 (FRTI) 任务，旨在利用有限的道路数据推断车道级交通信息，通过名为 RoadDiff 的两阶段框架，解决自动驾驶和交通管理中数据驱动模型的关键瓶颈。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UPP: Unified Point-Level Prompting for Robust Point Cloud Analysis",
        "summary": "Pre-trained point cloud analysis models have shown promising advancements in\nvarious downstream tasks, yet their effectiveness is typically suffering from\nlow-quality point cloud (i.e., noise and incompleteness), which is a common\nissue in real scenarios due to casual object occlusions and unsatisfactory data\ncollected by 3D sensors. To this end, existing methods focus on enhancing point\ncloud quality by developing dedicated denoising and completion models. However,\ndue to the isolation between the point cloud enhancement and downstream tasks,\nthese methods fail to work in various real-world domains. In addition, the\nconflicting objectives between denoising and completing tasks further limit the\nensemble paradigm to preserve critical geometric features. To tackle the above\nchallenges, we propose a unified point-level prompting method that reformulates\npoint cloud denoising and completion as a prompting mechanism, enabling robust\nanalysis in a parameter-efficient manner. We start by introducing a\nRectification Prompter to adapt to noisy points through the predicted\nrectification vector prompts, effectively filtering noise while preserving\nintricate geometric features essential for accurate analysis. Sequentially, we\nfurther incorporate a Completion Prompter to generate auxiliary point prompts\nbased on the rectified point clouds, facilitating their robustness and\nadaptability. Finally, a Shape-Aware Unit module is exploited to efficiently\nunify and capture the filtered geometric features for the downstream point\ncloud analysis.Extensive experiments on four datasets demonstrate the\nsuperiority and robustness of our method when handling noisy and incomplete\npoint cloud data against existing state-of-the-art methods. Our code is\nreleased at https://github.com/zhoujiahuan1991/ICCV2025-UPP.",
        "url": "http://arxiv.org/abs/2507.18997v1",
        "published_date": "2025-07-25T06:54:30+00:00",
        "updated_date": "2025-07-25T06:54:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zixiang Ai",
            "Zhenyu Cui",
            "Yuxin Peng",
            "Jiahuan Zhou"
        ],
        "tldr": "This paper introduces a unified point-level prompting method (UPP) to improve the robustness of point cloud analysis models against noise and incompleteness by using Rectification and Completion Prompters. The method shows improved performance in experiments on four datasets.",
        "tldr_zh": "本文提出了一种统一的点级提示方法（UPP），通过使用校正和补全提示器，提高点云分析模型对噪声和不完整性的鲁棒性。实验表明，该方法在四个数据集上表现出改进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HQ-SMem: Video Segmentation and Tracking Using Memory Efficient Object Embedding With Selective Update and Self-Supervised Distillation Feedback",
        "summary": "Video Object Segmentation (VOS) is foundational to numerous computer vision\napplications, including surveillance, autonomous driving, robotics and\ngenerative video editing. However, existing VOS models often struggle with\nprecise mask delineation, deformable objects, topologically transforming\nobjects, tracking drift and long video sequences. In this paper, we introduce\nHQ-SMem, for High Quality video segmentation and tracking using Smart Memory, a\nnovel method that enhances the performance of VOS base models by addressing\nthese limitations. Our approach incorporates three key innovations: (i)\nleveraging SAM with High-Quality masks (SAM-HQ) alongside appearance-based\ncandidate-selection to refine coarse segmentation masks, resulting in improved\nobject boundaries; (ii) implementing a dynamic smart memory mechanism that\nselectively stores relevant key frames while discarding redundant ones, thereby\noptimizing memory usage and processing efficiency for long-term videos; and\n(iii) dynamically updating the appearance model to effectively handle complex\ntopological object variations and reduce drift throughout the video. These\ncontributions mitigate several limitations of existing VOS models including,\ncoarse segmentations that mix-in background pixels, fixed memory update\nschedules, brittleness to drift and occlusions, and prompt ambiguity issues\nassociated with SAM. Extensive experiments conducted on multiple public\ndatasets and state-of-the-art base trackers demonstrate that our method\nconsistently ranks among the top two on VOTS and VOTSt 2024 datasets. Moreover,\nHQ-SMem sets new benchmarks on Long Video Dataset and LVOS, showcasing its\neffectiveness in challenging scenarios characterized by complex multi-object\ndynamics over extended temporal durations.",
        "url": "http://arxiv.org/abs/2507.18921v1",
        "published_date": "2025-07-25T03:28:05+00:00",
        "updated_date": "2025-07-25T03:28:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Elham Soltani Kazemi",
            "Imad Eddine Toubal",
            "Gani Rahmon",
            "Jaired Collins",
            "K. Palaniappan"
        ],
        "tldr": "The paper introduces HQ-SMem, a novel Video Object Segmentation (VOS) method that improves performance by using SAM-HQ for better mask delineation, a smart memory mechanism for long videos, and dynamic appearance model updates to reduce drift. It achieves state-of-the-art results on several datasets.",
        "tldr_zh": "该论文介绍了一种新的视频对象分割(VOS)方法HQ-SMem，通过使用SAM-HQ来提高掩码的精确度，使用智能内存机制来处理长视频，并动态更新外观模型以减少漂移，从而提高性能。该方法在多个数据集上取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Diffusion-FS: Multimodal Free-Space Prediction via Diffusion for Autonomous Driving",
        "summary": "Drivable Free-space prediction is a fundamental and crucial problem in\nautonomous driving. Recent works have addressed the problem by representing the\nentire non-obstacle road regions as the free-space. In contrast our aim is to\nestimate the driving corridors that are a navigable subset of the entire road\nregion. Unfortunately, existing corridor estimation methods directly assume a\nBEV-centric representation, which is hard to obtain. In contrast, we frame\ndrivable free-space corridor prediction as a pure image perception task, using\nonly monocular camera input. However such a formulation poses several\nchallenges as one doesn't have the corresponding data for such free-space\ncorridor segments in the image. Consequently, we develop a novel\nself-supervised approach for free-space sample generation by leveraging future\nego trajectories and front-view camera images, making the process of visual\ncorridor estimation dependent on the ego trajectory. We then employ a diffusion\nprocess to model the distribution of such segments in the image. However, the\nexisting binary mask-based representation for a segment poses many limitations.\nTherefore, we introduce ContourDiff, a specialized diffusion-based architecture\nthat denoises over contour points rather than relying on binary mask\nrepresentations, enabling structured and interpretable free-space predictions.\nWe evaluate our approach qualitatively and quantitatively on both nuScenes and\nCARLA, demonstrating its effectiveness in accurately predicting safe multimodal\nnavigable corridors in the image.",
        "url": "http://arxiv.org/abs/2507.18763v1",
        "published_date": "2025-07-24T19:30:55+00:00",
        "updated_date": "2025-07-24T19:30:55+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Keshav Gupta",
            "Tejas S. Stanley",
            "Pranjal Paul",
            "Arun K. Singh",
            "K. Madhava Krishna"
        ],
        "tldr": "This paper introduces Diffusion-FS, a self-supervised approach for predicting navigable free-space corridors in autonomous driving using monocular images and diffusion models operating on contour points rather than binary masks. The approach is evaluated on nuScenes and CARLA.",
        "tldr_zh": "本文介绍了一种名为 Diffusion-FS 的自监督方法，用于预测自动驾驶中可导航的自由空间走廊。该方法使用单目图像和扩散模型，且扩散模型作用于轮廓点而不是二值掩码。该方法在 nuScenes 和 CARLA 上进行了评估。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "SIDE: Sparse Information Disentanglement for Explainable Artificial Intelligence",
        "summary": "Understanding the decisions made by deep neural networks is essential in\nhigh-stakes domains such as medical imaging and autonomous driving. Yet, these\nmodels often lack transparency, particularly in computer vision.\nPrototypical-parts-based neural networks have emerged as a promising solution\nby offering concept-level explanations. However, most are limited to\nfine-grained classification tasks, with few exceptions such as InfoDisent.\nInfoDisent extends prototypical models to large-scale datasets like ImageNet,\nbut produces complex explanations.\n  We introduce Sparse Information Disentanglement for Explainability (SIDE), a\nnovel method that improves the interpretability of prototypical parts through a\ndedicated training and pruning scheme that enforces sparsity. Combined with\nsigmoid activations in place of softmax, this approach allows SIDE to associate\neach class with only a small set of relevant prototypes. Extensive experiments\nshow that SIDE matches the accuracy of existing methods while reducing\nexplanation size by over $90\\%$, substantially enhancing the understandability\nof prototype-based explanations.",
        "url": "http://arxiv.org/abs/2507.19321v1",
        "published_date": "2025-07-25T14:34:15+00:00",
        "updated_date": "2025-07-25T14:34:15+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Viktar Dubovik",
            "Łukasz Struski",
            "Jacek Tabor",
            "Dawid Rymarczyk"
        ],
        "tldr": "The paper introduces SIDE, a novel method for improving the interpretability of prototypical parts in neural networks through sparsity-enforcing training and pruning, achieving comparable accuracy with significantly smaller explanation sizes.",
        "tldr_zh": "该论文介绍了SIDE，一种通过强制稀疏性的训练和剪枝来提高神经网络中原型部分可解释性的新方法，在实现相当准确率的同时，显著减小了解释的大小。",
        "relevance_score": 6,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Perpetua: Multi-Hypothesis Persistence Modeling for Semi-Static Environments",
        "summary": "Many robotic systems require extended deployments in complex, dynamic\nenvironments. In such deployments, parts of the environment may change between\nsubsequent robot observations. Most robotic mapping or environment modeling\nalgorithms are incapable of representing dynamic features in a way that enables\npredicting their future state. Instead, they opt to filter certain state\nobservations, either by removing them or some form of weighted averaging. This\npaper introduces Perpetua, a method for modeling the dynamics of semi-static\nfeatures. Perpetua is able to: incorporate prior knowledge about the dynamics\nof the feature if it exists, track multiple hypotheses, and adapt over time to\nenable predicting of future feature states. Specifically, we chain together\nmixtures of \"persistence\" and \"emergence\" filters to model the probability that\nfeatures will disappear or reappear in a formal Bayesian framework. The\napproach is an efficient, scalable, general, and robust method for estimating\nthe states of features in an environment, both in the present as well as at\narbitrary future times. Through experiments on simulated and real-world data,\nwe find that Perpetua yields better accuracy than similar approaches while also\nbeing online adaptable and robust to missing observations.",
        "url": "http://arxiv.org/abs/2507.18808v2",
        "published_date": "2025-07-24T21:11:23+00:00",
        "updated_date": "2025-07-28T15:12:54+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Miguel Saavedra-Ruiz",
            "Samer B. Nashed",
            "Charlie Gauthier",
            "Liam Paull"
        ],
        "tldr": "Perpetua is a new method for modeling the dynamics of semi-static features in robotic environments, using a Bayesian framework with persistence and emergence filters to predict future states, outperforming existing approaches in accuracy and robustness.",
        "tldr_zh": "Perpetua是一种用于模拟机器人环境中半静态特征动态的新方法，它使用具有持久性和突现性滤波器的贝叶斯框架来预测未来状态，在准确性和稳健性方面优于现有方法。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]