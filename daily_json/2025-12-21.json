[
    {
        "title": "RadarGen: Automotive Radar Point Cloud Generation from Cameras",
        "summary": "We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird's-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.",
        "url": "http://arxiv.org/abs/2512.17897v1",
        "published_date": "2025-12-19T18:57:33+00:00",
        "updated_date": "2025-12-19T18:57:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Tomer Borreda",
            "Fangqiang Ding",
            "Sanja Fidler",
            "Shengyu Huang",
            "Or Litany"
        ],
        "tldr": "RadarGen is a diffusion model that synthesizes realistic automotive radar point clouds from camera images, leveraging BEV-aligned depth, semantic, and motion cues to guide the generation process.",
        "tldr_zh": "RadarGen是一个扩散模型，可以从相机图像中合成逼真的汽车雷达点云，并利用BEV对齐的深度、语义和运动线索来指导生成过程。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Diffusion Forcing for Multi-Agent Interaction Sequence Modeling",
        "summary": "Understanding and generating multi-person interactions is a fundamental challenge with broad implications for robotics and social computing. While humans naturally coordinate in groups, modeling such interactions remains difficult due to long temporal horizons, strong inter-agent dependencies, and variable group sizes. Existing motion generation methods are largely task-specific and do not generalize to flexible multi-agent generation. We introduce MAGNet (Multi-Agent Diffusion Forcing Transformer), a unified autoregressive diffusion framework for multi-agent motion generation that supports a wide range of interaction tasks through flexible conditioning and sampling. MAGNet performs dyadic prediction, partner inpainting, and full multi-agent motion generation within a single model, and can autoregressively generate ultra-long sequences spanning hundreds of v. Building on Diffusion Forcing, we introduce key modifications that explicitly model inter-agent coupling during autoregressive denoising, enabling coherent coordination across agents. As a result, MAGNet captures both tightly synchronized activities (e.g, dancing, boxing) and loosely structured social interactions. Our approach performs on par with specialized methods on dyadic benchmarks while naturally extending to polyadic scenarios involving three or more interacting people, enabled by a scalable architecture that is agnostic to the number of agents. We refer readers to the supplemental video, where the temporal dynamics and spatial coordination of generated interactions are best appreciated. Project page: https://von31.github.io/MAGNet/",
        "url": "http://arxiv.org/abs/2512.17900v1",
        "published_date": "2025-12-19T18:59:02+00:00",
        "updated_date": "2025-12-19T18:59:02+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Vongani H. Maluleke",
            "Kie Horiuchi",
            "Lea Wilken",
            "Evonne Ng",
            "Jitendra Malik",
            "Angjoo Kanazawa"
        ],
        "tldr": "The paper introduces MAGNet, a diffusion-based framework for generating multi-agent interaction sequences, demonstrating its ability to model various interaction types and generalize to different numbers of agents and long sequences.",
        "tldr_zh": "本文介绍了一种基于扩散的框架MAGNet，用于生成多智能体交互序列，展示了其建模各种交互类型并推广到不同数量的智能体和长序列的能力。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]