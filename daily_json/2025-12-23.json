[
    {
        "title": "CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis",
        "summary": "Automating crash video analysis is essential to leverage the growing availability of driving video data for traffic safety research and accountability attribution in autonomous driving. Crash video analysis is a challenging multitask problem due to the complex spatiotemporal dynamics of crash events in video data and the diverse analytical requirements involved. It requires capabilities spanning crash recognition, temporal grounding, and high-level video understanding. Existing models, however, cannot perform all these tasks within a unified framework, and effective training strategies for such models remain underexplored. To fill these gaps, this paper proposes CrashChat, a multimodal large language model (MLLM) for multitask traffic crash analysis, built upon VideoLLaMA3. CrashChat acquires domain-specific knowledge through instruction fine-tuning and employs a novel multitask learning strategy based on task decoupling and grouping, which maximizes the benefit of joint learning within and across task groups while mitigating negative transfer. Numerical experiments on consolidated public datasets demonstrate that CrashChat consistently outperforms existing MLLMs across model scales and traditional vision-based methods, achieving state-of-the-art performance. It reaches near-perfect accuracy in crash recognition, a 176\\% improvement in crash localization, and a 40\\% improvement in the more challenging pre-crash localization. Compared to general MLLMs, it substantially enhances textual accuracy and content coverage in crash description and reasoning tasks, with 0.18-0.41 increases in BLEU scores and 0.18-0.42 increases in ROUGE scores. Beyond its strong performance, CrashChat is a convenient, end-to-end analytical tool ready for practical implementation. The dataset and implementation code for CrashChat are available at https://github.com/Liangkd/CrashChat.",
        "url": "http://arxiv.org/abs/2512.18878v1",
        "published_date": "2025-12-21T20:39:31+00:00",
        "updated_date": "2025-12-21T20:39:31+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Kaidi Liang",
            "Ke Li",
            "Xianbiao Hu",
            "Ruwen Qin"
        ],
        "tldr": "The paper introduces CrashChat, a multimodal large language model built upon VideoLLaMA3, designed for multitask traffic crash video analysis, demonstrating significant improvements in crash recognition, localization, and description compared to existing models.",
        "tldr_zh": "该论文介绍了CrashChat，一个基于VideoLLaMA3的多模态大型语言模型，用于多任务交通事故视频分析，与现有模型相比，在事故识别、定位和描述方面表现出显著改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Point What You Mean: Visually Grounded Instruction Policy",
        "summary": "Vision-Language-Action (VLA) models align vision and language with embodied control, but their object referring ability remains limited when relying solely on text prompt, especially in cluttered or out-of-distribution (OOD) scenes. In this study, we introduce the Point-VLA, a plug-and-play policy that augments language instructions with explicit visual cues (e.g., bounding boxes) to resolve referential ambiguity and enable precise object-level grounding. To efficiently scale visually grounded datasets, we further develop an automatic data annotation pipeline requiring minimal human effort. We evaluate Point-VLA on diverse real-world referring tasks and observe consistently stronger performance than text-only instruction VLAs, particularly in cluttered or unseen-object scenarios, with robust generalization. These results demonstrate that Point-VLA effectively resolves object referring ambiguity through pixel-level visual grounding, achieving more generalizable embodied control.",
        "url": "http://arxiv.org/abs/2512.18933v1",
        "published_date": "2025-12-22T00:44:19+00:00",
        "updated_date": "2025-12-22T00:44:19+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Hang Yu",
            "Juntu Zhao",
            "Yufeng Liu",
            "Kaiyu Li",
            "Cheng Ma",
            "Di Zhang",
            "Yingdong Hu",
            "Guang Chen",
            "Junyuan Xie",
            "Junliang Guo",
            "Junqiao Zhao",
            "Yang Gao"
        ],
        "tldr": "The paper introduces Point-VLA, a policy that enhances Vision-Language-Action models with visual cues (bounding boxes) for improved object referring, particularly in complex scenes, and demonstrates its effectiveness through real-world experiments.",
        "tldr_zh": "该论文介绍了Point-VLA，一种通过视觉线索（例如边界框）增强视觉-语言-动作模型的策略，以提高对象指代能力，尤其是在复杂场景中，并通过真实世界的实验证明了其有效性。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]