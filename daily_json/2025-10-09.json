[
    {
        "title": "WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation",
        "summary": "Wrist-view observations are crucial for VLA models as they capture\nfine-grained hand-object interactions that directly enhance manipulation\nperformance. Yet large-scale datasets rarely include such recordings, resulting\nin a substantial gap between abundant anchor views and scarce wrist views.\nExisting world models cannot bridge this gap, as they require a wrist-view\nfirst frame and thus fail to generate wrist-view videos from anchor views\nalone. Amid this gap, recent visual geometry models such as VGGT emerge with\ngeometric and cross-view priors that make it possible to address extreme\nviewpoint shifts. Inspired by these insights, we propose WristWorld, the first\n4D world model that generates wrist-view videos solely from anchor views.\nWristWorld operates in two stages: (i) Reconstruction, which extends VGGT and\nincorporates our Spatial Projection Consistency (SPC) Loss to estimate\ngeometrically consistent wrist-view poses and 4D point clouds; (ii) Generation,\nwhich employs our video generation model to synthesize temporally coherent\nwrist-view videos from the reconstructed perspective. Experiments on Droid,\nCalvin, and Franka Panda demonstrate state-of-the-art video generation with\nsuperior spatial consistency, while also improving VLA performance, raising the\naverage task completion length on Calvin by 3.81% and closing 42.4% of the\nanchor-wrist view gap.",
        "url": "http://arxiv.org/abs/2510.07313v1",
        "published_date": "2025-10-08T17:59:08+00:00",
        "updated_date": "2025-10-08T17:59:08+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Zezhong Qian",
            "Xiaowei Chi",
            "Yuming Li",
            "Shizun Wang",
            "Zhiyuan Qin",
            "Xiaozhu Ju",
            "Sirui Han",
            "Shanghang Zhang"
        ],
        "tldr": "WristWorld is a novel 4D world model that generates wrist-view videos for robotic manipulation from anchor views, addressing the lack of wrist-view data and improving VLA performance.",
        "tldr_zh": "WristWorld 是一种新型的 4D 世界模型，可以从锚视图生成用于机器人操作的手腕视图视频，解决了手腕视图数据不足的问题，并提高了 VLA 的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics",
        "summary": "Vision-Language Models (VLMs) have shown remarkable capabilities in spatial\nreasoning, yet they remain fundamentally limited to qualitative precision and\nlack the computational precision required for real-world robotics. Current\napproaches fail to leverage metric cues from depth sensors and camera\ncalibration, instead reducing geometric problems to pattern recognition tasks\nthat cannot deliver the centimeter-level accuracy essential for robotic\nmanipulation. We present TIGeR (Tool-Integrated Geometric Reasoning), a novel\nframework that transforms VLMs from perceptual estimators to geometric\ncomputers by enabling them to generate and execute precise geometric\ncomputations through external tools. Rather than attempting to internalize\ncomplex geometric operations within neural networks, TIGeR empowers models to\nrecognize geometric reasoning requirements, synthesize appropriate\ncomputational code, and invoke specialized libraries for exact calculations. To\nsupport this paradigm, we introduce TIGeR-300K, a comprehensive\ntool-invocation-oriented dataset covering point transformations, pose\nestimation, trajectory generation, and spatial compatibility verification,\ncomplete with tool invocation sequences and intermediate computations. Through\na two-stage training pipeline combining supervised fine-tuning (SFT) and\nreinforcement fine-tuning (RFT) with our proposed hierarchical reward design,\nTIGeR achieves SOTA performance on geometric reasoning benchmarks while\ndemonstrating centimeter-level precision in real-world robotic manipulation\ntasks.",
        "url": "http://arxiv.org/abs/2510.07181v1",
        "published_date": "2025-10-08T16:20:23+00:00",
        "updated_date": "2025-10-08T16:20:23+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Yi Han",
            "Cheng Chi",
            "Enshen Zhou",
            "Shanyu Rong",
            "Jingkun An",
            "Pengwei Wang",
            "Zhongyuan Wang",
            "Lu Sheng",
            "Shanghang Zhang"
        ],
        "tldr": "TIGeR is a framework that enables VLMs to perform precise geometric computations for robotics by using external tools, achieving centimeter-level accuracy in real-world manipulation tasks.",
        "tldr_zh": "TIGeR是一个框架，它通过使用外部工具使视觉语言模型（VLMs）能够进行精确的几何计算，从而在现实世界的机器人操作任务中达到厘米级的精度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking",
        "summary": "Embodied Visual Tracking (EVT) is a fundamental ability that underpins\npractical applications, such as companion robots, guidance robots and service\nassistants, where continuously following moving targets is essential. Recent\nadvances have enabled language-guided tracking in complex and unstructured\nscenes. However, existing approaches lack explicit spatial reasoning and\neffective temporal memory, causing failures under severe occlusions or in the\npresence of similar-looking distractors. To address these challenges, we\npresent TrackVLA++, a novel Vision-Language-Action (VLA) model that enhances\nembodied visual tracking with two key modules, a spatial reasoning mechanism\nand a Target Identification Memory (TIM). The reasoning module introduces a\nChain-of-Thought paradigm, termed Polar-CoT, which infers the target's relative\nposition and encodes it as a compact polar-coordinate token for action\nprediction. Guided by these spatial priors, the TIM employs a gated update\nstrategy to preserve long-horizon target memory, ensuring spatiotemporal\nconsistency and mitigating target loss during extended occlusions. Extensive\nexperiments show that TrackVLA++ achieves state-of-the-art performance on\npublic benchmarks across both egocentric and multi-camera settings. On the\nchallenging EVT-Bench DT split, TrackVLA++ surpasses the previous leading\napproach by 5.1 and 12, respectively. Furthermore, TrackVLA++ exhibits strong\nzero-shot generalization, enabling robust real-world tracking in dynamic and\noccluded scenarios.",
        "url": "http://arxiv.org/abs/2510.07134v1",
        "published_date": "2025-10-08T15:29:17+00:00",
        "updated_date": "2025-10-08T15:29:17+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Jiahang Liu",
            "Yunpeng Qi",
            "Jiazhao Zhang",
            "Minghan Li",
            "Shaoan Wang",
            "Kui Wu",
            "Hanjing Ye",
            "Hong Zhang",
            "Zhibo Chen",
            "Fangwei Zhong",
            "Zhizheng Zhang",
            "He Wang"
        ],
        "tldr": "TrackVLA++ introduces a novel Vision-Language-Action model for embodied visual tracking, enhancing spatial reasoning and temporal memory to improve performance in challenging occlusion and distractor scenarios.",
        "tldr_zh": "TrackVLA++ 提出了一种新的视觉-语言-动作模型，用于具身视觉跟踪，通过增强空间推理和时间记忆，提高了在复杂遮挡和干扰场景下的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "OBJVanish: Physically Realizable Text-to-3D Adv. Generation of LiDAR-Invisible Objects",
        "summary": "LiDAR-based 3D object detectors are fundamental to autonomous driving, where\nfailing to detect objects poses severe safety risks. Developing effective 3D\nadversarial attacks is essential for thoroughly testing these detection systems\nand exposing their vulnerabilities before real-world deployment. However,\nexisting adversarial attacks that add optimized perturbations to 3D points have\ntwo critical limitations: they rarely cause complete object disappearance and\nprove difficult to implement in physical environments. We introduce the\ntext-to-3D adversarial generation method, a novel approach enabling physically\nrealizable attacks that can generate 3D models of objects truly invisible to\nLiDAR detectors and be easily realized in the real world. Specifically, we\npresent the first empirical study that systematically investigates the factors\ninfluencing detection vulnerability by manipulating the topology, connectivity,\nand intensity of individual pedestrian 3D models and combining pedestrians with\nmultiple objects within the CARLA simulation environment. Building on the\ninsights, we propose the physically-informed text-to-3D adversarial generation\n(Phy3DAdvGen) that systematically optimizes text prompts by iteratively\nrefining verbs, objects, and poses to produce LiDAR-invisible pedestrians. To\nensure physical realizability, we construct a comprehensive object pool\ncontaining 13 3D models of real objects and constrain Phy3DAdvGen to generate\n3D objects based on combinations of objects in this set. Extensive experiments\ndemonstrate that our approach can generate 3D pedestrians that evade six\nstate-of-the-art (SOTA) LiDAR 3D detectors in both CARLA simulation and\nphysical environments, thereby highlighting vulnerabilities in safety-critical\napplications.",
        "url": "http://arxiv.org/abs/2510.06952v1",
        "published_date": "2025-10-08T12:35:35+00:00",
        "updated_date": "2025-10-08T12:35:35+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bing Li",
            "Wuqi Wang",
            "Yanan Zhang",
            "Jingzheng Li",
            "Haigen Min",
            "Wei Feng",
            "Xingyu Zhao",
            "Jie Zhang",
            "Qing Guo"
        ],
        "tldr": "The paper introduces a text-to-3D adversarial generation method to create physically realizable 3D objects, specifically pedestrians, that are invisible to LiDAR detectors, exposing vulnerabilities in autonomous driving systems.",
        "tldr_zh": "该论文提出了一种文本到3D的对抗生成方法，用于创建物理上可实现的、对激光雷达探测器不可见的3D物体（特别是行人），从而暴露了自动驾驶系统中的漏洞。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "HARP-NeXt: High-Speed and Accurate Range-Point Fusion Network for 3D LiDAR Semantic Segmentation",
        "summary": "LiDAR semantic segmentation is crucial for autonomous vehicles and mobile\nrobots, requiring high accuracy and real-time processing, especially on\nresource-constrained embedded systems. Previous state-of-the-art methods often\nface a trade-off between accuracy and speed. Point-based and sparse\nconvolution-based methods are accurate but slow due to the complexity of\nneighbor searching and 3D convolutions. Projection-based methods are faster but\nlose critical geometric information during the 2D projection. Additionally,\nmany recent methods rely on test-time augmentation (TTA) to improve\nperformance, which further slows the inference. Moreover, the pre-processing\nphase across all methods increases execution time and is demanding on embedded\nplatforms. Therefore, we introduce HARP-NeXt, a high-speed and accurate LiDAR\nsemantic segmentation network. We first propose a novel pre-processing\nmethodology that significantly reduces computational overhead. Then, we design\nthe Conv-SE-NeXt feature extraction block to efficiently capture\nrepresentations without deep layer stacking per network stage. We also employ a\nmulti-scale range-point fusion backbone that leverages information at multiple\nabstraction levels to preserve essential geometric details, thereby enhancing\naccuracy. Experiments on the nuScenes and SemanticKITTI benchmarks show that\nHARP-NeXt achieves a superior speed-accuracy trade-off compared to all\nstate-of-the-art methods, and, without relying on ensemble models or TTA, is\ncomparable to the top-ranked PTv3, while running 24$\\times$ faster. The code is\navailable at https://github.com/SamirAbouHaidar/HARP-NeXt",
        "url": "http://arxiv.org/abs/2510.06876v1",
        "published_date": "2025-10-08T10:46:07+00:00",
        "updated_date": "2025-10-08T10:46:07+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Samir Abou Haidar",
            "Alexandre Chariot",
            "Mehdi Darouich",
            "Cyril Joly",
            "Jean-Emmanuel Deschaud"
        ],
        "tldr": "HARP-NeXt is a LiDAR semantic segmentation network that achieves a better speed-accuracy trade-off than existing methods by using a novel pre-processing method, a Conv-SE-NeXt feature extraction block, and a multi-scale range-point fusion backbone. It runs significantly faster than top-ranked methods without relying on test-time augmentation.",
        "tldr_zh": "HARP-NeXt 是一种 LiDAR 语义分割网络，通过使用一种新颖的预处理方法、Conv-SE-NeXt 特征提取块和多尺度范围点融合骨干网络，实现了比现有方法更好的速度-精度权衡。它运行速度明显快于排名靠前的方法，且不依赖于测试时增强。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Introspection in Learned Semantic Scene Graph Localisation",
        "summary": "This work investigates how semantics influence localisation performance and\nrobustness in a learned self-supervised, contrastive semantic localisation\nframework. After training a localisation network on both original and perturbed\nmaps, we conduct a thorough post-hoc introspection analysis to probe whether\nthe model filters environmental noise and prioritises distinctive landmarks\nover routine clutter. We validate various interpretability methods and present\na comparative reliability analysis. Integrated gradients and Attention Weights\nconsistently emerge as the most reliable probes of learned behaviour. A\nsemantic class ablation further reveals an implicit weighting in which frequent\nobjects are often down-weighted. Overall, the results indicate that the model\nlearns noise-robust, semantically salient relations about place definition,\nthereby enabling explainable registration under challenging visual and\nstructural variations.",
        "url": "http://arxiv.org/abs/2510.07053v1",
        "published_date": "2025-10-08T14:21:45+00:00",
        "updated_date": "2025-10-08T14:21:45+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.RO",
            "I.2.10; I.2.9; I.4.8; I.5.2; I.5.1"
        ],
        "authors": [
            "Manshika Charvi Bissessur",
            "Efimia Panagiotaki",
            "Daniele De Martini"
        ],
        "tldr": "The paper investigates how a self-supervised semantic localization framework learns to prioritize distinctive landmarks and filter noise, using interpretability methods to reveal the model's learned behavior and implicit weighting of semantic classes.",
        "tldr_zh": "本文研究了一个自监督语义定位框架如何学习优先考虑独特地标并过滤噪声，并使用可解释性方法来揭示模型的学习行为和语义类的隐式权重。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Learning Global Representation from Queries for Vectorized HD Map Construction",
        "summary": "The online construction of vectorized high-definition (HD) maps is a\ncornerstone of modern autonomous driving systems. State-of-the-art approaches,\nparticularly those based on the DETR framework, formulate this as an instance\ndetection problem. However, their reliance on independent, learnable object\nqueries results in a predominantly local query perspective, neglecting the\ninherent global representation within HD maps. In this work, we propose\n\\textbf{MapGR} (\\textbf{G}lobal \\textbf{R}epresentation learning for HD\n\\textbf{Map} construction), an architecture designed to learn and utilize a\nglobal representations from queries. Our method introduces two synergistic\nmodules: a Global Representation Learning (GRL) module, which encourages the\ndistribution of all queries to better align with the global map through a\ncarefully designed holistic segmentation task, and a Global Representation\nGuidance (GRG) module, which endows each individual query with explicit,\nglobal-level contextual information to facilitate its optimization. Evaluations\non the nuScenes and Argoverse2 datasets validate the efficacy of our approach,\ndemonstrating substantial improvements in mean Average Precision (mAP) compared\nto leading baselines.",
        "url": "http://arxiv.org/abs/2510.06969v1",
        "published_date": "2025-10-08T12:56:08+00:00",
        "updated_date": "2025-10-08T12:56:08+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Shoumeng Qiu",
            "Xinrun Li",
            "Yang Long",
            "Xiangyang Xue",
            "Varun Ojha",
            "Jian Pu"
        ],
        "tldr": "The paper introduces MapGR, a novel architecture for vectorized HD map construction that learns and utilizes global representations from queries, outperforming state-of-the-art methods on nuScenes and Argoverse2 datasets.",
        "tldr_zh": "该论文介绍了MapGR，一种用于矢量化高清地图构建的新型架构，它从查询中学习并利用全局表示，在nuScenes和Argoverse2数据集上优于最先进的方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniFField: A Generalizable Unified Neural Feature Field for Visual, Semantic, and Spatial Uncertainties in Any Scene",
        "summary": "Comprehensive visual, geometric, and semantic understanding of a 3D scene is\ncrucial for successful execution of robotic tasks, especially in unstructured\nand complex environments. Additionally, to make robust decisions, it is\nnecessary for the robot to evaluate the reliability of perceived information.\nWhile recent advances in 3D neural feature fields have enabled robots to\nleverage features from pretrained foundation models for tasks such as\nlanguage-guided manipulation and navigation, existing methods suffer from two\ncritical limitations: (i) they are typically scene-specific, and (ii) they lack\nthe ability to model uncertainty in their predictions. We present UniFField, a\nunified uncertainty-aware neural feature field that combines visual, semantic,\nand geometric features in a single generalizable representation while also\npredicting uncertainty in each modality. Our approach, which can be applied\nzero shot to any new environment, incrementally integrates RGB-D images into\nour voxel-based feature representation as the robot explores the scene,\nsimultaneously updating uncertainty estimation. We evaluate our uncertainty\nestimations to accurately describe the model prediction errors in scene\nreconstruction and semantic feature prediction. Furthermore, we successfully\nleverage our feature predictions and their respective uncertainty for an active\nobject search task using a mobile manipulator robot, demonstrating the\ncapability for robust decision-making.",
        "url": "http://arxiv.org/abs/2510.06754v1",
        "published_date": "2025-10-08T08:30:26+00:00",
        "updated_date": "2025-10-08T08:30:26+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Christian Maurer",
            "Snehal Jauhri",
            "Sophie Lueth",
            "Georgia Chalvatzaki"
        ],
        "tldr": "UniFField introduces a generalizable neural feature field that integrates visual, semantic, and geometric features with uncertainty estimation for any scene, enabling robust decision-making for robots in complex environments.",
        "tldr_zh": "UniFField 提出了一种通用的神经特征场，集成了视觉、语义和几何特征以及不确定性估计，适用于任何场景，从而为机器人复杂环境中的稳健决策提供支持。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Semantic Segmentation Algorithm Based on Light Field and LiDAR Fusion",
        "summary": "Semantic segmentation serves as a cornerstone of scene understanding in\nautonomous driving but continues to face significant challenges under complex\nconditions such as occlusion. Light field and LiDAR modalities provide\ncomplementary visual and spatial cues that are beneficial for robust\nperception; however, their effective integration is hindered by limited\nviewpoint diversity and inherent modality discrepancies. To address these\nchallenges, the first multimodal semantic segmentation dataset integrating\nlight field data and point cloud data is proposed. Based on this dataset, we\nproposed a multi-modal light field point-cloud fusion segmentation\nnetwork(Mlpfseg), incorporating feature completion and depth perception to\nsegment both camera images and LiDAR point clouds simultaneously. The feature\ncompletion module addresses the density mismatch between point clouds and image\npixels by performing differential reconstruction of point-cloud feature maps,\nenhancing the fusion of these modalities. The depth perception module improves\nthe segmentation of occluded objects by reinforcing attention scores for better\nocclusion awareness. Our method outperforms image-only segmentation by 1.71\nMean Intersection over Union(mIoU) and point cloud-only segmentation by 2.38\nmIoU, demonstrating its effectiveness.",
        "url": "http://arxiv.org/abs/2510.06687v1",
        "published_date": "2025-10-08T06:15:06+00:00",
        "updated_date": "2025-10-08T06:15:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jie Luo",
            "Yuxuan Jiang",
            "Xin Jin",
            "Mingyu Liu",
            "Yihui Fan"
        ],
        "tldr": "This paper introduces a new multimodal semantic segmentation dataset and a fusion network (Mlpfseg) that integrates light field and LiDAR data for improved scene understanding in autonomous driving, particularly in handling occlusions. The method outperforms single modality approaches in mIoU.",
        "tldr_zh": "本文介绍了一个新的多模态语义分割数据集和一个融合网络（Mlpfseg），该网络集成了光场和激光雷达数据，以提高自动驾驶中的场景理解能力，尤其是在处理遮挡方面。该方法在 mIoU 方面优于单一模态方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Active Next-Best-View Optimization for Risk-Averse Path Planning",
        "summary": "Safe navigation in uncertain environments requires planning methods that\nintegrate risk aversion with active perception. In this work, we present a\nunified framework that refines a coarse reference path by constructing\ntail-sensitive risk maps from Average Value-at-Risk statistics on an\nonline-updated 3D Gaussian-splat Radiance Field. These maps enable the\ngeneration of locally safe and feasible trajectories. In parallel, we formulate\nNext-Best-View (NBV) selection as an optimization problem on the SE(3) pose\nmanifold, where Riemannian gradient descent maximizes an expected information\ngain objective to reduce uncertainty most critical for imminent motion. Our\napproach advances the state-of-the-art by coupling risk-averse path refinement\nwith NBV planning, while introducing scalable gradient decompositions that\nsupport efficient online updates in complex environments. We demonstrate the\neffectiveness of the proposed framework through extensive computational\nstudies.",
        "url": "http://arxiv.org/abs/2510.06481v1",
        "published_date": "2025-10-07T21:41:28+00:00",
        "updated_date": "2025-10-07T21:41:28+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Amirhossein Mollaei Khass",
            "Guangyi Liu",
            "Vivek Pandey",
            "Wen Jiang",
            "Boshu Lei",
            "Kostas Daniilidis",
            "Nader Motee"
        ],
        "tldr": "This paper presents a framework for risk-averse path planning using Next-Best-View selection based on Average Value-at-Risk statistics on an online-updated 3D Gaussian-splat Radiance Field, maximizing information gain to reduce uncertainty.",
        "tldr_zh": "本文提出了一个基于在线更新的3D高斯-splat辐射场的平均风险价值统计，通过Next-Best-View选择进行风险规避路径规划的框架，最大化信息增益以减少不确定性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Real-Time Glass Detection and Reprojection using Sensor Fusion Onboard Aerial Robots",
        "summary": "Autonomous aerial robots are increasingly being deployed in real-world\nscenarios, where transparent obstacles present significant challenges to\nreliable navigation and mapping. These materials pose a unique problem for\ntraditional perception systems because they lack discernible features and can\ncause conventional depth sensors to fail, leading to inaccurate maps and\npotential collisions. To ensure safe navigation, robots must be able to\naccurately detect and map these transparent obstacles. Existing methods often\nrely on large, expensive sensors or algorithms that impose high computational\nburdens, making them unsuitable for low Size, Weight, and Power (SWaP) robots.\nIn this work, we propose a novel and computationally efficient framework for\ndetecting and mapping transparent obstacles onboard a sub-300g quadrotor. Our\nmethod fuses data from a Time-of-Flight (ToF) camera and an ultrasonic sensor\nwith a custom, lightweight 2D convolution model. This specialized approach\naccurately detects specular reflections and propagates their depth into\ncorresponding empty regions of the depth map, effectively rendering transparent\nobstacles visible. The entire pipeline operates in real-time, utilizing only a\nsmall fraction of a CPU core on an embedded processor. We validate our system\nthrough a series of experiments in both controlled and real-world environments,\ndemonstrating the utility of our method through experiments where the robot\nmaps indoor environments containing glass. Our work is, to our knowledge, the\nfirst of its kind to demonstrate a real-time, onboard transparent obstacle\nmapping system on a low-SWaP quadrotor using only the CPU.",
        "url": "http://arxiv.org/abs/2510.06518v1",
        "published_date": "2025-10-07T23:31:45+00:00",
        "updated_date": "2025-10-07T23:31:45+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.SY",
            "eess.SY"
        ],
        "authors": [
            "Malakhi Hopkins",
            "Varun Murali",
            "Vijay Kumar",
            "Camillo J Taylor"
        ],
        "tldr": "This paper presents a real-time, computationally efficient method for detecting and mapping transparent obstacles using sensor fusion on a low-SWaP aerial robot, addressing a significant challenge in autonomous navigation.",
        "tldr_zh": "本文提出了一种实时、计算高效的方法，通过传感器融合在低功耗无人机上检测和绘制透明障碍物地图，解决了自主导航中的一个重大挑战。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]