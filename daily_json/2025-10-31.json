[
    {
        "title": "Self-localization on a 3D map by fusing global and local features from a monocular camera",
        "summary": "Self-localization on a 3D map by using an inexpensive monocular camera is\nrequired to realize autonomous driving. Self-localization based on a camera\noften uses a convolutional neural network (CNN) that can extract local features\nthat are calculated by nearby pixels. However, when dynamic obstacles, such as\npeople, are present, CNN does not work well. This study proposes a new method\ncombining CNN with Vision Transformer, which excels at extracting global\nfeatures that show the relationship of patches on whole image. Experimental\nresults showed that, compared to the state-of-the-art method (SOTA), the\naccuracy improvement rate in a CG dataset with dynamic obstacles is 1.5 times\nhigher than that without dynamic obstacles. Moreover, the self-localization\nerror of our method is 20.1% smaller than that of SOTA on public datasets.\nAdditionally, our robot using our method can localize itself with 7.51cm error\non average, which is more accurate than SOTA.",
        "url": "http://arxiv.org/abs/2510.26170v1",
        "published_date": "2025-10-30T06:14:22+00:00",
        "updated_date": "2025-10-30T06:14:22+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Satoshi Kikuch",
            "Masaya Kato",
            "Tsuyoshi Tasaki"
        ],
        "tldr": "This paper proposes a novel self-localization method for autonomous driving that fuses CNN and Vision Transformer to improve accuracy in dynamic environments, outperforming state-of-the-art methods in both CG and real-world datasets.",
        "tldr_zh": "本文提出了一种用于自动驾驶的新型自定位方法，该方法融合了CNN和Vision Transformer，以提高在动态环境中的准确性，并在CG和真实世界数据集中均优于现有技术。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios",
        "summary": "Vision-based end-to-end (E2E) driving has garnered significant interest in\nthe research community due to its scalability and synergy with multimodal large\nlanguage models (MLLMs). However, current E2E driving benchmarks primarily\nfeature nominal scenarios, failing to adequately test the true potential of\nthese systems. Furthermore, existing open-loop evaluation metrics often fall\nshort in capturing the multi-modal nature of driving or effectively evaluating\nperformance in long-tail scenarios. To address these gaps, we introduce the\nWaymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021\ndriving segments (approximately 12 hours), specifically curated for challenging\nlong-tail scenarios that that are rare in daily life with an occurring\nfrequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the\nhigh-level routing information, ego states, and 360-degree camera views from 8\nsurrounding cameras. To evaluate the E2E driving performance on these long-tail\nsituations, we propose a novel open-loop evaluation metric: Rater Feedback\nScore (RFS). Unlike conventional metrics that measure the distance between\npredicted way points and the logs, RFS measures how closely the predicted\ntrajectory matches rater-annotated trajectory preference labels. We have\nreleased rater preference labels for all WOD-E2E validation set segments, while\nthe held out test set labels have been used for the 2025 WOD-E2E Challenge.\nThrough our work, we aim to foster state of the art research into\ngeneralizable, robust, and safe end-to-end autonomous driving agents capable of\nhandling complex real-world situations.",
        "url": "http://arxiv.org/abs/2510.26125v1",
        "published_date": "2025-10-30T04:25:33+00:00",
        "updated_date": "2025-10-30T04:25:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Runsheng Xu",
            "Hubert Lin",
            "Wonseok Jeon",
            "Hao Feng",
            "Yuliang Zou",
            "Liting Sun",
            "John Gorman",
            "Kate Tolstaya",
            "Sarah Tang",
            "Brandyn White",
            "Ben Sapp",
            "Mingxing Tan",
            "Jyh-Jing Hwang",
            "Drago Anguelov"
        ],
        "tldr": "The paper introduces WOD-E2E, a new driving dataset focused on challenging long-tail scenarios, along with a novel evaluation metric (RFS) based on rater feedback to better assess E2E driving performance in these situations.",
        "tldr_zh": "该论文介绍了一个新的驾驶数据集 WOD-E2E，专注于具有挑战性的长尾场景，并提出了一种基于评分员反馈的新型评估指标 (RFS)，以更好地评估在这些场景中的端到端驾驶性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles",
        "summary": "Autonomous Vehicles (AVs) are transforming the future of transportation\nthrough advances in intelligent perception, decision-making, and control\nsystems. However, their success is tied to one core capability, reliable object\ndetection in complex and multimodal environments. While recent breakthroughs in\nComputer Vision (CV) and Artificial Intelligence (AI) have driven remarkable\nprogress, the field still faces a critical challenge as knowledge remains\nfragmented across multimodal perception, contextual reasoning, and cooperative\nintelligence. This survey bridges that gap by delivering a forward-looking\nanalysis of object detection in AVs, emphasizing emerging paradigms such as\nVision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI\nrather than re-examining outdated techniques. We begin by systematically\nreviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR,\nand Radar) and their fusion strategies, highlighting not only their\ncapabilities and limitations in dynamic driving environments but also their\npotential to integrate with recent advances in LLM/VLM-driven perception\nframeworks. Next, we introduce a structured categorization of AV datasets that\nmoves beyond simple collections, positioning ego-vehicle, infrastructure-based,\nand cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a\ncross-analysis of data structures and characteristics. Ultimately, we analyze\ncutting-edge detection methodologies, ranging from 2D and 3D pipelines to\nhybrid sensor fusion, with particular attention to emerging transformer-driven\napproaches powered by Vision Transformers (ViTs), Large and Small Language\nModels (SLMs), and VLMs. By synthesizing these perspectives, our survey\ndelivers a clear roadmap of current capabilities, open challenges, and future\nopportunities.",
        "url": "http://arxiv.org/abs/2510.26641v1",
        "published_date": "2025-10-30T16:08:25+00:00",
        "updated_date": "2025-10-30T16:08:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sayed Pedram Haeri Boroujeni",
            "Niloufar Mehrabi",
            "Hazim Alzorgan",
            "Ahmad Sarlak",
            "Mahlagha Fazeli",
            "Abolfazl Razi"
        ],
        "tldr": "This survey paper provides a forward-looking analysis of object detection in autonomous vehicles, focusing on the integration of VLMs/LLMs and generative AI, and highlighting current capabilities, open challenges, and future opportunities.",
        "tldr_zh": "这篇综述性论文对自动驾驶中的目标检测进行了前瞻性分析，重点关注视觉语言模型（VLM）/大型语言模型（LLM）和生成式人工智能的集成，并强调了当前的能力、开放的挑战和未来的机遇。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PointSt3R: Point Tracking through 3D Grounded Correspondence",
        "summary": "Recent advances in foundational 3D reconstruction models, such as DUSt3R and\nMASt3R, have shown great potential in 2D and 3D correspondence in static\nscenes. In this paper, we propose to adapt them for the task of point tracking\nthrough 3D grounded correspondence. We first demonstrate that these models are\ncompetitive point trackers when focusing on static points, present in current\npoint tracking benchmarks ($+33.5\\%$ on EgoPoints vs. CoTracker2). We propose\nto combine the reconstruction loss with training for dynamic correspondence\nalong with a visibility head, and fine-tuning MASt3R for point tracking using a\nrelatively small amount of synthetic data. Importantly, we only train and\nevaluate on pairs of frames where one contains the query point, effectively\nremoving any temporal context. Using a mix of dynamic and static point\ncorrespondences, we achieve competitive or superior point tracking results on\nfour datasets (e.g. competitive on TAP-Vid-DAVIS 73.8 $\\delta_{avg}$ / 85.8\\%\nocclusion acc. for PointSt3R compared to 75.7 / 88.3\\% for CoTracker2; and\nsignificantly outperform CoTracker3 on EgoPoints 61.3 vs 54.2 and RGB-S 87.0 vs\n82.8). We also present results on 3D point tracking along with several\nablations on training datasets and percentage of dynamic correspondences.",
        "url": "http://arxiv.org/abs/2510.26443v1",
        "published_date": "2025-10-30T12:46:56+00:00",
        "updated_date": "2025-10-30T12:46:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rhodri Guerrier",
            "Adam W. Harley",
            "Dima Damen"
        ],
        "tldr": "The paper adapts recent 3D reconstruction models for point tracking, achieving competitive results on several datasets by combining reconstruction loss with dynamic correspondence training and fine-tuning on synthetic data.",
        "tldr_zh": "该论文将最新的3D重建模型用于点跟踪，通过将重建损失与动态对应训练相结合，并在合成数据上进行微调，在多个数据集上取得了具有竞争力的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian Splatting SLAM",
        "summary": "Autonomous robots in orchards require real-time 3D scene understanding\ndespite repetitive row geometry, seasonal appearance changes, and wind-driven\nfoliage motion. We present AgriGS-SLAM, a Visual--LiDAR SLAM framework that\ncouples direct LiDAR odometry and loop closures with multi-camera 3D Gaussian\nSplatting (3DGS) rendering. Batch rasterization across complementary viewpoints\nrecovers orchard structure under occlusions, while a unified gradient-driven\nmap lifecycle executed between keyframes preserves fine details and bounds\nmemory. Pose refinement is guided by a probabilistic LiDAR-based depth\nconsistency term, back-propagated through the camera projection to tighten\ngeometry-appearance coupling. We deploy the system on a field platform in apple\nand pear orchards across dormancy, flowering, and harvesting, using a\nstandardized trajectory protocol that evaluates both training-view and\nnovel-view synthesis to reduce 3DGS overfitting in evaluation. Across seasons\nand sites, AgriGS-SLAM delivers sharper, more stable reconstructions and\nsteadier trajectories than recent state-of-the-art 3DGS-SLAM baselines while\nmaintaining real-time performance on-tractor. While demonstrated in orchard\nmonitoring, the approach can be applied to other outdoor domains requiring\nrobust multimodal perception.",
        "url": "http://arxiv.org/abs/2510.26358v1",
        "published_date": "2025-10-30T11:08:23+00:00",
        "updated_date": "2025-10-30T11:08:23+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Mirko Usuelli",
            "David Rapado-Rincon",
            "Gert Kootstra",
            "Matteo Matteucci"
        ],
        "tldr": "The paper introduces AgriGS-SLAM, a Visual-LiDAR SLAM framework using multi-camera 3D Gaussian Splatting for robust orchard mapping across seasons, achieving real-time performance and improved accuracy compared to state-of-the-art methods.",
        "tldr_zh": "本文介绍了一种名为 AgriGS-SLAM 的视觉-激光雷达 SLAM 框架，该框架利用多相机 3D 高斯溅射技术实现对果园跨季节的鲁棒测绘，与最先进的方法相比，实现了实时性能和更高的精度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond Imitation: Constraint-Aware Trajectory Generation with Flow Matching For End-to-End Autonomous Driving",
        "summary": "Planning is a critical component of end-to-end autonomous driving. However,\nprevailing imitation learning methods often suffer from mode collapse, failing\nto produce diverse trajectory hypotheses. Meanwhile, existing generative\napproaches struggle to incorporate crucial safety and physical constraints\ndirectly into the generative process, necessitating an additional optimization\nstage to refine their outputs. To address these limitations, we propose CATG, a\nnovel planning framework that leverages Constrained Flow Matching. Concretely,\nCATG explicitly models the flow matching process, which inherently mitigates\nmode collapse and allows for flexible guidance from various conditioning\nsignals. Our primary contribution is the novel imposition of explicit\nconstraints directly within the flow matching process, ensuring that the\ngenerated trajectories adhere to vital safety and kinematic rules. Secondly,\nCATG parameterizes driving aggressiveness as a control signal during\ngeneration, enabling precise manipulation of trajectory style. Notably, on the\nNavSim v2 challenge, CATG achieved 2nd place with an EPDMS score of 51.31 and\nwas honored with the Innovation Award.",
        "url": "http://arxiv.org/abs/2510.26292v1",
        "published_date": "2025-10-30T09:24:34+00:00",
        "updated_date": "2025-10-30T09:24:34+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lin Liu",
            "Guanyi Yu",
            "Ziying Song",
            "Junqiao Li",
            "Caiyan Jia",
            "Feiyang Jia",
            "Peiliang Wu",
            "Yandan Luo"
        ],
        "tldr": "This paper introduces CATG, a novel autonomous driving planning framework that uses constrained flow matching to generate diverse, safe, and controllable trajectories, achieving strong performance in the NavSim v2 challenge.",
        "tldr_zh": "本文介绍了一种名为CATG的新型自动驾驶规划框架，该框架利用约束流匹配生成多样、安全且可控的轨迹，并在NavSim v2挑战赛中取得了优异的成绩。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM",
        "summary": "Attention models have recently emerged as a powerful approach, demonstrating\nsignificant progress in various fields. Visualization techniques, such as class\nactivation mapping, provide visual insights into the reasoning of convolutional\nneural networks (CNNs). Using network gradients, it is possible to identify\nregions where the network pays attention during image recognition tasks.\nFurthermore, these gradients can be combined with CNN features to localize more\ngeneralizable, task-specific attentive (salient) regions within scenes.\nHowever, explicit use of this gradient-based attention information integrated\ndirectly into CNN representations for semantic object understanding remains\nlimited. Such integration is particularly beneficial for visual tasks like\nsimultaneous localization and mapping (SLAM), where CNN representations\nenriched with spatially attentive object locations can enhance performance. In\nthis work, we propose utilizing task-specific network attention for RGB-D\nindoor SLAM. Specifically, we integrate layer-wise attention information\nderived from network gradients with CNN feature representations to improve\nframe association performance. Experimental results indicate improved\nperformance compared to baseline methods, particularly for large environments.",
        "url": "http://arxiv.org/abs/2510.26131v1",
        "published_date": "2025-10-30T04:31:56+00:00",
        "updated_date": "2025-10-30T04:31:56+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Ali Caglayan",
            "Nevrez Imamoglu",
            "Oguzhan Guclu",
            "Ali Osman Serhatoglu",
            "Ahmet Burak Can",
            "Ryosuke Nakamura"
        ],
        "tldr": "This paper proposes a method to improve RGB-D SLAM performance by integrating layer-wise attention information from network gradients with CNN feature representations for better frame association, showing improved performance in large environments.",
        "tldr_zh": "本文提出了一种通过整合来自网络梯度的逐层注意力信息与CNN特征表示来改进RGB-D SLAM性能的方法，从而实现更好的帧关联，并在大型环境中表现出更好的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "JOGS: Joint Optimization of Pose Estimation and 3D Gaussian Splatting",
        "summary": "Traditional novel view synthesis methods heavily rely on external camera pose\nestimation tools such as COLMAP, which often introduce computational\nbottlenecks and propagate errors. To address these challenges, we propose a\nunified framework that jointly optimizes 3D Gaussian points and camera poses\nwithout requiring pre-calibrated inputs. Our approach iteratively refines 3D\nGaussian parameters and updates camera poses through a novel co-optimization\nstrategy, ensuring simultaneous improvements in scene reconstruction fidelity\nand pose accuracy. The key innovation lies in decoupling the joint optimization\ninto two interleaved phases: first, updating 3D Gaussian parameters via\ndifferentiable rendering with fixed poses, and second, refining camera poses\nusing a customized 3D optical flow algorithm that incorporates geometric and\nphotometric constraints. This formulation progressively reduces projection\nerrors, particularly in challenging scenarios with large viewpoint variations\nand sparse feature distributions, where traditional methods struggle. Extensive\nevaluations on multiple datasets demonstrate that our approach significantly\noutperforms existing COLMAP-free techniques in reconstruction quality, and also\nsurpasses the standard COLMAP-based baseline in general.",
        "url": "http://arxiv.org/abs/2510.26117v1",
        "published_date": "2025-10-30T04:00:07+00:00",
        "updated_date": "2025-10-30T04:00:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuxuan Li",
            "Tao Wang",
            "Xianben Yang"
        ],
        "tldr": "This paper presents a method for jointly optimizing 3D Gaussian splatting and camera pose estimation, eliminating the need for COLMAP and improving reconstruction quality and pose accuracy, especially in challenging scenarios.",
        "tldr_zh": "本文提出了一种联合优化3D高斯溅射和相机姿态估计的方法，无需COLMAP，并提高了重建质量和姿态精度，尤其是在具有挑战性的场景中。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DARTS: A Drone-Based AI-Powered Real-Time Traffic Incident Detection System",
        "summary": "Rapid and reliable incident detection is critical for reducing crash-related\nfatalities, injuries, and congestion. However, conventional methods, such as\nclosed-circuit television, dashcam footage, and sensor-based detection,\nseparate detection from verification, suffer from limited flexibility, and\nrequire dense infrastructure or high penetration rates, restricting\nadaptability and scalability to shifting incident hotspots. To overcome these\nchallenges, we developed DARTS, a drone-based, AI-powered real-time traffic\nincident detection system. DARTS integrates drones' high mobility and aerial\nperspective for adaptive surveillance, thermal imaging for better\nlow-visibility performance and privacy protection, and a lightweight deep\nlearning framework for real-time vehicle trajectory extraction and incident\ndetection. The system achieved 99% detection accuracy on a self-collected\ndataset and supports simultaneous online visual verification, severity\nassessment, and incident-induced congestion propagation monitoring via a\nweb-based interface. In a field test on Interstate 75 in Florida, DARTS\ndetected and verified a rear-end collision 12 minutes earlier than the local\ntransportation management center and monitored incident-induced congestion\npropagation, suggesting potential to support faster emergency response and\nenable proactive traffic control to reduce congestion and secondary crash risk.\nCrucially, DARTS's flexible deployment architecture reduces dependence on\nfrequent physical patrols, indicating potential scalability and\ncost-effectiveness for use in remote areas and resource-constrained settings.\nThis study presents a promising step toward a more flexible and integrated\nreal-time traffic incident detection system, with significant implications for\nthe operational efficiency and responsiveness of modern transportation\nmanagement.",
        "url": "http://arxiv.org/abs/2510.26004v1",
        "published_date": "2025-10-29T22:32:16+00:00",
        "updated_date": "2025-10-29T22:32:16+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Bai Li",
            "Achilleas Kourtellis",
            "Rong Cao",
            "Joseph Post",
            "Brian Porter",
            "Yu Zhang"
        ],
        "tldr": "The paper presents DARTS, a drone-based AI system for real-time traffic incident detection, demonstrating faster detection and improved monitoring compared to conventional methods, potentially leading to quicker emergency response and proactive traffic control.",
        "tldr_zh": "该论文介绍了一种名为DARTS的基于无人机的人工智能实时交通事件检测系统，与传统方法相比，该系统展示了更快的检测速度和改进的监控能力，有可能加快应急响应和实现主动交通控制。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]