[
    {
        "title": "ROVR-Open-Dataset: A Large-Scale Depth Dataset for Autonomous Driving",
        "summary": "Depth estimation is a fundamental task for 3D scene understanding in\nautonomous driving, robotics, and augmented reality. Existing depth datasets,\nsuch as KITTI, nuScenes, and DDAD, have advanced the field but suffer from\nlimitations in diversity and scalability. As benchmark performance on these\ndatasets approaches saturation, there is an increasing need for a new\ngeneration of large-scale, diverse, and cost-efficient datasets to support the\nera of foundation models and multi-modal learning. To address these challenges,\nwe introduce a large-scale, diverse, frame-wise continuous dataset for depth\nestimation in dynamic outdoor driving environments, comprising 20K video frames\nto evaluate existing methods. Our lightweight acquisition pipeline ensures\nbroad scene coverage at low cost, while sparse yet statistically sufficient\nground truth enables robust training. Compared to existing datasets, ours\npresents greater diversity in driving scenarios and lower depth density,\ncreating new challenges for generalization. Benchmark experiments with standard\nmonocular depth estimation models validate the dataset's utility and highlight\nsubstantial performance gaps in challenging conditions, establishing a new\nplatform for advancing depth estimation research.",
        "url": "http://arxiv.org/abs/2508.13977v1",
        "published_date": "2025-08-19T16:13:49+00:00",
        "updated_date": "2025-08-19T16:13:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xianda Guo",
            "Ruijun Zhang",
            "Yiqun Duan",
            "Ruilin Wang",
            "Keyuan Zhou",
            "Wenzhao Zheng",
            "Wenke Huang",
            "Gangwei Xu",
            "Mike Horton",
            "Yuan Si",
            "Hao Zhao",
            "Long Chen"
        ],
        "tldr": "The paper introduces ROVR-Open-Dataset, a large-scale, diverse depth dataset for autonomous driving designed to address the limitations of existing datasets and support foundation models.",
        "tldr_zh": "该论文介绍了ROVR-Open-Dataset，一个大规模、多样化的深度数据集，用于自动驾驶，旨在解决现有数据集的局限性，并支持基础模型。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "MR6D: Benchmarking 6D Pose Estimation for Mobile Robots",
        "summary": "Existing 6D pose estimation datasets primarily focus on small household\nobjects typically handled by robot arm manipulators, limiting their relevance\nto mobile robotics. Mobile platforms often operate without manipulators,\ninteract with larger objects, and face challenges such as long-range\nperception, heavy self-occlusion, and diverse camera perspectives. While recent\nmodels generalize well to unseen objects, evaluations remain confined to\nhousehold-like settings that overlook these factors. We introduce MR6D, a\ndataset designed for 6D pose estimation for mobile robots in industrial\nenvironments. It includes 92 real-world scenes featuring 16 unique objects\nacross static and dynamic interactions. MR6D captures the challenges specific\nto mobile platforms, including distant viewpoints, varied object\nconfigurations, larger object sizes, and complex occlusion/self-occlusion\npatterns. Initial experiments reveal that current 6D pipelines underperform in\nthese settings, with 2D segmentation being another hurdle. MR6D establishes a\nfoundation for developing and evaluating pose estimation methods tailored to\nthe demands of mobile robotics. The dataset is available at\nhttps://huggingface.co/datasets/anas-gouda/mr6d.",
        "url": "http://arxiv.org/abs/2508.13775v1",
        "published_date": "2025-08-19T12:21:34+00:00",
        "updated_date": "2025-08-19T12:21:34+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Anas Gouda",
            "Shrutarv Awasthi",
            "Christian Blesing",
            "Lokeshwaran Manohar",
            "Frank Hoffmann",
            "Alice Kirchheim"
        ],
        "tldr": "The paper introduces MR6D, a new dataset specifically designed for 6D pose estimation for mobile robots in industrial environments, addressing the limitations of existing datasets focused on smaller household objects.",
        "tldr_zh": "该论文介绍了MR6D，一个新的数据集，专门为工业环境中移动机器人的6D姿态估计而设计，解决了现有数据集侧重于小型家用物体的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 10,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Unleashing Semantic and Geometric Priors for 3D Scene Completion",
        "summary": "Camera-based 3D semantic scene completion (SSC) provides dense geometric and\nsemantic perception for autonomous driving and robotic navigation. However,\nexisting methods rely on a coupled encoder to deliver both semantic and\ngeometric priors, which forces the model to make a trade-off between\nconflicting demands and limits its overall performance. To tackle these\nchallenges, we propose FoundationSSC, a novel framework that performs dual\ndecoupling at both the source and pathway levels. At the source level, we\nintroduce a foundation encoder that provides rich semantic feature priors for\nthe semantic branch and high-fidelity stereo cost volumes for the geometric\nbranch. At the pathway level, these priors are refined through specialised,\ndecoupled pathways, yielding superior semantic context and depth distributions.\nOur dual-decoupling design produces disentangled and refined inputs, which are\nthen utilised by a hybrid view transformation to generate complementary 3D\nfeatures. Additionally, we introduce a novel Axis-Aware Fusion (AAF) module\nthat addresses the often-overlooked challenge of fusing these features by\nanisotropically merging them into a unified representation. Extensive\nexperiments demonstrate the advantages of FoundationSSC, achieving simultaneous\nimprovements in both semantic and geometric metrics, surpassing prior bests by\n+0.23 mIoU and +2.03 IoU on SemanticKITTI. Additionally, we achieve\nstate-of-the-art performance on SSCBench-KITTI-360, with 21.78 mIoU and 48.61\nIoU. The code will be released upon acceptance.",
        "url": "http://arxiv.org/abs/2508.13601v1",
        "published_date": "2025-08-19T08:10:39+00:00",
        "updated_date": "2025-08-19T08:10:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shiyuan Chen",
            "Wei Sui",
            "Bohao Zhang",
            "Zeyd Boukhers",
            "John See",
            "Cong Yang"
        ],
        "tldr": "The paper introduces FoundationSSC, a novel framework for 3D semantic scene completion that uses dual decoupling and axis-aware fusion to achieve state-of-the-art performance on SemanticKITTI and SSCBench-KITTI-360 datasets, surpassing previous methods in both semantic and geometric metrics.",
        "tldr_zh": "该论文介绍了FoundationSSC，一种用于3D语义场景补全的新框架，它使用双重解耦和轴向感知融合，在SemanticKITTI和SSCBench-KITTI-360数据集上实现了最先进的性能，在语义和几何指标方面均超越了以往方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Online 3D Gaussian Splatting Modeling with Novel View Selection",
        "summary": "This study addresses the challenge of generating online 3D Gaussian Splatting\n(3DGS) models from RGB-only frames. Previous studies have employed dense SLAM\ntechniques to estimate 3D scenes from keyframes for 3DGS model construction.\nHowever, these methods are limited by their reliance solely on keyframes, which\nare insufficient to capture an entire scene, resulting in incomplete\nreconstructions. Moreover, building a generalizable model requires\nincorporating frames from diverse viewpoints to achieve broader scene coverage.\nHowever, online processing restricts the use of many frames or extensive\ntraining iterations. Therefore, we propose a novel method for high-quality 3DGS\nmodeling that improves model completeness through adaptive view selection. By\nanalyzing reconstruction quality online, our approach selects optimal\nnon-keyframes for additional training. By integrating both keyframes and\nselected non-keyframes, the method refines incomplete regions from diverse\nviewpoints, significantly enhancing completeness. We also present a framework\nthat incorporates an online multi-view stereo approach, ensuring consistency in\n3D information throughout the 3DGS modeling process. Experimental results\ndemonstrate that our method outperforms state-of-the-art methods, delivering\nexceptional performance in complex outdoor scenes.",
        "url": "http://arxiv.org/abs/2508.14014v1",
        "published_date": "2025-08-19T17:25:12+00:00",
        "updated_date": "2025-08-19T17:25:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Byeonggwon Lee",
            "Junkyu Park",
            "Khang Truong Giang",
            "Soohwan Song"
        ],
        "tldr": "This paper introduces a novel online 3D Gaussian Splatting modeling approach that uses adaptive view selection to improve model completeness by incorporating both keyframes and selected non-keyframes, outperforming state-of-the-art methods in complex outdoor scenes.",
        "tldr_zh": "本文提出了一种新的在线3D高斯溅射建模方法，该方法通过自适应视角选择，结合关键帧和选定的非关键帧来提高模型的完整性，并在复杂的户外场景中优于现有技术。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Self-Supervised Sparse Sensor Fusion for Long Range Perception",
        "summary": "Outside of urban hubs, autonomous cars and trucks have to master driving on\nintercity highways. Safe, long-distance highway travel at speeds exceeding 100\nkm/h demands perception distances of at least 250 m, which is about five times\nthe 50-100m typically addressed in city driving, to allow sufficient planning\nand braking margins. Increasing the perception ranges also allows to extend\nautonomy from light two-ton passenger vehicles to large-scale forty-ton trucks,\nwhich need a longer planning horizon due to their high inertia. However, most\nexisting perception approaches focus on shorter ranges and rely on Bird's Eye\nView (BEV) representations, which incur quadratic increases in memory and\ncompute costs as distance grows. To overcome this limitation, we built on top\nof a sparse representation and introduced an efficient 3D encoding of\nmulti-modal and temporal features, along with a novel self-supervised\npre-training scheme that enables large-scale learning from unlabeled\ncamera-LiDAR data. Our approach extends perception distances to 250 meters and\nachieves an 26.6% improvement in mAP in object detection and a decrease of\n30.5% in Chamfer Distance in LiDAR forecasting compared to existing methods,\nreaching distances up to 250 meters. Project Page:\nhttps://light.princeton.edu/lrs4fusion/",
        "url": "http://arxiv.org/abs/2508.13995v1",
        "published_date": "2025-08-19T16:40:29+00:00",
        "updated_date": "2025-08-19T16:40:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Edoardo Palladin",
            "Samuel Brucker",
            "Filippo Ghilotti",
            "Praveen Narayanan",
            "Mario Bijelic",
            "Felix Heide"
        ],
        "tldr": "This paper introduces a self-supervised sparse sensor fusion approach to extend perception distances in autonomous driving on highways to 250 meters, achieving significant improvements in object detection and LiDAR forecasting.",
        "tldr_zh": "本文介绍了一种自监督稀疏传感器融合方法，旨在将自动驾驶在高速公路上的感知距离扩展到250米，并在物体检测和激光雷达预测方面取得了显著改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RCGNet: RGB-based Category-Level 6D Object Pose Estimation with Geometric Guidance",
        "summary": "While most current RGB-D-based category-level object pose estimation methods\nachieve strong performance, they face significant challenges in scenes lacking\ndepth information. In this paper, we propose a novel category-level object pose\nestimation approach that relies solely on RGB images. This method enables\naccurate pose estimation in real-world scenarios without the need for depth\ndata. Specifically, we design a transformer-based neural network for\ncategory-level object pose estimation, where the transformer is employed to\npredict and fuse the geometric features of the target object. To ensure that\nthese predicted geometric features faithfully capture the object's geometry, we\nintroduce a geometric feature-guided algorithm, which enhances the network's\nability to effectively represent the object's geometric information. Finally,\nwe utilize the RANSAC-PnP algorithm to compute the object's pose, addressing\nthe challenges associated with variable object scales in pose estimation.\nExperimental results on benchmark datasets demonstrate that our approach is not\nonly highly efficient but also achieves superior accuracy compared to previous\nRGB-based methods. These promising results offer a new perspective for\nadvancing category-level object pose estimation using RGB images.",
        "url": "http://arxiv.org/abs/2508.13623v1",
        "published_date": "2025-08-19T08:34:17+00:00",
        "updated_date": "2025-08-19T08:34:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sheng Yu",
            "Di-Hua Zhai",
            "Yuanqing Xia"
        ],
        "tldr": "The paper proposes an RGB-based category-level 6D object pose estimation method using a transformer network and geometric feature guidance, achieving superior accuracy compared to previous RGB-based approaches.",
        "tldr_zh": "该论文提出了一种基于RGB的类别级6D物体姿态估计方法，使用Transformer网络和几何特征引导，与以往的基于RGB的方法相比，实现了更高的精度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Bridging Clear and Adverse Driving Conditions",
        "summary": "Autonomous Driving (AD) systems exhibit markedly degraded performance under\nadverse environmental conditions, such as low illumination and precipitation.\nThe underrepresentation of adverse conditions in AD datasets makes it\nchallenging to address this deficiency. To circumvent the prohibitive cost of\nacquiring and annotating adverse weather data, we propose a novel Domain\nAdaptation (DA) pipeline that transforms clear-weather images into fog, rain,\nsnow, and nighttime images. Here, we systematically develop and evaluate\nseveral novel data-generation pipelines, including simulation-only, GAN-based,\nand hybrid diffusion-GAN approaches, to synthesize photorealistic adverse\nimages from labelled clear images. We leverage an existing DA GAN, extend it to\nsupport auxiliary inputs, and develop a novel training recipe that leverages\nboth simulated and real images. The simulated images facilitate exact\nsupervision by providing perfectly matched image pairs, while the real images\nhelp bridge the simulation-to-real (sim2real) gap. We further introduce a\nmethod to mitigate hallucinations and artifacts in Stable-Diffusion\nImage-to-Image (img2img) outputs by blending them adaptively with their\nprogenitor images. We finetune downstream models on our synthetic data and\nevaluate them on the Adverse Conditions Dataset with Correspondences (ACDC). We\nachieve 1.85 percent overall improvement in semantic segmentation, and 4.62\npercent on nighttime, demonstrating the efficacy of our hybrid method for\nrobust AD perception under challenging conditions.",
        "url": "http://arxiv.org/abs/2508.13592v1",
        "published_date": "2025-08-19T07:58:05+00:00",
        "updated_date": "2025-08-19T07:58:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yoel Shapiro",
            "Yahia Showgan",
            "Koustav Mullick"
        ],
        "tldr": "This paper introduces a domain adaptation pipeline using simulation, GANs, and diffusion models to generate synthetic adverse weather images for training autonomous driving systems, demonstrating improved semantic segmentation performance on the ACDC dataset.",
        "tldr_zh": "该论文提出了一种使用模拟、GAN 和扩散模型的领域自适应管道，用于生成合成的恶劣天气图像，以训练自动驾驶系统，并在 ACDC 数据集上展示了改进的语义分割性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence",
        "summary": "Imitating tool manipulation from human videos offers an intuitive approach to\nteaching robots, while also providing a promising and scalable alternative to\nlabor-intensive teleoperation data collection for visuomotor policy learning.\nWhile humans can mimic tool manipulation behavior by observing others perform a\ntask just once and effortlessly transfer the skill to diverse tools for\nfunctionally equivalent tasks, current robots struggle to achieve this level of\ngeneralization. A key challenge lies in establishing function-level\ncorrespondences, considering the significant geometric variations among\nfunctionally similar tools, referred to as intra-function variations. To\naddress this challenge, we propose MimicFunc, a framework that establishes\nfunctional correspondences with function frame, a function-centric local\ncoordinate frame constructed with keypoint-based abstraction, for imitating\ntool manipulation skills. Experiments demonstrate that MimicFunc effectively\nenables the robot to generalize the skill from a single RGB-D human video to\nmanipulating novel tools for functionally equivalent tasks. Furthermore,\nleveraging MimicFunc's one-shot generalization capability, the generated\nrollouts can be used to train visuomotor policies without requiring\nlabor-intensive teleoperation data collection for novel objects. Our code and\nvideo are available at https://sites.google.com/view/mimicfunc.",
        "url": "http://arxiv.org/abs/2508.13534v1",
        "published_date": "2025-08-19T05:49:47+00:00",
        "updated_date": "2025-08-19T05:49:47+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Chao Tang",
            "Anxing Xiao",
            "Yuhong Deng",
            "Tianrun Hu",
            "Wenlong Dong",
            "Hanbo Zhang",
            "David Hsu",
            "Hong Zhang"
        ],
        "tldr": "The paper introduces MimicFunc, a framework for robots to imitate tool manipulation skills from a single human video, enabling generalization to novel tools via functional correspondence and keypoint-based abstraction. This allows for training visuomotor policies without extensive teleoperation data collection.",
        "tldr_zh": "该论文介绍了MimicFunc，一个让机器人从单个人类视频中模仿工具操作技能的框架，通过功能对应和基于关键点的抽象实现对新工具的泛化。这使得无需大量遥操作数据收集即可训练视觉运动策略。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CORENet: Cross-Modal 4D Radar Denoising Network with LiDAR Supervision for Autonomous Driving",
        "summary": "4D radar-based object detection has garnered great attention for its\nrobustness in adverse weather conditions and capacity to deliver rich spatial\ninformation across diverse driving scenarios. Nevertheless, the sparse and\nnoisy nature of 4D radar point clouds poses substantial challenges for\neffective perception. To address the limitation, we present CORENet, a novel\ncross-modal denoising framework that leverages LiDAR supervision to identify\nnoise patterns and extract discriminative features from raw 4D radar data.\nDesigned as a plug-and-play architecture, our solution enables seamless\nintegration into voxel-based detection frameworks without modifying existing\npipelines. Notably, the proposed method only utilizes LiDAR data for\ncross-modal supervision during training while maintaining full radar-only\noperation during inference. Extensive evaluation on the challenging Dual-Radar\ndataset, which is characterized by elevated noise level, demonstrates the\neffectiveness of our framework in enhancing detection robustness. Comprehensive\nexperiments validate that CORENet achieves superior performance compared to\nexisting mainstream approaches.",
        "url": "http://arxiv.org/abs/2508.13485v1",
        "published_date": "2025-08-19T03:30:21+00:00",
        "updated_date": "2025-08-19T03:30:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Fuyang Liu",
            "Jilin Mei",
            "Fangyuan Mao",
            "Chen Min",
            "Yan Xing",
            "Yu Hu"
        ],
        "tldr": "CORENet is a novel cross-modal denoising framework using LiDAR supervision during training to enhance 4D radar-based object detection in autonomous driving, achieving superior performance on the Dual-Radar dataset.",
        "tldr_zh": "CORENet是一个新颖的跨模态去噪框架，在训练期间利用激光雷达监督来增强自动驾驶中基于4D雷达的目标检测，并在Dual-Radar数据集上实现了卓越的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "STER-VLM: Spatio-Temporal With Enhanced Reference Vision-Language Models",
        "summary": "Vision-language models (VLMs) have emerged as powerful tools for enabling\nautomated traffic analysis; however, current approaches often demand\nsubstantial computational resources and struggle with fine-grained\nspatio-temporal understanding. This paper introduces STER-VLM, a\ncomputationally efficient framework that enhances VLM performance through (1)\ncaption decomposition to tackle spatial and temporal information separately,\n(2) temporal frame selection with best-view filtering for sufficient temporal\ninformation, and (3) reference-driven understanding for capturing fine-grained\nmotion and dynamic context and (4) curated visual/textual prompt techniques.\nExperimental results on the WTS \\cite{kong2024wts} and BDD \\cite{BDD} datasets\ndemonstrate substantial gains in semantic richness and traffic scene\ninterpretation. Our framework is validated through a decent test score of\n55.655 in the AI City Challenge 2025 Track 2, showing its effectiveness in\nadvancing resource-efficient and accurate traffic analysis for real-world\napplications.",
        "url": "http://arxiv.org/abs/2508.13470v1",
        "published_date": "2025-08-19T03:03:29+00:00",
        "updated_date": "2025-08-19T03:03:29+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Tinh-Anh Nguyen-Nhu",
            "Triet Dao Hoang Minh",
            "Dat To-Thanh",
            "Phuc Le-Gia",
            "Tuan Vo-Lan",
            "Tien-Huy Nguyen"
        ],
        "tldr": "STER-VLM enhances vision-language models for traffic analysis by using caption decomposition, temporal frame selection, reference-driven understanding, and visual/textual prompt techniques to improve performance and computational efficiency. The framework achieves strong results on traffic analysis benchmarks.",
        "tldr_zh": "STER-VLM 通过使用字幕分解、时间帧选择、参考驱动的理解以及视觉/文本提示技术来增强用于交通分析的视觉语言模型，从而提高性能和计算效率。该框架在交通分析基准测试中取得了优异的成果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Structured Prompting and Multi-Agent Knowledge Distillation for Traffic Video Interpretation and Risk Inference",
        "summary": "Comprehensive highway scene understanding and robust traffic risk inference\nare vital for advancing Intelligent Transportation Systems (ITS) and autonomous\ndriving. Traditional approaches often struggle with scalability and\ngeneralization, particularly under the complex and dynamic conditions of\nreal-world environments. To address these challenges, we introduce a novel\nstructured prompting and knowledge distillation framework that enables\nautomatic generation of high-quality traffic scene annotations and contextual\nrisk assessments. Our framework orchestrates two large Vision-Language Models\n(VLMs): GPT-4o and o3-mini, using a structured Chain-of-Thought (CoT) strategy\nto produce rich, multi-perspective outputs. These outputs serve as\nknowledge-enriched pseudo-annotations for supervised fine-tuning of a much\nsmaller student VLM. The resulting compact 3B-scale model, named VISTA (Vision\nfor Intelligent Scene and Traffic Analysis), is capable of understanding\nlow-resolution traffic videos and generating semantically faithful, risk-aware\ncaptions. Despite its significantly reduced parameter count, VISTA achieves\nstrong performance across established captioning metrics (BLEU-4, METEOR,\nROUGE-L, and CIDEr) when benchmarked against its teacher models. This\ndemonstrates that effective knowledge distillation and structured multi-agent\nsupervision can empower lightweight VLMs to capture complex reasoning\ncapabilities. The compact architecture of VISTA facilitates efficient\ndeployment on edge devices, enabling real-time risk monitoring without\nrequiring extensive infrastructure upgrades.",
        "url": "http://arxiv.org/abs/2508.13439v1",
        "published_date": "2025-08-19T01:44:02+00:00",
        "updated_date": "2025-08-19T01:44:02+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "eess.IV"
        ],
        "authors": [
            "Yunxiang Yang",
            "Ningning Xu",
            "Jidong J. Yang"
        ],
        "tldr": "The paper proposes a structured prompting and knowledge distillation framework using large VLMs to generate traffic scene annotations and risk assessments, resulting in a compact, efficient model (VISTA) suitable for edge deployment in ITS and autonomous driving applications.",
        "tldr_zh": "该论文提出了一种结构化提示和知识蒸馏框架，利用大型视觉语言模型生成交通场景注释和风险评估，从而产生一个紧凑高效的模型（VISTA），适用于智能交通系统和自动驾驶应用的边缘部署。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Prune2Drive: A Plug-and-Play Framework for Accelerating Vision-Language Models in Autonomous Driving",
        "summary": "Vision-Language Models (VLMs) have emerged as a promising paradigm in\nautonomous driving (AD), offering a unified framework for perception,\nreasoning, and decision-making by jointly modeling visual inputs and natural\nlanguage instructions. However, their deployment is hindered by the significant\ncomputational overhead incurred when processing high-resolution, multi-view\nimages, a standard setup in AD systems with six or more synchronized cameras.\nThis overhead stems from the large number of visual tokens generated during\nencoding, increasing inference latency and memory consumption due to the\nquadratic complexity of self-attention. To address these challenges, we propose\nPrune2Drive, a plug-and-play visual token pruning framework for multi-view VLMs\nin autonomous driving. Prune2Drive introduces two core innovations: (i) a\ndiversity-aware token selection mechanism inspired by farthest point sampling,\nwhich prioritizes semantic and spatial coverage across views rather than\nrelying solely on attention scores, and (ii) a view-adaptive pruning controller\nthat learns optimal pruning ratios for each camera view based on their\nimportance to downstream driving tasks. Unlike prior methods, Prune2Drive does\nnot require model retraining or access to attention maps, making it compatible\nwith modern efficient attention implementations. Extensive experiments on two\nlarge-scale multi-view driving benchmarks, DriveLM and DriveLMM-o1, show that\nPrune2Drive achieves significant speedups and memory savings while maintaining\nor improving task performance. When retaining only 10% of the visual tokens,\nour method achieves a 6.40$\\times$ speedup in the prefilling phase and consumes\n13.4% of the original FLOPs, with only a 3% performance drop on the DriveLM\nbenchmark.",
        "url": "http://arxiv.org/abs/2508.13305v1",
        "published_date": "2025-08-18T18:47:26+00:00",
        "updated_date": "2025-08-18T18:47:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Minhao Xiong",
            "Zichen Wen",
            "Zhuangcheng Gu",
            "Xuyang Liu",
            "Rui Zhang",
            "Hengrui Kang",
            "Jiabing Yang",
            "Junyuan Zhang",
            "Weijia Li",
            "Conghui He",
            "Yafei Wang",
            "Linfeng Zhang"
        ],
        "tldr": "Prune2Drive is a plug-and-play framework that accelerates vision-language models in autonomous driving by pruning visual tokens in a diversity-aware and view-adaptive manner, achieving significant speedups with minimal performance degradation without retraining.",
        "tldr_zh": "Prune2Drive 是一个即插即用的框架，通过以多样性感知和视角自适应的方式修剪视觉token来加速自动驾驶中的视觉-语言模型，在不重新训练的情况下实现显著加速，同时性能损失极小。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Self-Aware Adaptive Alignment: Enabling Accurate Perception for Intelligent Transportation Systems",
        "summary": "Achieving top-notch performance in Intelligent Transportation detection is a\ncritical research area. However, many challenges still need to be addressed\nwhen it comes to detecting in a cross-domain scenario. In this paper, we\npropose a Self-Aware Adaptive Alignment (SA3), by leveraging an efficient\nalignment mechanism and recognition strategy. Our proposed method employs a\nspecified attention-based alignment module trained on source and target domain\ndatasets to guide the image-level features alignment process, enabling the\nlocal-global adaptive alignment between the source domain and target domain.\nFeatures from both domains, whose channel importance is re-weighted, are fed\ninto the region proposal network, which facilitates the acquisition of salient\nregion features. Also, we introduce an instance-to-image level alignment module\nspecific to the target domain to adaptively mitigate the domain gap. To\nevaluate the proposed method, extensive experiments have been conducted on\npopular cross-domain object detection benchmarks. Experimental results show\nthat SA3 achieves superior results to the previous state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2508.13823v1",
        "published_date": "2025-08-19T13:33:03+00:00",
        "updated_date": "2025-08-19T13:33:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tong Xiang",
            "Hongxia Zhao",
            "Fenghua Zhu",
            "Yuanyuan Chen",
            "Yisheng Lv"
        ],
        "tldr": "The paper proposes a Self-Aware Adaptive Alignment (SA3) method for cross-domain object detection in Intelligent Transportation Systems, achieving state-of-the-art results on benchmark datasets.",
        "tldr_zh": "该论文提出了一种自感知自适应对齐 (SA3) 方法，用于智能交通系统中的跨域目标检测，并在基准数据集上取得了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "The 9th AI City Challenge",
        "summary": "The ninth AI City Challenge continues to advance real-world applications of\ncomputer vision and AI in transportation, industrial automation, and public\nsafety. The 2025 edition featured four tracks and saw a 17% increase in\nparticipation, with 245 teams from 15 countries registered on the evaluation\nserver. Public release of challenge datasets led to over 30,000 downloads to\ndate. Track 1 focused on multi-class 3D multi-camera tracking, involving\npeople, humanoids, autonomous mobile robots, and forklifts, using detailed\ncalibration and 3D bounding box annotations. Track 2 tackled video question\nanswering in traffic safety, with multi-camera incident understanding enriched\nby 3D gaze labels. Track 3 addressed fine-grained spatial reasoning in dynamic\nwarehouse environments, requiring AI systems to interpret RGB-D inputs and\nanswer spatial questions that combine perception, geometry, and language. Both\nTrack 1 and Track 3 datasets were generated in NVIDIA Omniverse. Track 4\nemphasized efficient road object detection from fisheye cameras, supporting\nlightweight, real-time deployment on edge devices. The evaluation framework\nenforced submission limits and used a partially held-out test set to ensure\nfair benchmarking. Final rankings were revealed after the competition\nconcluded, fostering reproducibility and mitigating overfitting. Several teams\nachieved top-tier results, setting new benchmarks in multiple tasks.",
        "url": "http://arxiv.org/abs/2508.13564v1",
        "published_date": "2025-08-19T06:55:06+00:00",
        "updated_date": "2025-08-19T06:55:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Zheng Tang",
            "Shuo Wang",
            "David C. Anastasiu",
            "Ming-Ching Chang",
            "Anuj Sharma",
            "Quan Kong",
            "Norimasa Kobori",
            "Munkhjargal Gochoo",
            "Ganzorig Batnasan",
            "Munkh-Erdene Otgonbold",
            "Fady Alnajjar",
            "Jun-Wei Hsieh",
            "Tomasz Kornuta",
            "Xiaolong Li",
            "Yilin Zhao",
            "Han Zhang",
            "Subhashree Radhakrishnan",
            "Arihant Jain",
            "Ratnesh Kumar",
            "Vidya N. Murali",
            "Yuxing Wang",
            "Sameer Satish Pusegaonkar",
            "Yizhou Wang",
            "Sujit Biswas",
            "Xunlei Wu",
            "Zhedong Zheng",
            "Pranamesh Chakraborty",
            "Rama Chellappa"
        ],
        "tldr": "The 9th AI City Challenge featured four tracks focusing on applying computer vision and AI to transportation, automation, and safety, with datasets and challenges relevant to 3D understanding, robotics, and autonomous driving. The challenge saw increased participation and benchmarked performance across several key tasks.",
        "tldr_zh": "第九届AI城市挑战赛包含四个赛道，重点在于将计算机视觉和人工智能应用于交通、自动化和安全领域，其中数据集和挑战与三维理解、机器人和自动驾驶相关。此次挑战赛参与人数增加，并对多个关键任务的性能进行了基准测试。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]