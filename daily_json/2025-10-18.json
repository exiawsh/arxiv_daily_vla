[
    {
        "title": "DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion",
        "summary": "We present DriveGen3D, a novel framework for generating high-quality and\nhighly controllable dynamic 3D driving scenes that addresses critical\nlimitations in existing methodologies. Current approaches to driving scene\nsynthesis either suffer from prohibitive computational demands for extended\ntemporal generation, focus exclusively on prolonged video synthesis without 3D\nrepresentation, or restrict themselves to static single-scene reconstruction.\nOur work bridges this methodological gap by integrating accelerated long-term\nvideo generation with large-scale dynamic scene reconstruction through\nmultimodal conditional control. DriveGen3D introduces a unified pipeline\nconsisting of two specialized components: FastDrive-DiT, an efficient video\ndiffusion transformer for high-resolution, temporally coherent video synthesis\nunder text and Bird's-Eye-View (BEV) layout guidance; and FastRecon3D, a\nfeed-forward reconstruction module that rapidly builds 3D Gaussian\nrepresentations across time, ensuring spatial-temporal consistency. Together,\nthese components enable real-time generation of extended driving videos (up to\n$424\\times800$ at 12 FPS) and corresponding dynamic 3D scenes, achieving SSIM\nof 0.811 and PSNR of 22.84 on novel view synthesis, all while maintaining\nparameter efficiency.",
        "url": "http://arxiv.org/abs/2510.15264v1",
        "published_date": "2025-10-17T03:00:08+00:00",
        "updated_date": "2025-10-17T03:00:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weijie Wang",
            "Jiagang Zhu",
            "Zeyu Zhang",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Guosheng Zhao",
            "Chaojun Ni",
            "Haoxiao Wang",
            "Guan Huang",
            "Xinze Chen",
            "Yukun Zhou",
            "Wenkang Qin",
            "Duochao Shi",
            "Haoyun Li",
            "Guanghong Jia",
            "Jiwen Lu"
        ],
        "tldr": "DriveGen3D introduces a framework for generating controllable, temporally consistent 3D driving scenes using a fast video diffusion transformer (FastDrive-DiT) and a feed-forward 3D reconstruction module (FastRecon3D), enabling real-time generation of extended driving videos with corresponding dynamic 3D scenes.",
        "tldr_zh": "DriveGen3D 提出了一个用于生成可控的、时间一致的 3D 驾驶场景的框架，该框架使用快速视频扩散变换器 (FastDrive-DiT) 和前馈 3D 重建模块 (FastRecon3D)，从而能够实时生成具有相应动态 3D 场景的扩展驾驶视频。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal",
        "summary": "Lens flare significantly degrades image quality, impacting critical computer\nvision tasks like object detection and autonomous driving. Recent Single Image\nFlare Removal (SIFR) methods perform poorly when off-frame light sources are\nincomplete or absent. We propose LightsOut, a diffusion-based outpainting\nframework tailored to enhance SIFR by reconstructing off-frame light sources.\nOur method leverages a multitask regression module and LoRA fine-tuned\ndiffusion model to ensure realistic and physically consistent outpainting\nresults. Comprehensive experiments demonstrate LightsOut consistently boosts\nthe performance of existing SIFR methods across challenging scenarios without\nadditional retraining, serving as a universally applicable plug-and-play\npreprocessing solution. Project page: https://ray-1026.github.io/lightsout/",
        "url": "http://arxiv.org/abs/2510.15868v1",
        "published_date": "2025-10-17T17:59:50+00:00",
        "updated_date": "2025-10-17T17:59:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shr-Ruei Tsai",
            "Wei-Cheng Chang",
            "Jie-Ying Lee",
            "Chih-Hai Su",
            "Yu-Lun Liu"
        ],
        "tldr": "LightsOut uses diffusion-based outpainting to reconstruct off-frame light sources, enhancing single image flare removal and improving computer vision tasks like autonomous driving by acting as a plug-and-play preprocessing step for existing methods.",
        "tldr_zh": "LightsOut 使用基于扩散的出画技术来重建画面外的光源，从而增强单图像眩光消除，并通过充当现有方法的即插即用预处理步骤来改善自动驾驶等计算机视觉任务。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Exploring Conditions for Diffusion models in Robotic Control",
        "summary": "While pre-trained visual representations have significantly advanced\nimitation learning, they are often task-agnostic as they remain frozen during\npolicy learning. In this work, we explore leveraging pre-trained text-to-image\ndiffusion models to obtain task-adaptive visual representations for robotic\ncontrol, without fine-tuning the model itself. However, we find that naively\napplying textual conditions - a successful strategy in other vision domains -\nyields minimal or even negative gains in control tasks. We attribute this to\nthe domain gap between the diffusion model's training data and robotic control\nenvironments, leading us to argue for conditions that consider the specific,\ndynamic visual information required for control. To this end, we propose ORCA,\nwhich introduces learnable task prompts that adapt to the control environment\nand visual prompts that capture fine-grained, frame-specific details. Through\nfacilitating task-adaptive representations with our newly devised conditions,\nour approach achieves state-of-the-art performance on various robotic control\nbenchmarks, significantly surpassing prior methods.",
        "url": "http://arxiv.org/abs/2510.15510v1",
        "published_date": "2025-10-17T10:24:14+00:00",
        "updated_date": "2025-10-17T10:24:14+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Heeseong Shin",
            "Byeongho Heo",
            "Dongyoon Han",
            "Seungryong Kim",
            "Taekyung Kim"
        ],
        "tldr": "This paper explores using pre-trained text-to-image diffusion models to generate task-adaptive visual representations for robotic control. It proposes a new conditioning method (ORCA) utilizing learnable task and visual prompts to achieve state-of-the-art performance on robotic control benchmarks.",
        "tldr_zh": "本文探索了利用预训练的文本到图像扩散模型，为机器人控制生成任务自适应的视觉表征。它提出了一种新的调节方法 (ORCA)，利用可学习的任务和视觉提示，在机器人控制基准测试中实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "MRASfM: Multi-Camera Reconstruction and Aggregation through Structure-from-Motion in Driving Scenes",
        "summary": "Structure from Motion (SfM) estimates camera poses and reconstructs point\nclouds, forming a foundation for various tasks. However, applying SfM to\ndriving scenes captured by multi-camera systems presents significant\ndifficulties, including unreliable pose estimation, excessive outliers in road\nsurface reconstruction, and low reconstruction efficiency. To address these\nlimitations, we propose a Multi-camera Reconstruction and Aggregation\nStructure-from-Motion (MRASfM) framework specifically designed for driving\nscenes. MRASfM enhances the reliability of camera pose estimation by leveraging\nthe fixed spatial relationships within the multi-camera system during the\nregistration process. To improve the quality of road surface reconstruction,\nour framework employs a plane model to effectively remove erroneous points from\nthe triangulated road surface. Moreover, treating the multi-camera set as a\nsingle unit in Bundle Adjustment (BA) helps reduce optimization variables to\nboost efficiency. In addition, MRASfM achieves multi-scene aggregation through\nscene association and assembly modules in a coarse-to-fine fashion. We deployed\nmulti-camera systems on actual vehicles to validate the generalizability of\nMRASfM across various scenes and its robustness in challenging conditions\nthrough real-world applications. Furthermore, large-scale validation results on\npublic datasets show the state-of-the-art performance of MRASfM, achieving\n0.124 absolute pose error on the nuScenes dataset.",
        "url": "http://arxiv.org/abs/2510.15467v1",
        "published_date": "2025-10-17T09:20:59+00:00",
        "updated_date": "2025-10-17T09:20:59+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lingfeng Xuan",
            "Chang Nie",
            "Yiqing Xu",
            "Zhe Liu",
            "Yanzi Miao",
            "Hesheng Wang"
        ],
        "tldr": "This paper presents MRASfM, a novel Structure-from-Motion framework tailored for multi-camera systems in driving scenes, addressing pose estimation, outlier removal, and efficiency challenges, and achieving state-of-the-art results on nuScenes.",
        "tldr_zh": "本文提出了 MRASfM，一种专为驾驶场景中的多摄像头系统设计的结构运动框架，解决了姿态估计、异常值移除和效率挑战，并在 nuScenes 数据集上取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FreqPDE: Rethinking Positional Depth Embedding for Multi-View 3D Object Detection Transformers",
        "summary": "Detecting 3D objects accurately from multi-view 2D images is a challenging\nyet essential task in the field of autonomous driving. Current methods resort\nto integrating depth prediction to recover the spatial information for object\nquery decoding, which necessitates explicit supervision from LiDAR points\nduring the training phase. However, the predicted depth quality is still\nunsatisfactory such as depth discontinuity of object boundaries and\nindistinction of small objects, which are mainly caused by the sparse\nsupervision of projected points and the use of high-level image features for\ndepth prediction. Besides, cross-view consistency and scale invariance are also\noverlooked in previous methods. In this paper, we introduce Frequency-aware\nPositional Depth Embedding (FreqPDE) to equip 2D image features with spatial\ninformation for 3D detection transformer decoder, which can be obtained through\nthree main modules. Specifically, the Frequency-aware Spatial Pyramid Encoder\n(FSPE) constructs a feature pyramid by combining high-frequency edge clues and\nlow-frequency semantics from different levels respectively. Then the Cross-view\nScale-invariant Depth Predictor (CSDP) estimates the pixel-level depth\ndistribution with cross-view and efficient channel attention mechanism.\nFinally, the Positional Depth Encoder (PDE) combines the 2D image features and\n3D position embeddings to generate the 3D depth-aware features for query\ndecoding. Additionally, hybrid depth supervision is adopted for complementary\ndepth learning from both metric and distribution aspects. Extensive experiments\nconducted on the nuScenes dataset demonstrate the effectiveness and superiority\nof our proposed method.",
        "url": "http://arxiv.org/abs/2510.15385v1",
        "published_date": "2025-10-17T07:36:54+00:00",
        "updated_date": "2025-10-17T07:36:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haisheng Su",
            "Junjie Zhang",
            "Feixiang Song",
            "Sanping Zhou",
            "Wei Wu",
            "Nanning Zheng",
            "Junchi Yan"
        ],
        "tldr": "This paper introduces FreqPDE, a novel approach for multi-view 3D object detection using transformers. It focuses on improving depth estimation with frequency-aware feature extraction, cross-view consistency, and hybrid depth supervision, demonstrating superior performance on the nuScenes dataset.",
        "tldr_zh": "本文介绍了 FreqPDE，一种用于多视图 3D 目标检测的新方法，它使用 transformer 模型。它侧重于通过频率感知特征提取、跨视图一致性和混合深度监督来改进深度估计，并在 nuScenes 数据集上表现出卓越的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SHARE: Scene-Human Aligned Reconstruction",
        "summary": "Animating realistic character interactions with the surrounding environment\nis important for autonomous agents in gaming, AR/VR, and robotics. However,\ncurrent methods for human motion reconstruction struggle with accurately\nplacing humans in 3D space. We introduce Scene-Human Aligned REconstruction\n(SHARE), a technique that leverages the scene geometry's inherent spatial cues\nto accurately ground human motion reconstruction. Each reconstruction relies\nsolely on a monocular RGB video from a stationary camera. SHARE first estimates\na human mesh and segmentation mask for every frame, alongside a scene point map\nat keyframes. It iteratively refines the human's positions at these keyframes\nby comparing the human mesh against the human point map extracted from the\nscene using the mask. Crucially, we also ensure that non-keyframe human meshes\nremain consistent by preserving their relative root joint positions to keyframe\nroot joints during optimization. Our approach enables more accurate 3D human\nplacement while reconstructing the surrounding scene, facilitating use cases on\nboth curated datasets and in-the-wild web videos. Extensive experiments\ndemonstrate that SHARE outperforms existing methods.",
        "url": "http://arxiv.org/abs/2510.15342v1",
        "published_date": "2025-10-17T06:12:10+00:00",
        "updated_date": "2025-10-17T06:12:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Joshua Li",
            "Brendan Chharawala",
            "Chang Shu",
            "Xue Bin Peng",
            "Pengcheng Xi"
        ],
        "tldr": "SHARE improves human motion reconstruction by aligning human meshes with scene geometry extracted from monocular RGB videos, resulting in more accurate 3D human placement.",
        "tldr_zh": "SHARE通过将人体网格与从单目RGB视频中提取的场景几何对齐，改进了人体运动重建，从而实现更精确的3D人体定位。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CuSfM: CUDA-Accelerated Structure-from-Motion",
        "summary": "Efficient and accurate camera pose estimation forms the foundational\nrequirement for dense reconstruction in autonomous navigation, robotic\nperception, and virtual simulation systems. This paper addresses the challenge\nvia cuSfM, a CUDA-accelerated offline Structure-from-Motion system that\nleverages GPU parallelization to efficiently employ computationally intensive\nyet highly accurate feature extractors, generating comprehensive and\nnon-redundant data associations for precise camera pose estimation and globally\nconsistent mapping. The system supports pose optimization, mapping, prior-map\nlocalization, and extrinsic refinement. It is designed for offline processing,\nwhere computational resources can be fully utilized to maximize accuracy.\nExperimental results demonstrate that cuSfM achieves significantly improved\naccuracy and processing speed compared to the widely used COLMAP method across\nvarious testing scenarios, while maintaining the high precision and global\nconsistency essential for offline SfM applications. The system is released as\nan open-source Python wrapper implementation, PyCuSfM, available at\nhttps://github.com/nvidia-isaac/pyCuSFM, to facilitate research and\napplications in computer vision and robotics.",
        "url": "http://arxiv.org/abs/2510.15271v1",
        "published_date": "2025-10-17T03:29:11+00:00",
        "updated_date": "2025-10-17T03:29:11+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Jingrui Yu",
            "Jun Liu",
            "Kefei Ren",
            "Joydeep Biswas",
            "Rurui Ye",
            "Keqiang Wu",
            "Chirag Majithia",
            "Di Zeng"
        ],
        "tldr": "The paper introduces cuSfM, a CUDA-accelerated offline Structure-from-Motion system that achieves improved accuracy and speed compared to COLMAP, and is released as an open-source Python wrapper.",
        "tldr_zh": "该论文介绍了一种CUDA加速的离线Structure-from-Motion系统cuSfM，与COLMAP相比，该系统实现了更高的精度和速度，并以开源Python封装的形式发布。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Generalized Dynamics Generation towards Scannable Physical World Model",
        "summary": "Digital twin worlds with realistic interactive dynamics presents a new\nopportunity to develop generalist embodied agents in scannable environments\nwith complex physical behaviors. To this end, we present GDGen (Generalized\nRepresentation for Generalized Dynamics Generation), a framework that takes a\npotential energy perspective to seamlessly integrate rigid body, articulated\nbody, and soft body dynamics into a unified, geometry-agnostic system. GDGen\noperates from the governing principle that the potential energy for any stable\nphysical system should be low. This fresh perspective allows us to treat the\nworld as one holistic entity and infer underlying physical properties from\nsimple motion observations. We extend classic elastodynamics by introducing\ndirectional stiffness to capture a broad spectrum of physical behaviors,\ncovering soft elastic, articulated, and rigid body systems. We propose a\nspecialized network to model the extended material property and employ a neural\nfield to represent deformation in a geometry-agnostic manner. Extensive\nexperiments demonstrate that GDGen robustly unifies diverse simulation\nparadigms, offering a versatile foundation for creating interactive virtual\nenvironments and training robotic agents in complex, dynamically rich\nscenarios.",
        "url": "http://arxiv.org/abs/2510.15041v1",
        "published_date": "2025-10-16T18:00:58+00:00",
        "updated_date": "2025-10-16T18:00:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yichen Li",
            "Zhiyi Li",
            "Brandon Feng",
            "Dinghuai Zhang",
            "Antonio Torralba"
        ],
        "tldr": "This paper introduces GDGen, a novel framework that unifies rigid, articulated, and soft body dynamics using a potential energy perspective, enabling the creation of realistic and interactive virtual environments for robot training.",
        "tldr_zh": "本文介绍了一种名为GDGen的新框架，该框架使用势能视角统一了刚体、铰接体和软体动力学，从而能够创建用于机器人训练的逼真且可交互的虚拟环境。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MAVR-Net: Robust Multi-View Learning for MAV Action Recognition with Cross-View Attention",
        "summary": "Recognizing the motion of Micro Aerial Vehicles (MAVs) is crucial for\nenabling cooperative perception and control in autonomous aerial swarms. Yet,\nvision-based recognition models relying only on RGB data often fail to capture\nthe complex spatial temporal characteristics of MAV motion, which limits their\nability to distinguish different actions. To overcome this problem, this paper\npresents MAVR-Net, a multi-view learning-based MAV action recognition\nframework. Unlike traditional single-view methods, the proposed approach\ncombines three complementary types of data, including raw RGB frames, optical\nflow, and segmentation masks, to improve the robustness and accuracy of MAV\nmotion recognition. Specifically, ResNet-based encoders are used to extract\ndiscriminative features from each view, and a multi-scale feature pyramid is\nadopted to preserve the spatiotemporal details of MAV motion patterns. To\nenhance the interaction between different views, a cross-view attention module\nis introduced to model the dependencies among various modalities and feature\nscales. In addition, a multi-view alignment loss is designed to ensure semantic\nconsistency and strengthen cross-view feature representations. Experimental\nresults on benchmark MAV action datasets show that our method clearly\noutperforms existing approaches, achieving 97.8\\%, 96.5\\%, and 92.8\\% accuracy\non the Short MAV, Medium MAV, and Long MAV datasets, respectively.",
        "url": "http://arxiv.org/abs/2510.15448v1",
        "published_date": "2025-10-17T09:04:51+00:00",
        "updated_date": "2025-10-17T09:04:51+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nengbo Zhang",
            "Hann Woei Ho"
        ],
        "tldr": "MAVR-Net is a multi-view learning framework for MAV action recognition that combines RGB, optical flow, and segmentation masks with cross-view attention and a multi-view alignment loss, achieving state-of-the-art results on MAV action datasets.",
        "tldr_zh": "MAVR-Net是一个用于MAV动作识别的多视角学习框架，它结合了RGB、光流和分割掩码，并采用跨视角注意力机制和多视角对齐损失，在MAV动作数据集上实现了最先进的结果。",
        "relevance_score": 7,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]