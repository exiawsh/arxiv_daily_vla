[
    {
        "title": "OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving",
        "summary": "Recent advances in vision-language models (VLMs) have demonstrated impressive\nspatial reasoning capabilities for autonomous driving, yet existing methods\npredominantly focus on static scene understanding while neglecting the\nessential temporal dimension of real-world driving scenarios. To address this\ncritical limitation, we propose the OmniReason framework, which establishes\nrobust spatiotemporal reasoning by jointly modeling dynamic 3D environments and\ntheir underlying decision-making processes. Our work makes two fundamental\nadvances: (1) We introduce OmniReason-Data, two large-scale\nvision-language-action (VLA) datasets with dense spatiotemporal annotations and\nnatural language explanations, generated through a novel\nhallucination-mitigated auto-labeling pipeline that ensures both physical\nplausibility and temporal coherence; (2) We develop the OmniReason-Agent\narchitecture, which integrates a sparse temporal memory module for persistent\nscene context modeling and an explanation generator that produces\nhuman-interpretable decision rationales, facilitated by our spatiotemporal\nknowledge distillation approach that effectively captures spatiotemporal causal\nreasoning patterns. Comprehensive experiments demonstrate state-of-the-art\nperformance, where OmniReason-Agent achieves significant improvements in both\nopen-loop planning tasks and visual question answering (VQA) benchmarks, while\nestablishing new capabilities for interpretable, temporally-aware autonomous\nvehicles operating in complex, dynamic environments.",
        "url": "http://arxiv.org/abs/2509.00789v1",
        "published_date": "2025-08-31T10:34:44+00:00",
        "updated_date": "2025-08-31T10:34:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Pei Liu",
            "Qingtian Ning",
            "Xinyan Lu",
            "Haipeng Liu",
            "Weiliang Ma",
            "Dangen She",
            "Peng Jia",
            "Xianpeng Lang",
            "Jun Ma"
        ],
        "tldr": "The paper introduces OmniReason, a framework with a large-scale dataset and agent architecture for spatiotemporal reasoning in autonomous driving, addressing the limitations of existing VLM methods that primarily focus on static scene understanding.",
        "tldr_zh": "该论文介绍了OmniReason框架，包括一个大规模数据集和一个智能体架构，用于自动驾驶中的时空推理，旨在解决现有视觉语言模型主要关注静态场景理解的局限性。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Galaxea Open-World Dataset and G0 Dual-System VLA Model",
        "summary": "We present Galaxea Open-World Dataset, a large-scale, diverse collection of\nrobot behaviors recorded in authentic human living and working environments.\nAll demonstrations are gathered using a consistent robotic embodiment, paired\nwith precise subtask-level language annotations to facilitate both training and\nevaluation. Building on this dataset, we introduce G0, a dual-system framework\nthat couples a Vision-Language Model (VLM) for multimodal planning with a\nVision-Language-Action (VLA) model for fine-grained execution. G0 is trained\nusing a three-stage curriculum: cross-embodiment pre-training,\nsingle-embodiment pre-training, and task-specific post-training. A\ncomprehensive benchmark spanning tabletop manipulation, few-shot learning, and\nlong-horizon mobile manipulation, demonstrates the effectiveness of our\napproach. In particular, we find that the single-embodiment pre-training stage,\ntogether with the Galaxea Open-World Dataset, plays a critical role in\nachieving strong performance.",
        "url": "http://arxiv.org/abs/2509.00576v1",
        "published_date": "2025-08-30T18:04:19+00:00",
        "updated_date": "2025-08-30T18:04:19+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Tao Jiang",
            "Tianyuan Yuan",
            "Yicheng Liu",
            "Chenhao Lu",
            "Jianning Cui",
            "Xiao Liu",
            "Shuiqi Cheng",
            "Jiyang Gao",
            "Huazhe Xu",
            "Hang Zhao"
        ],
        "tldr": "The paper introduces Galaxea, a large-scale robotic dataset with language annotations, and G0, a dual VLM-VLA system, demonstrating strong performance in various robotic tasks through a novel training curriculum.",
        "tldr_zh": "该论文介绍了 Galaxea，一个带有语言注释的大规模机器人数据集，以及 G0，一个双 VLM-VLA 系统，通过一种新颖的训练课程，在各种机器人任务中表现出强大的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]