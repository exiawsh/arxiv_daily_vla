[
    {
        "title": "FLUID: A Fine-Grained Lightweight Urban Signalized-Intersection Dataset of Dense Conflict Trajectories",
        "summary": "The trajectory data of traffic participants (TPs) is a fundamental resource\nfor evaluating traffic conditions and optimizing policies, especially at urban\nintersections. Although data acquisition using drones is efficient, existing\ndatasets still have limitations in scene representativeness, information\nrichness, and data fidelity. This study introduces FLUID, comprising a\nfine-grained trajectory dataset that captures dense conflicts at typical urban\nsignalized intersections, and a lightweight, full-pipeline framework for\ndrone-based trajectory processing. FLUID covers three distinct intersection\ntypes, with approximately 5 hours of recording time and featuring over 20,000\nTPs across 8 categories. Notably, the dataset averages two vehicle conflicts\nper minute, involving roughly 25% of all motor vehicles. FLUID provides\ncomprehensive data, including trajectories, traffic signals, maps, and raw\nvideos. Comparison with the DataFromSky platform and ground-truth measurements\nvalidates its high spatio-temporal accuracy. Through a detailed classification\nof motor vehicle conflicts and violations, FLUID reveals a diversity of\ninteractive behaviors, demonstrating its value for human preference mining,\ntraffic behavior modeling, and autonomous driving research.",
        "url": "http://arxiv.org/abs/2509.00497v1",
        "published_date": "2025-08-30T13:38:42+00:00",
        "updated_date": "2025-08-30T13:38:42+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Yiyang Chen",
            "Zhigang Wu",
            "Guohong Zheng",
            "Xuesong Wu",
            "Liwen Xu",
            "Haoyuan Tang",
            "Zhaocheng He",
            "Haipeng Zeng"
        ],
        "tldr": "The paper introduces FLUID, a new drone-captured, fine-grained, and dense conflict trajectory dataset for urban signalized intersections, validated for accuracy and useful for traffic behavior modeling and autonomous driving research.",
        "tldr_zh": "该论文介绍了FLUID，一个新型的无人机采集的，细粒度的，密集的城市信号交叉口冲突轨迹数据集，经过验证具有高精度，可用于交通行为建模和自动驾驶研究。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Embodied Spatial Intelligence: from Implicit Scene Modeling to Spatial Reasoning",
        "summary": "This thesis introduces \"Embodied Spatial Intelligence\" to address the\nchallenge of creating robots that can perceive and act in the real world based\non natural language instructions. To bridge the gap between Large Language\nModels (LLMs) and physical embodiment, we present contributions on two fronts:\nscene representation and spatial reasoning. For perception, we develop robust,\nscalable, and accurate scene representations using implicit neural models, with\ncontributions in self-supervised camera calibration, high-fidelity depth field\ngeneration, and large-scale reconstruction. For spatial reasoning, we enhance\nthe spatial capabilities of LLMs by introducing a novel navigation benchmark, a\nmethod for grounding language in 3D, and a state-feedback mechanism to improve\nlong-horizon decision-making. This work lays a foundation for robots that can\nrobustly perceive their surroundings and intelligently act upon complex,\nlanguage-based commands.",
        "url": "http://arxiv.org/abs/2509.00465v1",
        "published_date": "2025-08-30T11:42:26+00:00",
        "updated_date": "2025-08-30T11:42:26+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Jiading Fang"
        ],
        "tldr": "This thesis introduces \"Embodied Spatial Intelligence,\" focusing on bridging the gap between LLMs and robotics by developing implicit neural scene representations and enhancing LLMs' spatial reasoning capabilities for language-based robot control.",
        "tldr_zh": "该论文介绍了“具身空间智能”，专注于通过开发隐式神经场景表示和增强LLM的空间推理能力来弥合LLM和机器人技术之间的差距，以实现基于语言的机器人控制。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment",
        "summary": "Achieving human-like reasoning in deep learning models for complex tasks in\nunknown environments remains a critical challenge in embodied intelligence.\nWhile advanced vision-language models (VLMs) excel in static scene\nunderstanding, their limitations in spatio-temporal reasoning and adaptation to\ndynamic, open-set tasks like task-oriented navigation and embodied question\nanswering (EQA) persist due to inadequate modeling of fine-grained\nspatio-temporal cues and physical world comprehension. To address this, we\npropose VEME, a novel cross-modal alignment method that enhances generalization\nin unseen scenes by learning an ego-centric, experience-centered world model.\nOur framework integrates three key components: (1) a cross-modal alignment\nframework bridging objects, spatial representations, and visual semantics with\nspatio-temporal cues to enhance VLM in-context learning; (2) a dynamic,\nimplicit cognitive map activated by world embedding to enable task-relevant\ngeometric-semantic memory recall; and (3) an instruction-based navigation and\nreasoning framework leveraging embodied priors for long-term planning and\nefficient exploration. By embedding geometry-aware spatio-temporal episodic\nexperiences, our method significantly improves reasoning and planning in\ndynamic environments. Experimental results on VSI-Bench and VLN-CE demonstrate\n1%-3% accuracy and exploration efficiency improvement compared to traditional\napproaches.",
        "url": "http://arxiv.org/abs/2509.00210v1",
        "published_date": "2025-08-29T19:47:25+00:00",
        "updated_date": "2025-08-29T19:47:25+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jinzhou Tang",
            "Jusheng zhang",
            "Sidi Liu",
            "Waikit Xiu",
            "Qinhan Lv",
            "Xiying Li"
        ],
        "tldr": "The paper introduces VEME, a cross-modal alignment method that enhances VLMs for embodied AI tasks by learning an ego-centric world model with geometric-semantic spatio-temporal priors, improving reasoning and planning in dynamic environments.",
        "tldr_zh": "该论文介绍了一种名为VEME的跨模态对齐方法，通过学习具有几何语义时空先验的以自我为中心的世界模型，增强了VLM在具身人工智能任务中的性能，从而提高了动态环境中的推理和规划能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reinforcement Learning of Dolly-In Filming Using a Ground-Based Robot",
        "summary": "Free-roaming dollies enhance filmmaking with dynamic movement, but challenges\nin automated camera control remain unresolved. Our study advances this field by\napplying Reinforcement Learning (RL) to automate dolly-in shots using\nfree-roaming ground-based filming robots, overcoming traditional control\nhurdles. We demonstrate the effectiveness of combined control for precise film\ntasks by comparing it to independent control strategies. Our robust RL pipeline\nsurpasses traditional Proportional-Derivative controller performance in\nsimulation and proves its efficacy in real-world tests on a modified ROSBot 2.0\nplatform equipped with a camera turret. This validates our approach's\npracticality and sets the stage for further research in complex filming\nscenarios, contributing significantly to the fusion of technology with\ncinematic creativity. This work presents a leap forward in the field and opens\nnew avenues for research and development, effectively bridging the gap between\ntechnological advancement and creative filmmaking.",
        "url": "http://arxiv.org/abs/2509.00564v1",
        "published_date": "2025-08-30T17:14:11+00:00",
        "updated_date": "2025-08-30T17:14:11+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Philip Lorimer",
            "Jack Saunders",
            "Alan Hunter",
            "Wenbin Li"
        ],
        "tldr": "This paper uses reinforcement learning to control a ground-based robot for automated dolly-in filming, outperforming traditional controllers in simulation and real-world tests.",
        "tldr_zh": "本文使用强化学习控制地面机器人，实现自动变焦拍摄，并在仿真和实际测试中优于传统控制器。",
        "relevance_score": 6,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]