[
    {
        "title": "Keeping it Local, Tiny and Real: Automated Report Generation on Edge Computing Devices for Mechatronic-Based Cognitive Systems",
        "summary": "Recent advancements in Deep Learning enable hardware-based cognitive systems,\nthat is, mechatronic systems in general and robotics in particular with\nintegrated Artificial Intelligence, to interact with dynamic and unstructured\nenvironments. While the results are impressive, the application of such systems\nto critical tasks like autonomous driving as well as service and care robotics\nnecessitate the evaluation of large amount of heterogeneous data. Automated\nreport generation for Mobile Robotics can play a crucial role in facilitating\nthe evaluation and acceptance of such systems in various domains. In this\npaper, we propose a pipeline for generating automated reports in natural\nlanguage utilizing various multi-modal sensors that solely relies on local\nmodels capable of being deployed on edge computing devices, thus preserving the\nprivacy of all actors involved and eliminating the need for external services.\nIn particular, we evaluate our implementation on a diverse dataset spanning\nmultiple domains including indoor, outdoor and urban environments, providing\nquantitative as well as qualitative evaluation results. Various generated\nexample reports and other supplementary materials are available via a public\nrepository.",
        "url": "http://arxiv.org/abs/2511.02507v1",
        "published_date": "2025-11-04T11:49:41+00:00",
        "updated_date": "2025-11-04T11:49:41+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Nicolas Schuler",
            "Lea Dewald",
            "Jürgen Graf"
        ],
        "tldr": "This paper introduces a pipeline for automated report generation in natural language for mobile robots using multi-modal sensors and edge computing, preserving privacy and eliminating reliance on external services. They evaluate their method on diverse datasets and provide quantitative and qualitative results.",
        "tldr_zh": "本文提出了一种用于移动机器人的自然语言自动报告生成流程，该流程使用多模态传感器和边缘计算，保护隐私并消除对外部服务的依赖。他们在不同的数据集上评估了他们的方法，并提供了定量和定性的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Self-Supervised Moving Object Segmentation of Sparse and Noisy Radar Point Clouds",
        "summary": "Moving object segmentation is a crucial task for safe and reliable autonomous\nmobile systems like self-driving cars, improving the reliability and robustness\nof subsequent tasks like SLAM or path planning. While the segmentation of\ncamera or LiDAR data is widely researched and achieves great results, it often\nintroduces an increased latency by requiring the accumulation of temporal\nsequences to gain the necessary temporal context. Radar sensors overcome this\nproblem with their ability to provide a direct measurement of a point's Doppler\nvelocity, which can be exploited for single-scan moving object segmentation.\nHowever, radar point clouds are often sparse and noisy, making data annotation\nfor use in supervised learning very tedious, time-consuming, and\ncost-intensive. To overcome this problem, we address the task of\nself-supervised moving object segmentation of sparse and noisy radar point\nclouds. We follow a two-step approach of contrastive self-supervised\nrepresentation learning with subsequent supervised fine-tuning using limited\namounts of annotated data. We propose a novel clustering-based contrastive loss\nfunction with cluster refinement based on dynamic points removal to pretrain\nthe network to produce motion-aware representations of the radar data. Our\nmethod improves label efficiency after fine-tuning, effectively boosting\nstate-of-the-art performance by self-supervised pretraining.",
        "url": "http://arxiv.org/abs/2511.02395v1",
        "published_date": "2025-11-04T09:21:45+00:00",
        "updated_date": "2025-11-04T09:21:45+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Leon Schwarzer",
            "Matthias Zeller",
            "Daniel Casado Herraez",
            "Simon Dierl",
            "Michael Heidingsfeld",
            "Cyrill Stachniss"
        ],
        "tldr": "This paper presents a self-supervised approach for moving object segmentation in sparse and noisy radar point clouds, using contrastive learning and limited labeled data to improve performance in autonomous driving scenarios.",
        "tldr_zh": "本文提出了一种自监督方法，用于在稀疏和嘈杂的雷达点云中分割移动物体，使用对比学习和少量标记数据来提高自动驾驶场景中的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "3D Point Cloud Object Detection on Edge Devices for Split Computing",
        "summary": "The field of autonomous driving technology is rapidly advancing, with deep\nlearning being a key component. Particularly in the field of sensing, 3D point\ncloud data collected by LiDAR is utilized to run deep neural network models for\n3D object detection. However, these state-of-the-art models are complex,\nleading to longer processing times and increased power consumption on edge\ndevices. The objective of this study is to address these issues by leveraging\nSplit Computing, a distributed machine learning inference method. Split\nComputing aims to lessen the computational burden on edge devices, thereby\nreducing processing time and power consumption. Furthermore, it minimizes the\nrisk of data breaches by only transmitting intermediate data from the deep\nneural network model. Experimental results show that splitting after\nvoxelization reduces the inference time by 70.8% and the edge device execution\ntime by 90.0%. When splitting within the network, the inference time is reduced\nby up to 57.1%, and the edge device execution time is reduced by up to 69.5%.",
        "url": "http://arxiv.org/abs/2511.02293v1",
        "published_date": "2025-11-04T06:15:24+00:00",
        "updated_date": "2025-11-04T06:15:24+00:00",
        "categories": [
            "cs.DC",
            "cs.CV",
            "C.2.4; I.2.10"
        ],
        "authors": [
            "Taisuke Noguchi",
            "Takuya Azumi"
        ],
        "tldr": "This paper explores using Split Computing to reduce the computational burden of 3D point cloud object detection on edge devices for autonomous driving, demonstrating significant reductions in inference and execution time.",
        "tldr_zh": "本文探讨了使用拆分计算来减少自动驾驶边缘设备上3D点云目标检测的计算负担，实验结果表明推理和执行时间显著减少。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From the Laboratory to Real-World Application: Evaluating Zero-Shot Scene Interpretation on Edge Devices for Mobile Robotics",
        "summary": "Video Understanding, Scene Interpretation and Commonsense Reasoning are\nhighly challenging tasks enabling the interpretation of visual information,\nallowing agents to perceive, interact with and make rational decisions in its\nenvironment. Large Language Models (LLMs) and Visual Language Models (VLMs)\nhave shown remarkable advancements in these areas in recent years, enabling\ndomain-specific applications as well as zero-shot open vocabulary tasks,\ncombining multiple domains. However, the required computational complexity\nposes challenges for their application on edge devices and in the context of\nMobile Robotics, especially considering the trade-off between accuracy and\ninference time. In this paper, we investigate the capabilities of\nstate-of-the-art VLMs for the task of Scene Interpretation and Action\nRecognition, with special regard to small VLMs capable of being deployed to\nedge devices in the context of Mobile Robotics. The proposed pipeline is\nevaluated on a diverse dataset consisting of various real-world cityscape,\non-campus and indoor scenarios. The experimental evaluation discusses the\npotential of these small models on edge devices, with particular emphasis on\nchallenges, weaknesses, inherent model biases and the application of the gained\ninformation. Supplementary material is provided via the following repository:\nhttps://datahub.rz.rptu.de/hstr-csrl-public/publications/scene-interpretation-on-edge-devices/",
        "url": "http://arxiv.org/abs/2511.02427v1",
        "published_date": "2025-11-04T09:58:29+00:00",
        "updated_date": "2025-11-04T09:58:29+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Nicolas Schuler",
            "Lea Dewald",
            "Nick Baldig",
            "Jürgen Graf"
        ],
        "tldr": "This paper evaluates the performance of small Visual Language Models (VLMs) on edge devices for scene interpretation and action recognition in mobile robotics, focusing on real-world scenarios and model biases.",
        "tldr_zh": "本文评估了小型视觉语言模型（VLMs）在边缘设备上进行场景理解和动作识别的性能，应用于移动机器人领域，重点关注真实场景和模型偏差。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]