[
    {
        "title": "3D Aware Region Prompted Vision Language Model",
        "summary": "We present Spatial Region 3D (SR-3D) aware vision-language model that\nconnects single-view 2D images and multi-view 3D data through a shared visual\ntoken space. SR-3D supports flexible region prompting, allowing users to\nannotate regions with bounding boxes, segmentation masks on any frame, or\ndirectly in 3D, without the need for exhaustive multi-frame labeling. We\nachieve this by enriching 2D visual features with 3D positional embeddings,\nwhich allows the 3D model to draw upon strong 2D priors for more accurate\nspatial reasoning across frames, even when objects of interest do not co-occur\nwithin the same view. Extensive experiments on both general 2D vision language\nand specialized 3D spatial benchmarks demonstrate that SR-3D achieves\nstate-of-the-art performance, underscoring its effectiveness for unifying 2D\nand 3D representation space on scene understanding. Moreover, we observe\napplicability to in-the-wild videos without sensory 3D inputs or ground-truth\n3D annotations, where SR-3D accurately infers spatial relationships and metric\nmeasurements.",
        "url": "http://arxiv.org/abs/2509.13317v1",
        "published_date": "2025-09-16T17:59:06+00:00",
        "updated_date": "2025-09-16T17:59:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "An-Chieh Cheng",
            "Yang Fu",
            "Yukang Chen",
            "Zhijian Liu",
            "Xiaolong Li",
            "Subhashree Radhakrishnan",
            "Song Han",
            "Yao Lu",
            "Jan Kautz",
            "Pavlo Molchanov",
            "Hongxu Yin",
            "Xiaolong Wang",
            "Sifei Liu"
        ],
        "tldr": "The paper introduces SR-3D, a vision-language model that unifies 2D and 3D data by enabling flexible region prompting for improved spatial reasoning and scene understanding, achieving state-of-the-art performance on both 2D and 3D benchmarks.",
        "tldr_zh": "该论文介绍了SR-3D，一种视觉语言模型，通过支持灵活的区域提示，统一了2D和3D数据，从而提高了空间推理和场景理解能力，并在2D和3D基准测试中取得了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Road Obstacle Video Segmentation",
        "summary": "With the growing deployment of autonomous driving agents, the detection and\nsegmentation of road obstacles have become critical to ensure safe autonomous\nnavigation. However, existing road-obstacle segmentation methods are applied on\nindividual frames, overlooking the temporal nature of the problem, leading to\ninconsistent prediction maps between consecutive frames. In this work, we\ndemonstrate that the road-obstacle segmentation task is inherently temporal,\nsince the segmentation maps for consecutive frames are strongly correlated. To\naddress this, we curate and adapt four evaluation benchmarks for road-obstacle\nvideo segmentation and evaluate 11 state-of-the-art image- and video-based\nsegmentation methods on these benchmarks. Moreover, we introduce two strong\nbaseline methods based on vision foundation models. Our approach establishes a\nnew state-of-the-art in road-obstacle video segmentation for long-range video\nsequences, providing valuable insights and direction for future research.",
        "url": "http://arxiv.org/abs/2509.13181v1",
        "published_date": "2025-09-16T15:34:43+00:00",
        "updated_date": "2025-09-16T15:34:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shyam Nandan Rai",
            "Shyamgopal Karthik",
            "Mariana-Iuliana Georgescu",
            "Barbara Caputo",
            "Carlo Masone",
            "Zeynep Akata"
        ],
        "tldr": "This paper addresses the temporal inconsistency in road obstacle segmentation by introducing video-based methods and benchmarks, achieving new state-of-the-art results using vision foundation models.",
        "tldr_zh": "本文通过引入基于视频的方法和基准，解决了道路障碍物分割中的时间不一致性问题，并使用视觉基础模型实现了新的最先进结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation",
        "summary": "4D radar super-resolution, which aims to reconstruct sparse and noisy point\nclouds into dense and geometrically consistent representations, is a\nfoundational problem in autonomous perception. However, existing methods often\nsuffer from high training cost or rely on complex diffusion-based sampling,\nresulting in high inference latency and poor generalization, making it\ndifficult to balance accuracy and efficiency. To address these limitations, we\npropose MSDNet, a multi-stage distillation framework that efficiently transfers\ndense LiDAR priors to 4D radar features to achieve both high reconstruction\nquality and computational efficiency. The first stage performs\nreconstruction-guided feature distillation, aligning and densifying the\nstudent's features through feature reconstruction. In the second stage, we\npropose diffusion-guided feature distillation, which treats the stage-one\ndistilled features as a noisy version of the teacher's representations and\nrefines them via a lightweight diffusion network. Furthermore, we introduce a\nnoise adapter that adaptively aligns the noise level of the feature with a\npredefined diffusion timestep, enabling a more precise denoising. Extensive\nexperiments on the VoD and in-house datasets demonstrate that MSDNet achieves\nboth high-fidelity reconstruction and low-latency inference in the task of 4D\nradar point cloud super-resolution, and consistently improves performance on\ndownstream tasks. The code will be publicly available upon publication.",
        "url": "http://arxiv.org/abs/2509.13149v1",
        "published_date": "2025-09-16T15:05:11+00:00",
        "updated_date": "2025-09-16T15:05:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Minqing Huang",
            "Shouyi Lu",
            "Boyuan Zheng",
            "Ziyao Li",
            "Xiao Tang",
            "Guirong Zhuo"
        ],
        "tldr": "The paper introduces MSDNet, a multi-stage distillation framework for efficient 4D radar super-resolution using LiDAR priors, achieving high reconstruction quality and low-latency inference by combining reconstruction-guided and diffusion-guided feature distillation.",
        "tldr_zh": "该论文介绍了 MSDNet，一个多阶段蒸馏框架，它利用激光雷达先验知识实现高效的 4D 雷达超分辨率，通过结合重建引导和扩散引导的特征蒸馏，实现了高质量的重建和低延迟的推理。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Advancing Real-World Parking Slot Detection with Large-Scale Dataset and Semi-Supervised Baseline",
        "summary": "As automatic parking systems evolve, the accurate detection of parking slots\nhas become increasingly critical. This study focuses on parking slot detection\nusing surround-view cameras, which offer a comprehensive bird's-eye view of the\nparking environment. However, the current datasets are limited in scale, and\nthe scenes they contain are seldom disrupted by real-world noise (e.g., light,\nocclusion, etc.). Moreover, manual data annotation is prone to errors and\nomissions due to the complexity of real-world conditions, significantly\nincreasing the cost of annotating large-scale datasets. To address these\nissues, we first construct a large-scale parking slot detection dataset (named\nCRPS-D), which includes various lighting distributions, diverse weather\nconditions, and challenging parking slot variants. Compared with existing\ndatasets, the proposed dataset boasts the largest data scale and consists of a\nhigher density of parking slots, particularly featuring more slanted parking\nslots. Additionally, we develop a semi-supervised baseline for parking slot\ndetection, termed SS-PSD, to further improve performance by exploiting\nunlabeled data. To our knowledge, this is the first semi-supervised approach in\nparking slot detection, which is built on the teacher-student model with\nconfidence-guided mask consistency and adaptive feature perturbation.\nExperimental results demonstrate the superiority of SS-PSD over the existing\nstate-of-the-art (SoTA) solutions on both the proposed dataset and the existing\ndataset. Particularly, the more unlabeled data there is, the more significant\nthe gains brought by our semi-supervised scheme. The relevant source codes and\nthe dataset have been made publicly available at\nhttps://github.com/zzh362/CRPS-D.",
        "url": "http://arxiv.org/abs/2509.13133v1",
        "published_date": "2025-09-16T14:50:19+00:00",
        "updated_date": "2025-09-16T14:50:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhihao Zhang",
            "Chunyu Lin",
            "Lang Nie",
            "Jiyuan Wang",
            "Yao Zhao"
        ],
        "tldr": "This paper introduces a large-scale parking slot detection dataset (CRPS-D) and a novel semi-supervised baseline (SS-PSD) that leverages unlabeled data to improve parking slot detection accuracy, demonstrating superior performance over existing methods.",
        "tldr_zh": "本文介绍了一个大型停车位检测数据集 (CRPS-D) 和一种新的半监督基线方法 (SS-PSD)，该方法利用未标记数据来提高停车位检测精度，并展示了优于现有方法的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Weakly and Self-Supervised Class-Agnostic Motion Prediction for Autonomous Driving",
        "summary": "Understanding motion in dynamic environments is critical for autonomous\ndriving, thereby motivating research on class-agnostic motion prediction. In\nthis work, we investigate weakly and self-supervised class-agnostic motion\nprediction from LiDAR point clouds. Outdoor scenes typically consist of mobile\nforegrounds and static backgrounds, allowing motion understanding to be\nassociated with scene parsing. Based on this observation, we propose a novel\nweakly supervised paradigm that replaces motion annotations with fully or\npartially annotated (1%, 0.1%) foreground/background masks for supervision. To\nthis end, we develop a weakly supervised approach utilizing\nforeground/background cues to guide the self-supervised learning of motion\nprediction models. Since foreground motion generally occurs in non-ground\nregions, non-ground/ground masks can serve as an alternative to\nforeground/background masks, further reducing annotation effort. Leveraging\nnon-ground/ground cues, we propose two additional approaches: a weakly\nsupervised method requiring fewer (0.01%) foreground/background annotations,\nand a self-supervised method without annotations. Furthermore, we design a\nRobust Consistency-aware Chamfer Distance loss that incorporates multi-frame\ninformation and robust penalty functions to suppress outliers in\nself-supervised learning. Experiments show that our weakly and self-supervised\nmodels outperform existing self-supervised counterparts, and our weakly\nsupervised models even rival some supervised ones. This demonstrates that our\napproaches effectively balance annotation effort and performance.",
        "url": "http://arxiv.org/abs/2509.13116v1",
        "published_date": "2025-09-16T14:22:45+00:00",
        "updated_date": "2025-09-16T14:22:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruibo Li",
            "Hanyu Shi",
            "Zhe Wang",
            "Guosheng Lin"
        ],
        "tldr": "The paper introduces weakly and self-supervised methods for class-agnostic motion prediction in autonomous driving using LiDAR point clouds, achieving comparable or superior performance to existing self-supervised and some supervised methods with significantly reduced annotation effort.",
        "tldr_zh": "本文提出了一种弱监督和自监督方法，用于自动驾驶中基于LiDAR点云的类无关运动预测，以显著减少标注工作量的同时，实现了与现有自监督方法甚至部分监督方法相当或更好的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era",
        "summary": "Omnidirectional vision, using 360-degree vision to understand the\nenvironment, has become increasingly critical across domains like robotics,\nindustrial inspection, and environmental monitoring. Compared to traditional\npinhole vision, omnidirectional vision provides holistic environmental\nawareness, significantly enhancing the completeness of scene perception and the\nreliability of decision-making. However, foundational research in this area has\nhistorically lagged behind traditional pinhole vision. This talk presents an\nemerging trend in the embodied AI era: the rapid development of omnidirectional\nvision, driven by growing industrial demand and academic interest. We highlight\nrecent breakthroughs in omnidirectional generation, omnidirectional perception,\nomnidirectional understanding, and related datasets. Drawing on insights from\nboth academia and industry, we propose an ideal panoramic system architecture\nin the embodied AI era, PANORAMA, which consists of four key subsystems.\nMoreover, we offer in-depth opinions related to emerging trends and\ncross-community impacts at the intersection of panoramic vision and embodied\nAI, along with the future roadmap and open challenges. This overview\nsynthesizes state-of-the-art advancements and outlines challenges and\nopportunities for future research in building robust, general-purpose\nomnidirectional AI systems in the embodied AI era.",
        "url": "http://arxiv.org/abs/2509.12989v1",
        "published_date": "2025-09-16T11:54:37+00:00",
        "updated_date": "2025-09-16T11:54:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xu Zheng",
            "Chenfei Liao",
            "Ziqiao Weng",
            "Kaiyu Lei",
            "Zihao Dongfang",
            "Haocong He",
            "Yuanhuiyi Lyu",
            "Lutao Jiang",
            "Lu Qi",
            "Li Chen",
            "Danda Pani Paudel",
            "Kailun Yang",
            "Linfeng Zhang",
            "Luc Van Gool",
            "Xuming Hu"
        ],
        "tldr": "This paper presents PANORAMA, an overview of the emerging trend of omnidirectional vision in embodied AI, highlighting recent advancements, proposing a system architecture, and outlining future research directions and challenges.",
        "tldr_zh": "本文介绍了全景视觉在具身智能领域的新兴趋势，重点介绍了最新进展，提出了一个系统架构，并概述了未来的研究方向和挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MATTER: Multiscale Attention for Registration Error Regression",
        "summary": "Point cloud registration (PCR) is crucial for many downstream tasks, such as\nsimultaneous localization and mapping (SLAM) and object tracking. This makes\ndetecting and quantifying registration misalignment, i.e.,~{\\it PCR quality\nvalidation}, an important task. All existing methods treat validation as a\nclassification task, aiming to assign the PCR quality to a few classes. In this\nwork, we instead use regression for PCR validation, allowing for a more\nfine-grained quantification of the registration quality. We also extend\npreviously used misalignment-related features by using multiscale extraction\nand attention-based aggregation. This leads to accurate and robust registration\nerror estimation on diverse datasets, especially for point clouds with\nheterogeneous spatial densities. Furthermore, when used to guide a mapping\ndownstream task, our method significantly improves the mapping quality for a\ngiven amount of re-registered frames, compared to the state-of-the-art\nclassification-based method.",
        "url": "http://arxiv.org/abs/2509.12924v1",
        "published_date": "2025-09-16T10:21:36+00:00",
        "updated_date": "2025-09-16T10:21:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shipeng Liu",
            "Ziliang Xiong",
            "Khac-Hoang Ngo",
            "Per-Erik Forssén"
        ],
        "tldr": "The paper introduces a regression-based approach, MATTER, for point cloud registration (PCR) quality validation using multiscale attention, improving the accuracy and robustness of registration error estimation, especially for point clouds with heterogeneous densities. This method shows promising results when integrated into a mapping task.",
        "tldr_zh": "该论文提出了一种基于回归的点云配准质量验证方法MATTER，它利用多尺度注意力来提高配准误差估计的准确性和鲁棒性，尤其适用于具有异构密度的点云。 该方法在集成到映射任务中时显示出良好的效果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unleashing the Power of Discrete-Time State Representation: Ultrafast Target-based IMU-Camera Spatial-Temporal Calibration",
        "summary": "Visual-inertial fusion is crucial for a large amount of intelligent and\nautonomous applications, such as robot navigation and augmented reality. To\nbootstrap and achieve optimal state estimation, the spatial-temporal\ndisplacements between IMU and cameras must be calibrated in advance. Most\nexisting calibration methods adopt continuous-time state representation, more\nspecifically the B-spline. Despite these methods achieve precise\nspatial-temporal calibration, they suffer from high computational cost caused\nby continuous-time state representation. To this end, we propose a novel and\nextremely efficient calibration method that unleashes the power of\ndiscrete-time state representation. Moreover, the weakness of discrete-time\nstate representation in temporal calibration is tackled in this paper. With the\nincreasing production of drones, cellphones and other visual-inertial\nplatforms, if one million devices need calibration around the world, saving one\nminute for the calibration of each device means saving 2083 work days in total.\nTo benefit both the research and industry communities, our code will be\nopen-source.",
        "url": "http://arxiv.org/abs/2509.12846v1",
        "published_date": "2025-09-16T09:05:46+00:00",
        "updated_date": "2025-09-16T09:05:46+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Junlin Song",
            "Antoine Richard",
            "Miguel Olivares-Mendez"
        ],
        "tldr": "This paper presents a novel and efficient IMU-camera spatial-temporal calibration method using discrete-time state representation, addressing the computational cost issues of continuous-time methods. It emphasizes the practical impact of faster calibration on large-scale deployment.",
        "tldr_zh": "本文提出了一种新颖高效的基于离散时间状态表示的IMU-相机空间-时间标定方法，解决了连续时间方法计算成本高的问题。 它强调了更快的校准对大规模部署的实际影响。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Foundational Models for Single-Chip Radar",
        "summary": "mmWave radars are compact, inexpensive, and durable sensors that are robust\nto occlusions and work regardless of environmental conditions, such as weather\nand darkness. However, this comes at the cost of poor angular resolution,\nespecially for inexpensive single-chip radars, which are typically used in\nautomotive and indoor sensing applications. Although many have proposed\nlearning-based methods to mitigate this weakness, no standardized foundational\nmodels or large datasets for the mmWave radar have emerged, and practitioners\nhave largely trained task-specific models from scratch using relatively small\ndatasets.\n  In this paper, we collect (to our knowledge) the largest available raw radar\ndataset with 1M samples (29 hours) and train a foundational model for 4D\nsingle-chip radar, which can predict 3D occupancy and semantic segmentation\nwith quality that is typically only possible with much higher resolution\nsensors. We demonstrate that our Generalizable Radar Transformer (GRT)\ngeneralizes across diverse settings, can be fine-tuned for different tasks, and\nshows logarithmic data scaling of 20\\% per $10\\times$ data. We also run\nextensive ablations on common design decisions, and find that using raw radar\ndata significantly outperforms widely-used lossy representations, equivalent to\na $10\\times$ increase in training data. Finally, we roughly estimate that\n$\\approx$100M samples (3000 hours) of data are required to fully exploit the\npotential of GRT.",
        "url": "http://arxiv.org/abs/2509.12482v1",
        "published_date": "2025-09-15T22:06:17+00:00",
        "updated_date": "2025-09-15T22:06:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianshu Huang",
            "Akarsh Prabhakara",
            "Chuhan Chen",
            "Jay Karhade",
            "Deva Ramanan",
            "Matthew O'Toole",
            "Anthony Rowe"
        ],
        "tldr": "This paper introduces a large raw radar dataset and a foundational model (GRT) for single-chip radar, demonstrating its effectiveness in 3D occupancy and semantic segmentation tasks and showing strong generalization and data scaling properties. The authors also suggest that scaling the dataset to 100M samples could unlock further improvements.",
        "tldr_zh": "本文介绍了一个大型原始雷达数据集和一个用于单芯片雷达的基础模型 (GRT)，展示了其在 3D 占用和语义分割任务中的有效性，并展示了强大的泛化和数据缩放特性。作者还认为将数据集扩展到 1 亿个样本可以进一步改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]