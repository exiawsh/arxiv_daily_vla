[
    {
        "title": "UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs",
        "summary": "Although transformers have demonstrated remarkable capabilities across\nvarious domains, their quadratic attention mechanisms introduce significant\ncomputational overhead when processing long-sequence data. In this paper, we\npresent a unified autonomous driving model, UniLION, which efficiently handles\nlarge-scale LiDAR point clouds, high-resolution multi-view images, and even\ntemporal sequences based on the linear group RNN operator (i.e., performs\nlinear RNN for grouped features). Remarkably, UniLION serves as a single\nversatile architecture that can seamlessly support multiple specialized\nvariants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal\ntemporal fusion configurations) without requiring explicit temporal or\nmulti-modal fusion modules. Moreover, UniLION consistently delivers competitive\nand even state-of-the-art performance across a wide range of core tasks,\nincluding 3D perception (e.g., 3D object detection, 3D object tracking, 3D\noccupancy prediction, BEV map segmentation), prediction (e.g., motion\nprediction), and planning (e.g., end-to-end planning). This unified paradigm\nnaturally simplifies the design of multi-modal and multi-task autonomous\ndriving systems while maintaining superior performance. Ultimately, we hope\nUniLION offers a fresh perspective on the development of 3D foundation models\nin autonomous driving. Code is available at\nhttps://github.com/happinesslz/UniLION",
        "url": "http://arxiv.org/abs/2511.01768v1",
        "published_date": "2025-11-03T17:24:19+00:00",
        "updated_date": "2025-11-03T17:24:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhe Liu",
            "Jinghua Hou",
            "Xiaoqing Ye",
            "Jingdong Wang",
            "Hengshuang Zhao",
            "Xiang Bai"
        ],
        "tldr": "The paper introduces UniLION, a unified autonomous driving model using linear group RNNs for efficient processing of large-scale LiDAR, multi-view images, and temporal sequences, achieving competitive performance across various tasks without explicit fusion modules.",
        "tldr_zh": "该论文介绍了UniLION，一种统一的自动驾驶模型，它使用线性组RNN有效地处理大规模LiDAR点云、多视角图像和时间序列，无需显式的融合模块即可在各种任务中实现有竞争力的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "3EED: Ground Everything Everywhere in 3D",
        "summary": "Visual grounding in 3D is the key for embodied agents to localize\nlanguage-referred objects in open-world environments. However, existing\nbenchmarks are limited to indoor focus, single-platform constraints, and small\nscale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark\nfeaturing RGB and LiDAR data from vehicle, drone, and quadruped platforms. We\nprovide over 128,000 objects and 22,000 validated referring expressions across\ndiverse outdoor scenes -- 10x larger than existing datasets. We develop a\nscalable annotation pipeline combining vision-language model prompting with\nhuman verification to ensure high-quality spatial grounding. To support\ncross-platform learning, we propose platform-aware normalization and\ncross-modal alignment techniques, and establish benchmark protocols for\nin-domain and cross-platform evaluations. Our findings reveal significant\nperformance gaps, highlighting the challenges and opportunities of\ngeneralizable 3D grounding. The 3EED dataset and benchmark toolkit are released\nto advance future research in language-driven 3D embodied perception.",
        "url": "http://arxiv.org/abs/2511.01755v1",
        "published_date": "2025-11-03T17:05:22+00:00",
        "updated_date": "2025-11-03T17:05:22+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Rong Li",
            "Yuhao Dong",
            "Tianshuai Hu",
            "Ao Liang",
            "Youquan Liu",
            "Dongyue Lu",
            "Liang Pan",
            "Lingdong Kong",
            "Junwei Liang",
            "Ziwei Liu"
        ],
        "tldr": "The paper introduces 3EED, a large-scale, multi-platform 3D grounding benchmark designed for embodied agents operating in diverse outdoor environments, addressing limitations of existing datasets.",
        "tldr_zh": "该论文介绍了3EED，一个大规模、多平台的3D视觉定位基准，专为在各种户外环境中运行的具身智能体设计，旨在解决现有数据集的局限性。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process",
        "summary": "Vision-language-action (VLA) models aim to understand natural language\ninstructions and visual observations and to execute corresponding actions as an\nembodied agent. Recent work integrates future images into the\nunderstanding-acting loop, yielding unified VLAs that jointly understand,\ngenerate, and act -- reading text and images and producing future images and\nactions. However, these models either rely on external experts for modality\nunification or treat image generation and action prediction as separate\nprocesses, limiting the benefits of direct synergy between these tasks. Our\ncore philosophy is to optimize generation and action jointly through a\nsynchronous denoising process, where the iterative refinement enables actions\nto evolve from initialization, under constant and sufficient visual guidance.\nWe ground this philosophy in our proposed Unified Diffusion VLA and Joint\nDiscrete Denoising Diffusion Process (JD3P), which is a joint diffusion process\nthat integrates multiple modalities into a single denoising trajectory to serve\nas the key mechanism enabling understanding, generation, and acting to be\nintrinsically synergistic. Our model and theory are built on a unified\ntokenized space of all modalities and a hybrid attention mechanism. We further\npropose a two-stage training pipeline and several inference-time techniques\nthat optimize performance and efficiency. Our approach achieves\nstate-of-the-art performance on benchmarks such as CALVIN, LIBERO, and\nSimplerEnv with 4$\\times$ faster inference than autoregressive methods, and we\ndemonstrate its effectiveness through in-depth analysis and real-world\nevaluations. Our project page is available at\nhttps://irpn-eai.github.io/UD-VLA.github.io/.",
        "url": "http://arxiv.org/abs/2511.01718v1",
        "published_date": "2025-11-03T16:26:54+00:00",
        "updated_date": "2025-11-03T16:26:54+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Jiayi Chen",
            "Wenxuan Song",
            "Pengxiang Ding",
            "Ziyang Zhou",
            "Han Zhao",
            "Feilong Tang",
            "Donglin Wang",
            "Haoang Li"
        ],
        "tldr": "This paper introduces a Unified Diffusion VLA model with a Joint Discrete Denoising Diffusion Process (JD3P) for vision-language-action tasks, achieving SOTA results with faster inference by jointly optimizing generation and action.",
        "tldr_zh": "本文介绍了一种统一扩散VLA模型，该模型具有联合离散去噪扩散过程（JD3P），用于视觉-语言-动作任务，通过联合优化生成和动作，实现了最先进的结果，并具有更快的推理速度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "OmniVLA: Unifiying Multi-Sensor Perception for Physically-Grounded Multimodal VLA",
        "summary": "Vision-language-action (VLA) models have shown strong generalization for\naction prediction through large-scale vision-language pretraining. However,\nmost existing models rely solely on RGB cameras, limiting their perception and,\nconsequently, manipulation capabilities. We present OmniVLA, an omni-modality\nVLA model that integrates novel sensing modalities for physically-grounded\nspatial intelligence beyond RGB perception. The core of our approach is the\nsensor-masked image, a unified representation that overlays spatially grounded\nand physically meaningful masks onto the RGB images, derived from sensors\nincluding an infrared camera, a mmWave radar, and a microphone array. This\nimage-native unification keeps sensor input close to RGB statistics to\nfacilitate training, provides a uniform interface across sensor hardware, and\nenables data-efficient learning with lightweight per-sensor projectors. Built\non this, we present a multisensory vision-language-action model architecture\nand train the model based on an RGB-pretrained VLA backbone. We evaluate\nOmniVLA on challenging real-world tasks where sensor-modality perception is\nneeded to guide the manipulation. OmniVLA achieves an average task success rate\nof 84%, significantly outperforms both RGB-only and raw-sensor-input baseline\nmodels by 59% and 28% respectively, meanwhile showing higher learning\nefficiency and stronger generalization capability.",
        "url": "http://arxiv.org/abs/2511.01210v1",
        "published_date": "2025-11-03T04:10:44+00:00",
        "updated_date": "2025-11-03T04:10:44+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Heyu Guo",
            "Shanmu Wang",
            "Ruichun Ma",
            "Shiqi Jiang",
            "Yasaman Ghasempour",
            "Omid Abari",
            "Baining Guo",
            "Lili Qi"
        ],
        "tldr": "OmniVLA introduces a unified multi-sensor VLA model using a sensor-masked image representation, significantly improving performance on real-world manipulation tasks compared to RGB-only and raw-sensor baselines.",
        "tldr_zh": "OmniVLA 引入了一个统一的多传感器 VLA 模型，使用传感器掩码图像表示，与仅使用 RGB 和原始传感器基线相比，显着提高了在现实世界操作任务中的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models",
        "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have\nsignificantly improved 2D visual understanding, prompting interest in their\napplication to complex 3D reasoning tasks. However, it remains unclear whether\nthese models can effectively capture the detailed spatial information required\nfor robust real-world performance, especially cross-view consistency, a key\nrequirement for accurate 3D reasoning. Considering this issue, we introduce\nViewpoint Learning, a task designed to evaluate and improve the spatial\nreasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,\nconsisting of 100K object-centric image pairs with diverse viewpoints and\ncorresponding question-answer pairs. Our approach employs a two-stage\nfine-tuning strategy: first, foundational knowledge is injected to the baseline\nMLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in\nsignificant improvements across multiple tasks; second, generalization is\nenhanced through Reinforcement Learning using the Group Relative Policy\nOptimization (GRPO) algorithm on a broader set of questions. Additionally, we\nintroduce a hybrid cold-start initialization method designed to simultaneously\nlearn viewpoint representations and maintain coherent reasoning thinking.\nExperimental results show that our approach significantly activates the spatial\nreasoning ability of MLLM, improving performance on both in-domain and\nout-of-domain reasoning tasks. Our findings highlight the value of developing\nfoundational spatial skills in MLLMs, supporting future progress in robotics,\nautonomous systems, and 3D scene understanding.",
        "url": "http://arxiv.org/abs/2511.01618v1",
        "published_date": "2025-11-03T14:27:00+00:00",
        "updated_date": "2025-11-03T14:27:00+00:00",
        "categories": [
            "cs.CV",
            "cs.CL"
        ],
        "authors": [
            "Xiaoyu Zhan",
            "Wenxuan Huang",
            "Hao Sun",
            "Xinyu Fu",
            "Changfeng Ma",
            "Shaosheng Cao",
            "Bohan Jia",
            "Shaohui Lin",
            "Zhenfei Yin",
            "Lei Bai",
            "Wanli Ouyang",
            "Yuanqi Li",
            "Jie Guo",
            "Yanwen Guo"
        ],
        "tldr": "This paper introduces Viewpoint Learning and the Viewpoint-100K dataset to enhance the spatial reasoning capabilities of MLLMs, using a two-stage fine-tuning approach (SFT + RL) and a hybrid cold-start initialization method.",
        "tldr_zh": "该论文介绍了视点学习和 Viewpoint-100K 数据集，旨在通过两阶段微调方法（SFT + RL）和混合冷启动初始化方法来提升 MLLM 的空间推理能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MARS: Multi-Agent Robotic System with Multimodal Large Language Models for Assistive Intelligence",
        "summary": "Multimodal large language models (MLLMs) have shown remarkable capabilities\nin cross-modal understanding and reasoning, offering new opportunities for\nintelligent assistive systems, yet existing systems still struggle with\nrisk-aware planning, user personalization, and grounding language plans into\nexecutable skills in cluttered homes. We introduce MARS - a Multi-Agent Robotic\nSystem powered by MLLMs for assistive intelligence and designed for smart home\nrobots supporting people with disabilities. The system integrates four agents:\na visual perception agent for extracting semantic and spatial features from\nenvironment images, a risk assessment agent for identifying and prioritizing\nhazards, a planning agent for generating executable action sequences, and an\nevaluation agent for iterative optimization. By combining multimodal perception\nwith hierarchical multi-agent decision-making, the framework enables adaptive,\nrisk-aware, and personalized assistance in dynamic indoor environments.\nExperiments on multiple datasets demonstrate the superior overall performance\nof the proposed system in risk-aware planning and coordinated multi-agent\nexecution compared with state-of-the-art multimodal models. The proposed\napproach also highlights the potential of collaborative AI for practical\nassistive scenarios and provides a generalizable methodology for deploying\nMLLM-enabled multi-agent systems in real-world environments.",
        "url": "http://arxiv.org/abs/2511.01594v1",
        "published_date": "2025-11-03T13:58:37+00:00",
        "updated_date": "2025-11-03T13:58:37+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "I.2.9; I.2.11; I.2.6; I.4.8"
        ],
        "authors": [
            "Renjun Gao",
            "Peiyan Zhong"
        ],
        "tldr": "The paper introduces MARS, a multi-agent robotic system using MLLMs for assistive intelligence in smart homes, focusing on risk-aware planning and personalized assistance. It demonstrates improved performance in experiments involving risk-aware planning and multi-agent execution.",
        "tldr_zh": "该论文介绍了MARS，一个使用MLLM的多智能体机器人系统，用于智能家居中的辅助智能，重点关注风险感知规划和个性化辅助。实验表明，在风险感知规划和多智能体执行方面，该系统表现出优越的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model",
        "summary": "Vision-Language-Action models (VLAs) are emerging as powerful tools for\nlearning generalizable visuomotor control policies. However, current VLAs are\nmostly trained on large-scale image-text-action data and remain limited in two\nkey ways: (i) they struggle with pixel-level scene understanding, and (ii) they\nrely heavily on textual prompts, which reduces their flexibility in real-world\nsettings. To address these challenges, we introduce PixelVLA, the first VLA\nmodel designed to support both pixel-level reasoning and multimodal prompting\nwith text and visual inputs. Our approach is built on a new visuomotor\ninstruction tuning framework that integrates a multiscale pixel-aware encoder\nwith a visual prompting encoder. To train PixelVLA effectively, we further\npropose a two-stage automated annotation pipeline that generates Pixel-160K, a\nlarge-scale dataset with pixel-level annotations derived from existing robot\ndata. Experiments on three standard VLA benchmarks and two VLA model variants\nshow that PixelVLA improves manipulation success rates by 10.1%-17.8% over\nOpenVLA, while requiring only 1.5% of its pretraining cost. These results\ndemonstrate that PixelVLA can be integrated into existing VLAs to enable more\naccurate, efficient, and versatile robot control in complex environments. The\ndataset and code will be released as open source.",
        "url": "http://arxiv.org/abs/2511.01571v1",
        "published_date": "2025-11-03T13:39:37+00:00",
        "updated_date": "2025-11-03T13:39:37+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Wenqi Liang",
            "Gan Sun",
            "Yao He",
            "Jiahua Dong",
            "Suyan Dai",
            "Ivan Laptev",
            "Salman Khan",
            "Yang Cong"
        ],
        "tldr": "PixelVLA is a new VLA model designed for pixel-level reasoning and multimodal prompting, trained on a novel pixel-level annotated dataset. It achieves improved manipulation success rates while significantly reducing pretraining costs.",
        "tldr_zh": "PixelVLA是一种新的VLA模型，专为像素级推理和多模态提示而设计，并使用新的像素级标注数据集进行训练。它在显著降低预训练成本的同时，提高了操作成功率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Driving scenario generation and evaluation using a structured layer representation and foundational models",
        "summary": "Rare and challenging driving scenarios are critical for autonomous vehicle\ndevelopment. Since they are difficult to encounter, simulating or generating\nthem using generative models is a popular approach. Following previous efforts\nto structure driving scenario representations in a layer model, we propose a\nstructured five-layer model to improve the evaluation and generation of rare\nscenarios. We use this model alongside large foundational models to generate\nnew driving scenarios using a data augmentation strategy. Unlike previous\nrepresentations, our structure introduces subclasses and characteristics for\nevery agent of the scenario, allowing us to compare them using an embedding\nspecific to our layer-model. We study and adapt two metrics to evaluate the\nrelevance of a synthetic dataset in the context of a structured representation:\nthe diversity score estimates how different the scenarios of a dataset are from\none another, while the originality score calculates how similar a synthetic\ndataset is from a real reference set. This paper showcases both metrics in\ndifferent generation setup, as well as a qualitative evaluation of synthetic\nvideos generated from structured scenario descriptions. The code and extended\nresults can be found at https://github.com/Valgiz/5LMSG.",
        "url": "http://arxiv.org/abs/2511.01541v1",
        "published_date": "2025-11-03T13:04:55+00:00",
        "updated_date": "2025-11-03T13:04:55+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Arthur Hubert",
            "Gamal Elghazaly",
            "Raphaël Frank"
        ],
        "tldr": "This paper introduces a structured five-layer model and foundational models for generating diverse and original driving scenarios, with metrics for evaluation. They also provide code and extended results.",
        "tldr_zh": "本文介绍了一种结构化的五层模型和基础模型，用于生成多样化和原始的驾驶场景，并提供评估指标。他们还提供了代码和扩展结果。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning",
        "summary": "Unsupervised learning of depth and ego-motion, two fundamental 3D perception\ntasks, has made significant strides in recent years. However, most methods\ntreat ego-motion as an auxiliary task, either mixing all motion types or\nexcluding depth-independent rotational motions in supervision. Such designs\nlimit the incorporation of strong geometric constraints, reducing reliability\nand robustness under diverse conditions. This study introduces a discriminative\ntreatment of motion components, leveraging the geometric regularities of their\nrespective rigid flows to benefit both depth and ego-motion estimation. Given\nconsecutive video frames, network outputs first align the optical axes and\nimaging planes of the source and target cameras. Optical flows between frames\nare transformed through these alignments, and deviations are quantified to\nimpose geometric constraints individually on each ego-motion component,\nenabling more targeted refinement. These alignments further reformulate the\njoint learning process into coaxial and coplanar forms, where depth and each\ntranslation component can be mutually derived through closed-form geometric\nrelationships, introducing complementary constraints that improve depth\nrobustness. DiMoDE, a general depth and ego-motion joint learning framework\nincorporating these designs, achieves state-of-the-art performance on multiple\npublic datasets and a newly collected diverse real-world dataset, particularly\nunder challenging conditions. Our source code will be publicly available at\nmias.group/DiMoDE upon publication.",
        "url": "http://arxiv.org/abs/2511.01502v1",
        "published_date": "2025-11-03T12:14:52+00:00",
        "updated_date": "2025-11-03T12:14:52+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Mengtan Zhang",
            "Zizhan Guo",
            "Hongbo Zhao",
            "Yi Feng",
            "Zuyi Xiong",
            "Yue Wang",
            "Shaoyi Du",
            "Hanli Wang",
            "Rui Fan"
        ],
        "tldr": "This paper introduces a novel unsupervised depth and ego-motion learning framework (DiMoDE) that discriminatively treats motion components, improving robustness and accuracy by leveraging geometric constraints and achieving state-of-the-art results.",
        "tldr_zh": "本文介绍了一种新的无监督深度和自我运动学习框架 (DiMoDE)，该框架有区别地处理运动分量，通过利用几何约束来提高鲁棒性和准确性，并达到最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation",
        "summary": "Object pose estimation is a fundamental problem in robotics and computer\nvision, yet it remains challenging due to partial observability, occlusions,\nand object symmetries, which inevitably lead to pose ambiguity and multiple\nhypotheses consistent with the same observation. While deterministic deep\nnetworks achieve impressive performance under well-constrained conditions, they\nare often overconfident and fail to capture the multi-modality of the\nunderlying pose distribution. To address these challenges, we propose a novel\nprobabilistic framework that leverages flow matching on the SE(3) manifold for\nestimating 6D object pose distributions. Unlike existing methods that regress a\nsingle deterministic output, our approach models the full pose distribution\nwith a sample-based estimate and enables reasoning about uncertainty in\nambiguous cases such as symmetric objects or severe occlusions. We achieve\nstate-of-the-art results on Real275, YCB-V, and LM-O, and demonstrate how our\nsample-based pose estimates can be leveraged in downstream robotic manipulation\ntasks such as active perception for disambiguating uncertain viewpoints or\nguiding grasp synthesis in an uncertainty-aware manner.",
        "url": "http://arxiv.org/abs/2511.01501v1",
        "published_date": "2025-11-03T12:11:35+00:00",
        "updated_date": "2025-11-03T12:11:35+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Yufeng Jin",
            "Niklas Funk",
            "Vignesh Prasad",
            "Zechu Li",
            "Mathias Franzius",
            "Jan Peters",
            "Georgia Chalvatzaki"
        ],
        "tldr": "This paper introduces SE(3)-PoseFlow, a novel probabilistic framework using flow matching on the SE(3) manifold to estimate 6D object pose distributions, achieving state-of-the-art results and enabling uncertainty-aware robotic manipulation.",
        "tldr_zh": "本文介绍了SE(3)-PoseFlow，一种新颖的概率框架，它利用SE(3)流形上的流匹配来估计6D物体姿态分布，实现了最先进的结果，并实现了不确定性感知的机器人操作。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop",
        "summary": "LiDAR semantic segmentation degrades in adverse weather because refraction,\nscattering, and point dropouts corrupt geometry. Prior work in weather\nsimulation, mixing-based augmentation, domain randomization, and uncertainty or\nboundary regularization improves robustness but still overlooks structural\nvulnerabilities near boundaries, corners, and sparse regions. We present a\nLight Geometry-aware adapter. The module aligns azimuth and applies horizontal\ncircular padding to preserve neighbor continuity across the 0~360 degree\nwrap-around boundary. A local-window K-Nearest Neighbors gathers nearby points\nand computes simple local statistics, which are compressed into compact\ngeometry-aware cues. During training, these cues drive region-aware\nregularization that stabilizes predictions in structurally fragile areas. The\nadapter is plug and play, complements augmentation, and can be enabled only\nduring training with negligible inference cost. We adopt a source-only\ncross-weather setup where models train on SemanticKITTI and are evaluated on\nSemanticSTF without target labels or fine-tuning. The adapter improves mIoU by\n7.9 percentage points over the data-centric augmentation baseline and by 0.6\npoints over the class-centric regularization baseline. These results indicate\nthat geometry-driven regularization is a key direction for all-weather LiDAR\nsegmentation.",
        "url": "http://arxiv.org/abs/2511.01250v1",
        "published_date": "2025-11-03T05:44:07+00:00",
        "updated_date": "2025-11-03T05:44:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "YoungJae Cheong",
            "Jhonghyun An"
        ],
        "tldr": "This paper introduces a geometry-aware adapter for LiDAR semantic segmentation that improves robustness in adverse weather conditions by focusing on structural vulnerabilities. It achieves this through region-aware regularization during training and a plug-and-play design for negligible inference cost.",
        "tldr_zh": "本文提出了一种几何感知适配器，用于激光雷达语义分割，通过关注结构脆弱性来提高在恶劣天气条件下的鲁棒性。它通过训练期间的区域感知正则化和可忽略推理成本的即插即用设计来实现这一点。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping",
        "summary": "Reconstructing large-scale colored point clouds is an important task in\nrobotics, supporting perception, navigation, and scene understanding. Despite\nadvances in LiDAR inertial visual odometry (LIVO), its performance remains\nhighly sensitive to extrinsic calibration. Meanwhile, 3D vision foundation\nmodels, such as VGGT, suffer from limited scalability in large environments and\ninherently lack metric scale. To overcome these limitations, we propose\nLiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry with\nthe state-of-the-art VGGT model through a two-stage coarse- to-fine fusion\npipeline: First, a pre-fusion module with robust initialization refinement\nefficiently estimates VGGT poses and point clouds with coarse metric scale\nwithin each session. Then, a post-fusion module enhances cross-modal 3D\nsimilarity transformation, using bounding-box-based regularization to reduce\nscale distortions caused by inconsistent FOVs between LiDAR and camera sensors.\nExtensive experiments across multiple datasets demonstrate that LiDAR-VGGT\nachieves dense, globally consistent colored point clouds and outperforms both\nVGGT-based methods and LIVO baselines. The implementation of our proposed novel\ncolor point cloud evaluation toolkit will be released as open source.",
        "url": "http://arxiv.org/abs/2511.01186v1",
        "published_date": "2025-11-03T03:24:28+00:00",
        "updated_date": "2025-11-03T03:24:28+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Lijie Wang",
            "Lianjie Guo",
            "Ziyi Xu",
            "Qianhao Wang",
            "Fei Gao",
            "Xieyuanli Chen"
        ],
        "tldr": "This paper presents LiDAR-VGGT, a novel framework that tightly integrates LiDAR inertial odometry with VGGT using a coarse-to-fine fusion pipeline to achieve globally consistent and metric-scale dense colored point clouds, outperforming existing methods.",
        "tldr_zh": "本文提出了LiDAR-VGGT，一种新颖的框架，通过粗到精的融合流程将激光雷达惯性里程计与VGGT紧密结合，以实现全局一致且具有度量比例的密集彩色点云，性能优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Terrain-Enhanced Resolution-aware Refinement Attention for Off-Road Segmentation",
        "summary": "Off-road semantic segmentation suffers from thick, inconsistent boundaries,\nsparse supervision for rare classes, and pervasive label noise. Designs that\nfuse only at low resolution blur edges and propagate local errors, whereas\nmaintaining high-resolution pathways or repeating high-resolution fusions is\ncostly and fragile to noise. We introduce a resolutionaware token decoder that\nbalances global semantics, local consistency, and boundary fidelity under\nimperfect supervision. Most computation occurs at a low-resolution bottleneck;\na gated cross-attention injects fine-scale detail, and only a sparse,\nuncertainty-selected set of pixels is refined. The components are co-designed\nand tightly integrated: global self-attention with lightweight dilated\ndepthwise refinement restores local coherence; a gated cross-attention\nintegrates fine-scale features from a standard high-resolution encoder stream\nwithout amplifying noise; and a class-aware point refinement corrects residual\nambiguities with negligible overhead. During training, we add a boundary-band\nconsistency regularizer that encourages coherent predictions in a thin\nneighborhood around annotated edges, with no inference-time cost. Overall, the\nresults indicate competitive performance and improved stability across\ntransitions.",
        "url": "http://arxiv.org/abs/2511.01434v1",
        "published_date": "2025-11-03T10:36:57+00:00",
        "updated_date": "2025-11-03T10:36:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Seongkyu Choi",
            "Jhonghyun An"
        ],
        "tldr": "This paper introduces a resolution-aware token decoder for off-road semantic segmentation that balances global semantics, local consistency, and boundary fidelity using a gated cross-attention mechanism and boundary consistency regularization.",
        "tldr_zh": "本文提出了一种用于越野语义分割的、分辨率感知的 Token 解码器，该解码器利用门控交叉注意力机制和边界一致性正则化来平衡全局语义、局部一致性和边界保真度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    },
    {
        "title": "Saliency-Guided Domain Adaptation for Left-Hand Driving in Autonomous Steering",
        "summary": "Domain adaptation is required for automated driving models to generalize well\nacross diverse road conditions. This paper explores a training method for\ndomain adaptation to adapt PilotNet, an end-to-end deep learning-based model,\nfor left-hand driving conditions using real-world Australian highway data. Four\ntraining methods were evaluated: (1) a baseline model trained on U.S.\nright-hand driving data, (2) a model trained on flipped U.S. data, (3) a model\npretrained on U.S. data and then fine-tuned on Australian highways, and (4) a\nmodel pretrained on flipped U.S. data and then finetuned on Australian\nhighways. This setup examines whether incorporating flipped data enhances the\nmodel adaptation by providing an initial left-hand driving alignment. The paper\ncompares model performance regarding steering prediction accuracy and\nattention, using saliency-based analysis to measure attention shifts across\nsignificant road regions. Results show that pretraining on flipped data alone\nworsens prediction stability due to misaligned feature representations, but\nsignificantly improves adaptation when followed by fine-tuning, leading to\nlower prediction error and stronger focus on left-side cues. To validate this\napproach across different architectures, the same experiments were done on\nResNet, which confirmed similar adaptation trends. These findings emphasize the\nimportance of preprocessing techniques, such as flipped-data pretraining,\nfollowed by fine-tuning to improve model adaptation with minimal retraining\nrequirements.",
        "url": "http://arxiv.org/abs/2511.01223v1",
        "published_date": "2025-11-03T04:46:17+00:00",
        "updated_date": "2025-11-03T04:46:17+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Zahra Mehraban",
            "Sebastien Glaser",
            "Michael Milford",
            "Ronald Schroeter"
        ],
        "tldr": "This paper explores a domain adaptation technique for autonomous driving, specifically adapting a steering model (PilotNet and ResNet) to left-hand driving by pretraining on flipped right-hand driving data and then fine-tuning on Australian highway data, demonstrating improved performance and attention on left-side cues.",
        "tldr_zh": "本文探讨了一种自动驾驶领域的领域自适应技术，通过在翻转的右手驾驶数据上进行预训练，然后在澳大利亚高速公路数据上进行微调，将转向模型（PilotNet和ResNet）调整为左手驾驶，结果表明性能得到了改善，并且更加关注左侧提示。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]