[
    {
        "title": "Changes in Real Time: Online Scene Change Detection with Multi-View Fusion",
        "summary": "Online Scene Change Detection (SCD) is an extremely challenging problem that requires an agent to detect relevant changes on the fly while observing the scene from unconstrained viewpoints. Existing online SCD methods are significantly less accurate than offline approaches. We present the first online SCD approach that is pose-agnostic, label-free, and ensures multi-view consistency, while operating at over 10 FPS and achieving new state-of-the-art performance, surpassing even the best offline approaches. Our method introduces a new self-supervised fusion loss to infer scene changes from multiple cues and observations, PnP-based fast pose estimation against the reference scene, and a fast change-guided update strategy for the 3D Gaussian Splatting scene representation. Extensive experiments on complex real-world datasets demonstrate that our approach outperforms both online and offline baselines.",
        "url": "http://arxiv.org/abs/2511.12370v1",
        "published_date": "2025-11-15T22:12:16+00:00",
        "updated_date": "2025-11-15T22:12:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chamuditha Jayanga Galappaththige",
            "Jason Lai",
            "Lloyd Windrim",
            "Donald Dansereau",
            "Niko Sünderhauf",
            "Dimity Miller"
        ],
        "tldr": "This paper presents a novel online scene change detection method that is pose-agnostic, label-free, and ensures multi-view consistency, achieving state-of-the-art performance even surpassing offline approaches, while running at over 10 FPS.",
        "tldr_zh": "本文提出了一种新的在线场景变化检测方法，该方法具有姿态无关性、无标签性，并确保多视角一致性，实现了最先进的性能，甚至超越了离线方法，同时运行速度超过 10 FPS。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Fast Reasoning Segmentation for Images and Videos",
        "summary": "Reasoning segmentation enables open-set object segmentation via implicit text queries, therefore serving as a foundation for embodied agents that should operate autonomously in real-world environments. However, existing methods for reasoning segmentation require multimodal large language models with billions of parameters that exceed the computational capabilities of edge devices that typically deploy the embodied AI systems. Distillation offers a pathway to compress these models while preserving their capabilities. Yet, existing distillation approaches fail to transfer the multi-step reasoning capabilities that reasoning segmentation demands, as they focus on matching output predictions and intermediate features rather than preserving reasoning chains. The emerging paradigm of reasoning over digital twin representations presents an opportunity for more effective distillation by re-framing the problem. Consequently, we propose FastReasonSeg, which employs digital twin representations that decouple perception from reasoning to enable more effective distillation. Our distillation scheme first relies on supervised fine-tuning on teacher-generated reasoning chains. Then it is followed by reinforcement fine-tuning with joint rewards evaluating both segmentation accuracy and reasoning quality alignment. Experiments on two video (JiTBench, RVTBench) and two image benchmarks (ReasonSeg, LLM-Seg40K) demonstrate that our FastReasonSeg achieves state-of-the-art reasoning segmentation performance. Moreover, the distilled 0.6B variant outperforms models with 20 times more parameters while achieving 7.79 FPS throughput with only 2.1GB memory consumption. This efficiency enables deployment in resource-constrained environments to enable real-time reasoning segmentation.",
        "url": "http://arxiv.org/abs/2511.12368v1",
        "published_date": "2025-11-15T22:06:29+00:00",
        "updated_date": "2025-11-15T22:06:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiqing Shen",
            "Mathias Unberath"
        ],
        "tldr": "The paper introduces FastReasonSeg, a method for distilling reasoning segmentation models using digital twin representations and a novel distillation scheme, achieving state-of-the-art performance with significantly reduced computational requirements for edge deployment.",
        "tldr_zh": "该论文介绍了 FastReasonSeg，一种使用数字孪生表示和新颖的蒸馏方案来蒸馏推理分割模型的方法，以显著降低的计算要求实现了最先进的性能，适用于边缘部署。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Towards Temporal Fusion Beyond the Field of View for Camera-based Semantic Scene Completion",
        "summary": "Recent camera-based 3D semantic scene completion (SSC) methods have increasingly explored leveraging temporal cues to enrich the features of the current frame. However, while these approaches primarily focus on enhancing in-frame regions, they often struggle to reconstruct critical out-of-frame areas near the sides of the ego-vehicle, although previous frames commonly contain valuable contextual information about these unseen regions. To address this limitation, we propose the Current-Centric Contextual 3D Fusion (C3DFusion) module, which generates hidden region-aware 3D feature geometry by explicitly aligning 3D-lifted point features from both current and historical frames. C3DFusion performs enhanced temporal fusion through two complementary techniques-historical context blurring and current-centric feature densification-which suppress noise from inaccurately warped historical point features by attenuating their scale, and enhance current point features by increasing their volumetric contribution. Simply integrated into standard SSC architectures, C3DFusion demonstrates strong effectiveness, significantly outperforming state-of-the-art methods on the SemanticKITTI and SSCBench-KITTI-360 datasets. Furthermore, it exhibits robust generalization, achieving notable performance gains when applied to other baseline models.",
        "url": "http://arxiv.org/abs/2511.12498v1",
        "published_date": "2025-11-16T08:16:25+00:00",
        "updated_date": "2025-11-16T08:16:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jongseong Bae",
            "Junwoo Ha",
            "Jinnyeong Heo",
            "Yeongin Lee",
            "Ha Young Kim"
        ],
        "tldr": "This paper introduces a novel temporal fusion module, C3DFusion, to improve camera-based 3D semantic scene completion by explicitly fusing current and historical frame features, particularly for out-of-frame regions, and demonstrates significant performance gains on standard datasets.",
        "tldr_zh": "本文提出了一种新颖的时序融合模块C3DFusion，通过显式融合当前帧和历史帧的特征，特别是在帧外区域，来改进基于相机的3D语义场景补全，并在标准数据集上展示了显著的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "VLA-R: Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving",
        "summary": "Exploring open-world situations in an end-to-end manner is a promising yet challenging task due to the need for strong generalization capabilities. In particular, end-to-end autonomous driving in unstructured outdoor environments often encounters conditions that were unfamiliar during training. In this work, we present Vision-Language Action Retrieval (VLA-R), an open-world end-to-end autonomous driving (OW-E2EAD) framework that integrates open-world perception with a novel vision-action retrieval paradigm. We leverage a frozen vision-language model for open-world detection and segmentation to obtain multi-scale, prompt-guided, and interpretable perception features without domain-specific tuning. A Q-Former bottleneck aggregates fine-grained visual representations with language-aligned visual features, bridging perception and action domains. To learn transferable driving behaviors, we introduce a vision-action contrastive learning scheme that aligns vision-language and action embeddings for effective open-world reasoning and action retrieval. Our experiments on a real-world robotic platform demonstrate strong generalization and exploratory performance in unstructured, unseen environments, even with limited data. Demo videos are provided in the supplementary material.",
        "url": "http://arxiv.org/abs/2511.12405v1",
        "published_date": "2025-11-16T00:55:28+00:00",
        "updated_date": "2025-11-16T00:55:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hyunki Seong",
            "Seongwoo Moon",
            "Hojin Ahn",
            "Jehun Kang",
            "David Hyunchul Shim"
        ],
        "tldr": "This paper introduces VLA-R, a vision-language action retrieval framework for open-world end-to-end autonomous driving that uses a frozen vision-language model for perception and a contrastive learning scheme for action retrieval, demonstrating strong generalization in real-world experiments.",
        "tldr_zh": "本文介绍了一种名为VLA-R的视觉-语言动作检索框架，用于开放世界端到端自动驾驶。该框架利用冻结的视觉-语言模型进行感知，并采用对比学习方案进行动作检索，在真实世界的实验中表现出强大的泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]