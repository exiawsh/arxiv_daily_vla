[
    {
        "title": "Efficient Active Training for Deep LiDAR Odometry",
        "summary": "Robust and efficient deep LiDAR odometry models are crucial for accurate\nlocalization and 3D reconstruction, but typically require extensive and diverse\ntraining data to adapt to diverse environments, leading to inefficiencies. To\ntackle this, we introduce an active training framework designed to selectively\nextract training data from diverse environments, thereby reducing the training\nload and enhancing model generalization. Our framework is based on two key\nstrategies: Initial Training Set Selection (ITSS) and Active Incremental\nSelection (AIS). ITSS begins by breaking down motion sequences from general\nweather into nodes and edges for detailed trajectory analysis, prioritizing\ndiverse sequences to form a rich initial training dataset for training the base\nmodel. For complex sequences that are difficult to analyze, especially under\nchallenging snowy weather conditions, AIS uses scene reconstruction and\nprediction inconsistency to iteratively select training samples, refining the\nmodel to handle a wide range of real-world scenarios. Experiments across\ndatasets and weather conditions validate our approach's effectiveness. Notably,\nour method matches the performance of full-dataset training with just 52\\% of\nthe sequence volume, demonstrating the training efficiency and robustness of\nour active training paradigm. By optimizing the training process, our approach\nsets the stage for more agile and reliable LiDAR odometry systems, capable of\nnavigating diverse environmental conditions with greater precision.",
        "url": "http://arxiv.org/abs/2509.03211v1",
        "published_date": "2025-09-03T11:00:17+00:00",
        "updated_date": "2025-09-03T11:00:17+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Beibei Zhou",
            "Zhiyuan Zhang",
            "Zhenbo Song",
            "Jianhui Guo",
            "Hui Kong"
        ],
        "tldr": "The paper introduces an active training framework (ITSS & AIS) for deep LiDAR odometry, achieving comparable performance to full-dataset training with significantly less data by intelligently selecting training samples.",
        "tldr_zh": "该论文介绍了一种用于深度激光雷达里程计的主动训练框架（ITSS & AIS），通过智能选择训练样本，以显著减少的数据量实现了与完整数据集训练相当的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "sam-llm: interpretable lane change trajectoryprediction via parametric finetuning",
        "summary": "This work introduces SAM-LLM, a novel hybrid architecture that bridges the\ngap between the contextual reasoning of Large Language Models (LLMs) and the\nphysical precision of kinematic lane change models for autonomous driving. The\nsystem is designed for interpretable lane change trajectory prediction by\nfinetuning an LLM to output the core physical parameters of a trajectory model\ninstead of raw coordinates. For lane-keeping scenarios, the model predicts\ndiscrete coordinates, but for lane change maneuvers, it generates the\nparameters for an enhanced Sinusoidal Acceleration Model (SAM), including\nlateral displacement, maneuver duration, initial lateral velocity, and\nlongitudinal velocity change. This parametric approach yields a complete,\ncontinuous, and physically plausible trajectory model that is inherently\ninterpretable and computationally efficient, achieving an 80% reduction in\noutput size compared to coordinate-based methods. The SAM-LLM achieves a\nstate-of-the-art overall intention prediction accuracy of 98.73%, demonstrating\nperformance equivalent to traditional LLM predictors while offering significant\nadvantages in explainability and resource efficiency.",
        "url": "http://arxiv.org/abs/2509.03462v1",
        "published_date": "2025-09-03T16:37:49+00:00",
        "updated_date": "2025-09-03T16:37:49+00:00",
        "categories": [
            "cs.AI",
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Zhuo Cao",
            "Yunxiao Shi",
            "Min Xu"
        ],
        "tldr": "The paper introduces SAM-LLM, a hybrid architecture combining LLMs and kinematic models for interpretable and efficient lane change trajectory prediction by predicting parameters of a sinusoidal acceleration model, achieving high intention prediction accuracy with reduced output size.",
        "tldr_zh": "该论文介绍了一种混合架构SAM-LLM，它结合了LLM和运动学模型，通过预测正弦加速度模型的参数来实现可解释且高效的车道变换轨迹预测，在高精度意图预测的同时，减少了输出规模。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PI3DETR: Parametric Instance Detection of 3D Point Cloud Edges with a Geometry-Aware 3DETR",
        "summary": "We present PI3DETR, an end-to-end framework that directly predicts 3D\nparametric curve instances from raw point clouds, avoiding the intermediate\nrepresentations and multi-stage processing common in prior work. Extending\n3DETR, our model introduces a geometry-aware matching strategy and specialized\nloss functions that enable unified detection of differently parameterized curve\ntypes, including cubic B\\'ezier curves, line segments, circles, and arcs, in a\nsingle forward pass. Optional post-processing steps further refine predictions\nwithout adding complexity. This streamlined design improves robustness to noise\nand varying sampling densities, addressing critical challenges in real world\nLiDAR and 3D sensing scenarios. PI3DETR sets a new state-of-the-art on the ABC\ndataset and generalizes effectively to real sensor data, offering a simple yet\npowerful solution for 3D edge and curve estimation.",
        "url": "http://arxiv.org/abs/2509.03262v1",
        "published_date": "2025-09-03T12:24:25+00:00",
        "updated_date": "2025-09-03T12:24:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fabio F. Oberweger",
            "Michael Schwingshackl",
            "Vanessa Staderini"
        ],
        "tldr": "PI3DETR is a novel end-to-end framework extending 3DETR for direct prediction of 3D parametric curve instances from point clouds, achieving state-of-the-art results on the ABC dataset and generalizing well to real sensor data.",
        "tldr_zh": "PI3DETR是一个新的端到端框架，扩展了3DETR，用于直接从点云预测3D参数曲线实例，在ABC数据集上实现了最先进的结果，并能很好地推广到真实传感器数据。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Uncertainty-aware Test-Time Training (UT$^3$) for Efficient On-the-fly Domain Adaptive Dense Regression",
        "summary": "Deep neural networks (DNNs) are increasingly being used in autonomous\nsystems. However, DNNs do not generalize well to domain shift. Adapting to a\ncontinuously evolving environment is a safety-critical challenge inevitably\nfaced by all autonomous systems deployed to the real world. Recent work on\ntest-time training proposes methods that adapt to a new test distribution on\nthe fly by optimizing the DNN model for each test input using self-supervision.\nHowever, these techniques result in a sharp increase in inference time as\nmultiple forward and backward passes are required for a single test sample (for\ntest-time training) before finally making the prediction based on the\nfine-tuned features. This is undesirable for real-world robotics applications\nwhere these models may be deployed to resource constraint hardware with strong\nlatency requirements. In this work, we propose a new framework (called UT$^3$)\nthat leverages test-time training for improved performance in the presence of\ncontinuous domain shift while also decreasing the inference time, making it\nsuitable for real-world applications. Our method proposes an uncertainty-aware\nself-supervision task for efficient test-time training that leverages the\nquantified uncertainty to selectively apply the training leading to sharp\nimprovements in the inference time while performing comparably to standard\ntest-time training protocol. Our proposed protocol offers a continuous setting\nto identify the selected keyframes, allowing the end-user to control how often\nto apply test-time training. We demonstrate the efficacy of our method on a\ndense regression task - monocular depth estimation.",
        "url": "http://arxiv.org/abs/2509.03012v1",
        "published_date": "2025-09-03T04:41:43+00:00",
        "updated_date": "2025-09-03T04:41:43+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Uddeshya Upadhyay"
        ],
        "tldr": "This paper introduces Uncertainty-aware Test-Time Training (UT$^3$), a method for efficient online domain adaptation in dense regression tasks, specifically addressing the inference time overhead of existing test-time training approaches by using uncertainty to selectively apply training.",
        "tldr_zh": "该论文介绍了一种不确定性感知测试时训练（UT$^3$）方法，用于密集回归任务中高效的在线领域自适应，通过利用不确定性来选择性地应用训练，从而解决了现有测试时训练方法的推理时间开销问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DUViN: Diffusion-Based Underwater Visual Navigation via Knowledge-Transferred Depth Features",
        "summary": "Autonomous underwater navigation remains a challenging problem due to limited\nsensing capabilities and the difficulty of constructing accurate maps in\nunderwater environments. In this paper, we propose a Diffusion-based Underwater\nVisual Navigation policy via knowledge-transferred depth features, named DUViN,\nwhich enables vision-based end-to-end 4-DoF motion control for underwater\nvehicles in unknown environments. DUViN guides the vehicle to avoid obstacles\nand maintain a safe and perception awareness altitude relative to the terrain\nwithout relying on pre-built maps. To address the difficulty of collecting\nlarge-scale underwater navigation datasets, we propose a method that ensures\nrobust generalization under domain shifts from in-air to underwater\nenvironments by leveraging depth features and introducing a novel model\ntransfer strategy. Specifically, our training framework consists of two phases:\nwe first train the diffusion-based visual navigation policy on in-air datasets\nusing a pre-trained depth feature extractor. Secondly, we retrain the extractor\non an underwater depth estimation task and integrate the adapted extractor into\nthe trained navigation policy from the first step. Experiments in both\nsimulated and real-world underwater environments demonstrate the effectiveness\nand generalization of our approach. The experimental videos are available at\nhttps://www.youtube.com/playlist?list=PLqt2s-RyCf1gfXJgFzKjmwIqYhrP4I-7Y.",
        "url": "http://arxiv.org/abs/2509.02983v1",
        "published_date": "2025-09-03T03:43:12+00:00",
        "updated_date": "2025-09-03T03:43:12+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Jinghe Yang",
            "Minh-Quan Le",
            "Mingming Gong",
            "Ye Pu"
        ],
        "tldr": "The paper introduces DUViN, a diffusion-based visual navigation policy for underwater vehicles that uses knowledge-transferred depth features to enable end-to-end 4-DoF motion control in unknown environments, addressing the challenges of limited underwater sensing and dataset scarcity through a two-phase training approach.",
        "tldr_zh": "本文介绍了DUViN，一种基于扩散的水下视觉导航策略，它利用知识迁移的深度特征，在未知环境中实现水下车辆的端到端4自由度运动控制。该方法通过两阶段训练方法，解决了水下感知能力有限和数据集稀缺的挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "KEPT: Knowledge-Enhanced Prediction of Trajectories from Consecutive Driving Frames with Vision-Language Models",
        "summary": "Accurate short-horizon trajectory prediction is pivotal for safe and reliable\nautonomous driving, yet existing vision-language models (VLMs) often fail to\neffectively ground their reasoning in scene dynamics and domain knowledge. To\naddress this challenge, this paper introduces KEPT, a knowledge-enhanced VLM\nframework that predicts ego trajectories directly from consecutive front-view\ndriving frames. KEPT couples a temporal frequency-spatial fusion (TFSF) video\nencoder, trained via self-supervised learning with hard-negative mining, with a\nscalable k-means + HNSW retrieval stack that supplies scene-aligned exemplars.\nRetrieved priors are embedded into chain-of-thought (CoT) prompts with explicit\nplanning constraints, while a triple-stage fine-tuning schedule incrementally\naligns the language head to metric spatial cues, physically feasible motion,\nand temporally conditioned front-view planning. Evaluated on nuScenes dataset,\nKEPT achieves state-of-the-art performance across open-loop protocols: under\nNoAvg, it achieves 0.70m average L2 with a 0.21\\% collision rate; under TemAvg\nwith lightweight ego status, it attains 0.31m average L2 and a 0.07\\% collision\nrate. Ablation studies show that all three fine-tuning stages contribute\ncomplementary benefits, and that using Top-2 retrieved exemplars yields the\nbest accuracy-safety trade-off. The k-means-clustered HNSW index delivers\nsub-millisecond retrieval latency, supporting practical deployment. These\nresults indicate that retrieval-augmented, CoT-guided VLMs offer a promising,\ndata-efficient pathway toward interpretable and trustworthy autonomous driving.",
        "url": "http://arxiv.org/abs/2509.02966v1",
        "published_date": "2025-09-03T03:10:42+00:00",
        "updated_date": "2025-09-03T03:10:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yujin Wang",
            "Tianyi Wang",
            "Quanfeng Liu",
            "Wenxian Fan",
            "Junfeng Jiao",
            "Christian Claudel",
            "Yunbing Yan",
            "Bingzhao Gao",
            "Jianqiang Wang",
            "Hong Chen"
        ],
        "tldr": "KEPT is a knowledge-enhanced vision-language model (VLM) framework for predicting ego-trajectories from driving frames, using retrieved scene exemplars and chain-of-thought prompting to achieve state-of-the-art performance on nuScenes.",
        "tldr_zh": "KEPT是一个知识增强的视觉语言模型框架，用于从驾驶帧预测自我轨迹，它使用检索到的场景示例和思维链提示，在nuScenes数据集上实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "High-Fidelity Digital Twins for Bridging the Sim2Real Gap in LiDAR-Based ITS Perception",
        "summary": "Sim2Real domain transfer offers a cost-effective and scalable approach for\ndeveloping LiDAR-based perception (e.g., object detection, tracking,\nsegmentation) in Intelligent Transportation Systems (ITS). However, perception\nmodels trained in simulation often under perform on real-world data due to\ndistributional shifts. To address this Sim2Real gap, this paper proposes a\nhigh-fidelity digital twin (HiFi DT) framework that incorporates real-world\nbackground geometry, lane-level road topology, and sensor-specific\nspecifications and placement. We formalize the domain adaptation challenge\nunderlying Sim2Real learning and present a systematic method for constructing\nsimulation environments that yield in-domain synthetic data. An off-the-shelf\n3D object detector is trained on HiFi DT-generated synthetic data and evaluated\non real data. Our experiments show that the DT-trained model outperforms the\nequivalent model trained on real data by 4.8%. To understand this gain, we\nquantify distributional alignment between synthetic and real data using\nmultiple metrics, including Chamfer Distance (CD), Maximum Mean Discrepancy\n(MMD), Earth Mover's Distance (EMD), and Fr'echet Distance (FD), at both\nraw-input and latent-feature levels. Results demonstrate that HiFi DTs\nsubstantially reduce domain shift and improve generalization across diverse\nevaluation scenarios. These findings underscore the significant role of digital\ntwins in enabling reliable, simulation-based LiDAR perception for real-world\nITS applications.",
        "url": "http://arxiv.org/abs/2509.02904v1",
        "published_date": "2025-09-03T00:12:58+00:00",
        "updated_date": "2025-09-03T00:12:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Muhammad Shahbaz",
            "Shaurya Agarwal"
        ],
        "tldr": "This paper proposes a high-fidelity digital twin framework to bridge the Sim2Real gap for LiDAR-based perception in ITS, showing improved performance compared to training on real data by reducing domain shift.",
        "tldr_zh": "该论文提出了一个高保真数字孪生框架，用于弥合智能交通系统中基于激光雷达感知的Sim2Real差距，通过减少领域转移，展示了相比于在真实数据上训练的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PercepTwin: Modeling High-Fidelity Digital Twins for Sim2Real LiDAR-based Perception for Intelligent Transportation Systems",
        "summary": "LiDAR-based perception in intelligent transportation systems (ITS), for tasks\nsuch as object detection, tracking, and semantic and instance segmentation, is\npredominantly solved by deep neural network models which often require\nlarge-scale labeled datasets during training to achieve generalization.\nHowever, creating these datasets is costly. time consuming and require human\nlabor before the datasets are ready for training models. This hinders\nscalability of the LiDAR-based perception systems in ITS. Sim2Real learning\noffers scalable alternative, however, its effectiveness is dependent on the\nfidelity of the source simulation(s) to real-world, in terms of environment\nstructure, actor dynamics, and sensor emulations. In response, this paper\nintroduces a rigorous and reproducible methodology for creating large-scale,\nhigh-quality synthetic datasets using High-Fidelity Digital Twins (HiFi DTs).\nThe proposed workflow outlines the steps, tools, and best practices for\ndigitally replicating real-world environments, encompassing static geometry\nmodeling, road infrastructure replication, and dynamic traffic scenario\ngeneration. Leveraging open-source and readily available resources such as\nsatellite imagery and OpenStreetMap data, alongside specific sensor\nconfigurations, this paper provides practical, detailed guidance for\nconstructing robust synthetic environments. These environments subsequently\nfacilitate scalable, cost-effective, and diverse dataset generation, forming a\nreliable foundation for robust Sim2Real learning.",
        "url": "http://arxiv.org/abs/2509.02903v1",
        "published_date": "2025-09-03T00:12:15+00:00",
        "updated_date": "2025-09-03T00:12:15+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Muhammad Shahbaz",
            "Shaurya Agarwal"
        ],
        "tldr": "This paper introduces a methodology for creating high-fidelity digital twins to generate synthetic LiDAR datasets for Sim2Real learning in intelligent transportation systems, aiming to address the challenge of costly data annotation.",
        "tldr_zh": "本文介绍了一种创建高保真数字孪生的方法，用于为智能交通系统中的Sim2Real学习生成合成LiDAR数据集，旨在解决数据标注成本高昂的问题。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LiGuard: A Streamlined Open-Source Framework for Rapid & Interactive Lidar Research",
        "summary": "There is a growing interest in the development of lidar-based autonomous\nmobility and Intelligent Transportation Systems (ITS). To operate and research\non lidar data, researchers often develop code specific to application niche.\nThis approach leads to duplication of efforts across studies that, in many\ncases, share multiple methodological steps such as data input/output (I/O),\npre/post processing, and common algorithms in multi-stage solutions. Moreover,\nslight changes in data, algorithms, and/or research focus may force major\nrevisions in the code. To address these challenges, we present LiGuard, an\nopen-source software framework that allows researchers to: 1) rapidly develop\ncode for their lidar-based projects by providing built-in support for data I/O,\npre/post processing, and commonly used algorithms, 2) interactively\nadd/remove/reorder custom algorithms and adjust their parameters, and 3)\nvisualize results for classification, detection, segmentation, and tracking\ntasks. Moreover, because it creates all the code files in structured\ndirectories, it allows easy sharing of entire projects or even the individual\ncomponents to be reused by other researchers. The effectiveness of LiGuard is\ndemonstrated via case studies.",
        "url": "http://arxiv.org/abs/2509.02902v1",
        "published_date": "2025-09-03T00:10:53+00:00",
        "updated_date": "2025-09-03T00:10:53+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Muhammad Shahbaz",
            "Shaurya Agarwal"
        ],
        "tldr": "LiGuard is an open-source framework designed to streamline lidar-based research by providing built-in functionalities for data I/O, processing, and common algorithms, enabling rapid development, interactive modification, and result visualization.",
        "tldr_zh": "LiGuard是一个开源框架，旨在通过提供内置的数据输入/输出、处理和常用算法功能来简化基于激光雷达的研究，从而实现快速开发、交互式修改和结果可视化。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]