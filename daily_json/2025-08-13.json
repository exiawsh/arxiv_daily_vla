[
    {
        "title": "DiffPhysCam: Differentiable Physics-Based Camera Simulation for Inverse Rendering and Embodied AI",
        "summary": "We introduce DiffPhysCam, a differentiable camera simulator designed to\nsupport robotics and embodied AI applications by enabling gradient-based\noptimization in visual perception pipelines. Generating synthetic images that\nclosely mimic those from real cameras is essential for training visual models\nand enabling end-to-end visuomotor learning. Moreover, differentiable rendering\nallows inverse reconstruction of real-world scenes as digital twins,\nfacilitating simulation-based robotics training. However, existing virtual\ncameras offer limited control over intrinsic settings, poorly capture optical\nartifacts, and lack tunable calibration parameters -- hindering sim-to-real\ntransfer. DiffPhysCam addresses these limitations through a multi-stage\npipeline that provides fine-grained control over camera settings, models key\noptical effects such as defocus blur, and supports calibration with real-world\ndata. It enables both forward rendering for image synthesis and inverse\nrendering for 3D scene reconstruction, including mesh and material texture\noptimization. We show that DiffPhysCam enhances robotic perception performance\nin synthetic image tasks. As an illustrative example, we create a digital twin\nof a real-world scene using inverse rendering, simulate it in a multi-physics\nenvironment, and demonstrate navigation of an autonomous ground vehicle using\nimages generated by DiffPhysCam.",
        "url": "http://arxiv.org/abs/2508.08831v1",
        "published_date": "2025-08-12T10:38:20+00:00",
        "updated_date": "2025-08-12T10:38:20+00:00",
        "categories": [
            "cs.GR",
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Bo-Hsun Chen",
            "Nevindu M. Batagoda",
            "Dan Negrut"
        ],
        "tldr": "DiffPhysCam is a differentiable camera simulator with fine-grained control and realistic optical effects, enabling gradient-based optimization for inverse rendering and embodied AI, facilitating sim-to-real transfer.",
        "tldr_zh": "DiffPhysCam是一个可微相机模拟器，具有精细的控制和逼真的光学效果，支持基于梯度的优化，用于逆向渲染和具身人工智能，从而促进从仿真到现实的转移。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "VLM-3D:End-to-End Vision-Language Models for Open-World 3D Perception",
        "summary": "Open-set perception in complex traffic environments poses a critical\nchallenge for autonomous driving systems, particularly in identifying\npreviously unseen object categories, which is vital for ensuring safety. Visual\nLanguage Models (VLMs), with their rich world knowledge and strong semantic\nreasoning capabilities, offer new possibilities for addressing this task.\nHowever, existing approaches typically leverage VLMs to extract visual features\nand couple them with traditional object detectors, resulting in multi-stage\nerror propagation that hinders perception accuracy. To overcome this\nlimitation, we propose VLM-3D, the first end-to-end framework that enables VLMs\nto perform 3D geometric perception in autonomous driving scenarios. VLM-3D\nincorporates Low-Rank Adaptation (LoRA) to efficiently adapt VLMs to driving\ntasks with minimal computational overhead, and introduces a joint\nsemantic-geometric loss design: token-level semantic loss is applied during\nearly training to ensure stable convergence, while 3D IoU loss is introduced in\nlater stages to refine the accuracy of 3D bounding box predictions. Evaluations\non the nuScenes dataset demonstrate that the proposed joint semantic-geometric\nloss in VLM-3D leads to a 12.8% improvement in perception accuracy, fully\nvalidating the effectiveness and advancement of our method.",
        "url": "http://arxiv.org/abs/2508.09061v1",
        "published_date": "2025-08-12T16:25:27+00:00",
        "updated_date": "2025-08-12T16:25:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Fuhao Chang",
            "Shuxin Li",
            "Yabei Li",
            "Lei He"
        ],
        "tldr": "The paper introduces VLM-3D, an end-to-end framework that adapts Vision-Language Models (VLMs) for 3D object detection in autonomous driving, using a novel joint semantic-geometric loss for improved accuracy.",
        "tldr_zh": "该论文介绍了VLM-3D，一个端到端框架，它将视觉语言模型（VLM）用于自动驾驶中的3D物体检测，并使用一种新颖的联合语义-几何损失来提高准确性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding",
        "summary": "Vision-Language-Action models have demonstrated remarkable capabilities in\npredicting agent movements within virtual environments and real-world scenarios\nbased on visual observations and textual instructions. Although recent research\nhas focused on enhancing spatial and temporal understanding independently, this\npaper presents a novel approach that integrates both aspects through visual\nprompting. We introduce a method that projects visual traces of key points from\nobservations onto depth maps, enabling models to capture both spatial and\ntemporal information simultaneously. The experiments in SimplerEnv show that\nthe mean number of tasks successfully solved increased for 4% compared to\nSpatialVLA and 19% compared to TraceVLA. Furthermore, we show that this\nenhancement can be achieved with minimal training data, making it particularly\nvaluable for real-world applications where data collection is challenging. The\nproject page is available at https://ampiromax.github.io/ST-VLA.",
        "url": "http://arxiv.org/abs/2508.09032v1",
        "published_date": "2025-08-12T15:53:45+00:00",
        "updated_date": "2025-08-12T15:53:45+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Maxim A. Patratskiy",
            "Alexey K. Kovalev",
            "Aleksandr I. Panov"
        ],
        "tldr": "This paper introduces a novel visual prompting method called Spatial Traces that integrates spatial and temporal information for Vision-Language-Action models, achieving improved task completion with minimal training data.",
        "tldr_zh": "本文提出了一种名为Spatial Traces的新型视觉提示方法，该方法集成了视觉-语言-动作模型的空间和时间信息，以最少的训练数据实现了改进的任务完成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Pseudo Global Fusion Paradigm-Based Cross-View Network for LiDAR-Based Place Recognition",
        "summary": "LiDAR-based Place Recognition (LPR) remains a critical task in Embodied\nArtificial Intelligence (AI) and Autonomous Driving, primarily addressing\nlocalization challenges in GPS-denied environments and supporting loop closure\ndetection. Existing approaches reduce place recognition to a Euclidean\ndistance-based metric learning task, neglecting the feature space's intrinsic\nstructures and intra-class variances. Such Euclidean-centric formulation\ninherently limits the model's capacity to capture nonlinear data distributions,\nleading to suboptimal performance in complex environments and temporal-varying\nscenarios. To address these challenges, we propose a novel cross-view network\nbased on an innovative fusion paradigm. Our framework introduces a\npseudo-global information guidance mechanism that coordinates multi-modal\nbranches to perform feature learning within a unified semantic space.\nConcurrently, we propose a Manifold Adaptation and Pairwise Variance-Locality\nLearning Metric that constructs a Symmetric Positive Definite (SPD) matrix to\ncompute Mahalanobis distance, superseding traditional Euclidean distance\nmetrics. This geometric formulation enables the model to accurately\ncharacterize intrinsic data distributions and capture complex inter-class\ndependencies within the feature space. Experimental results demonstrate that\nthe proposed algorithm achieves competitive performance, particularly excelling\nin complex environmental conditions.",
        "url": "http://arxiv.org/abs/2508.08917v1",
        "published_date": "2025-08-12T13:12:48+00:00",
        "updated_date": "2025-08-12T13:12:48+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jintao Cheng",
            "Jiehao Luo",
            "Xieyuanli Chen",
            "Jin Wu",
            "Rui Fan",
            "Xiaoyu Tang",
            "Wei Zhang"
        ],
        "tldr": "This paper introduces a novel cross-view LiDAR-based place recognition network that uses a pseudo-global information fusion mechanism and a manifold-aware metric learning approach to improve performance in complex environments.",
        "tldr_zh": "本文介绍了一种新的基于激光雷达的跨视角地点识别网络，该网络采用伪全局信息融合机制和流形感知的度量学习方法，以提高在复杂环境中的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GaussianUpdate: Continual 3D Gaussian Splatting Update for Changing Environments",
        "summary": "Novel view synthesis with neural models has advanced rapidly in recent years,\nyet adapting these models to scene changes remains an open problem. Existing\nmethods are either labor-intensive, requiring extensive model retraining, or\nfail to capture detailed types of changes over time. In this paper, we present\nGaussianUpdate, a novel approach that combines 3D Gaussian representation with\ncontinual learning to address these challenges. Our method effectively updates\nthe Gaussian radiance fields with current data while preserving information\nfrom past scenes. Unlike existing methods, GaussianUpdate explicitly models\ndifferent types of changes through a novel multi-stage update strategy.\nAdditionally, we introduce a visibility-aware continual learning approach with\ngenerative replay, enabling self-aware updating without the need to store\nimages. The experiments on the benchmark dataset demonstrate our method\nachieves superior and real-time rendering with the capability of visualizing\nchanges over different times",
        "url": "http://arxiv.org/abs/2508.08867v1",
        "published_date": "2025-08-12T11:50:37+00:00",
        "updated_date": "2025-08-12T11:50:37+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lin Zeng",
            "Boming Zhao",
            "Jiarui Hu",
            "Xujie Shen",
            "Ziqiang Dang",
            "Hujun Bao",
            "Zhaopeng Cui"
        ],
        "tldr": "GaussianUpdate introduces a novel continual learning approach for 3D Gaussian Splatting, enabling real-time rendering of changing environments while preserving past information and explicitly modeling different change types.",
        "tldr_zh": "GaussianUpdate 提出了一种新颖的持续学习方法，用于 3D 高斯溅射，能够实时渲染不断变化的环境，同时保留过去的信息并明确地对不同类型的变化进行建模。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ROD: RGB-Only Fast and Efficient Off-road Freespace Detection",
        "summary": "Off-road freespace detection is more challenging than on-road scenarios\nbecause of the blurred boundaries of traversable areas. Previous\nstate-of-the-art (SOTA) methods employ multi-modal fusion of RGB images and\nLiDAR data. However, due to the significant increase in inference time when\ncalculating surface normal maps from LiDAR data, multi-modal methods are not\nsuitable for real-time applications, particularly in real-world scenarios where\nhigher FPS is required compared to slow navigation. This paper presents a novel\nRGB-only approach for off-road freespace detection, named ROD, eliminating the\nreliance on LiDAR data and its computational demands. Specifically, we utilize\na pre-trained Vision Transformer (ViT) to extract rich features from RGB\nimages. Additionally, we design a lightweight yet efficient decoder, which\ntogether improve both precision and inference speed. ROD establishes a new SOTA\non ORFD and RELLIS-3D datasets, as well as an inference speed of 50 FPS,\nsignificantly outperforming prior models.",
        "url": "http://arxiv.org/abs/2508.08697v1",
        "published_date": "2025-08-12T07:41:20+00:00",
        "updated_date": "2025-08-12T07:41:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tong Sun",
            "Hongliang Ye",
            "Jilin Mei",
            "Liang Chen",
            "Fangzhou Zhao",
            "Leiqiang Zong",
            "Yu Hu"
        ],
        "tldr": "The paper introduces ROD, a novel RGB-only approach for fast and efficient off-road freespace detection using a Vision Transformer and a lightweight decoder, achieving state-of-the-art results and high FPS on relevant datasets.",
        "tldr_zh": "该论文介绍了一种名为ROD的新型RGB图像方法，用于快速高效的越野自由空间检测，使用Vision Transformer和一个轻量级的解码器，在相关数据集上实现了最先进的结果和高FPS。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]