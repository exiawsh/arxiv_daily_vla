[
    {
        "title": "Simulating the Visual World with Artificial Intelligence: A Roadmap",
        "summary": "The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a \"window\" into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.",
        "url": "http://arxiv.org/abs/2511.08585v1",
        "published_date": "2025-11-11T18:59:50+00:00",
        "updated_date": "2025-11-12T02:05:57+00:00",
        "categories": [
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Jingtong Yue",
            "Ziqi Huang",
            "Zhaoxi Chen",
            "Xintao Wang",
            "Pengfei Wan",
            "Ziwei Liu"
        ],
        "tldr": "This paper surveys the evolution of video generation towards video foundation models that act as implicit world models, simulating physical dynamics and agent interactions, with applications in robotics and autonomous driving.",
        "tldr_zh": "本文综述了视频生成向视频基础模型演进的过程，这些模型充当隐式世界模型，模拟物理动力学和智能体交互，并在机器人和自动驾驶领域有应用。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "SMF-VO: Direct Ego-Motion Estimation via Sparse Motion Fields",
        "summary": "Traditional Visual Odometry (VO) and Visual Inertial Odometry (VIO) methods rely on a 'pose-centric' paradigm, which computes absolute camera poses from the local map thus requires large-scale landmark maintenance and continuous map optimization. This approach is computationally expensive, limiting their real-time performance on resource-constrained devices. To overcome these limitations, we introduce Sparse Motion Field Visual Odometry (SMF-VO), a lightweight, 'motion-centric' framework. Our approach directly estimates instantaneous linear and angular velocity from sparse optical flow, bypassing the need for explicit pose estimation or expensive landmark tracking. We also employed a generalized 3D ray-based motion field formulation that works accurately with various camera models, including wide-field-of-view lenses. SMF-VO demonstrates superior efficiency and competitive accuracy on benchmark datasets, achieving over 100 FPS on a Raspberry Pi 5 using only a CPU. Our work establishes a scalable and efficient alternative to conventional methods, making it highly suitable for mobile robotics and wearable devices.",
        "url": "http://arxiv.org/abs/2511.09072v1",
        "published_date": "2025-11-12T07:47:22+00:00",
        "updated_date": "2025-11-13T01:30:23+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Sangheon Yang",
            "Yeongin Yoon",
            "Hong Mo Jung",
            "Jongwoo Lim"
        ],
        "tldr": "This paper presents SMF-VO, a computationally efficient 'motion-centric' visual odometry framework that directly estimates velocity from sparse optical flow, achieving real-time performance on resource-constrained devices.",
        "tldr_zh": "该论文提出了SMF-VO，一个计算高效的“运动中心”视觉里程计框架，它直接从稀疏光流估计速度，在资源受限的设备上实现实时性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UniMM-V2X: MoE-Enhanced Multi-Level Fusion for End-to-End Cooperative Autonomous Driving",
        "summary": "Autonomous driving holds transformative potential but remains fundamentally constrained by the limited perception and isolated decision-making with standalone intelligence. While recent multi-agent approaches introduce cooperation, they often focus merely on perception-level tasks, overlooking the alignment with downstream planning and control, or fall short in leveraging the full capacity of the recent emerging end-to-end autonomous driving. In this paper, we present UniMM-V2X, a novel end-to-end multi-agent framework that enables hierarchical cooperation across perception, prediction, and planning. At the core of our framework is a multi-level fusion strategy that unifies perception and prediction cooperation, allowing agents to share queries and reason cooperatively for consistent and safe decision-making. To adapt to diverse downstream tasks and further enhance the quality of multi-level fusion, we incorporate a Mixture-of-Experts (MoE) architecture to dynamically enhance the BEV representations. We further extend MoE into the decoder to better capture diverse motion patterns. Extensive experiments on the DAIR-V2X dataset demonstrate our approach achieves state-of-the-art (SOTA) performance with a 39.7% improvement in perception accuracy, a 7.2% reduction in prediction error, and a 33.2% improvement in planning performance compared with UniV2X, showcasing the strength of our MoE-enhanced multi-level cooperative paradigm.",
        "url": "http://arxiv.org/abs/2511.09013v1",
        "published_date": "2025-11-12T06:02:41+00:00",
        "updated_date": "2025-11-13T01:26:05+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Ziyi Song",
            "Chen Xia",
            "Chenbing Wang",
            "Haibao Yu",
            "Sheng Zhou",
            "Zhisheng Niu"
        ],
        "tldr": "UniMM-V2X introduces a novel end-to-end multi-agent autonomous driving framework with a multi-level fusion strategy enhanced by a Mixture-of-Experts architecture, achieving SOTA performance on the DAIR-V2X dataset across perception, prediction, and planning.",
        "tldr_zh": "UniMM-V2X 提出了一种新颖的端到端多智能体自动驾驶框架，采用多层次融合策略，并利用混合专家（MoE）架构进行增强，在 DAIR-V2X 数据集上实现了最先进的感知、预测和规划性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation",
        "summary": "Pre-trained Vision-Language-Action (VLA) models have achieved remarkable success in improving robustness and generalization for end-to-end robotic manipulation. However, these models struggle with long-horizon tasks due to their lack of memory and reliance solely on immediate sensory inputs. To address this limitation, we propose Memory-Augmented Prompting for Vision-Language-Action model (MAP-VLA), a novel framework that empowers pre-trained VLA models with demonstration-derived memory prompts to augment action generation for long-horizon robotic manipulation tasks. To achieve this, MAP-VLA first constructs a memory library from historical demonstrations, where each memory unit captures information about a specific stage of a task. These memory units are implemented as learnable soft prompts optimized through prompt tuning. Then, during real-time task execution, MAP-VLA retrieves relevant memory through trajectory similarity matching and dynamically integrates it into the VLA model for augmented action generation. Importantly, this prompt tuning and retrieval augmentation approach operates as a plug-and-play module for a frozen VLA model, offering a lightweight and flexible solution to improve task performance. Experimental results show that MAP-VLA delivers up to 7.0% absolute performance gains in the simulation benchmark and 25.0% on real robot evaluations for long-horizon tasks, surpassing the current state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2511.09516v1",
        "published_date": "2025-11-12T17:56:13+00:00",
        "updated_date": "2025-11-13T01:59:20+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Runhao Li",
            "Wenkai Guo",
            "Zhenyu Wu",
            "Changyuan Wang",
            "Haoyuan Deng",
            "Zhenyu Weng",
            "Yap-Peng Tan",
            "Ziwei Wang"
        ],
        "tldr": "The paper introduces MAP-VLA, a memory-augmented prompting framework that enhances pre-trained VLA models for long-horizon robotic manipulation by leveraging demonstration-derived memory prompts, achieving significant performance gains in both simulation and real-world experiments.",
        "tldr_zh": "该论文提出了MAP-VLA，一种记忆增强的提示框架，通过利用从演示中获得的记忆提示来增强预训练的VLA模型，从而改进长时程机器人操作，并在仿真和真实实验中取得了显著的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RePose-NeRF: Robust Radiance Fields for Mesh Reconstruction under Noisy Camera Poses",
        "summary": "Accurate 3D reconstruction from multi-view images is essential for downstream robotic tasks such as navigation, manipulation, and environment understanding. However, obtaining precise camera poses in real-world settings remains challenging, even when calibration parameters are known. This limits the practicality of existing NeRF-based methods that rely heavily on accurate extrinsic estimates. Furthermore, their implicit volumetric representations differ significantly from the widely adopted polygonal meshes, making rendering and manipulation inefficient in standard 3D software. In this work, we propose a robust framework that reconstructs high-quality, editable 3D meshes directly from multi-view images with noisy extrinsic parameters. Our approach jointly refines camera poses while learning an implicit scene representation that captures fine geometric detail and photorealistic appearance. The resulting meshes are compatible with common 3D graphics and robotics tools, enabling efficient downstream use. Experiments on standard benchmarks demonstrate that our method achieves accurate and robust 3D reconstruction under pose uncertainty, bridging the gap between neural implicit representations and practical robotic applications.",
        "url": "http://arxiv.org/abs/2511.08545v1",
        "published_date": "2025-11-11T18:25:58+00:00",
        "updated_date": "2025-11-12T02:04:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sriram Srinivasan",
            "Gautam Ramachandra"
        ],
        "tldr": "RePose-NeRF addresses the problem of 3D mesh reconstruction from multi-view images with noisy camera poses using a robust framework that jointly refines camera poses and learns an implicit scene representation, resulting in editable meshes suitable for robotic applications.",
        "tldr_zh": "RePose-NeRF 解决使用嘈杂相机姿态从多视角图像进行 3D 网格重建的问题，它使用一个稳健的框架，该框架联合细化相机姿态并学习隐式场景表示，从而生成适用于机器人应用的可编辑网格。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Expand Your SCOPE: Semantic Cognition over Potential-Based Exploration for Embodied Visual Navigation",
        "summary": "Embodied visual navigation remains a challenging task, as agents must explore unknown environments with limited knowledge. Existing zero-shot studies have shown that incorporating memory mechanisms to support goal-directed behavior can improve long-horizon planning performance. However, they overlook visual frontier boundaries, which fundamentally dictate future trajectories and observations, and fall short of inferring the relationship between partial visual observations and navigation goals. In this paper, we propose Semantic Cognition Over Potential-based Exploration (SCOPE), a zero-shot framework that explicitly leverages frontier information to drive potential-based exploration, enabling more informed and goal-relevant decisions. SCOPE estimates exploration potential with a Vision-Language Model and organizes it into a spatio-temporal potential graph, capturing boundary dynamics to support long-horizon planning. In addition, SCOPE incorporates a self-reconsideration mechanism that revisits and refines prior decisions, enhancing reliability and reducing overconfident errors. Experimental results on two diverse embodied navigation tasks show that SCOPE outperforms state-of-the-art baselines by 4.6\\% in accuracy. Further analysis demonstrates that its core components lead to improved calibration, stronger generalization, and higher decision quality.",
        "url": "http://arxiv.org/abs/2511.08935v1",
        "published_date": "2025-11-12T03:23:09+00:00",
        "updated_date": "2025-11-13T01:19:30+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Ningnan Wang",
            "Weihuang Chen",
            "Liming Chen",
            "Haoxuan Ji",
            "Zhongyu Guo",
            "Xuchong Zhang",
            "Hongbin Sun"
        ],
        "tldr": "The paper introduces SCOPE, a zero-shot embodied visual navigation framework that leverages frontier information and a spatio-temporal potential graph for improved exploration and goal-relevant decision-making, outperforming state-of-the-art baselines.",
        "tldr_zh": "该论文介绍了SCOPE，一个零样本具身视觉导航框架，利用边界信息和时空潜在图，以改进探索和目标相关的决策，性能优于最先进的基线。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "HOTFLoc++: End-to-End Hierarchical LiDAR Place Recognition, Re-Ranking, and 6-DoF Metric Localisation in Forests",
        "summary": "This article presents HOTFLoc++, an end-to-end framework for LiDAR place recognition, re-ranking, and 6-DoF metric localisation in forests. Leveraging an octree-based transformer, our approach extracts hierarchical local descriptors at multiple granularities to increase robustness to clutter, self-similarity, and viewpoint changes in challenging scenarios, including ground-to-ground and ground-to-aerial in forest and urban environments. We propose a learnable multi-scale geometric verification module to reduce re-ranking failures in the presence of degraded single-scale correspondences. Our coarse-to-fine registration approach achieves comparable or lower localisation errors to baselines, with runtime improvements of two orders of magnitude over RANSAC for dense point clouds. Experimental results on public datasets show the superiority of our approach compared to state-of-the-art methods, achieving an average Recall@1 of 90.7% on CS-Wild-Places: an improvement of 29.6 percentage points over baselines, while maintaining high performance on single-source benchmarks with an average Recall@1 of 91.7% and 96.0% on Wild-Places and MulRan, respectively. Our method achieves under 2 m and 5 degrees error for 97.2% of 6-DoF registration attempts, with our multi-scale re-ranking module reducing localisation errors by ~2$\\times$ on average. The code will be available upon acceptance.",
        "url": "http://arxiv.org/abs/2511.09170v1",
        "published_date": "2025-11-12T10:10:23+00:00",
        "updated_date": "2025-11-13T01:37:19+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Ethan Griffiths",
            "Maryam Haghighat",
            "Simon Denman",
            "Clinton Fookes",
            "Milad Ramezani"
        ],
        "tldr": "The paper introduces HOTFLoc++, an end-to-end LiDAR place recognition and 6-DoF localization framework using a hierarchical octree-based transformer and multi-scale geometric verification, achieving state-of-the-art performance, especially in challenging forest environments.",
        "tldr_zh": "该论文介绍了HOTFLoc++，一个端到端的激光雷达地点识别和6自由度定位框架，它使用分层八叉树 Transformer 和多尺度几何验证，实现了最先进的性能，尤其是在具有挑战性的森林环境中。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FQ-PETR: Fully Quantized Position Embedding Transformation for Multi-View 3D Object Detection",
        "summary": "Camera-based multi-view 3D detection is crucial for autonomous driving. PETR and its variants (PETRs) excel in benchmarks but face deployment challenges due to high computational cost and memory footprint. Quantization is an effective technique for compressing deep neural networks by reducing the bit width of weights and activations. However, directly applying existing quantization methods to PETRs leads to severe accuracy degradation. This issue primarily arises from two key challenges: (1) significant magnitude disparity between multi-modal features-specifically, image features and camera-ray positional embeddings (PE), and (2) the inefficiency and approximation error of quantizing non-linear operators, which commonly rely on hardware-unfriendly computations. In this paper, we propose FQ-PETR, a fully quantized framework for PETRs, featuring three key innovations: (1) Quantization-Friendly LiDAR-ray Position Embedding (QFPE): Replacing multi-point sampling with LiDAR-prior-guided single-point sampling and anchor-based embedding eliminates problematic non-linearities (e.g., inverse-sigmoid) and aligns PE scale with image features, preserving accuracy. (2) Dual-Lookup Table (DULUT): This algorithm approximates complex non-linear functions using two cascaded linear LUTs, achieving high fidelity with minimal entries and no specialized hardware. (3) Quantization After Numerical Stabilization (QANS): Performing quantization after softmax numerical stabilization mitigates attention distortion from large inputs. On PETRs (e.g. PETR, StreamPETR, PETRv2, MV2d), FQ-PETR under W8A8 achieves near-floating-point accuracy (1% degradation) while reducing latency by up to 75%, significantly outperforming existing PTQ and QAT baselines.",
        "url": "http://arxiv.org/abs/2511.09347v1",
        "published_date": "2025-11-12T13:59:28+00:00",
        "updated_date": "2025-11-13T01:47:57+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiangyong Yu",
            "Changyong Shu",
            "Sifan Zhou",
            "Zichen Yu",
            "Xing Hu",
            "Yan Chen",
            "Dawei Yang"
        ],
        "tldr": "This paper introduces FQ-PETR, a fully quantized framework for multi-view 3D object detection using camera data, which addresses the accuracy degradation issues when directly quantizing PETR-based models by incorporating quantization-friendly position embeddings, dual lookup tables, and quantization after numerical stabilization, achieving near-floating-point accuracy with significant latency reduction.",
        "tldr_zh": "本文介绍了FQ-PETR，一个用于多视图3D目标检测的全量化框架，通过结合量化友好的位置嵌入、双查找表和数值稳定后的量化，解决了直接量化基于PETR模型的精度下降问题，实现了接近浮点精度的同时显著降低了延迟。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]