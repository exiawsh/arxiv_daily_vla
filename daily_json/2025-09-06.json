[
    {
        "title": "WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool",
        "summary": "We present WinT3R, a feed-forward reconstruction model capable of online\nprediction of precise camera poses and high-quality point maps. Previous\nmethods suffer from a trade-off between reconstruction quality and real-time\nperformance. To address this, we first introduce a sliding window mechanism\nthat ensures sufficient information exchange among frames within the window,\nthereby improving the quality of geometric predictions without large\ncomputation. In addition, we leverage a compact representation of cameras and\nmaintain a global camera token pool, which enhances the reliability of camera\npose estimation without sacrificing efficiency. These designs enable WinT3R to\nachieve state-of-the-art performance in terms of online reconstruction quality,\ncamera pose estimation, and reconstruction speed, as validated by extensive\nexperiments on diverse datasets. Code and model are publicly available at\nhttps://github.com/LiZizun/WinT3R.",
        "url": "http://arxiv.org/abs/2509.05296v1",
        "published_date": "2025-09-05T17:59:47+00:00",
        "updated_date": "2025-09-05T17:59:47+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Zizun Li",
            "Jianjun Zhou",
            "Yifan Wang",
            "Haoyu Guo",
            "Wenzheng Chang",
            "Yang Zhou",
            "Haoyi Zhu",
            "Junyi Chen",
            "Chunhua Shen",
            "Tong He"
        ],
        "tldr": "WinT3R is a novel feed-forward reconstruction model utilizing a sliding window and camera token pool for real-time, high-quality camera pose estimation and point map reconstruction, achieving state-of-the-art performance.",
        "tldr_zh": "WinT3R是一种新颖的前馈重建模型，它利用滑动窗口和相机令牌池来实现实时的、高质量的相机姿态估计和点云地图重建，并实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Robust Model Predictive Control Design for Autonomous Vehicles with Perception-based Observers",
        "summary": "This paper presents a robust model predictive control (MPC) framework that\nexplicitly addresses the non-Gaussian noise inherent in deep learning-based\nperception modules used for state estimation. Recognizing that accurate\nuncertainty quantification of the perception module is essential for safe\nfeedback control, our approach departs from the conventional assumption of\nzero-mean noise quantification of the perception error. Instead, it employs\nset-based state estimation with constrained zonotopes to capture biased,\nheavy-tailed uncertainties while maintaining bounded estimation errors. To\nimprove computational efficiency, the robust MPC is reformulated as a linear\nprogram (LP), using a Minkowski-Lyapunov-based cost function with an added\nslack variable to prevent degenerate solutions. Closed-loop stability is\nensured through Minkowski-Lyapunov inequalities and contractive zonotopic\ninvariant sets. The largest stabilizing terminal set and its corresponding\nfeedback gain are then derived via an ellipsoidal approximation of the\nzonotopes. The proposed framework is validated through both simulations and\nhardware experiments on an omnidirectional mobile robot along with a camera and\na convolutional neural network-based perception module implemented within a\nROS2 framework. The results demonstrate that the perception-aware MPC provides\nstable and accurate control performance under heavy-tailed noise conditions,\nsignificantly outperforming traditional Gaussian-noise-based designs in terms\nof both state estimation error bounding and overall control performance.",
        "url": "http://arxiv.org/abs/2509.05201v1",
        "published_date": "2025-09-05T16:03:57+00:00",
        "updated_date": "2025-09-05T16:03:57+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG",
            "cs.SY",
            "eess.SY"
        ],
        "authors": [
            "Nariman Niknejad",
            "Gokul S. Sankar",
            "Bahare Kiumarsi",
            "Hamidreza Modares"
        ],
        "tldr": "The paper introduces a robust MPC framework for autonomous vehicles that addresses non-Gaussian noise in perception modules by using set-based state estimation with constrained zonotopes and a Minkowski-Lyapunov-based cost function, validated through simulations and hardware experiments.",
        "tldr_zh": "本文提出了一种用于自动驾驶汽车的鲁棒MPC框架，通过使用基于集合的状态估计（带约束的zonotope）和基于Minkowski-Lyapunov的代价函数来解决感知模块中的非高斯噪声，并通过仿真和硬件实验进行了验证。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet",
        "summary": "The classification of 3D point clouds is crucial for applications such as\nautonomous driving, robotics, and augmented reality. However, the commonly used\nModelNet40 dataset suffers from limitations such as inconsistent labeling, 2D\ndata, size mismatches, and inadequate class differentiation, which hinder model\nperformance. This paper introduces ModelNet-R, a meticulously refined version\nof ModelNet40 designed to address these issues and serve as a more reliable\nbenchmark. Additionally, this paper proposes Point-SkipNet, a lightweight\ngraph-based neural network that leverages efficient sampling, neighborhood\ngrouping, and skip connections to achieve high classification accuracy with\nreduced computational overhead. Extensive experiments demonstrate that models\ntrained in ModelNet-R exhibit significant performance improvements. Notably,\nPoint-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a\nsubstantially lower parameter count compared to contemporary models. This\nresearch highlights the crucial role of dataset quality in optimizing model\nefficiency for 3D point cloud classification. For more details, see the code\nat: https://github.com/m-saeid/ModeNetR_PointSkipNet.",
        "url": "http://arxiv.org/abs/2509.05198v1",
        "published_date": "2025-09-05T15:57:36+00:00",
        "updated_date": "2025-09-05T15:57:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Mohammad Saeid",
            "Amir Salarpour",
            "Pedram MohajerAnsari"
        ],
        "tldr": "This paper introduces ModelNet-R, a refined dataset for 3D point cloud classification, and Point-SkipNet, a lightweight graph neural network that achieves state-of-the-art accuracy on the new dataset with fewer parameters.",
        "tldr_zh": "本文介绍了ModelNet-R，一个用于3D点云分类的精炼数据集，以及Point-SkipNet，一个轻量级的图神经网络，它以更少的参数在新数据集上实现了最先进的精度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing",
        "summary": "Accurate 3D instance segmentation is crucial for high-quality scene\nunderstanding in the 3D vision domain. However, 3D instance segmentation based\non 2D-to-3D lifting approaches struggle to produce precise instance-level\nsegmentation, due to accumulated errors introduced during the lifting process\nfrom ambiguous semantic guidance and insufficient depth constraints. To tackle\nthese challenges, we propose splitting and growing reliable semantic mask for\nhigh-fidelity 3D instance segmentation (SGS-3D), a novel \"split-then-grow\"\nframework that first purifies and splits ambiguous lifted masks using geometric\nprimitives, and then grows them into complete instances within the scene.\nUnlike existing approaches that directly rely on raw lifted masks and sacrifice\nsegmentation accuracy, SGS-3D serves as a training-free refinement method that\njointly fuses semantic and geometric information, enabling effective\ncooperation between the two levels of representation. Specifically, for\nsemantic guidance, we introduce a mask filtering strategy that leverages the\nco-occurrence of 3D geometry primitives to identify and remove ambiguous masks,\nthereby ensuring more reliable semantic consistency with the 3D object\ninstances. For the geometric refinement, we construct fine-grained object\ninstances by exploiting both spatial continuity and high-level features,\nparticularly in the case of semantic ambiguity between distinct objects.\nExperimental results on ScanNet200, ScanNet++, and KITTI-360 demonstrate that\nSGS-3D substantially improves segmentation accuracy and robustness against\ninaccurate masks from pre-trained models, yielding high-fidelity object\ninstances while maintaining strong generalization across diverse indoor and\noutdoor environments. Code is available in the supplementary materials.",
        "url": "http://arxiv.org/abs/2509.05144v1",
        "published_date": "2025-09-05T14:37:31+00:00",
        "updated_date": "2025-09-05T14:37:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chaolei Wang",
            "Yang Luo",
            "Jing Du",
            "Siyu Chen",
            "Yiping Chen",
            "Ting Han"
        ],
        "tldr": "The paper introduces SGS-3D, a novel \"split-then-grow\" framework for high-fidelity 3D instance segmentation that refines lifted masks using geometric primitives and semantic information, achieving improved accuracy and robustness across diverse datasets.",
        "tldr_zh": "本文提出了一种名为SGS-3D的新颖的“分割-然后-生长”框架，用于高保真3D实例分割。该框架利用几何基元和语义信息来优化提升的掩码，从而在各种数据集中实现了更高的精度和鲁棒性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CoRe-GS: Coarse-to-Refined Gaussian Splatting with Semantic Object Focus",
        "summary": "Mobile reconstruction for autonomous aerial robotics holds strong potential\nfor critical applications such as tele-guidance and disaster response. These\ntasks demand both accurate 3D reconstruction and fast scene processing. Instead\nof reconstructing the entire scene in detail, it is often more efficient to\nfocus on specific objects, i.e., points of interest (PoIs). Mobile robots\nequipped with advanced sensing can usually detect these early during data\nacquisition or preliminary analysis, reducing the need for full-scene\noptimization. Gaussian Splatting (GS) has recently shown promise in delivering\nhigh-quality novel view synthesis and 3D representation by an incremental\nlearning process. Extending GS with scene editing, semantics adds useful\nper-splat features to isolate objects effectively.\n  Semantic 3D Gaussian editing can already be achieved before the full training\ncycle is completed, reducing the overall training time. Moreover, the\nsemantically relevant area, the PoI, is usually already known during capturing.\nTo balance high-quality reconstruction with reduced training time, we propose\nCoRe-GS. We first generate a coarse segmentation-ready scene with semantic GS\nand then refine it for the semantic object using our novel color-based\neffective filtering for effective object isolation. This is speeding up the\ntraining process to be about a quarter less than a full training cycle for\nsemantic GS. We evaluate our approach on two datasets, SCRREAM (real-world,\noutdoor) and NeRDS 360 (synthetic, indoor), showing reduced runtime and higher\nnovel-view-synthesis quality.",
        "url": "http://arxiv.org/abs/2509.04859v1",
        "published_date": "2025-09-05T07:21:26+00:00",
        "updated_date": "2025-09-05T07:21:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hannah Schieber",
            "Dominik Frischmann",
            "Simon Boche",
            "Victor Schaack",
            "Angela Schoellig",
            "Stefan Leutenegger",
            "Daniel Roth"
        ],
        "tldr": "CoRe-GS accelerates semantic Gaussian Splatting for mobile robot reconstruction by focusing refinement on objects of interest, achieving faster training and higher quality novel view synthesis.",
        "tldr_zh": "CoRe-GS通过聚焦于感兴趣物体的细化，加速了移动机器人重建的语义高斯溅射过程，实现了更快的训练和更高质量的新视角合成。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhancing Self-Driving Segmentation in Adverse Weather Conditions: A Dual Uncertainty-Aware Training Approach to SAM Optimization",
        "summary": "Recent advances in vision foundation models, such as the Segment Anything\nModel (SAM) and its successor SAM2, have achieved state-of-the-art performance\non general image segmentation benchmarks. However, these models struggle in\nadverse weather conditions where visual ambiguity is high, largely due to their\nlack of uncertainty quantification. Inspired by progress in medical imaging,\nwhere uncertainty-aware training has improved reliability in ambiguous cases,\nwe investigate two approaches to enhance segmentation robustness for autonomous\ndriving. First, we introduce a multi-step finetuning procedure for SAM2 that\nincorporates uncertainty metrics directly into the loss function, improving\noverall scene recognition. Second, we adapt the Uncertainty-Aware Adapter\n(UAT), originally designed for medical image segmentation, to driving contexts.\nWe evaluate both methods on CamVid, BDD100K, and GTA driving datasets.\nExperiments show that UAT-SAM outperforms standard SAM in extreme weather,\nwhile SAM2 with uncertainty-aware loss achieves improved performance across\ndiverse driving scenes. These findings underscore the value of explicit\nuncertainty modeling for safety-critical autonomous driving in challenging\nenvironments.",
        "url": "http://arxiv.org/abs/2509.04735v1",
        "published_date": "2025-09-05T01:24:42+00:00",
        "updated_date": "2025-09-05T01:24:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Dharsan Ravindran",
            "Kevin Wang",
            "Zhuoyuan Cao",
            "Saleh Abdelrahman",
            "Jeffery Wu"
        ],
        "tldr": "This paper explores uncertainty-aware training methods to improve SAM and SAM2 segmentation performance in adverse weather conditions for autonomous driving, demonstrating promising results on standard driving datasets.",
        "tldr_zh": "本文探索了不确定性感知训练方法，以提高SAM和SAM2在恶劣天气条件下自动驾驶的分割性能，并在标准驾驶数据集上展示了有希望的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Domain Adaptation for Different Sensor Configurations in 3D Object Detection",
        "summary": "Recent advances in autonomous driving have underscored the importance of\naccurate 3D object detection, with LiDAR playing a central role due to its\nrobustness under diverse visibility conditions. However, different vehicle\nplatforms often deploy distinct sensor configurations, causing performance\ndegradation when models trained on one configuration are applied to another\nbecause of shifts in the point cloud distribution. Prior work on multi-dataset\ntraining and domain adaptation for 3D object detection has largely addressed\nenvironmental domain gaps and density variation within a single LiDAR; in\ncontrast, the domain gap for different sensor configurations remains largely\nunexplored. In this work, we address domain adaptation across different sensor\nconfigurations in 3D object detection. We propose two techniques: Downstream\nFine-tuning (dataset-specific fine-tuning after multi-dataset training) and\nPartial Layer Fine-tuning (updating only a subset of layers to improve\ncross-configuration generalization). Using paired datasets collected in the\nsame geographic region with multiple sensor configurations, we show that joint\ntraining with Downstream Fine-tuning and Partial Layer Fine-tuning consistently\noutperforms naive joint training for each configuration. Our findings provide a\npractical and scalable solution for adapting 3D object detection models to the\ndiverse vehicle platforms.",
        "url": "http://arxiv.org/abs/2509.04711v1",
        "published_date": "2025-09-04T23:54:25+00:00",
        "updated_date": "2025-09-04T23:54:25+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Satoshi Tanaka",
            "Kok Seang Tan",
            "Isamu Yamashita"
        ],
        "tldr": "This paper addresses the problem of domain adaptation in 3D object detection caused by different LiDAR sensor configurations on autonomous vehicles, proposing downstream fine-tuning and partial layer fine-tuning to improve cross-configuration generalization.",
        "tldr_zh": "本文解决了自动驾驶中因不同激光雷达传感器配置导致的3D目标检测领域自适应问题，提出了下游微调和部分层微调方法，以提高跨配置的泛化能力。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards an Accurate and Effective Robot Vision (The Problem of Topological Localization for Mobile Robots)",
        "summary": "Topological localization is a fundamental problem in mobile robotics, since\nrobots must be able to determine their position in order to accomplish tasks.\nVisual localization and place recognition are challenging due to perceptual\nambiguity, sensor noise, and illumination variations. This work addresses\ntopological localization in an office environment using only images acquired\nwith a perspective color camera mounted on a robot platform, without relying on\ntemporal continuity of image sequences. We evaluate state-of-the-art visual\ndescriptors, including Color Histograms, SIFT, ASIFT, RGB-SIFT, and\nBag-of-Visual-Words approaches inspired by text retrieval. Our contributions\ninclude a systematic, quantitative comparison of these features, distance\nmeasures, and classifiers. Performance was analyzed using standard evaluation\nmetrics and visualizations, extending previous experiments. Results demonstrate\nthe advantages of proper configurations of appearance descriptors, similarity\nmeasures, and classifiers. The quality of these configurations was further\nvalidated in the Robot Vision task of the ImageCLEF evaluation campaign, where\nthe system identified the most likely location of novel image sequences. Future\nwork will explore hierarchical models, ranking methods, and feature\ncombinations to build more robust localization systems, reducing training and\nruntime while avoiding the curse of dimensionality. Ultimately, this aims\ntoward integrated, real-time localization across varied illumination and longer\nroutes.",
        "url": "http://arxiv.org/abs/2509.04948v1",
        "published_date": "2025-09-05T09:14:59+00:00",
        "updated_date": "2025-09-05T09:14:59+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Emanuela Boros"
        ],
        "tldr": "This paper presents a comparative analysis of various visual descriptors, distance measures, and classifiers for topological localization of mobile robots using a single color camera in an office environment, aiming to improve robustness and efficiency.",
        "tldr_zh": "本文对移动机器人在办公环境中，使用单个彩色相机进行拓扑定位的各种视觉描述符、距离度量和分类器进行了比较分析，旨在提高鲁棒性和效率。",
        "relevance_score": 7,
        "novelty_claim_score": 5,
        "clarity_score": 8,
        "potential_impact_score": 6,
        "overall_priority_score": 6
    }
]