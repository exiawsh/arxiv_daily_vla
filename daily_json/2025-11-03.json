[
    {
        "title": "Fast-SmartWay: Panoramic-Free End-to-End Zero-Shot Vision-and-Language Navigation",
        "summary": "Recent advances in Vision-and-Language Navigation in Continuous Environments\n(VLN-CE) have leveraged multimodal large language models (MLLMs) to achieve\nzero-shot navigation. However, existing methods often rely on panoramic\nobservations and two-stage pipelines involving waypoint predictors, which\nintroduce significant latency and limit real-world applicability. In this work,\nwe propose Fast-SmartWay, an end-to-end zero-shot VLN-CE framework that\neliminates the need for panoramic views and waypoint predictors. Our approach\nuses only three frontal RGB-D images combined with natural language\ninstructions, enabling MLLMs to directly predict actions. To enhance decision\nrobustness, we introduce an Uncertainty-Aware Reasoning module that integrates\n(i) a Disambiguation Module for avoiding local optima, and (ii) a Future-Past\nBidirectional Reasoning mechanism for globally coherent planning. Experiments\non both simulated and real-robot environments demonstrate that our method\nsignificantly reduces per-step latency while achieving competitive or superior\nperformance compared to panoramic-view baselines. These results demonstrate the\npracticality and effectiveness of Fast-SmartWay for real-world zero-shot\nembodied navigation.",
        "url": "http://arxiv.org/abs/2511.00933v1",
        "published_date": "2025-11-02T13:21:54+00:00",
        "updated_date": "2025-11-02T13:21:54+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Xiangyu Shi",
            "Zerui Li",
            "Yanyuan Qiao",
            "Qi Wu"
        ],
        "tldr": "The paper introduces Fast-SmartWay, an end-to-end zero-shot Vision-and-Language Navigation framework that uses only frontal RGB-D images, eliminating panoramic views and waypoint predictors for faster and more robust navigation in real-world environments.",
        "tldr_zh": "该论文介绍了一种名为Fast-SmartWay的端到端零样本视觉语言导航框架，该框架仅使用正面RGB-D图像，无需全景视图和航点预测器，从而在实际环境中实现更快、更稳健的导航。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Layer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion",
        "summary": "In autonomous driving, transparency in the decision-making of perception\nmodels is critical, as even a single misperception can be catastrophic. Yet\nwith multi-sensor inputs, it is difficult to determine how each modality\ncontributes to a prediction because sensor information becomes entangled within\nthe fusion network. We introduce Layer-Wise Modality Decomposition (LMD), a\npost-hoc, model-agnostic interpretability method that disentangles\nmodality-specific information across all layers of a pretrained fusion model.\nTo our knowledge, LMD is the first approach to attribute the predictions of a\nperception model to individual input modalities in a sensor-fusion system for\nautonomous driving. We evaluate LMD on pretrained fusion models under\ncamera-radar, camera-LiDAR, and camera-radar-LiDAR settings for autonomous\ndriving. Its effectiveness is validated using structured perturbation-based\nmetrics and modality-wise visual decompositions, demonstrating practical\napplicability to interpreting high-capacity multimodal architectures. Code is\navailable at https://github.com/detxter-jvb/Layer-Wise-Modality-Decomposition.",
        "url": "http://arxiv.org/abs/2511.00859v1",
        "published_date": "2025-11-02T08:52:24+00:00",
        "updated_date": "2025-11-02T08:52:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jaehyun Park",
            "Konyul Park",
            "Daehun Kim",
            "Junseo Park",
            "Jun Won Choi"
        ],
        "tldr": "This paper introduces Layer-Wise Modality Decomposition (LMD), a novel post-hoc method for attributing predictions of multi-sensor fusion models in autonomous driving to individual modalities, enhancing interpretability.",
        "tldr_zh": "该论文介绍了层级模态分解（LMD），一种新颖的事后方法，用于将自动驾驶中多传感器融合模型的预测归因于各个模态，从而增强可解释性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction",
        "summary": "Predicting pedestrian crossing intentions is crucial for the navigation of\nmobile robots and intelligent vehicles. Although recent deep learning-based\nmodels have shown significant success in forecasting intentions, few consider\nincomplete observation under occlusion scenarios. To tackle this challenge, we\npropose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded\nmotion patterns and leverages them to guide future intention prediction. During\nthe denoising stage, we introduce an occlusion-aware diffusion transformer\narchitecture to estimate noise features associated with occluded patterns,\nthereby enhancing the model's ability to capture contextual relationships in\noccluded semantic scenarios. Furthermore, an occlusion mask-guided reverse\nprocess is introduced to effectively utilize observation information, reducing\nthe accumulation of prediction errors and enhancing the accuracy of\nreconstructed motion features. The performance of the proposed method under\nvarious occlusion scenarios is comprehensively evaluated and compared with\nexisting methods on popular benchmarks, namely PIE and JAAD. Extensive\nexperimental results demonstrate that the proposed method achieves more robust\nperformance than existing methods in the literature.",
        "url": "http://arxiv.org/abs/2511.00858v1",
        "published_date": "2025-11-02T08:49:07+00:00",
        "updated_date": "2025-11-02T08:49:07+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG"
        ],
        "authors": [
            "Yu Liu",
            "Zhijie Liu",
            "Zedong Yang",
            "You-Fu Li",
            "He Kong"
        ],
        "tldr": "This paper introduces an Occlusion-Aware Diffusion Model (ODM) for pedestrian intention prediction, specifically addressing the challenge of incomplete observations due to occlusion and demonstrating improved performance on standard datasets.",
        "tldr_zh": "本文介绍了一种用于行人意图预测的遮挡感知扩散模型（ODM），专门解决由于遮挡导致的不完整观察的挑战，并在标准数据集上展示了改进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking",
        "summary": "3D instance segmentation is an important task for real-world applications. To\navoid costly manual annotations, existing methods have explored generating\npseudo labels by transferring 2D masks from foundation models to 3D. However,\nthis approach is often suboptimal since the video frames are processed\nindependently. This causes inconsistent segmentation granularity and\nconflicting 3D pseudo labels, which degrades the accuracy of final\nsegmentation. To address this, we introduce a Granularity-Consistent automatic\n2D Mask Tracking approach that maintains temporal correspondences across\nframes, eliminating conflicting pseudo labels. Combined with a three-stage\ncurriculum learning framework, our approach progressively trains from\nfragmented single-view data to unified multi-view annotations, ultimately\nglobally coherent full-scene supervision. This structured learning pipeline\nenables the model to progressively expose to pseudo-labels of increasing\nconsistency. Thus, we can robustly distill a consistent 3D representation from\ninitially fragmented and contradictory 2D priors. Experimental results\ndemonstrated that our method effectively generated consistent and accurate 3D\nsegmentations. Furthermore, the proposed method achieved state-of-the-art\nresults on standard benchmarks and open-vocabulary ability.",
        "url": "http://arxiv.org/abs/2511.00785v1",
        "published_date": "2025-11-02T03:52:42+00:00",
        "updated_date": "2025-11-02T03:52:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Juan Wang",
            "Yasutomo Kawanishi",
            "Tomo Miyazaki",
            "Zhijie Wang",
            "Shinichiro Omachi"
        ],
        "tldr": "This paper introduces a method for class-agnostic 3D instance segmentation using a granularity-consistent 2D mask tracking approach to generate more consistent pseudo-labels and improve 3D segmentation accuracy, achieving state-of-the-art results.",
        "tldr_zh": "本文提出了一种类无关的3D实例分割方法，该方法使用粒度一致的2D掩码跟踪方法来生成更一致的伪标签，从而提高3D分割的准确性，并取得了最先进的成果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards classification-based representation learning for place recognition on LiDAR scans",
        "summary": "Place recognition is a crucial task in autonomous driving, allowing vehicles\nto determine their position using sensor data. While most existing methods rely\non contrastive learning, we explore an alternative approach by framing place\nrecognition as a multi-class classification problem. Our method assigns\ndiscrete location labels to LiDAR scans and trains an encoder-decoder model to\nclassify each scan's position directly. We evaluate this approach on the\nNuScenes dataset and show that it achieves competitive performance compared to\ncontrastive learning-based methods while offering advantages in training\nefficiency and stability.",
        "url": "http://arxiv.org/abs/2511.00738v2",
        "published_date": "2025-11-01T23:24:11+00:00",
        "updated_date": "2025-11-04T09:35:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Maksim Konoplia",
            "Dmitrii Khizbullin"
        ],
        "tldr": "This paper explores a classification-based approach to place recognition using LiDAR scans, achieving competitive performance with contrastive methods while improving training efficiency and stability.",
        "tldr_zh": "本文探索了一种基于分类的激光雷达扫描位置识别方法，在提高训练效率和稳定性的同时，实现了与对比方法相当的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Been There, Scanned That: Nostalgia-Driven LiDAR Compression for Self-Driving Cars",
        "summary": "An autonomous vehicle can generate several terabytes of sensor data per day.\nA significant portion of this data consists of 3D point clouds produced by\ndepth sensors such as LiDARs. This data must be transferred to cloud storage,\nwhere it is utilized for training machine learning models or conducting\nanalyses, such as forensic investigations in the event of an accident. To\nreduce network and storage costs, this paper introduces DejaView. Although\nprior work uses interframe redundancies to compress data, DejaView searches for\nand uses redundancies on larger temporal scales (days and months) for more\neffective compression. We designed DejaView with the insight that the operating\narea of autonomous vehicles is limited and that vehicles mostly traverse the\nsame routes daily. Consequently, the 3D data they collect daily is likely\nsimilar to the data they have captured in the past. To capture this, the core\nof DejaView is a diff operation that compactly represents point clouds as delta\nw.r.t. 3D data from the past. Using two months of LiDAR data, an end-to-end\nimplementation of DejaView can compress point clouds by a factor of 210 at a\nreconstruction error of only 15 cm.",
        "url": "http://arxiv.org/abs/2511.00652v1",
        "published_date": "2025-11-01T18:25:08+00:00",
        "updated_date": "2025-11-01T18:25:08+00:00",
        "categories": [
            "eess.IV",
            "cs.CV"
        ],
        "authors": [
            "Ali Khalid",
            "Jaiaid Mobin",
            "Sumanth Rao Appala",
            "Avinash Maurya",
            "Stephany Berrio Perez",
            "M. Mustafa Rafique",
            "Fawad Ahmad"
        ],
        "tldr": "The paper introduces DejaView, a LiDAR compression technique for autonomous vehicles that leverages temporal redundancies across days and months by comparing current point clouds to historical data, achieving a compression factor of 210 with minimal reconstruction error.",
        "tldr_zh": "该论文介绍了 DejaView，一种用于自动驾驶汽车的 LiDAR 压缩技术，它通过比较当前点云与历史数据，利用跨越数天和数月的时域冗余，实现了 210 倍的压缩率，且重建误差极小。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]