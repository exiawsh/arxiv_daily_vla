[
    {
        "title": "SimScale: Learning to Drive via Real-World Simulation at Scale",
        "summary": "Achieving fully autonomous driving systems requires learning rational decisions in a wide span of scenarios, including safety-critical and out-of-distribution ones. However, such cases are underrepresented in real-world corpus collected by human experts. To complement for the lack of data diversity, we introduce a novel and scalable simulation framework capable of synthesizing massive unseen states upon existing driving logs. Our pipeline utilizes advanced neural rendering with a reactive environment to generate high-fidelity multi-view observations controlled by the perturbed ego trajectory. Furthermore, we develop a pseudo-expert trajectory generation mechanism for these newly simulated states to provide action supervision. Upon the synthesized data, we find that a simple co-training strategy on both real-world and simulated samples can lead to significant improvements in both robustness and generalization for various planning methods on challenging real-world benchmarks, up to +6.8 EPDMS on navhard and +2.9 on navtest. More importantly, such policy improvement scales smoothly by increasing simulation data only, even without extra real-world data streaming in. We further reveal several crucial findings of such a sim-real learning system, which we term SimScale, including the design of pseudo-experts and the scaling properties for different policy architectures. Our simulation data and code would be released.",
        "url": "http://arxiv.org/abs/2511.23369v1",
        "published_date": "2025-11-28T17:17:38+00:00",
        "updated_date": "2025-11-28T17:17:38+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Haochen Tian",
            "Tianyu Li",
            "Haochen Liu",
            "Jiazhi Yang",
            "Yihang Qiu",
            "Guang Li",
            "Junli Wang",
            "Yinfeng Gao",
            "Zhang Zhang",
            "Liang Wang",
            "Hangjun Ye",
            "Tieniu Tan",
            "Long Chen",
            "Hongyang Li"
        ],
        "tldr": "The paper introduces SimScale, a novel and scalable simulation framework for autonomous driving that synthesizes diverse driving scenarios and uses co-training to improve the robustness and generalization of planning methods, showing significant performance gains on real-world benchmarks.",
        "tldr_zh": "该论文介绍了SimScale，一个新颖且可扩展的自动驾驶仿真框架，可以合成多样化的驾驶场景，并通过联合训练提高规划方法的鲁棒性和泛化能力，并在真实世界基准测试中显示出显著的性能提升。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "DiskChunGS: Large-Scale 3D Gaussian SLAM Through Chunk-Based Memory Management",
        "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated impressive results for novel view synthesis with real-time rendering capabilities. However, integrating 3DGS with SLAM systems faces a fundamental scalability limitation: methods are constrained by GPU memory capacity, restricting reconstruction to small-scale environments. We present DiskChunGS, a scalable 3DGS SLAM system that overcomes this bottleneck through an out-of-core approach that partitions scenes into spatial chunks and maintains only active regions in GPU memory while storing inactive areas on disk. Our architecture integrates seamlessly with existing SLAM frameworks for pose estimation and loop closure, enabling globally consistent reconstruction at scale. We validate DiskChunGS on indoor scenes (Replica, TUM-RGBD), urban driving scenarios (KITTI), and resource-constrained Nvidia Jetson platforms. Our method uniquely completes all 11 KITTI sequences without memory failures while achieving superior visual quality, demonstrating that algorithmic innovation can overcome the memory constraints that have limited previous 3DGS SLAM methods.",
        "url": "http://arxiv.org/abs/2511.23030v1",
        "published_date": "2025-11-28T09:52:49+00:00",
        "updated_date": "2025-11-28T09:52:49+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Casimir Feldmann",
            "Maximum Wilder-Smith",
            "Vaishakh Patil",
            "Michael Oechsle",
            "Michael Niemeyer",
            "Keisuke Tateno",
            "Marco Hutter"
        ],
        "tldr": "DiskChunGS overcomes the GPU memory limitations of 3D Gaussian Splatting SLAM by using an out-of-core approach, partitioning scenes into chunks and storing inactive areas on disk, enabling large-scale reconstruction.",
        "tldr_zh": "DiskChunGS通过使用核外方法克服了3D高斯溅射SLAM的GPU内存限制，将场景划分为块并将非活动区域存储在磁盘上，从而实现大规模重建。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "HMR3D: Hierarchical Multimodal Representation for 3D Scene Understanding with Large Vision-Language Model",
        "summary": "Recent advances in large vision-language models (VLMs) have shown significant promise for 3D scene understanding. Existing VLM-based approaches typically align 3D scene features with the VLM's embedding space. However, this implicit alignment often yields suboptimal performance due to the scarcity of 3D data and the inherent complexity of spatial relationships in 3D environments. To address these limitations, we propose a novel hierarchical multimodal representation for 3D scene reasoning that explicitly aligns with VLMs at the input space by leveraging both multi-view images and text descriptions. The text descriptions capture spatial relationships by referencing the 3D coordinates of detected objects, while the multi-view images include a top-down perspective and four directional views (forward, left, right, and backward), ensuring comprehensive scene coverage. Additionally, we introduce a hierarchical feature representation that aggregates patch-level image features into view-level and scene-level representations, enabling the model to reason over both local and global scene context. Experimental results on both situated 3D Q&A and general 3D Q&A benchmarks demonstrate the effectiveness of our approach.",
        "url": "http://arxiv.org/abs/2511.22961v1",
        "published_date": "2025-11-28T08:06:20+00:00",
        "updated_date": "2025-11-28T08:06:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chen Li",
            "Eric Peh",
            "Basura Fernando"
        ],
        "tldr": "This paper introduces a hierarchical multimodal representation (HMR3D) that explicitly aligns 3D scene features with VLMs using multi-view images and text descriptions, improving performance on 3D question answering tasks.",
        "tldr_zh": "该论文介绍了一种分层多模态表示（HMR3D），它通过使用多视角图像和文本描述将3D场景特征与VLM显式对齐，从而提高3D问答任务的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Emergent Extreme-View Geometry in 3D Foundation Models",
        "summary": "3D foundation models (3DFMs) have recently transformed 3D vision, enabling joint prediction of depths, poses, and point maps directly from images. Yet their ability to reason under extreme, non-overlapping views remains largely unexplored. In this work, we study their internal representations and find that 3DFMs exhibit an emergent understanding of extreme-view geometry, despite never being trained for such conditions. To further enhance these capabilities, we introduce a lightweight alignment scheme that refines their internal 3D representation by tuning only a small subset of backbone bias terms, leaving all decoder heads frozen. This targeted adaptation substantially improves relative pose estimation under extreme viewpoints without degrading per-image depth or point quality. Additionally, we contribute MegaUnScene, a new benchmark of Internet scenes unseen by existing 3DFMs, with dedicated test splits for both relative pose estimation and dense 3D reconstruction. All code and data will be released.",
        "url": "http://arxiv.org/abs/2511.22686v1",
        "published_date": "2025-11-27T18:40:03+00:00",
        "updated_date": "2025-11-27T18:40:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiwen Zhang",
            "Joseph Tung",
            "Ruojin Cai",
            "David Fouhey",
            "Hadar Averbuch-Elor"
        ],
        "tldr": "This paper explores the emergent understanding of extreme-view geometry in 3D foundation models (3DFMs), enhances it with a lightweight alignment scheme, and introduces a new benchmark, MegaUnScene, for evaluating performance under extreme viewpoints.",
        "tldr_zh": "该论文探索了3D基础模型(3DFMs)中对极端视角几何体的涌现理解，通过轻量级对齐方案增强了这种理解，并引入了一个新的基准MegaUnScene，用于评估极端视角下的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Robust 3DGS-based SLAM via Adaptive Kernel Smoothing",
        "summary": "In this paper, we challenge the conventional notion in 3DGS-SLAM that rendering quality is the primary determinant of tracking accuracy. We argue that, compared to solely pursuing a perfect scene representation, it is more critical to enhance the robustness of the rasterization process against parameter errors to ensure stable camera pose tracking. To address this challenge, we propose a novel approach that leverages a smooth kernel strategy to enhance the robustness of 3DGS-based SLAM. Unlike conventional methods that focus solely on minimizing rendering error, our core insight is to make the rasterization process more resilient to imperfections in the 3DGS parameters. We hypothesize that by allowing each Gaussian to influence a smoother, wider distribution of pixels during rendering, we can mitigate the detrimental effects of parameter noise from outlier Gaussians. This approach intentionally introduces a controlled blur to the rendered image, which acts as a regularization term, stabilizing the subsequent pose optimization. While a complete redesign of the rasterization pipeline is an ideal solution, we propose a practical and effective alternative that is readily integrated into existing 3DGS frameworks. Our method, termed Corrective Blurry KNN (CB-KNN), adaptively modifies the RGB values and locations of the K-nearest neighboring Gaussians within a local region. This dynamic adjustment generates a smoother local rendering, reducing the impact of erroneous GS parameters on the overall image. Experimental results demonstrate that our approach, while maintaining the overall quality of the scene reconstruction (mapping), significantly improves the robustness and accuracy of camera pose tracking.",
        "url": "http://arxiv.org/abs/2511.23221v1",
        "published_date": "2025-11-28T14:28:05+00:00",
        "updated_date": "2025-11-28T14:28:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shouhe Zhang",
            "Dayong Ren",
            "Sensen Song",
            "Wenjie Li",
            "Piaopiao Yu",
            "Yurong Qian"
        ],
        "tldr": "This paper proposes a novel 3D Gaussian Splatting (3DGS)-based SLAM approach, Corrective Blurry KNN (CB-KNN), that enhances tracking robustness by adaptively smoothing the rasterization process, making it less sensitive to parameter errors and outlier Gaussians.",
        "tldr_zh": "本文提出了一种基于3D高斯溅射(3DGS)的新型SLAM方法，即纠正模糊KNN(CB-KNN)，通过自适应地平滑栅格化过程来增强跟踪的鲁棒性，使其对参数误差和异常高斯不太敏感。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Taming the Light: Illumination-Invariant Semantic 3DGS-SLAM",
        "summary": "Extreme exposure degrades both the 3D map reconstruction and semantic segmentation accuracy, which is particularly detrimental to tightly-coupled systems. To achieve illumination invariance, we propose a novel semantic SLAM framework with two designs. First, the Intrinsic Appearance Normalization (IAN) module proactively disentangles the scene's intrinsic properties, such as albedo, from transient lighting. By learning a standardized, illumination-invariant appearance model, it assigns a stable and consistent color representation to each Gaussian primitive. Second, the Dynamic Radiance Balancing Loss (DRB-Loss) reactively handles frames with extreme exposure. It activates only when an image's exposure is poor, operating directly on the radiance field to guide targeted optimization. This prevents error accumulation from extreme lighting without compromising performance under normal conditions. The synergy between IAN's proactive invariance and DRB-Loss's reactive correction endows our system with unprecedented robustness. Evaluations on public datasets demonstrate state-of-the-art performance in camera tracking, map quality, and semantic and geometric accuracy.",
        "url": "http://arxiv.org/abs/2511.22968v1",
        "published_date": "2025-11-28T08:19:40+00:00",
        "updated_date": "2025-11-28T08:19:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shouhe Zhang",
            "Dayong Ren",
            "Sensen Song",
            "Yurong Qian",
            "Zhenhong Jia"
        ],
        "tldr": "This paper introduces a novel semantic SLAM framework, 3DGS-SLAM, that addresses the challenges of extreme exposure in 3D map reconstruction and semantic segmentation by using Intrinsic Appearance Normalization (IAN) and Dynamic Radiance Balancing Loss (DRB-Loss). The system achieves state-of-the-art performance in camera tracking, map quality, and semantic accuracy.",
        "tldr_zh": "这篇论文提出了一种新的语义SLAM框架，3DGS-SLAM，通过使用Intrinsic Appearance Normalization (IAN)和Dynamic Radiance Balancing Loss (DRB-Loss)解决了极端曝光对3D地图重建和语义分割的挑战。该系统在相机跟踪、地图质量和语义准确性方面取得了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RobotSeg: A Model and Dataset for Segmenting Robots in Image and Video",
        "summary": "Accurate robot segmentation is a fundamental capability for robotic perception. It enables precise visual servoing for VLA systems, scalable robot-centric data augmentation, accurate real-to-sim transfer, and reliable safety monitoring in dynamic human-robot environments. Despite the strong capabilities of modern segmentation models, surprisingly it remains challenging to segment robots. This is due to robot embodiment diversity, appearance ambiguity, structural complexity, and rapid shape changes. Embracing these challenges, we introduce RobotSeg, a foundation model for robot segmentation in image and video. RobotSeg is built upon the versatile SAM 2 foundation model but addresses its three limitations for robot segmentation, namely the lack of adaptation to articulated robots, reliance on manual prompts, and the need for per-frame training mask annotations, by introducing a structure-enhanced memory associator, a robot prompt generator, and a label-efficient training strategy. These innovations collectively enable a structure-aware, automatic, and label-efficient solution. We further construct the video robot segmentation (VRS) dataset comprising over 2.8k videos (138k frames) with diverse robot embodiments and environments. Extensive experiments demonstrate that RobotSeg achieves state-of-the-art performance on both images and videos, establishing a strong foundation for future advances in robot perception.",
        "url": "http://arxiv.org/abs/2511.22950v1",
        "published_date": "2025-11-28T07:51:02+00:00",
        "updated_date": "2025-11-28T07:51:02+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Haiyang Mei",
            "Qiming Huang",
            "Hai Ci",
            "Mike Zheng Shou"
        ],
        "tldr": "The paper introduces RobotSeg, a foundation model and dataset for robot segmentation in images and videos, addressing limitations of existing models by incorporating structure-aware learning, automatic prompt generation, and label-efficient training.",
        "tldr_zh": "该论文介绍了RobotSeg，一个用于图像和视频中机器人分割的基础模型和数据集。它通过引入结构感知学习、自动提示生成和标签高效训练来解决现有模型的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DM$^3$T: Harmonizing Modalities via Diffusion for Multi-Object Tracking",
        "summary": "Multi-object tracking (MOT) is a fundamental task in computer vision with critical applications in autonomous driving and robotics. Multimodal MOT that integrates visible light and thermal infrared information is particularly essential for robust autonomous driving systems. However, effectively fusing these heterogeneous modalities is challenging. Simple strategies like concatenation or addition often fail to bridge the significant non-linear distribution gap between their feature representations, which can lead to modality conflicts and degrade tracking accuracy. Drawing inspiration from the connection between multimodal MOT and the iterative refinement in diffusion models, this paper proposes DM$^3$T, a novel framework that reformulates multimodal fusion as an iterative feature alignment process to generate accurate and temporally coherent object trajectories. Our approach performs iterative cross-modal harmonization through a proposed Cross-Modal Diffusion Fusion (C-MDF) module. In this process, features from both modalities provide mutual guidance, iteratively projecting them onto a shared, consistent feature manifold. This enables the learning of complementary information and achieves deeper fusion compared to conventional methods. Additionally, we introduce a plug-and-play Diffusion Refiner (DR) to enhance and refine the unified feature representation. To further improve tracking robustness, we design a Hierarchical Tracker that adaptively handles confidence estimation. DM$^3$T unifies object detection, state estimation, and data association into a comprehensive online tracking framework without complex post-processing. Extensive experiments on the VT-MOT benchmark demonstrate that our method achieves 41.7 HOTA, representing a 1.54% relative improvement over existing state-of-the-art methods. The code and models are available at https://vranlee.github.io/DM-3-T/.",
        "url": "http://arxiv.org/abs/2511.22896v1",
        "published_date": "2025-11-28T06:02:58+00:00",
        "updated_date": "2025-11-28T06:02:58+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Weiran Li",
            "Yeqiang Liu",
            "Yijie Wei",
            "Mina Han",
            "Qiannan Guo",
            "Zhenbo Li"
        ],
        "tldr": "The paper introduces DM$^3$T, a diffusion-based framework for multimodal multi-object tracking (MOT) using visible and thermal infrared data, achieving state-of-the-art results on the VT-MOT benchmark by iteratively aligning cross-modal features.",
        "tldr_zh": "该论文介绍了DM$^3$T，一个基于扩散的多模态多目标跟踪（MOT）框架，使用可见光和热红外数据，通过迭代对齐跨模态特征，在VT-MOT基准测试上取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SUPER-AD: Semantic Uncertainty-aware Planning for End-to-End Robust Autonomous Driving",
        "summary": "End-to-End (E2E) planning has become a powerful paradigm for autonomous driving, yet current systems remain fundamentally uncertainty-blind. They assume perception outputs are fully reliable, even in ambiguous or poorly observed scenes, leaving the planner without an explicit measure of uncertainty. To address this limitation, we propose a camera-only E2E framework that estimates aleatoric uncertainty directly in BEV space and incorporates it into planning. Our method produces a dense, uncertainty-aware drivability map that captures both semantic structure and geometric layout at pixel-level resolution. To further promote safe and rule-compliant behavior, we introduce a lane-following regularization that encodes lane structure and traffic norms. This prior stabilizes trajectory planning under normal conditions while preserving the flexibility needed for maneuvers such as overtaking or lane changes. Together, these components enable robust and interpretable trajectory planning, even under challenging uncertainty conditions. Evaluated on the NAVSIM benchmark, our method achieves state-of-the-art performance, delivering substantial gains on both the challenging NAVHARD and NAVSAFE subsets. These results demonstrate that our principled aleatoric uncertainty modeling combined with driving priors significantly advances the safety and reliability of camera-only E2E autonomous driving.",
        "url": "http://arxiv.org/abs/2511.22865v1",
        "published_date": "2025-11-28T03:50:44+00:00",
        "updated_date": "2025-11-28T03:50:44+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Wonjeong Ryu",
            "Seungjun Yu",
            "Seokha Moon",
            "Hojun Choi",
            "Junsung Park",
            "Jinkyu Kim",
            "Hyunjung Shim"
        ],
        "tldr": "This paper introduces a camera-only end-to-end autonomous driving framework that incorporates aleatoric uncertainty estimation in BEV space and lane-following regularization for robust and safe planning, achieving state-of-the-art results on the NAVSIM benchmark.",
        "tldr_zh": "本文提出了一种仅使用摄像头的端到端自动驾驶框架，该框架结合了BEV空间中的不确定性估计和车道保持正则化，以实现鲁棒和安全的规划，并在NAVSIM基准测试上取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mechanistic Finetuning of Vision-Language-Action Models via Few-Shot Demonstrations",
        "summary": "Vision-Language Action (VLAs) models promise to extend the remarkable success of vision-language models (VLMs) to robotics. Yet, unlike VLMs in the vision-language domain, VLAs for robotics require finetuning to contend with varying physical factors like robot embodiment, environment characteristics, and spatial relationships of each task. Existing fine-tuning methods lack specificity, adapting the same set of parameters regardless of a task's visual, linguistic, and physical characteristics. Inspired by functional specificity in neuroscience, we hypothesize that it is more effective to finetune sparse model representations specific to a given task. In this work, we introduce Robotic Steering, a finetuning approach grounded in mechanistic interpretability that leverages few-shot demonstrations to identify and selectively finetune task-specific attention heads aligned with the physical, visual, and linguistic requirements of robotic tasks. Through comprehensive on-robot evaluations with a Franka Emika robot arm, we demonstrate that Robotic Steering outperforms LoRA while achieving superior robustness under task variation, reduced computational cost, and enhanced interpretability for adapting VLAs to diverse robotic tasks.",
        "url": "http://arxiv.org/abs/2511.22697v1",
        "published_date": "2025-11-27T18:50:21+00:00",
        "updated_date": "2025-11-27T18:50:21+00:00",
        "categories": [
            "cs.RO",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Chancharik Mitra",
            "Yusen Luo",
            "Raj Saravanan",
            "Dantong Niu",
            "Anirudh Pai",
            "Jesse Thomason",
            "Trevor Darrell",
            "Abrar Anwar",
            "Deva Ramanan",
            "Roei Herzig"
        ],
        "tldr": "The paper introduces Robotic Steering, a mechanistic interpretability-based finetuning approach for VLA models that selectively finetunes task-specific attention heads, demonstrating improved performance, robustness, and interpretability compared to LoRA in robot arm tasks.",
        "tldr_zh": "该论文介绍了一种基于机械可解释性的VLA模型微调方法Robotic Steering，它选择性地微调任务特定的注意力头，与LoRA相比，在机器人手臂任务中表现出更好的性能、鲁棒性和可解释性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Toward Automatic Safe Driving Instruction: A Large-Scale Vision Language Model Approach",
        "summary": "Large-scale Vision Language Models (LVLMs) exhibit advanced capabilities in tasks that require visual information, including object detection. These capabilities have promising applications in various industrial domains, such as autonomous driving. For example, LVLMs can generate safety-oriented descriptions of videos captured by road-facing cameras. However, ensuring comprehensive safety requires monitoring driver-facing views as well to detect risky events, such as the use of mobiles while driving. Thus, the ability to process synchronized inputs is necessary from both driver-facing and road-facing cameras. In this study, we develop models and investigate the capabilities of LVLMs by constructing a dataset and evaluating their performance on this dataset. Our experimental results demonstrate that while pre-trained LVLMs have limited effectiveness, fine-tuned LVLMs can generate accurate and safety-aware driving instructions. Nonetheless, several challenges remain, particularly in detecting subtle or complex events in the video. Our findings and error analysis provide valuable insights that can contribute to the improvement of LVLM-based systems in this domain.",
        "url": "http://arxiv.org/abs/2511.23311v1",
        "published_date": "2025-11-28T16:09:36+00:00",
        "updated_date": "2025-11-28T16:09:36+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL"
        ],
        "authors": [
            "Haruki Sakajo",
            "Hiroshi Takato",
            "Hiroshi Tsutsui",
            "Komei Soda",
            "Hidetaka Kamigaito",
            "Taro Watanabe"
        ],
        "tldr": "This paper explores the use of fine-tuned LVLMs to generate safety-aware driving instructions using synchronized road-facing and driver-facing camera inputs, identifying limitations in detecting subtle events.",
        "tldr_zh": "本文探讨了使用微调的LVLMs，通过同步的前向道路和驾驶员面部摄像头输入来生成安全驾驶指令，并发现了在检测细微事件方面的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]