[
    {
        "title": "Bridging Perspectives: Foundation Model Guided BEV Maps for 3D Object Detection and Tracking",
        "summary": "Camera-based 3D object detection and tracking are essential for perception in\nautonomous driving. Current state-of-the-art approaches often rely exclusively\non either perspective-view (PV) or bird's-eye-view (BEV) features, limiting\ntheir ability to leverage both fine-grained object details and spatially\nstructured scene representations. In this work, we propose DualViewDistill, a\nhybrid detection and tracking framework that incorporates both PV and BEV\ncamera image features to leverage their complementary strengths. Our approach\nintroduces BEV maps guided by foundation models, leveraging descriptive DINOv2\nfeatures that are distilled into BEV representations through a novel\ndistillation process. By integrating PV features with BEV maps enriched with\nsemantic and geometric features from DINOv2, our model leverages this hybrid\nrepresentation via deformable aggregation to enhance 3D object detection and\ntracking. Extensive experiments on the nuScenes and Argoverse 2 benchmarks\ndemonstrate that DualViewDistill achieves state-of-the-art performance. The\nresults showcase the potential of foundation model BEV maps to enable more\nreliable perception for autonomous driving. We make the code and pre-trained\nmodels available at https://dualviewdistill.cs.uni-freiburg.de .",
        "url": "http://arxiv.org/abs/2510.10287v1",
        "published_date": "2025-10-11T17:01:42+00:00",
        "updated_date": "2025-10-11T17:01:42+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Markus Käppeler",
            "Özgün Çiçek",
            "Daniele Cattaneo",
            "Claudius Gläser",
            "Yakov Miron",
            "Abhinav Valada"
        ],
        "tldr": "This paper introduces DualViewDistill, a novel 3D object detection and tracking framework that combines perspective-view and bird's-eye-view features, utilizing foundation model-guided BEV maps for improved performance on autonomous driving benchmarks.",
        "tldr_zh": "本文介绍了一种名为DualViewDistill的新型3D物体检测和跟踪框架，该框架结合了透视视图和鸟瞰视图特征，并利用基础模型引导的BEV地图，从而在自动驾驶基准测试中实现了更高的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model",
        "summary": "Successful generalist Vision-Language-Action (VLA) models rely on effective\ntraining across diverse robotic platforms with large-scale, cross-embodiment,\nheterogeneous datasets. To facilitate and leverage the heterogeneity in rich,\ndiverse robotic data sources, we propose a novel Soft Prompt approach with\nminimally added parameters, by infusing prompt learning concepts into\ncross-embodiment robot learning and introducing separate sets of learnable\nembeddings for each distinct data source. These embeddings serve as\nembodiment-specific prompts, which in unity empower VLA models with effective\nexploitation of varying cross-embodiment features. Our new X-VLA, a neat\nflow-matching-based VLA architecture, relies exclusively on soft-prompted\nstandard Transformer encoders, enjoying both scalability and simplicity.\nEvaluated across 6 simulations as well as 3 real-world robots, our 0.9B\ninstantiation-X-VLA-0.9B simultaneously achieves SOTA performance over a sweep\nof benchmarks, demonstrating superior results on a wide axes of capabilities,\nfrom flexible dexterity to quick adaptation across embodiments, environments,\nand tasks. Website: https://thu-air-dream.github.io/X-VLA/",
        "url": "http://arxiv.org/abs/2510.10274v1",
        "published_date": "2025-10-11T16:20:17+00:00",
        "updated_date": "2025-10-11T16:20:17+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Jinliang Zheng",
            "Jianxiong Li",
            "Zhihao Wang",
            "Dongxiu Liu",
            "Xirui Kang",
            "Yuchun Feng",
            "Yinan Zheng",
            "Jiayin Zou",
            "Yilun Chen",
            "Jia Zeng",
            "Ya-Qin Zhang",
            "Jiangmiao Pang",
            "Jingjing Liu",
            "Tai Wang",
            "Xianyuan Zhan"
        ],
        "tldr": "The paper introduces X-VLA, a novel Vision-Language-Action model that uses soft prompts with Transformers to effectively leverage heterogeneous, cross-embodiment robotic data, achieving state-of-the-art performance on various benchmarks.",
        "tldr_zh": "该论文介绍了X-VLA，一种新颖的视觉-语言-动作模型，它使用带有Transformer的软提示来有效地利用异构的、跨具身性的机器人数据，并在各种基准测试中实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Style-Based Metric for Quantifying the Synthetic-to-Real Gap in Autonomous Driving Image Datasets",
        "summary": "Ensuring the reliability of autonomous driving perception systems requires\nextensive environment-based testing, yet real-world execution is often\nimpractical. Synthetic datasets have therefore emerged as a promising\nalternative, offering advantages such as cost-effectiveness, bias free\nlabeling, and controllable scenarios. However, the domain gap between synthetic\nand real-world datasets remains a critical bottleneck for the generalization of\nAI-based autonomous driving models. Quantifying this synthetic-to-real gap is\nthus essential for evaluating dataset utility and guiding the design of more\neffective training pipelines. In this paper, we establish a systematic\nframework for quantifying the synthetic-to-real gap in autonomous driving\nsystems, and propose Style Embedding Distribution Discrepancy (SEDD) as a novel\nevaluation metric. Our framework combines Gram matrix-based style extraction\nwith metric learning optimized for intra-class compactness and inter-class\nseparation to extract style embeddings. Furthermore, we establish a benchmark\nusing publicly available datasets. Experiments are conducted on a variety of\ndatasets and sim-to-real methods, and the results show that our method is\ncapable of quantifying the synthetic-to-real gap. This work provides a\nstandardized quality control tool that enables systematic diagnosis and\ntargeted enhancement of synthetic datasets, advancing future development of\ndata-driven autonomous driving systems.",
        "url": "http://arxiv.org/abs/2510.10203v1",
        "published_date": "2025-10-11T13:09:41+00:00",
        "updated_date": "2025-10-11T13:09:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dingyi Yao",
            "Xinyao Han",
            "Ruibo Ming",
            "Zhihang Song",
            "Lihui Peng",
            "Jianming Hu",
            "Danya Yao",
            "Yi Zhang"
        ],
        "tldr": "This paper introduces a new metric, SEDD, to quantify the synthetic-to-real gap in autonomous driving image datasets using style embeddings, providing a tool for dataset evaluation and enhancement.",
        "tldr_zh": "本文提出了一种新的度量方法SEDD，利用风格嵌入来量化自动驾驶图像数据集中从合成到真实的差距，为数据集评估和增强提供工具。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "B2N3D: Progressive Learning from Binary to N-ary Relationships for 3D Object Grounding",
        "summary": "Localizing 3D objects using natural language is essential for robotic scene\nunderstanding. The descriptions often involve multiple spatial relationships to\ndistinguish similar objects, making 3D-language alignment difficult. Current\nmethods only model relationships for pairwise objects, ignoring the global\nperceptual significance of n-ary combinations in multi-modal relational\nunderstanding. To address this, we propose a novel progressive relational\nlearning framework for 3D object grounding. We extend relational learning from\nbinary to n-ary to identify visual relations that match the referential\ndescription globally. Given the absence of specific annotations for referred\nobjects in the training data, we design a grouped supervision loss to\nfacilitate n-ary relational learning. In the scene graph created with n-ary\nrelationships, we use a multi-modal network with hybrid attention mechanisms to\nfurther localize the target within the n-ary combinations. Experiments and\nablation studies on the ReferIt3D and ScanRefer benchmarks demonstrate that our\nmethod outperforms the state-of-the-art, and proves the advantages of the n-ary\nrelational perception in 3D localization.",
        "url": "http://arxiv.org/abs/2510.10194v1",
        "published_date": "2025-10-11T12:17:12+00:00",
        "updated_date": "2025-10-11T12:17:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Feng Xiao",
            "Hongbin Xu",
            "Hai Ci",
            "Wenxiong Kang"
        ],
        "tldr": "This paper introduces a novel framework, B2N3D, for 3D object grounding that uses progressive learning from binary to n-ary relationships to improve 3D-language alignment and object localization, achieving state-of-the-art results on ReferIt3D and ScanRefer benchmarks.",
        "tldr_zh": "该论文提出了一种新颖的框架B2N3D，用于3D对象定位，该框架使用从二元到n元关系的渐进式学习来改进3D语言对齐和对象定位，并在ReferIt3D和ScanRefer基准测试中取得了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Dejavu: Post-Deployment Learning for Embodied Agents via Experience Feedback",
        "summary": "Embodied agents face a fundamental limitation: once deployed in real-world\nenvironments to perform specific tasks, they are unable to acquire new useful\nknowledge to enhance task performance. In this paper, we propose a general\npost-deployment learning framework called Dejavu, which employs an Experience\nFeedback Network (EFN) and augments the frozen Vision-Language-Action (VLA)\npolicy with retrieved execution memories. EFN automatically identifies\ncontextually successful prior action experiences and conditions action\nprediction on this retrieved guidance. We adopt reinforcement learning with\nsemantic similarity rewards on EFN to ensure that the predicted actions align\nwith past successful behaviors under current observations. During deployment,\nEFN continually enriches its memory with new trajectories, enabling the agent\nto exhibit \"learning from experience\" despite fixed weights. Experiments across\ndiverse embodied tasks show that EFN significantly improves adaptability,\nrobustness, and success rates over frozen baselines. These results highlight a\npromising path toward embodied agents that continually refine their behavior\nafter deployment.",
        "url": "http://arxiv.org/abs/2510.10181v1",
        "published_date": "2025-10-11T11:43:58+00:00",
        "updated_date": "2025-10-11T11:43:58+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Shaokai Wu",
            "Yanbiao Ji",
            "Qiuchang Li",
            "Zhiyi Zhang",
            "Qichen He",
            "Wenyuan Xie",
            "Guodong Zhang",
            "Bayram Bayramli",
            "Yue Ding",
            "Hongtao Lu"
        ],
        "tldr": "The paper introduces Dejavu, a post-deployment learning framework for embodied agents that utilizes an Experience Feedback Network (EFN) to improve adaptability and robustness by retrieving and learning from past successful experiences. This allows agents to continually refine their behavior even with fixed weights.",
        "tldr_zh": "该论文介绍了一种名为Dejavu的后部署学习框架，用于具身智能体。该框架利用经验反馈网络（EFN），通过检索并学习过去的成功经验来提高适应性和鲁棒性。这使得智能体即使在权重固定的情况下也能不断改进其行为。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HccePose(BF): Predicting Front \\& Back Surfaces to Construct Ultra-Dense 2D-3D Correspondences for Pose Estimation",
        "summary": "In pose estimation for seen objects, a prevalent pipeline involves using\nneural networks to predict dense 3D coordinates of the object surface on 2D\nimages, which are then used to establish dense 2D-3D correspondences. However,\ncurrent methods primarily focus on more efficient encoding techniques to\nimprove the precision of predicted 3D coordinates on the object's front\nsurface, overlooking the potential benefits of incorporating the back surface\nand interior of the object. To better utilize the full surface and interior of\nthe object, this study predicts 3D coordinates of both the object's front and\nback surfaces and densely samples 3D coordinates between them. This process\ncreates ultra-dense 2D-3D correspondences, effectively enhancing pose\nestimation accuracy based on the Perspective-n-Point (PnP) algorithm.\nAdditionally, we propose Hierarchical Continuous Coordinate Encoding (HCCE) to\nprovide a more accurate and efficient representation of front and back surface\ncoordinates. Experimental results show that, compared to existing\nstate-of-the-art (SOTA) methods on the BOP website, the proposed approach\noutperforms across seven classic BOP core datasets. Code is available at\nhttps://github.com/WangYuLin-SEU/HCCEPose.",
        "url": "http://arxiv.org/abs/2510.10177v1",
        "published_date": "2025-10-11T11:29:53+00:00",
        "updated_date": "2025-10-11T11:29:53+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Yulin Wang",
            "Mengting Hu",
            "Hongli Li",
            "Chen Luo"
        ],
        "tldr": "This paper introduces HccePose(BF), a pose estimation method that predicts both front and back surfaces of objects and uses Hierarchical Continuous Coordinate Encoding (HCCE) to create ultra-dense 2D-3D correspondences, achieving SOTA results on BOP datasets.",
        "tldr_zh": "本文介绍了 HccePose(BF)，一种姿态估计方法，它预测物体的前后表面，并使用分层连续坐标编码 (HCCE) 来创建超密集 2D-3D 对应关系，在 BOP 数据集上实现了 SOTA 结果。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]