[
    {
        "title": "EventTracer: Fast Path Tracing-based Event Stream Rendering",
        "summary": "Simulating event streams from 3D scenes has become a common practice in\nevent-based vision research, as it meets the demand for large-scale, high\ntemporal frequency data without setting up expensive hardware devices or\nundertaking extensive data collections. Yet existing methods in this direction\ntypically work with noiseless RGB frames that are costly to render, and\ntherefore they can only achieve a temporal resolution equivalent to 100-300\nFPS, far lower than that of real-world event data. In this work, we propose\nEventTracer, a path tracing-based rendering pipeline that simulates\nhigh-fidelity event sequences from complex 3D scenes in an efficient and\nphysics-aware manner. Specifically, we speed up the rendering process via low\nsample-per-pixel (SPP) path tracing, and train a lightweight event spiking\nnetwork to denoise the resulting RGB videos into realistic event sequences. To\ncapture the physical properties of event streams, the network is equipped with\na bipolar leaky integrate-and-fired (BiLIF) spiking unit and trained with a\nbidirectional earth mover distance (EMD) loss. Our EventTracer pipeline runs at\na speed of about 4 minutes per second of 720p video, and it inherits the merit\nof accurate spatiotemporal modeling from its path tracing backbone. We show in\ntwo downstream tasks that EventTracer captures better scene details and\ndemonstrates a greater similarity to real-world event data than other event\nsimulators, which establishes it as a promising tool for creating large-scale\nevent-RGB datasets at a low cost, narrowing the sim-to-real gap in event-based\nvision, and boosting various application scenarios such as robotics, autonomous\ndriving, and VRAR.",
        "url": "http://arxiv.org/abs/2508.18071v1",
        "published_date": "2025-08-25T14:33:09+00:00",
        "updated_date": "2025-08-25T14:33:09+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhenyang Li",
            "Xiaoyang Bai",
            "Jinfan Lu",
            "Pengfei Shen",
            "Edmund Y. Lam",
            "Yifan Peng"
        ],
        "tldr": "EventTracer is a path tracing-based pipeline for efficiently simulating high-fidelity event streams from 3D scenes by combining low-SPP path tracing with a spiking network for denoising, achieving superior results in downstream tasks compared to existing simulators.",
        "tldr_zh": "EventTracer是一种基于路径追踪的管道，通过将低SPP路径追踪与用于降噪的脉冲神经网络相结合，可以有效地从3D场景中模拟高保真事件流。与现有的模拟器相比，该方法在下游任务中取得了更好的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework",
        "summary": "Traversability estimation is critical for enabling robots to navigate across\ndiverse terrains and environments. While recent self-supervised learning\nmethods achieve promising results, they often fail to capture the\ncharacteristics of non-traversable regions. Moreover, most prior works\nconcentrate on a single modality, overlooking the complementary strengths\noffered by integrating heterogeneous sensory modalities for more robust\ntraversability estimation. To address these limitations, we propose a\nmultimodal self-supervised framework for traversability labeling and\nestimation. First, our annotation pipeline integrates footprint, LiDAR, and\ncamera data as prompts for a vision foundation model, generating traversability\nlabels that account for both semantic and geometric cues. Then, leveraging\nthese labels, we train a dual-stream network that jointly learns from different\nmodalities in a decoupled manner, enhancing its capacity to recognize diverse\ntraversability patterns. In addition, we incorporate sparse LiDAR-based\nsupervision to mitigate the noise introduced by pseudo labels. Finally,\nextensive experiments conducted across urban, off-road, and campus environments\ndemonstrate the effectiveness of our approach. The proposed automatic labeling\nmethod consistently achieves around 88% IoU across diverse datasets. Compared\nto existing self-supervised state-of-the-art methods, our multimodal\ntraversability estimation network yields consistently higher IoU, improving by\n1.6-3.5% on all evaluated datasets.",
        "url": "http://arxiv.org/abs/2508.18249v1",
        "published_date": "2025-08-25T17:40:16+00:00",
        "updated_date": "2025-08-25T17:40:16+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Zipeng Fang",
            "Yanbo Wang",
            "Lei Zhao",
            "Weidong Chen"
        ],
        "tldr": "This paper proposes a multimodal self-supervised framework for traversability estimation, using a vision foundation model to generate labels from footprint, LiDAR, and camera data, and then training a dual-stream network. It demonstrates improved IoU compared to existing methods across diverse datasets.",
        "tldr_zh": "本文提出了一种用于可通行性估计的多模态自监督框架，该框架使用视觉基础模型从足迹、激光雷达和相机数据生成标签，然后训练一个双流网络。实验表明，与现有方法相比，该方法在各种数据集上都提高了 IoU。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Enhanced Drift-Aware Computer Vision Architecture for Autonomous Driving",
        "summary": "The use of computer vision in automotive is a trending research in which\nsafety and security are a primary concern. In particular, for autonomous\ndriving, preventing road accidents requires highly accurate object detection\nunder diverse conditions. To address this issue, recently the International\nOrganization for Standardization (ISO) released the 8800 norm, providing\nstructured frameworks for managing associated AI relevant risks. However,\nchallenging scenarios such as adverse weather or low lighting often introduce\ndata drift, leading to degraded model performance and potential safety\nviolations. In this work, we present a novel hybrid computer vision\narchitecture trained with thousands of synthetic image data from the road\nenvironment to improve robustness in unseen drifted environments. Our dual mode\nframework utilized YOLO version 8 for swift detection and incorporated a\nfive-layer CNN for verification. The system functioned in sequence and improved\nthe detection accuracy by more than 90\\% when tested with drift-augmented road\nimages. The focus was to demonstrate how such a hybrid model can provide better\nroad safety when working together in a hybrid structure.",
        "url": "http://arxiv.org/abs/2508.17975v1",
        "published_date": "2025-08-25T12:43:29+00:00",
        "updated_date": "2025-08-25T12:43:29+00:00",
        "categories": [
            "cs.CV",
            "math.LO"
        ],
        "authors": [
            "Md Shahi Amran Hossain",
            "Abu Shad Ahammed",
            "Sayeri Mukherjee",
            "Roman Obermaisser"
        ],
        "tldr": "This paper introduces a hybrid computer vision architecture using YOLOv8 and a CNN for improved object detection accuracy in autonomous driving scenarios with data drift, achieving over 90% accuracy on drift-augmented images.",
        "tldr_zh": "本文介绍了一种混合计算机视觉架构，使用YOLOv8和一个CNN来提高自动驾驶场景中数据漂移时的目标检测精度，在漂移增强图像上实现了超过90%的精度。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm",
        "summary": "This paper introduces a holistic perception system for internal and external\nmonitoring of autonomous vehicles, with the aim of demonstrating a novel\nAI-leveraged self-adaptive framework of advanced vehicle technologies and\nsolutions that optimize perception and experience on-board. Internal monitoring\nsystem relies on a multi-camera setup designed for predicting and identifying\ndriver and occupant behavior through facial recognition, exploiting in addition\na large language model as virtual assistant. Moreover, the in-cabin monitoring\nsystem includes AI-empowered smart sensors that measure air-quality and perform\nthermal comfort analysis for efficient on and off-boarding. On the other hand,\nexternal monitoring system perceives the surrounding environment of vehicle,\nthrough a LiDAR-based cost-efficient semantic segmentation approach, that\nperforms highly accurate and efficient super-resolution on low-quality raw 3D\npoint clouds. The holistic perception framework is developed in the context of\nEU's Horizon Europe programm AutoTRUST, and has been integrated and deployed on\na real electric vehicle provided by ALKE. Experimental validation and\nevaluation at the integration site of Joint Research Centre at Ispra, Italy,\nhighlights increased performance and efficiency of the modular blocks of the\nproposed perception architecture.",
        "url": "http://arxiv.org/abs/2508.17969v1",
        "published_date": "2025-08-25T12:32:13+00:00",
        "updated_date": "2025-08-25T12:32:13+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Alexandros Gkillas",
            "Christos Anagnostopoulos",
            "Nikos Piperigkos",
            "Dimitris Tsiktsiris",
            "Theofilos Christodoulou",
            "Theofanis Siamatras",
            "Dimitrios Triantafyllou",
            "Christos Basdekis",
            "Theoktisti Marinopoulou",
            "Panagiotis Lepentsiotis",
            "Elefterios Blitsis",
            "Aggeliki Zacharaki",
            "Nearchos Stylianidis",
            "Leonidas Katelaris",
            "Lamberto Salvan",
            "Aris S. Lalos",
            "Christos Laoudias",
            "Antonios Lalas",
            "Konstantinos Votis"
        ],
        "tldr": "This paper presents a holistic perception system (AutoTRUST) for autonomous vehicles, integrating internal (driver/occupant monitoring with LLM assistant) and external (LiDAR-based semantic segmentation) monitoring, and validated on a real electric vehicle.",
        "tldr_zh": "本文介绍了一个用于自动驾驶汽车的整体感知系统（AutoTRUST），集成了内部（驾驶员/乘客监控与LLM助手）和外部（基于LiDAR的语义分割）监控，并在真实的电动汽车上进行了验证。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulation",
        "summary": "Event cameras offer microsecond latency, high dynamic range, and low power\nconsumption, making them ideal for real-time robotic perception under\nchallenging conditions such as motion blur, occlusion, and illumination\nchanges. However, despite their advantages, synthetic event-based vision\nremains largely unexplored in mainstream robotics simulators. This lack of\nsimulation setup hinders the evaluation of event-driven approaches for robotic\nmanipulation and navigation tasks. This work presents an open-source,\nuser-friendly v2e robotics operating system (ROS) package for Gazebo simulation\nthat enables seamless event stream generation from RGB camera feeds. The\npackage is used to investigate event-based robotic policies (ERP) for real-time\nnavigation and manipulation. Two representative scenarios are evaluated: (1)\nobject following with a mobile robot and (2) object detection and grasping with\na robotic manipulator. Transformer-based ERPs are trained by behavior cloning\nand compared to RGB-based counterparts under various operating conditions.\nExperimental results show that event-guided policies consistently deliver\ncompetitive advantages. The results highlight the potential of event-driven\nperception to improve real-time robotic navigation and manipulation, providing\na foundation for broader integration of event cameras into robotic policy\nlearning. The GitHub repo for the dataset and code:\nhttps://eventbasedvision.github.io/SEBVS/",
        "url": "http://arxiv.org/abs/2508.17643v1",
        "published_date": "2025-08-25T04:14:04+00:00",
        "updated_date": "2025-08-25T04:14:04+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Krishna Vinod",
            "Prithvi Jai Ramesh",
            "Pavan Kumar B N",
            "Bharatesh Chakravarthi"
        ],
        "tldr": "This paper introduces an open-source ROS package for simulating event cameras in Gazebo, enabling the development and evaluation of event-based robotic policies for navigation and manipulation, demonstrating performance advantages over RGB-based approaches. It also provides a dataset and code.",
        "tldr_zh": "该论文介绍了一个开源的ROS软件包，用于在Gazebo中模拟事件相机，从而能够开发和评估基于事件的机器人导航和操作策略，并展示了相对于基于RGB的方法的性能优势。同时，它还提供了一个数据集和代码。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Finding Outliers in a Haystack: Anomaly Detection for Large Pointcloud Scenes",
        "summary": "LiDAR scanning in outdoor scenes acquires accurate distance measurements over\nwide areas, producing large-scale point clouds. Application examples for this\ndata include robotics, automotive vehicles, and land surveillance. During such\napplications, outlier objects from outside the training data will inevitably\nappear. Our research contributes a novel approach to open-set segmentation,\nleveraging the learnings of object defect-detection research. We also draw on\nthe Mamba architecture's strong performance in utilising long-range\ndependencies and scalability to large data. Combining both, we create a\nreconstruction based approach for the task of outdoor scene open-set\nsegmentation. We show that our approach improves performance not only when\napplied to our our own open-set segmentation method, but also when applied to\nexisting methods. Furthermore we contribute a Mamba based architecture which is\ncompetitive with existing voxel-convolution based methods on challenging,\nlarge-scale pointclouds.",
        "url": "http://arxiv.org/abs/2508.17634v1",
        "published_date": "2025-08-25T03:47:33+00:00",
        "updated_date": "2025-08-25T03:47:33+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Ryan Faulkner",
            "Ian Reid",
            "Simon Ratcliffe",
            "Tat-Jun Chin"
        ],
        "tldr": "This paper introduces a Mamba-based reconstruction approach for open-set segmentation in large-scale outdoor point clouds, demonstrating improved performance in anomaly detection compared to existing methods.",
        "tldr_zh": "该论文提出了一种基于 Mamba 的重建方法，用于大规模室外点云中的开放集分割，并在异常检测方面表现出比现有方法更好的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GWM: Towards Scalable Gaussian World Models for Robotic Manipulation",
        "summary": "Training robot policies within a learned world model is trending due to the\ninefficiency of real-world interactions. The established image-based world\nmodels and policies have shown prior success, but lack robust geometric\ninformation that requires consistent spatial and physical understanding of the\nthree-dimensional world, even pre-trained on internet-scale video sources. To\nthis end, we propose a novel branch of world model named Gaussian World Model\n(GWM) for robotic manipulation, which reconstructs the future state by\ninferring the propagation of Gaussian primitives under the effect of robot\nactions. At its core is a latent Diffusion Transformer (DiT) combined with a 3D\nvariational autoencoder, enabling fine-grained scene-level future state\nreconstruction with Gaussian Splatting. GWM can not only enhance the visual\nrepresentation for imitation learning agent by self-supervised future\nprediction training, but can serve as a neural simulator that supports\nmodel-based reinforcement learning. Both simulated and real-world experiments\ndepict that GWM can precisely predict future scenes conditioned on diverse\nrobot actions, and can be further utilized to train policies that outperform\nthe state-of-the-art by impressive margins, showcasing the initial data scaling\npotential of 3D world model.",
        "url": "http://arxiv.org/abs/2508.17600v1",
        "published_date": "2025-08-25T02:01:09+00:00",
        "updated_date": "2025-08-25T02:01:09+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Guanxing Lu",
            "Baoxiong Jia",
            "Puhao Li",
            "Yixin Chen",
            "Ziwei Wang",
            "Yansong Tang",
            "Siyuan Huang"
        ],
        "tldr": "This paper introduces Gaussian World Model (GWM), a novel world model for robotic manipulation that reconstructs future states by inferring the propagation of Gaussian primitives using a latent Diffusion Transformer and a 3D variational autoencoder, enabling fine-grained scene-level future state reconstruction. It shows improved performance over state-of-the-art methods.",
        "tldr_zh": "本文介绍了一种名为高斯世界模型(GWM)的新型机器人操作世界模型，它通过推断高斯基元的传播来重建未来状态，使用潜在扩散转换器和3D变分自动编码器，实现精细的场景级未来状态重建。结果表明，该方法优于最先进的方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TinyGiantVLM: A Lightweight Vision-Language Architecture for Spatial Reasoning under Resource Constraints",
        "summary": "Reasoning about fine-grained spatial relationships in warehouse-scale\nenvironments poses a significant challenge for existing vision-language models\n(VLMs), which often struggle to comprehend 3D layouts, object arrangements, and\nmultimodal cues in real-world industrial settings. In this paper, we present\nTinyGiantVLM, a lightweight and modular two-stage framework designed for\nphysical spatial reasoning, distinguishing itself from traditional geographic\nreasoning in complex logistics scenes. Our approach encodes both global and\nregion-level features from RGB and depth modalities using pretrained visual\nbackbones. To effectively handle the complexity of high-modality inputs and\ndiverse question types, we incorporate a Mixture-of-Experts (MoE) fusion\nmodule, which dynamically combines spatial representations to support\ndownstream reasoning tasks and improve convergence. Training is conducted in a\ntwo-phase strategy: the first phase focuses on generating free-form answers to\nenhance spatial reasoning ability, while the second phase uses normalized\nanswers for evaluation. Evaluated on Track 3 of the AI City Challenge 2025, our\n64M-parameter base model achieved 5th place on the leaderboard with a score of\n66.8861, demonstrating strong performance in bridging visual perception and\nspatial understanding in industrial environments. We further present an\n80M-parameter variant with expanded MoE capacity, which demonstrates improved\nperformance on spatial reasoning tasks.",
        "url": "http://arxiv.org/abs/2508.17595v1",
        "published_date": "2025-08-25T01:36:22+00:00",
        "updated_date": "2025-08-25T01:36:22+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Vinh-Thuan Ly",
            "Hoang M. Truong",
            "Xuan-Huong Nguyen"
        ],
        "tldr": "The paper introduces TinyGiantVLM, a lightweight vision-language model for spatial reasoning in industrial settings, achieving competitive results on the AI City Challenge 2025. It leverages a two-stage training approach and a Mixture-of-Experts (MoE) fusion module to handle multimodal data and complex question types.",
        "tldr_zh": "该论文介绍了 TinyGiantVLM，一种用于工业环境中空间推理的轻量级视觉语言模型，在 AI City Challenge 2025 中取得了有竞争力的结果。它利用两阶段训练方法和混合专家（MoE）融合模块来处理多模态数据和复杂的问题类型。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]