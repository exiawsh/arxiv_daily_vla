[
    {
        "title": "Reinforced Embodied Active Defense: Exploiting Adaptive Interaction for Robust Visual Perception in Adversarial 3D Environments",
        "summary": "Adversarial attacks in 3D environments have emerged as a critical threat to\nthe reliability of visual perception systems, particularly in safety-sensitive\napplications such as identity verification and autonomous driving. These\nattacks employ adversarial patches and 3D objects to manipulate deep neural\nnetwork (DNN) predictions by exploiting vulnerabilities within complex scenes.\nExisting defense mechanisms, such as adversarial training and purification,\nprimarily employ passive strategies to enhance robustness. However, these\napproaches often rely on pre-defined assumptions about adversarial tactics,\nlimiting their adaptability in dynamic 3D settings. To address these\nchallenges, we introduce Reinforced Embodied Active Defense (Rein-EAD), a\nproactive defense framework that leverages adaptive exploration and interaction\nwith the environment to improve perception robustness in 3D adversarial\ncontexts. By implementing a multi-step objective that balances immediate\nprediction accuracy with predictive entropy minimization, Rein-EAD optimizes\ndefense strategies over a multi-step horizon. Additionally, Rein-EAD involves\nan uncertainty-oriented reward-shaping mechanism that facilitates efficient\npolicy updates, thereby reducing computational overhead and supporting\nreal-world applicability without the need for differentiable environments.\nComprehensive experiments validate the effectiveness of Rein-EAD, demonstrating\na substantial reduction in attack success rates while preserving standard\naccuracy across diverse tasks. Notably, Rein-EAD exhibits robust generalization\nto unseen and adaptive attacks, making it suitable for real-world complex\ntasks, including 3D object classification, face recognition and autonomous\ndriving.",
        "url": "http://arxiv.org/abs/2507.18484v1",
        "published_date": "2025-07-24T14:56:21+00:00",
        "updated_date": "2025-07-24T14:56:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xiao Yang",
            "Lingxuan Wu",
            "Lizhong Wang",
            "Chengyang Ying",
            "Hang Su",
            "Jun Zhu"
        ],
        "tldr": "The paper introduces Reinforced Embodied Active Defense (Rein-EAD), a proactive defense framework using reinforcement learning to improve the robustness of visual perception systems in 3D adversarial environments by adaptively interacting with the environment to minimize prediction error and uncertainty.",
        "tldr_zh": "该论文介绍了Reinforced Embodied Active Defense (Rein-EAD)，一种主动防御框架，利用强化学习通过自适应地与环境交互，来提高3D对抗环境中视觉感知系统的鲁棒性，从而最小化预测误差和不确定性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting",
        "summary": "Spatial intelligence is emerging as a transformative frontier in AI, yet it\nremains constrained by the scarcity of large-scale 3D datasets. Unlike the\nabundant 2D imagery, acquiring 3D data typically requires specialized sensors\nand laborious annotation. In this work, we present a scalable pipeline that\nconverts single-view images into comprehensive, scale- and appearance-realistic\n3D representations - including point clouds, camera poses, depth maps, and\npseudo-RGBD - via integrated depth estimation, camera calibration, and scale\ncalibration. Our method bridges the gap between the vast repository of imagery\nand the increasing demand for spatial scene understanding. By automatically\ngenerating authentic, scale-aware 3D data from images, we significantly reduce\ndata collection costs and open new avenues for advancing spatial intelligence.\nWe release two generated spatial datasets, i.e., COCO-3D and Objects365-v2-3D,\nand demonstrate through extensive experiments that our generated data can\nbenefit various 3D tasks, ranging from fundamental perception to MLLM-based\nreasoning. These results validate our pipeline as an effective solution for\ndeveloping AI systems capable of perceiving, understanding, and interacting\nwith physical environments.",
        "url": "http://arxiv.org/abs/2507.18678v1",
        "published_date": "2025-07-24T14:53:26+00:00",
        "updated_date": "2025-07-24T14:53:26+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Xingyu Miao",
            "Haoran Duan",
            "Quanhao Qian",
            "Jiuniu Wang",
            "Yang Long",
            "Ling Shao",
            "Deli Zhao",
            "Ran Xu",
            "Gongjie Zhang"
        ],
        "tldr": "The paper introduces a scalable pipeline for converting 2D images into realistic, scale-aware 3D representations, addressing the lack of large-scale 3D datasets and facilitating advancements in spatial intelligence and MLLM-based reasoning.",
        "tldr_zh": "该论文介绍了一个可扩展的流程，用于将2D图像转换为逼真的、具有尺度感知的3D表示，解决了大规模3D数据集的缺乏问题，并促进了空间智能和基于MLLM的推理的进步。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "HybridTM: Combining Transformer and Mamba for 3D Semantic Segmentation",
        "summary": "Transformer-based methods have demonstrated remarkable capabilities in 3D\nsemantic segmentation through their powerful attention mechanisms, but the\nquadratic complexity limits their modeling of long-range dependencies in\nlarge-scale point clouds. While recent Mamba-based approaches offer efficient\nprocessing with linear complexity, they struggle with feature representation\nwhen extracting 3D features. However, effectively combining these complementary\nstrengths remains an open challenge in this field. In this paper, we propose\nHybridTM, the first hybrid architecture that integrates Transformer and Mamba\nfor 3D semantic segmentation. In addition, we propose the Inner Layer Hybrid\nStrategy, which combines attention and Mamba at a finer granularity, enabling\nsimultaneous capture of long-range dependencies and fine-grained local\nfeatures. Extensive experiments demonstrate the effectiveness and\ngeneralization of our HybridTM on diverse indoor and outdoor datasets.\nFurthermore, our HybridTM achieves state-of-the-art performance on ScanNet,\nScanNet200, and nuScenes benchmarks. The code will be made available at\nhttps://github.com/deepinact/HybridTM.",
        "url": "http://arxiv.org/abs/2507.18575v1",
        "published_date": "2025-07-24T16:48:50+00:00",
        "updated_date": "2025-07-24T16:48:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinyu Wang",
            "Jinghua Hou",
            "Zhe Liu",
            "Yingying Zhu"
        ],
        "tldr": "This paper introduces HybridTM, a novel architecture combining Transformers and Mamba for 3D semantic segmentation, achieving state-of-the-art results on several benchmarks by leveraging their complementary strengths.",
        "tldr_zh": "本文介绍了 HybridTM，一种结合 Transformer 和 Mamba 的新型架构，用于 3D 语义分割。通过利用两者的互补优势，在多个基准测试中实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GaussianFusionOcc: A Seamless Sensor Fusion Approach for 3D Occupancy Prediction Using 3D Gaussians",
        "summary": "3D semantic occupancy prediction is one of the crucial tasks of autonomous\ndriving. It enables precise and safe interpretation and navigation in complex\nenvironments. Reliable predictions rely on effective sensor fusion, as\ndifferent modalities can contain complementary information. Unlike conventional\nmethods that depend on dense grid representations, our approach,\nGaussianFusionOcc, uses semantic 3D Gaussians alongside an innovative sensor\nfusion mechanism. Seamless integration of data from camera, LiDAR, and radar\nsensors enables more precise and scalable occupancy prediction, while 3D\nGaussian representation significantly improves memory efficiency and inference\nspeed. GaussianFusionOcc employs modality-agnostic deformable attention to\nextract essential features from each sensor type, which are then used to refine\nGaussian properties, resulting in a more accurate representation of the\nenvironment. Extensive testing with various sensor combinations demonstrates\nthe versatility of our approach. By leveraging the robustness of multi-modal\nfusion and the efficiency of Gaussian representation, GaussianFusionOcc\noutperforms current state-of-the-art models.",
        "url": "http://arxiv.org/abs/2507.18522v1",
        "published_date": "2025-07-24T15:46:38+00:00",
        "updated_date": "2025-07-24T15:46:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tomislav Pavković",
            "Mohammad-Ali Nikouei Mahani",
            "Johannes Niedermayer",
            "Johannes Betz"
        ],
        "tldr": "GaussianFusionOcc uses 3D Gaussians and deformable attention for multi-sensor fusion (camera, LiDAR, radar) to improve 3D semantic occupancy prediction for autonomous driving, achieving better performance and efficiency than state-of-the-art methods.",
        "tldr_zh": "GaussianFusionOcc使用3D高斯和可变形注意力机制进行多传感器融合（摄像头、激光雷达、雷达），以改进自动驾驶的3D语义占用预测，与现有最佳方法相比，实现了更好的性能和效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Delving into Mapping Uncertainty for Mapless Trajectory Prediction",
        "summary": "Recent advances in autonomous driving are moving towards mapless approaches,\nwhere High-Definition (HD) maps are generated online directly from sensor data,\nreducing the need for expensive labeling and maintenance. However, the\nreliability of these online-generated maps remains uncertain. While\nincorporating map uncertainty into downstream trajectory prediction tasks has\nshown potential for performance improvements, current strategies provide\nlimited insights into the specific scenarios where this uncertainty is\nbeneficial. In this work, we first analyze the driving scenarios in which\nmapping uncertainty has the greatest positive impact on trajectory prediction\nand identify a critical, previously overlooked factor: the agent's kinematic\nstate. Building on these insights, we propose a novel Proprioceptive Scenario\nGating that adaptively integrates map uncertainty into trajectory prediction\nbased on forecasts of the ego vehicle's future kinematics. This lightweight,\nself-supervised approach enhances the synergy between online mapping and\ntrajectory prediction, providing interpretability around where uncertainty is\nadvantageous and outperforming previous integration methods. Additionally, we\nintroduce a Covariance-based Map Uncertainty approach that better aligns with\nmap geometry, further improving trajectory prediction. Extensive ablation\nstudies confirm the effectiveness of our approach, achieving up to 23.6%\nimprovement in mapless trajectory prediction performance over the\nstate-of-the-art method using the real-world nuScenes driving dataset. Our\ncode, data, and models are publicly available at\nhttps://github.com/Ethan-Zheng136/Map-Uncertainty-for-Trajectory-Prediction.",
        "url": "http://arxiv.org/abs/2507.18498v1",
        "published_date": "2025-07-24T15:13:11+00:00",
        "updated_date": "2025-07-24T15:13:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zongzheng Zhang",
            "Xuchong Qiu",
            "Boran Zhang",
            "Guantian Zheng",
            "Xunjiang Gu",
            "Guoxuan Chi",
            "Huan-ang Gao",
            "Leichen Wang",
            "Ziming Liu",
            "Xinrun Li",
            "Igor Gilitschenski",
            "Hongyang Li",
            "Hang Zhao",
            "Hao Zhao"
        ],
        "tldr": "The paper proposes a novel approach to integrate mapping uncertainty into mapless trajectory prediction by considering the agent's kinematic state and introducing a proprioceptive scenario gating mechanism. It achieves significant performance improvements on the nuScenes dataset.",
        "tldr_zh": "该论文提出了一种新方法，通过考虑智能体的运动学状态并引入本体感受场景门控机制，将地图不确定性整合到无地图轨迹预测中。 在nuScenes数据集上实现了显著的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CRUISE: Cooperative Reconstruction and Editing in V2X Scenarios using Gaussian Splatting",
        "summary": "Vehicle-to-everything (V2X) communication plays a crucial role in autonomous\ndriving, enabling cooperation between vehicles and infrastructure. While\nsimulation has significantly contributed to various autonomous driving tasks,\nits potential for data generation and augmentation in V2X scenarios remains\nunderexplored. In this paper, we introduce CRUISE, a comprehensive\nreconstruction-and-synthesis framework designed for V2X driving environments.\nCRUISE employs decomposed Gaussian Splatting to accurately reconstruct\nreal-world scenes while supporting flexible editing. By decomposing dynamic\ntraffic participants into editable Gaussian representations, CRUISE allows for\nseamless modification and augmentation of driving scenes. Furthermore, the\nframework renders images from both ego-vehicle and infrastructure views,\nenabling large-scale V2X dataset augmentation for training and evaluation. Our\nexperimental results demonstrate that: 1) CRUISE reconstructs real-world V2X\ndriving scenes with high fidelity; 2) using CRUISE improves 3D detection across\nego-vehicle, infrastructure, and cooperative views, as well as cooperative 3D\ntracking on the V2X-Seq benchmark; and 3) CRUISE effectively generates\nchallenging corner cases.",
        "url": "http://arxiv.org/abs/2507.18473v1",
        "published_date": "2025-07-24T14:48:44+00:00",
        "updated_date": "2025-07-24T14:48:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haoran Xu",
            "Saining Zhang",
            "Peishuo Li",
            "Baijun Ye",
            "Xiaoxue Chen",
            "Huan-ang Gao",
            "Jv Zheng",
            "Xiaowei Song",
            "Ziqiao Peng",
            "Run Miao",
            "Jinrang Jia",
            "Yifeng Shi",
            "Guangqi Yi",
            "Hang Zhao",
            "Hao Tang",
            "Hongyang Li",
            "Kaicheng Yu",
            "Hao Zhao"
        ],
        "tldr": "The paper introduces CRUISE, a Gaussian Splatting-based framework for reconstructing, editing, and augmenting V2X driving scenarios, demonstrating improved 3D detection and tracking performance and corner case generation.",
        "tldr_zh": "该论文介绍了CRUISE，一个基于高斯溅射的框架，用于重建、编辑和增强V2X驾驶场景，展示了改进的3D检测和跟踪性能以及极端情况生成。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Revisiting Physically Realizable Adversarial Object Attack against LiDAR-based Detection: Clarifying Problem Formulation and Experimental Protocols",
        "summary": "Adversarial robustness in LiDAR-based 3D object detection is a critical\nresearch area due to its widespread application in real-world scenarios. While\nmany digital attacks manipulate point clouds or meshes, they often lack\nphysical realizability, limiting their practical impact. Physical adversarial\nobject attacks remain underexplored and suffer from poor reproducibility due to\ninconsistent setups and hardware differences. To address this, we propose a\ndevice-agnostic, standardized framework that abstracts key elements of physical\nadversarial object attacks, supports diverse methods, and provides open-source\ncode with benchmarking protocols in simulation and real-world settings. Our\nframework enables fair comparison, accelerates research, and is validated by\nsuccessfully transferring simulated attacks to a physical LiDAR system. Beyond\nthe framework, we offer insights into factors influencing attack success and\nadvance understanding of adversarial robustness in real-world LiDAR perception.",
        "url": "http://arxiv.org/abs/2507.18457v1",
        "published_date": "2025-07-24T14:37:00+00:00",
        "updated_date": "2025-07-24T14:37:00+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Luo Cheng",
            "Hanwei Zhang",
            "Lijun Zhang",
            "Holger Hermanns"
        ],
        "tldr": "This paper introduces a device-agnostic framework and benchmarking protocols for physically realizable adversarial object attacks on LiDAR-based 3D object detection systems, addressing issues of reproducibility and fair comparison in this underexplored area.",
        "tldr_zh": "本文提出了一个设备无关的框架和基准测试协议，用于研究基于激光雷达的3D目标检测系统中可物理实现的对抗性目标攻击，旨在解决该领域研究不足且重现性差的问题，并促进公平比较。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PDB-Eval: An Evaluation of Large Multimodal Models for Description and Explanation of Personalized Driving Behavior",
        "summary": "Understanding a driver's behavior and intentions is important for potential\nrisk assessment and early accident prevention. Safety and driver assistance\nsystems can be tailored to individual drivers' behavior, significantly\nenhancing their effectiveness. However, existing datasets are limited in\ndescribing and explaining general vehicle movements based on external visual\nevidence. This paper introduces a benchmark, PDB-Eval, for a detailed\nunderstanding of Personalized Driver Behavior, and aligning Large Multimodal\nModels (MLLMs) with driving comprehension and reasoning. Our benchmark consists\nof two main components, PDB-X and PDB-QA. PDB-X can evaluate MLLMs'\nunderstanding of temporal driving scenes. Our dataset is designed to find valid\nvisual evidence from the external view to explain the driver's behavior from\nthe internal view. To align MLLMs' reasoning abilities with driving tasks, we\npropose PDB-QA as a visual explanation question-answering task for MLLM\ninstruction fine-tuning. As a generic learning task for generative models like\nMLLMs, PDB-QA can bridge the domain gap without harming MLLMs'\ngeneralizability. Our evaluation indicates that fine-tuning MLLMs on\nfine-grained descriptions and explanations can effectively bridge the gap\nbetween MLLMs and the driving domain, which improves zero-shot performance on\nquestion-answering tasks by up to 73.2%. We further evaluate the MLLMs\nfine-tuned on PDB-X in Brain4Cars' intention prediction and AIDE's recognition\ntasks. We observe up to 12.5% performance improvements on the turn intention\nprediction task in Brain4Cars, and consistent performance improvements up to\n11.0% on all tasks in AIDE.",
        "url": "http://arxiv.org/abs/2507.18447v1",
        "published_date": "2025-07-24T14:33:06+00:00",
        "updated_date": "2025-07-24T14:33:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Junda Wu",
            "Jessica Echterhoff",
            "Kyungtae Han",
            "Amr Abdelraouf",
            "Rohit Gupta",
            "Julian McAuley"
        ],
        "tldr": "The paper introduces PDB-Eval, a benchmark for evaluating Large Multimodal Models (MLLMs) in understanding and explaining personalized driving behavior, demonstrating improved performance in intention prediction and recognition tasks after fine-tuning.",
        "tldr_zh": "该论文介绍了PDB-Eval，一个用于评估大型多模态模型（MLLM）在理解和解释个性化驾驶行为方面的基准，展示了在微调后，模型在意图预测和识别任务中的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Boosting Multi-View Indoor 3D Object Detection via Adaptive 3D Volume Construction",
        "summary": "This work presents SGCDet, a novel multi-view indoor 3D object detection\nframework based on adaptive 3D volume construction. Unlike previous approaches\nthat restrict the receptive field of voxels to fixed locations on images, we\nintroduce a geometry and context aware aggregation module to integrate\ngeometric and contextual information within adaptive regions in each image and\ndynamically adjust the contributions from different views, enhancing the\nrepresentation capability of voxel features. Furthermore, we propose a sparse\nvolume construction strategy that adaptively identifies and selects voxels with\nhigh occupancy probabilities for feature refinement, minimizing redundant\ncomputation in free space. Benefiting from the above designs, our framework\nachieves effective and efficient volume construction in an adaptive way. Better\nstill, our network can be supervised using only 3D bounding boxes, eliminating\nthe dependence on ground-truth scene geometry. Experimental results demonstrate\nthat SGCDet achieves state-of-the-art performance on the ScanNet, ScanNet200\nand ARKitScenes datasets. The source code is available at\nhttps://github.com/RM-Zhang/SGCDet.",
        "url": "http://arxiv.org/abs/2507.18331v1",
        "published_date": "2025-07-24T11:58:01+00:00",
        "updated_date": "2025-07-24T11:58:01+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Runmin Zhang",
            "Zhu Yu",
            "Si-Yuan Cao",
            "Lingyu Zhu",
            "Guangyi Zhang",
            "Xiaokai Bai",
            "Hui-Liang Shen"
        ],
        "tldr": "SGCDet introduces a novel multi-view 3D object detection framework with adaptive 3D volume construction and a sparse volume strategy, achieving state-of-the-art results on multiple datasets without requiring ground-truth scene geometry.",
        "tldr_zh": "SGCDet提出了一种新的多视角3D物体检测框架，通过自适应的3D体素构建和稀疏体素策略，在多个数据集上实现了最先进的性能，且不需要真实的场景几何信息。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Adaptive Articulated Object Manipulation On The Fly with Foundation Model Reasoning and Part Grounding",
        "summary": "Articulated objects pose diverse manipulation challenges for robots. Since\ntheir internal structures are not directly observable, robots must adaptively\nexplore and refine actions to generate successful manipulation trajectories.\nWhile existing works have attempted cross-category generalization in adaptive\narticulated object manipulation, two major challenges persist: (1) the\ngeometric diversity of real-world articulated objects complicates visual\nperception and understanding, and (2) variations in object functions and\nmechanisms hinder the development of a unified adaptive manipulation strategy.\nTo address these challenges, we propose AdaRPG, a novel framework that\nleverages foundation models to extract object parts, which exhibit greater\nlocal geometric similarity than entire objects, thereby enhancing visual\naffordance generalization for functional primitive skills. To support this, we\nconstruct a part-level affordance annotation dataset to train the affordance\nmodel. Additionally, AdaRPG utilizes the common knowledge embedded in\nfoundation models to reason about complex mechanisms and generate high-level\ncontrol codes that invoke primitive skill functions based on part affordance\ninference. Simulation and real-world experiments demonstrate AdaRPG's strong\ngeneralization ability across novel articulated object categories.",
        "url": "http://arxiv.org/abs/2507.18276v1",
        "published_date": "2025-07-24T10:25:58+00:00",
        "updated_date": "2025-07-24T10:25:58+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Xiaojie Zhang",
            "Yuanfei Wang",
            "Ruihai Wu",
            "Kunqi Xu",
            "Yu Li",
            "Liuyu Xiang",
            "Hao Dong",
            "Zhaofeng He"
        ],
        "tldr": "The paper introduces AdaRPG, a framework leveraging foundation models for adaptive articulated object manipulation by extracting object parts for enhanced visual affordance generalization and reasoning about mechanisms. It demonstrates strong generalization across novel articulated object categories.",
        "tldr_zh": "该论文提出了AdaRPG框架，利用基础模型进行自适应铰接物体操作，通过提取物体部件来增强视觉可供性泛化和推理机制。实验表明，该框架在新的铰接物体类别中具有很强的泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation",
        "summary": "Semantics-driven 3D spatial constraints align highlevel semantic\nrepresentations with low-level action spaces, facilitating the unification of\ntask understanding and execution in robotic manipulation. The synergistic\nreasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation\nModels (VFMs) enables cross-modal 3D spatial constraint construction.\nNevertheless, existing methods have three key limitations: (1) coarse semantic\ngranularity in constraint modeling, (2) lack of real-time closed-loop planning,\n(3) compromised robustness in semantically diverse environments. To address\nthese challenges, we propose ReSem3D, a unified manipulation framework for\nsemantically diverse environments, leveraging the synergy between VFMs and\nMLLMs to achieve fine-grained visual grounding and dynamically constructs\nhierarchical 3D spatial constraints for real-time manipulation. Specifically,\nthe framework is driven by hierarchical recursive reasoning in MLLMs, which\ninteract with VFMs to automatically construct 3D spatial constraints from\nnatural language instructions and RGB-D observations in two stages: part-level\nextraction and region-level refinement. Subsequently, these constraints are\nencoded as real-time optimization objectives in joint space, enabling reactive\nbehavior to dynamic disturbances. Extensive simulation and real-world\nexperiments are conducted in semantically rich household and sparse chemical\nlab environments. The results demonstrate that ReSem3D performs diverse\nmanipulation tasks under zero-shot conditions, exhibiting strong adaptability\nand generalization. Code and videos are available at\nhttps://github.com/scy-v/ReSem3D and https://resem3d.github.io.",
        "url": "http://arxiv.org/abs/2507.18262v2",
        "published_date": "2025-07-24T10:07:31+00:00",
        "updated_date": "2025-07-25T17:54:43+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.HC",
            "cs.LG"
        ],
        "authors": [
            "Chenyu Su",
            "Weiwei Shang",
            "Chen Qian",
            "Fei Zhang",
            "Shuang Cong"
        ],
        "tldr": "The paper introduces ReSem3D, a novel robotic manipulation framework that uses MLLMs and VFMs to construct fine-grained 3D spatial constraints from language instructions and RGB-D data, enabling robust and generalizable manipulation in diverse environments.",
        "tldr_zh": "该论文介绍了一种名为 ReSem3D 的新型机器人操作框架，该框架利用 MLLM 和 VFM 从语言指令和 RGB-D 数据构建细粒度的 3D 空间约束，从而在各种环境中实现鲁棒且可泛化的操作。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "LONG3R: Long Sequence Streaming 3D Reconstruction",
        "summary": "Recent advancements in multi-view scene reconstruction have been significant,\nyet existing methods face limitations when processing streams of input images.\nThese methods either rely on time-consuming offline optimization or are\nrestricted to shorter sequences, hindering their applicability in real-time\nscenarios. In this work, we propose LONG3R (LOng sequence streaming 3D\nReconstruction), a novel model designed for streaming multi-view 3D scene\nreconstruction over longer sequences. Our model achieves real-time processing\nby operating recurrently, maintaining and updating memory with each new\nobservation. We first employ a memory gating mechanism to filter relevant\nmemory, which, together with a new observation, is fed into a dual-source\nrefined decoder for coarse-to-fine interaction. To effectively capture\nlong-sequence memory, we propose a 3D spatio-temporal memory that dynamically\nprunes redundant spatial information while adaptively adjusting resolution\nalong the scene. To enhance our model's performance on long sequences while\nmaintaining training efficiency, we employ a two-stage curriculum training\nstrategy, each stage targeting specific capabilities. Experiments demonstrate\nthat LONG3R outperforms state-of-the-art streaming methods, particularly for\nlonger sequences, while maintaining real-time inference speed. Project page:\nhttps://zgchen33.github.io/LONG3R/.",
        "url": "http://arxiv.org/abs/2507.18255v1",
        "published_date": "2025-07-24T09:55:20+00:00",
        "updated_date": "2025-07-24T09:55:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhuoguang Chen",
            "Minghui Qin",
            "Tianyuan Yuan",
            "Zhe Liu",
            "Hang Zhao"
        ],
        "tldr": "The paper introduces LONG3R, a novel recurrent model for real-time, streaming multi-view 3D scene reconstruction over long sequences, using memory gating and spatio-temporal memory management with curriculum learning.",
        "tldr_zh": "该论文介绍了LONG3R，一种用于长序列实时流式多视图3D场景重建的新型循环模型，它使用记忆门控和时空记忆管理与课程学习。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "DATA: Domain-And-Time Alignment for High-Quality Feature Fusion in Collaborative Perception",
        "summary": "Feature-level fusion shows promise in collaborative perception (CP) through\nbalanced performance and communication bandwidth trade-off. However, its\neffectiveness critically relies on input feature quality. The acquisition of\nhigh-quality features faces domain gaps from hardware diversity and deployment\nconditions, alongside temporal misalignment from transmission delays. These\nchallenges degrade feature quality with cumulative effects throughout the\ncollaborative network. In this paper, we present the Domain-And-Time Alignment\n(DATA) network, designed to systematically align features while maximizing\ntheir semantic representations for fusion. Specifically, we propose a\nConsistency-preserving Domain Alignment Module (CDAM) that reduces domain gaps\nthrough proximal-region hierarchical downsampling and observability-constrained\ndiscriminator. We further propose a Progressive Temporal Alignment Module\n(PTAM) to handle transmission delays via multi-scale motion modeling and\ntwo-stage compensation. Building upon the aligned features, an Instance-focused\nFeature Aggregation Module (IFAM) is developed to enhance semantic\nrepresentations. Extensive experiments demonstrate that DATA achieves\nstate-of-the-art performance on three typical datasets, maintaining robustness\nwith severe communication delays and pose errors. The code will be released at\nhttps://github.com/ChengchangTian/DATA.",
        "url": "http://arxiv.org/abs/2507.18237v1",
        "published_date": "2025-07-24T09:24:29+00:00",
        "updated_date": "2025-07-24T09:24:29+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chengchang Tian",
            "Jianwei Ma",
            "Yan Huang",
            "Zhanye Chen",
            "Honghao Wei",
            "Hui Zhang",
            "Wei Hong"
        ],
        "tldr": "The paper introduces DATA, a novel network for collaborative perception that addresses domain and temporal misalignment issues in feature-level fusion using domain and time alignment modules and instance-focused feature aggregation, achieving state-of-the-art performance.",
        "tldr_zh": "该论文介绍了DATA，一种用于协同感知的新型网络，它使用域和时间对齐模块以及实例聚焦的特征聚合来解决特征级融合中的域和时间不对齐问题，实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "3D Test-time Adaptation via Graph Spectral Driven Point Shift",
        "summary": "While test-time adaptation (TTA) methods effectively address domain shifts by\ndynamically adapting pre-trained models to target domain data during online\ninference, their application to 3D point clouds is hindered by their irregular\nand unordered structure. Current 3D TTA methods often rely on computationally\nexpensive spatial-domain optimizations and may require additional training\ndata. In contrast, we propose Graph Spectral Domain Test-Time Adaptation\n(GSDTTA), a novel approach for 3D point cloud classification that shifts\nadaptation to the graph spectral domain, enabling more efficient adaptation by\ncapturing global structural properties with fewer parameters. Point clouds in\ntarget domain are represented as outlier-aware graphs and transformed into\ngraph spectral domain by Graph Fourier Transform (GFT). For efficiency,\nadaptation is performed by optimizing only the lowest 10% of frequency\ncomponents, which capture the majority of the point cloud's energy. An inverse\nGFT (IGFT) is then applied to reconstruct the adapted point cloud with the\ngraph spectral-driven point shift. This process is enhanced by an\neigenmap-guided self-training strategy that iteratively refines both the\nspectral adjustments and the model parameters. Experimental results and\nablation studies on benchmark datasets demonstrate the effectiveness of GSDTTA,\noutperforming existing TTA methods for 3D point cloud classification.",
        "url": "http://arxiv.org/abs/2507.18225v1",
        "published_date": "2025-07-24T09:18:39+00:00",
        "updated_date": "2025-07-24T09:18:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xin Wei",
            "Qin Yang",
            "Yijie Fang",
            "Mingrui Zhu",
            "Nannan Wang"
        ],
        "tldr": "This paper introduces GSDTTA, a novel test-time adaptation method for 3D point cloud classification that operates in the graph spectral domain, achieving efficiency by adapting only the lowest frequency components and using an eigenmap-guided self-training strategy.",
        "tldr_zh": "本文介绍了一种新的3D点云分类测试时自适应方法GSDTTA，该方法在图谱域中运行，通过仅调整最低频率分量并采用特征图引导的自训练策略来实现效率。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Unsupervised Domain Adaptation for 3D LiDAR Semantic Segmentation Using Contrastive Learning and Multi-Model Pseudo Labeling",
        "summary": "Addressing performance degradation in 3D LiDAR semantic segmentation due to\ndomain shifts (e.g., sensor type, geographical location) is crucial for\nautonomous systems, yet manual annotation of target data is prohibitive. This\nstudy addresses the challenge using Unsupervised Domain Adaptation (UDA) and\nintroduces a novel two-stage framework to tackle it. Initially, unsupervised\ncontrastive learning at the segment level is used to pre-train a backbone\nnetwork, enabling it to learn robust, domain-invariant features without labels.\nSubsequently, a multi-model pseudo-labeling strategy is introduced, utilizing\nan ensemble of diverse state-of-the-art architectures (including projection,\nvoxel, hybrid, and cylinder-based methods). Predictions from these models are\naggregated via hard voting to generate high-quality, refined pseudo-labels for\nthe unlabeled target domain, mitigating single-model biases. The contrastively\npre-trained network is then fine-tuned using these robust pseudo-labels.\nExperiments adapting from SemanticKITTI to unlabeled target datasets\n(SemanticPOSS, SemanticSlamantic) demonstrate significant improvements in\nsegmentation accuracy compared to direct transfer and single-model UDA\napproaches. These results highlight the effectiveness of combining contrastive\npre-training with refined ensemble pseudo-labeling for bridging complex domain\ngaps without requiring target domain annotations.",
        "url": "http://arxiv.org/abs/2507.18176v1",
        "published_date": "2025-07-24T08:21:43+00:00",
        "updated_date": "2025-07-24T08:21:43+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Abhishek Kaushik",
            "Norbert Haala",
            "Uwe Soergel"
        ],
        "tldr": "This paper proposes a two-stage unsupervised domain adaptation framework for 3D LiDAR semantic segmentation, using contrastive learning for feature pre-training and multi-model pseudo-labeling for robust fine-tuning on unlabeled target data.",
        "tldr_zh": "本文提出了一种用于 3D LiDAR 语义分割的两阶段无监督域自适应框架，该框架使用对比学习进行特征预训练，并使用多模型伪标签进行稳健的微调，从而处理无标签目标数据。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Real-Time Object Detection and Classification using YOLO for Edge FPGAs",
        "summary": "Object detection and classification are crucial tasks across various\napplication domains, particularly in the development of safe and reliable\nAdvanced Driver Assistance Systems (ADAS). Existing deep learning-based methods\nsuch as Convolutional Neural Networks (CNNs), Single Shot Detectors (SSDs), and\nYou Only Look Once (YOLO) have demonstrated high performance in terms of\naccuracy and computational speed when deployed on Field-Programmable Gate\nArrays (FPGAs). However, despite these advances, state-of-the-art YOLO-based\nobject detection and classification systems continue to face challenges in\nachieving resource efficiency suitable for edge FPGA platforms. To address this\nlimitation, this paper presents a resource-efficient real-time object detection\nand classification system based on YOLOv5 optimized for FPGA deployment. The\nproposed system is trained on the COCO and GTSRD datasets and implemented on\nthe Xilinx Kria KV260 FPGA board. Experimental results demonstrate a\nclassification accuracy of 99%, with a power consumption of 3.5W and a\nprocessing speed of 9 frames per second (FPS). These findings highlight the\neffectiveness of the proposed approach in enabling real-time,\nresource-efficient object detection and classification for edge computing\napplications.",
        "url": "http://arxiv.org/abs/2507.18174v1",
        "published_date": "2025-07-24T08:17:37+00:00",
        "updated_date": "2025-07-24T08:17:37+00:00",
        "categories": [
            "cs.CV",
            "cs.AR"
        ],
        "authors": [
            "Rashed Al Amin",
            "Roman Obermaisser"
        ],
        "tldr": "This paper presents a resource-efficient YOLOv5-based object detection system optimized for edge FPGA deployment, achieving 99% accuracy at 9 FPS and 3.5W power consumption on a Xilinx Kria KV260 board.",
        "tldr_zh": "本文提出了一种基于YOLOv5的资源高效的目标检测系统，专为边缘FPGA部署而优化，在Xilinx Kria KV260板上实现了99%的准确率、9 FPS的处理速度和3.5W的功耗。",
        "relevance_score": 7,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]