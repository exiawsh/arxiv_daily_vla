[
    {
        "title": "Opening the Sim-to-Real Door for Humanoid Pixel-to-Action Policy Transfer",
        "summary": "Recent progress in GPU-accelerated, photorealistic simulation has opened a scalable data-generation path for robot learning, where massive physics and visual randomization allow policies to generalize beyond curated environments. Building on these advances, we develop a teacher-student-bootstrap learning framework for vision-based humanoid loco-manipulation, using articulated-object interaction as a representative high-difficulty benchmark. Our approach introduces a staged-reset exploration strategy that stabilizes long-horizon privileged-policy training, and a GRPO-based fine-tuning procedure that mitigates partial observability and improves closed-loop consistency in sim-to-real RL. Trained entirely on simulation data, the resulting policy achieves robust zero-shot performance across diverse door types and outperforms human teleoperators by up to 31.7% in task completion time under the same whole-body control stack. This represents the first humanoid sim-to-real policy capable of diverse articulated loco-manipulation using pure RGB perception.",
        "url": "http://arxiv.org/abs/2512.01061v1",
        "published_date": "2025-11-30T20:07:13+00:00",
        "updated_date": "2025-11-30T20:07:13+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Haoru Xue",
            "Tairan He",
            "Zi Wang",
            "Qingwei Ben",
            "Wenli Xiao",
            "Zhengyi Luo",
            "Xingye Da",
            "Fernando Castañeda",
            "Guanya Shi",
            "Shankar Sastry",
            "Linxi \"Jim\" Fan",
            "Yuke Zhu"
        ],
        "tldr": "This paper presents a sim-to-real framework for humanoid robots to perform loco-manipulation tasks, specifically opening doors, using only RGB perception, achieving superior performance compared to human teleoperators.",
        "tldr_zh": "本文提出了一种用于人形机器人的模拟到真实框架，使其能够仅使用RGB感知执行loco-manipulation任务，特别是开门，并且性能优于人类遥操作员。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FOM-Nav: Frontier-Object Maps for Object Goal Navigation",
        "summary": "This paper addresses the Object Goal Navigation problem, where a robot must efficiently find a target object in an unknown environment. Existing implicit memory-based methods struggle with long-term memory retention and planning, while explicit map-based approaches lack rich semantic information. To address these challenges, we propose FOM-Nav, a modular framework that enhances exploration efficiency through Frontier-Object Maps and vision-language models. Our Frontier-Object Maps are built online and jointly encode spatial frontiers and fine-grained object information. Using this representation, a vision-language model performs multimodal scene understanding and high-level goal prediction, which is executed by a low-level planner for efficient trajectory generation. To train FOM-Nav, we automatically construct large-scale navigation datasets from real-world scanned environments. Extensive experiments validate the effectiveness of our model design and constructed dataset. FOM-Nav achieves state-of-the-art performance on the MP3D and HM3D benchmarks, particularly in navigation efficiency metric SPL, and yields promising results on a real robot.",
        "url": "http://arxiv.org/abs/2512.01009v1",
        "published_date": "2025-11-30T18:16:09+00:00",
        "updated_date": "2025-11-30T18:16:09+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Thomas Chabal",
            "Shizhe Chen",
            "Jean Ponce",
            "Cordelia Schmid"
        ],
        "tldr": "The paper introduces FOM-Nav, a framework for object goal navigation that combines Frontier-Object Maps with vision-language models for improved exploration and state-of-the-art performance on standard benchmarks.",
        "tldr_zh": "该论文介绍了FOM-Nav，一个用于目标物体导航的框架，它结合了Frontier-Object Maps和视觉-语言模型，以提高探索效率，并在标准基准测试中实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Estimation of Kinematic Motion from Dashcam Footage",
        "summary": "The goal of this paper is to explore the accuracy of dashcam footage to predict the actual kinematic motion of a car-like vehicle. Our approach uses ground truth information from the vehicle's on-board data stream, through the controller area network, and a time-synchronized dashboard camera, mounted to a consumer-grade vehicle, for 18 hours of footage and driving. The contributions of the paper include neural network models that allow us to quantify the accuracy of predicting the vehicle speed and yaw, as well as the presence of a lead vehicle, and its relative distance and speed. In addition, the paper describes how other researchers can gather their own data to perform similar experiments, using open-source tools and off-the-shelf technology.",
        "url": "http://arxiv.org/abs/2512.01104v1",
        "published_date": "2025-11-30T22:07:40+00:00",
        "updated_date": "2025-11-30T22:07:40+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Evelyn Zhang",
            "Alex Richardson",
            "Jonathan Sprinkle"
        ],
        "tldr": "This paper explores the accuracy of using dashcam footage to predict a vehicle's kinematic motion, including speed, yaw, and lead vehicle presence/behavior, using neural networks and open-source tools.",
        "tldr_zh": "本文探讨了使用行车记录仪拍摄的视频预测车辆运动学运动的准确性，包括速度、偏航以及前方车辆的存在/行为，使用了神经网络和开源工具。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]