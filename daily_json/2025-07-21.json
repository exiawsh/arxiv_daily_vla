[
    {
        "title": "TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP",
        "summary": "3D visual grounding allows an embodied agent to understand visual information\nin real-world 3D environments based on human instructions, which is crucial for\nembodied intelligence. Existing 3D visual grounding methods typically rely on\nseparate encoders for different modalities (e.g., RGB images, text, and 3D\npoint clouds), resulting in large and complex models that are inefficient to\ntrain. While some approaches use pre-trained 2D multi-modal models like CLIP\nfor 3D tasks, they still struggle with aligning point cloud data to 2D\nencoders. As a result, these methods continue to depend on 3D encoders for\nfeature extraction, further increasing model complexity and training\ninefficiency. In this paper, we propose a unified 2D pre-trained multi-modal\nnetwork to process all three modalities (RGB images, text, and point clouds),\nsignificantly simplifying the architecture. By leveraging a 2D CLIP bi-modal\nmodel with adapter-based fine-tuning, this framework effectively adapts to the\ntri-modal setting, improving both adaptability and performance across\nmodalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module\nis designed to fuse geometric multi-scale features from point clouds and\nimages. We then integrate textual features for final modality fusion and\nintroduce a multi-modal decoder to facilitate deep cross-modal understanding.\nTogether, our method achieves unified feature extraction and fusion across the\nthree modalities, enabling an end-to-end 3D visual grounding model. Compared to\nthe baseline, our method reduces the number of trainable parameters by\napproximately 58\\%, while achieving a 6.52\\% improvement in the 3D detection\ntask and a 6.25\\% improvement in the 3D visual grounding task.",
        "url": "http://arxiv.org/abs/2507.14904v1",
        "published_date": "2025-07-20T10:28:06+00:00",
        "updated_date": "2025-07-20T10:28:06+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Fan Li",
            "Zanyi Wang",
            "Zeyi Huang",
            "Guang Dai",
            "Jingdong Wang",
            "Mengmeng Wang"
        ],
        "tldr": "The paper introduces TriCLIP-3D, a parameter-efficient framework for 3D visual grounding that unifies feature extraction across RGB images, text, and point clouds using a 2D CLIP model and a Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module, achieving performance improvements with fewer trainable parameters.",
        "tldr_zh": "该论文介绍了 TriCLIP-3D，一个参数高效的 3D 视觉定位框架，它使用 2D CLIP 模型和一个几何感知 2D-3D 特征恢复和融合 (GARF) 模块，统一了 RGB 图像、文本和点云的特征提取，并在减少可训练参数的同时实现了性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Light Future: Multimodal Action Frame Prediction via InstructPix2Pix",
        "summary": "Predicting future motion trajectories is a critical capability across domains\nsuch as robotics, autonomous systems, and human activity forecasting, enabling\nsafer and more intelligent decision-making. This paper proposes a novel,\nefficient, and lightweight approach for robot action prediction, offering\nsignificantly reduced computational cost and inference latency compared to\nconventional video prediction models. Importantly, it pioneers the adaptation\nof the InstructPix2Pix model for forecasting future visual frames in robotic\ntasks, extending its utility beyond static image editing. We implement a deep\nlearning-based visual prediction framework that forecasts what a robot will\nobserve 100 frames (10 seconds) into the future, given a current image and a\ntextual instruction. We repurpose and fine-tune the InstructPix2Pix model to\naccept both visual and textual inputs, enabling multimodal future frame\nprediction. Experiments on the RoboTWin dataset (generated based on real-world\nscenarios) demonstrate that our method achieves superior SSIM and PSNR compared\nto state-of-the-art baselines in robot action prediction tasks. Unlike\nconventional video prediction models that require multiple input frames, heavy\ncomputation, and slow inference latency, our approach only needs a single image\nand a text prompt as input. This lightweight design enables faster inference,\nreduced GPU demands, and flexible multimodal control, particularly valuable for\napplications like robotics and sports motion trajectory analytics, where motion\ntrajectory precision is prioritized over visual fidelity.",
        "url": "http://arxiv.org/abs/2507.14809v1",
        "published_date": "2025-07-20T03:57:18+00:00",
        "updated_date": "2025-07-20T03:57:18+00:00",
        "categories": [
            "cs.CV",
            "cs.MM",
            "cs.RO",
            "I.2.10; I.4.8"
        ],
        "authors": [
            "Zesen Zhong",
            "Duomin Zhang",
            "Yijia Li"
        ],
        "tldr": "The paper proposes a lightweight approach for robot action prediction using a fine-tuned InstructPix2Pix model that forecasts future visual frames from a single image and a text prompt, achieving superior SSIM and PSNR compared to state-of-the-art baselines.",
        "tldr_zh": "该论文提出了一种轻量级的机器人动作预测方法，使用微调的InstructPix2Pix模型，通过单张图像和文本提示预测未来的视觉帧，与最先进的基线相比，实现了更高的SSIM和PSNR。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]