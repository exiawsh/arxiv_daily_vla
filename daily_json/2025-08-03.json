[
    {
        "title": "A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding",
        "summary": "Visual grounding aims to identify objects or regions in a scene based on\nnatural language descriptions, essential for spatially aware perception in\nautonomous driving. However, existing visual grounding tasks typically depend\non bounding boxes that often fail to capture fine-grained details. Not all\nvoxels within a bounding box are occupied, resulting in inaccurate object\nrepresentations. To address this, we introduce a benchmark for 3D occupancy\ngrounding in challenging outdoor scenes. Built on the nuScenes dataset, it\nintegrates natural language with voxel-level occupancy annotations, offering\nmore precise object perception compared to the traditional grounding task.\nMoreover, we propose GroundingOcc, an end-to-end model designed for 3D\noccupancy grounding through multi-modal learning. It combines visual, textual,\nand point cloud features to predict object location and occupancy information\nfrom coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder\nfor feature extraction, an occupancy head for voxel-wise predictions, and a\ngrounding head to refine localization. Additionally, a 2D grounding module and\na depth estimation module enhance geometric understanding, thereby boosting\nmodel performance. Extensive experiments on the benchmark demonstrate that our\nmethod outperforms existing baselines on 3D occupancy grounding. The dataset is\navailable at https://github.com/RONINGOD/GroundingOcc.",
        "url": "http://arxiv.org/abs/2508.01197v1",
        "published_date": "2025-08-02T05:05:50+00:00",
        "updated_date": "2025-08-02T05:05:50+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Zhan Shi",
            "Song Wang",
            "Junbo Chen",
            "Jianke Zhu"
        ],
        "tldr": "This paper introduces a new benchmark and model (GroundingOcc) for 3D occupancy grounding using multi-modal data (visual, textual, point cloud) to achieve finer-grained object perception for autonomous driving.",
        "tldr_zh": "本文介绍了一个新的基准测试和模型（GroundingOcc），用于使用多模态数据（视觉，文本，点云）进行3D occupancy grounding，以实现更精细的自动驾驶物体感知。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "OpenGS-Fusion: Open-Vocabulary Dense Mapping with Hybrid 3D Gaussian Splatting for Refined Object-Level Understanding",
        "summary": "Recent advancements in 3D scene understanding have made significant strides\nin enabling interaction with scenes using open-vocabulary queries, particularly\nfor VR/AR and robotic applications. Nevertheless, existing methods are hindered\nby rigid offline pipelines and the inability to provide precise 3D object-level\nunderstanding given open-ended queries. In this paper, we present\nOpenGS-Fusion, an innovative open-vocabulary dense mapping framework that\nimproves semantic modeling and refines object-level understanding.\nOpenGS-Fusion combines 3D Gaussian representation with a Truncated Signed\nDistance Field to facilitate lossless fusion of semantic features on-the-fly.\nFurthermore, we introduce a novel multimodal language-guided approach named\nMLLM-Assisted Adaptive Thresholding, which refines the segmentation of 3D\nobjects by adaptively adjusting similarity thresholds, achieving an improvement\n17\\% in 3D mIoU compared to the fixed threshold strategy. Extensive experiments\ndemonstrate that our method outperforms existing methods in 3D object\nunderstanding and scene reconstruction quality, as well as showcasing its\neffectiveness in language-guided scene interaction. The code is available at\nhttps://young-bit.github.io/opengs-fusion.github.io/ .",
        "url": "http://arxiv.org/abs/2508.01150v1",
        "published_date": "2025-08-02T02:22:36+00:00",
        "updated_date": "2025-08-02T02:22:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Dianyi Yang",
            "Xihan Wang",
            "Yu Gao",
            "Shiyang Liu",
            "Bohan Ren",
            "Yufeng Yue",
            "Yi Yang"
        ],
        "tldr": "OpenGS-Fusion is a novel framework combining 3D Gaussian Splatting and Truncated Signed Distance Fields for on-the-fly open-vocabulary dense mapping and refined object-level understanding, enhanced by a multimodal language-guided thresholding approach.",
        "tldr_zh": "OpenGS-Fusion是一个新颖的框架，结合了3D高斯溅射和截断符号距离场，用于实时开放词汇密集映射和精细的对象级理解，并通过多模态语言引导的阈值处理方法进行增强。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "CP-FREEZER: Latency Attacks against Vehicular Cooperative Perception",
        "summary": "Cooperative perception (CP) enhances situational awareness of connected and\nautonomous vehicles by exchanging and combining messages from multiple agents.\nWhile prior work has explored adversarial integrity attacks that degrade\nperceptual accuracy, little is known about CP's robustness against attacks on\ntimeliness (or availability), a safety-critical requirement for autonomous\ndriving. In this paper, we present CP-FREEZER, the first latency attack that\nmaximizes the computation delay of CP algorithms by injecting adversarial\nperturbation via V2V messages. Our attack resolves several unique challenges,\nincluding the non-differentiability of point cloud preprocessing, asynchronous\nknowledge of the victim's input due to transmission delays, and uses a novel\nloss function that effectively maximizes the execution time of the CP pipeline.\nExtensive experiments show that CP-FREEZER increases end-to-end CP latency by\nover $90\\times$, pushing per-frame processing time beyond 3 seconds with a 100%\nsuccess rate on our real-world vehicle testbed. Our findings reveal a critical\nthreat to the availability of CP systems, highlighting the urgent need for\nrobust defenses.",
        "url": "http://arxiv.org/abs/2508.01062v1",
        "published_date": "2025-08-01T20:34:36+00:00",
        "updated_date": "2025-08-01T20:34:36+00:00",
        "categories": [
            "cs.CR",
            "cs.CV"
        ],
        "authors": [
            "Chenyi Wang",
            "Ruoyu Song",
            "Raymond Muller",
            "Jean-Philippe Monteuuis",
            "Z. Berkay Celik",
            "Jonathan Petit",
            "Ryan Gerdes",
            "Ming Li"
        ],
        "tldr": "The paper introduces CP-FREEZER, a latency attack against vehicular cooperative perception (CP) that significantly increases processing time by injecting adversarial perturbations into V2V messages, exposing vulnerabilities in CP system availability.",
        "tldr_zh": "该论文介绍了CP-FREEZER，一种针对车载协同感知（CP）的延迟攻击，通过在V2V消息中注入对抗性扰动来显著增加处理时间，从而暴露了CP系统可用性的漏洞。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "3DRot: 3D Rotation Augmentation for RGB-Based 3D Tasks",
        "summary": "RGB-based 3D tasks, e.g., 3D detection, depth estimation, 3D keypoint\nestimation, still suffer from scarce, expensive annotations and a thin\naugmentation toolbox, since most image transforms, including resize and\nrotation, disrupt geometric consistency. In this paper, we introduce 3DRot, a\nplug-and-play augmentation that rotates and mirrors images about the camera's\noptical center while synchronously updating RGB images, camera intrinsics,\nobject poses, and 3D annotations to preserve projective geometry-achieving\ngeometry-consistent rotations and reflections without relying on any scene\ndepth. We validate 3DRot with a classical 3D task, monocular 3D detection. On\nSUN RGB-D dataset, 3DRot raises $IoU_{3D}$ from 43.21 to 44.51, cuts rotation\nerror (ROT) from 22.91$^\\circ$ to 20.93$^\\circ$, and boosts $mAP_{0.5}$ from\n35.70 to 38.11. As a comparison, Cube R-CNN adds 3 other datasets together with\nSUN RGB-D for monocular 3D estimation, with a similar mechanism and test\ndataset, increases $IoU_{3D}$ from 36.2 to 37.8, boosts $mAP_{0.5}$ from 34.7\nto 35.4. Because it operates purely through camera-space transforms, 3DRot is\nreadily transferable to other 3D tasks.",
        "url": "http://arxiv.org/abs/2508.01423v1",
        "published_date": "2025-08-02T16:08:16+00:00",
        "updated_date": "2025-08-02T16:08:16+00:00",
        "categories": [
            "cs.CV",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Shitian Yang",
            "Deyu Li",
            "Xiaoke Jiang",
            "Lei Zhang"
        ],
        "tldr": "The paper introduces 3DRot, a geometry-consistent data augmentation technique for RGB-based 3D tasks that rotates and mirrors images while updating camera intrinsics and object poses, demonstrating improvements on monocular 3D detection.",
        "tldr_zh": "该论文介绍了 3DRot，一种几何一致的数据增强技术，用于基于RGB的3D任务。该方法旋转和镜像图像，同时更新相机内参和物体姿态，并在单目3D检测上展示了改进。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "RoadMamba: A Dual Branch Visual State Space Model for Road Surface Classification",
        "summary": "Acquiring the road surface conditions in advance based on visual technologies\nprovides effective information for the planning and control system of\nautonomous vehicles, thus improving the safety and driving comfort of the\nvehicles. Recently, the Mamba architecture based on state-space models has\nshown remarkable performance in visual processing tasks, benefiting from the\nefficient global receptive field. However, existing Mamba architectures\nstruggle to achieve state-of-the-art visual road surface classification due to\ntheir lack of effective extraction of the local texture of the road surface. In\nthis paper, we explore for the first time the potential of visual Mamba\narchitectures for road surface classification task and propose a method that\neffectively combines local and global perception, called RoadMamba.\nSpecifically, we utilize the Dual State Space Model (DualSSM) to effectively\nextract the global semantics and local texture of the road surface and decode\nand fuse the dual features through the Dual Attention Fusion (DAF). In\naddition, we propose a dual auxiliary loss to explicitly constrain dual\nbranches, preventing the network from relying only on global semantic\ninformation from the deep large receptive field and ignoring the local texture.\nThe proposed RoadMamba achieves the state-of-the-art performance in experiments\non a large-scale road surface classification dataset containing 1 million\nsamples.",
        "url": "http://arxiv.org/abs/2508.01210v1",
        "published_date": "2025-08-02T05:54:38+00:00",
        "updated_date": "2025-08-02T05:54:38+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianze Wang",
            "Zhang Zhang",
            "Chao Yue",
            "Nuoran Li",
            "Chao Sun"
        ],
        "tldr": "RoadMamba introduces a novel Dual State Space Model (DualSSM) architecture, combined with Dual Attention Fusion (DAF) and dual auxiliary loss, for improved road surface classification, achieving state-of-the-art performance on a large dataset.",
        "tldr_zh": "RoadMamba 提出了一种新颖的双状态空间模型（DualSSM）架构，结合双重注意力融合（DAF）和双重辅助损失，以改进路面分类，并在大型数据集上实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "No Pose at All: Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views",
        "summary": "We introduce SPFSplat, an efficient framework for 3D Gaussian splatting from\nsparse multi-view images, requiring no ground-truth poses during training or\ninference. It employs a shared feature extraction backbone, enabling\nsimultaneous prediction of 3D Gaussian primitives and camera poses in a\ncanonical space from unposed inputs within a single feed-forward step.\nAlongside the rendering loss based on estimated novel-view poses, a\nreprojection loss is integrated to enforce the learning of pixel-aligned\nGaussian primitives for enhanced geometric constraints. This pose-free training\nparadigm and efficient one-step feed-forward design make SPFSplat well-suited\nfor practical applications. Remarkably, despite the absence of pose\nsupervision, SPFSplat achieves state-of-the-art performance in novel view\nsynthesis even under significant viewpoint changes and limited image overlap.\nIt also surpasses recent methods trained with geometry priors in relative pose\nestimation. Code and trained models are available on our project page:\nhttps://ranrhuang.github.io/spfsplat/.",
        "url": "http://arxiv.org/abs/2508.01171v1",
        "published_date": "2025-08-02T03:19:13+00:00",
        "updated_date": "2025-08-02T03:19:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ranran Huang",
            "Krystian Mikolajczyk"
        ],
        "tldr": "SPFSplat enables 3D Gaussian splatting from sparse multi-view images without pose information, achieving state-of-the-art novel view synthesis.",
        "tldr_zh": "SPFSplat 实现了从稀疏多视图图像进行三维高斯 Splatting，无需姿态信息，并实现了最先进的新视角合成。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DELTAv2: Accelerating Dense 3D Tracking",
        "summary": "We propose a novel algorithm for accelerating dense long-term 3D point\ntracking in videos. Through analysis of existing state-of-the-art methods, we\nidentify two major computational bottlenecks. First, transformer-based\niterative tracking becomes expensive when handling a large number of\ntrajectories. To address this, we introduce a coarse-to-fine strategy that\nbegins tracking with a small subset of points and progressively expands the set\nof tracked trajectories. The newly added trajectories are initialized using a\nlearnable interpolation module, which is trained end-to-end alongside the\ntracking network. Second, we propose an optimization that significantly reduces\nthe cost of correlation feature computation, another key bottleneck in prior\nmethods. Together, these improvements lead to a 5-100x speedup over existing\napproaches while maintaining state-of-the-art tracking accuracy.",
        "url": "http://arxiv.org/abs/2508.01170v1",
        "published_date": "2025-08-02T03:15:47+00:00",
        "updated_date": "2025-08-02T03:15:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tuan Duc Ngo",
            "Ashkan Mirzaei",
            "Guocheng Qian",
            "Hanwen Liang",
            "Chuang Gan",
            "Evangelos Kalogerakis",
            "Peter Wonka",
            "Chaoyang Wang"
        ],
        "tldr": "DELTAv2 accelerates dense 3D point tracking in videos by addressing computational bottlenecks in transformer-based methods through a coarse-to-fine tracking strategy and optimized correlation feature computation, achieving significant speedups while maintaining accuracy.",
        "tldr_zh": "DELTAv2通过粗到精的跟踪策略和优化的相关特征计算，解决了基于Transformer方法中存在的计算瓶颈，从而加速了视频中密集的3D点跟踪，并在保持精度的同时实现了显著的加速。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Hestia: Hierarchical Next-Best-View Exploration for Systematic Intelligent Autonomous Data Collection",
        "summary": "Advances in 3D reconstruction and novel view synthesis have enabled\nefficient, photorealistic rendering, but the data collection process remains\nlargely manual, making it time-consuming and labor-intensive. To address the\nchallenges, this study introduces Hierarchical Next-Best-View Exploration for\nSystematic Intelligent Autonomous Data Collection (Hestia), which leverages\nreinforcement learning to learn a generalizable policy for 5-DoF next-best\nviewpoint prediction. Unlike prior approaches, Hestia systematically defines\nthe next-best-view task by proposing core components such as dataset choice,\nobservation design, action space, reward calculation, and learning schemes,\nforming a foundation for the planner. Hestia goes beyond prior next-best-view\napproaches and traditional capture systems through integration and validation\nin a real-world setup, where a drone serves as a mobile sensor for active scene\nexploration. Experimental results show that Hestia performs robustly across\nthree datasets and translated object settings in the NVIDIA IsaacLab\nenvironment, and proves feasible for real-world deployment.",
        "url": "http://arxiv.org/abs/2508.01014v1",
        "published_date": "2025-08-01T18:27:23+00:00",
        "updated_date": "2025-08-01T18:27:23+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Cheng-You Lu",
            "Zhuoli Zhuang",
            "Nguyen Thanh Trung Le",
            "Da Xiao",
            "Yu-Cheng Chang",
            "Thomas Do",
            "Srinath Sridhar",
            "Chin-teng Lin"
        ],
        "tldr": "The paper introduces Hestia, a reinforcement learning-based approach for autonomous next-best-view planning for 3D data collection using a drone, validated in simulation and real-world settings.",
        "tldr_zh": "本文介绍了一种名为Hestia的基于强化学习的自主下一最佳视角规划方法，用于使用无人机进行3D数据收集，并在仿真和真实环境中进行了验证。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "3D Reconstruction via Incremental Structure From Motion",
        "summary": "Accurate 3D reconstruction from unstructured image collections is a key\nrequirement in applications such as robotics, mapping, and scene understanding.\nWhile global Structure from Motion (SfM) techniques rely on full image\nconnectivity and can be sensitive to noise or missing data, incremental SfM\noffers a more flexible alternative. By progressively incorporating new views\ninto the reconstruction, it enables the system to recover scene structure and\ncamera motion even in sparse or partially overlapping datasets. In this paper,\nwe present a detailed implementation of the incremental SfM pipeline, focusing\non the consistency of geometric estimation and the effect of iterative\nrefinement through bundle adjustment. We demonstrate the approach using a real\ndataset and assess reconstruction quality through reprojection error and camera\ntrajectory coherence. The results support the practical utility of incremental\nSfM as a reliable method for sparse 3D reconstruction in visually structured\nenvironments.",
        "url": "http://arxiv.org/abs/2508.01019v1",
        "published_date": "2025-08-01T18:45:05+00:00",
        "updated_date": "2025-08-01T18:45:05+00:00",
        "categories": [
            "cs.CV",
            "math.OC"
        ],
        "authors": [
            "Muhammad Zeeshan",
            "Umer Zaki",
            "Syed Ahmed Pasha",
            "Zaar Khizar"
        ],
        "tldr": "This paper presents a detailed implementation of incremental Structure from Motion (SfM) for 3D reconstruction from unstructured image collections, focusing on geometric consistency and iterative refinement, and demonstrating its utility on a real dataset.",
        "tldr_zh": "本文详细介绍了增量式运动结构 (SfM) 的实现，用于从非结构化图像集合中进行 3D 重建，重点关注几何一致性和迭代优化，并在真实数据集上展示了其效用。",
        "relevance_score": 8,
        "novelty_claim_score": 5,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]