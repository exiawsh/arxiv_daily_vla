[
    {
        "title": "SAMFusion: Sensor-Adaptive Multimodal Fusion for 3D Object Detection in Adverse Weather",
        "summary": "Multimodal sensor fusion is an essential capability for autonomous robots,\nenabling object detection and decision-making in the presence of failing or\nuncertain inputs. While recent fusion methods excel in normal environmental\nconditions, these approaches fail in adverse weather, e.g., heavy fog, snow, or\nobstructions due to soiling. We introduce a novel multi-sensor fusion approach\ntailored to adverse weather conditions. In addition to fusing RGB and LiDAR\nsensors, which are employed in recent autonomous driving literature, our sensor\nfusion stack is also capable of learning from NIR gated camera and radar\nmodalities to tackle low light and inclement weather. We fuse multimodal sensor\ndata through attentive, depth-based blending schemes, with learned refinement\non the Bird's Eye View (BEV) plane to combine image and range features\neffectively. Our detections are predicted by a transformer decoder that weighs\nmodalities based on distance and visibility. We demonstrate that our method\nimproves the reliability of multimodal sensor fusion in autonomous vehicles\nunder challenging weather conditions, bridging the gap between ideal conditions\nand real-world edge cases. Our approach improves average precision by 17.2 AP\ncompared to the next best method for vulnerable pedestrians in long distances\nand challenging foggy scenes. Our project page is available at\nhttps://light.princeton.edu/samfusion/",
        "url": "http://arxiv.org/abs/2508.16408v1",
        "published_date": "2025-08-22T14:20:46+00:00",
        "updated_date": "2025-08-22T14:20:46+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Edoardo Palladin",
            "Roland Dietze",
            "Praveen Narayanan",
            "Mario Bijelic",
            "Felix Heide"
        ],
        "tldr": "The paper introduces SAMFusion, a novel multi-sensor fusion approach using RGB, LiDAR, NIR gated camera and radar to improve 3D object detection for autonomous vehicles in adverse weather conditions, demonstrating a significant improvement in average precision compared to existing methods.",
        "tldr_zh": "该论文介绍了 SAMFusion，一种新型多传感器融合方法，它使用 RGB、LiDAR、NIR 门控相机和雷达来提高自动驾驶车辆在恶劣天气条件下的 3D 对象检测能力，与现有方法相比，平均精度有了显著提高。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation",
        "summary": "Recent advancements in video generation have substantially improved visual\nquality and temporal coherence, making these models increasingly appealing for\napplications such as autonomous driving, particularly in the context of driving\nsimulation and so-called \"world models\". In this work, we investigate the\neffects of existing fine-tuning video generation approaches on structured\ndriving datasets and uncover a potential trade-off: although visual fidelity\nimproves, spatial accuracy in modeling dynamic elements may degrade. We\nattribute this degradation to a shift in the alignment between visual quality\nand dynamic understanding objectives. In datasets with diverse scene structures\nwithin temporal space, where objects or perspective shift in varied ways, these\nobjectives tend to highly correlated. However, the very regular and repetitive\nnature of driving scenes allows visual quality to improve by modeling dominant\nscene motion patterns, without necessarily preserving fine-grained dynamic\nbehavior. As a result, fine-tuning encourages the model to prioritize\nsurface-level realism over dynamic accuracy. To further examine this\nphenomenon, we show that simple continual learning strategies, such as replay\nfrom diverse domains, can offer a balanced alternative by preserving spatial\naccuracy while maintaining strong visual quality.",
        "url": "http://arxiv.org/abs/2508.16512v1",
        "published_date": "2025-08-22T16:35:19+00:00",
        "updated_date": "2025-08-22T16:35:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chun-Peng Chang",
            "Chen-Yu Wang",
            "Julian Schmidt",
            "Holger Caesar",
            "Alain Pagani"
        ],
        "tldr": "This paper investigates the trade-off between visual fidelity and spatial accuracy when fine-tuning video generation models for driving simulation, finding that fine-tuning can degrade dynamic element modeling and proposing continual learning as a solution.",
        "tldr_zh": "该论文研究了在微调用于驾驶模拟的视频生成模型时，视觉保真度和空间精度之间的权衡。研究发现微调会降低动态元素建模的性能，并提出了持续学习作为解决方案。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CoVeRaP: Cooperative Vehicular Perception through mmWave FMCW Radars",
        "summary": "Automotive FMCW radars remain reliable in rain and glare, yet their sparse,\nnoisy point clouds constrain 3-D object detection. We therefore release\nCoVeRaP, a 21 k-frame cooperative dataset that time-aligns radar, camera, and\nGPS streams from multiple vehicles across diverse manoeuvres. Built on this\ndata, we propose a unified cooperative-perception framework with middle- and\nlate-fusion options. Its baseline network employs a multi-branch PointNet-style\nencoder enhanced with self-attention to fuse spatial, Doppler, and intensity\ncues into a common latent space, which a decoder converts into 3-D bounding\nboxes and per-point depth confidence. Experiments show that middle fusion with\nintensity encoding boosts mean Average Precision by up to 9x at IoU 0.9 and\nconsistently outperforms single-vehicle baselines. CoVeRaP thus establishes the\nfirst reproducible benchmark for multi-vehicle FMCW-radar perception and\ndemonstrates that affordable radar sharing markedly improves detection\nrobustness. Dataset and code are publicly available to encourage further\nresearch.",
        "url": "http://arxiv.org/abs/2508.16030v1",
        "published_date": "2025-08-22T01:14:27+00:00",
        "updated_date": "2025-08-22T01:14:27+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.NI"
        ],
        "authors": [
            "Jinyue Song",
            "Hansol Ku",
            "Jayneel Vora",
            "Nelson Lee",
            "Ahmad Kamari",
            "Prasant Mohapatra",
            "Parth Pathak"
        ],
        "tldr": "The paper introduces CoVeRaP, a multi-vehicle radar and camera dataset for cooperative perception, and a baseline framework demonstrating improved 3D object detection through radar sharing, especially with intensity encoding.",
        "tldr_zh": "该论文介绍了CoVeRaP，一个用于协同感知的多车辆雷达和相机数据集，以及一个基线框架，该框架展示了通过雷达共享改进的3D物体检测，尤其是在强度编码方面。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]