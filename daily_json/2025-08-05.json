[
    {
        "title": "Dream-to-Recon: Monocular 3D Reconstruction with Diffusion-Depth Distillation from Single Images",
        "summary": "Volumetric scene reconstruction from a single image is crucial for a broad\nrange of applications like autonomous driving and robotics. Recent volumetric\nreconstruction methods achieve impressive results, but generally require\nexpensive 3D ground truth or multi-view supervision. We propose to leverage\npre-trained 2D diffusion models and depth prediction models to generate\nsynthetic scene geometry from a single image. This can then be used to distill\na feed-forward scene reconstruction model. Our experiments on the challenging\nKITTI-360 and Waymo datasets demonstrate that our method matches or outperforms\nstate-of-the-art baselines that use multi-view supervision, and offers unique\nadvantages, for example regarding dynamic scenes.",
        "url": "http://arxiv.org/abs/2508.02323v1",
        "published_date": "2025-08-04T11:43:12+00:00",
        "updated_date": "2025-08-04T11:43:12+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Philipp Wulff",
            "Felix Wimbauer",
            "Dominik Muhle",
            "Daniel Cremers"
        ],
        "tldr": "This paper proposes a novel single-image 3D reconstruction method using pre-trained 2D diffusion and depth prediction models to generate synthetic 3D scene geometry, achieving state-of-the-art performance on autonomous driving datasets.",
        "tldr_zh": "本文提出了一种新颖的单图像3D重建方法，该方法使用预训练的2D扩散和深度预测模型来生成合成的3D场景几何，在自动驾驶数据集上实现了最先进的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "AID4AD: Aerial Image Data for Automated Driving Perception",
        "summary": "This work investigates the integration of spatially aligned aerial imagery\ninto perception tasks for automated vehicles (AVs). As a central contribution,\nwe present AID4AD, a publicly available dataset that augments the nuScenes\ndataset with high-resolution aerial imagery precisely aligned to its local\ncoordinate system. The alignment is performed using SLAM-based point cloud maps\nprovided by nuScenes, establishing a direct link between aerial data and\nnuScenes local coordinate system. To ensure spatial fidelity, we propose an\nalignment workflow that corrects for localization and projection distortions. A\nmanual quality control process further refines the dataset by identifying a set\nof high-quality alignments, which we publish as ground truth to support future\nresearch on automated registration. We demonstrate the practical value of\nAID4AD in two representative tasks: in online map construction, aerial imagery\nserves as a complementary input that improves the mapping process; in motion\nprediction, it functions as a structured environmental representation that\nreplaces high-definition maps. Experiments show that aerial imagery leads to a\n15-23% improvement in map construction accuracy and a 2% gain in trajectory\nprediction performance. These results highlight the potential of aerial imagery\nas a scalable and adaptable source of environmental context in automated\nvehicle systems, particularly in scenarios where high-definition maps are\nunavailable, outdated, or costly to maintain. AID4AD, along with evaluation\ncode and pretrained models, is publicly released to foster further research in\nthis direction: https://github.com/DriverlessMobility/AID4AD.",
        "url": "http://arxiv.org/abs/2508.02140v1",
        "published_date": "2025-08-04T07:38:18+00:00",
        "updated_date": "2025-08-04T07:38:18+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Daniel Lengerer",
            "Mathias Pechinger",
            "Klaus Bogenberger",
            "Carsten Markgraf"
        ],
        "tldr": "The paper introduces AID4AD, a new dataset augmenting nuScenes with spatially aligned aerial imagery, and demonstrates its utility for online map construction and motion prediction, achieving performance improvements in both tasks.",
        "tldr_zh": "该论文介绍了AID4AD数据集，它通过空间对齐的航拍图像增强了nuScenes数据集，并展示了其在在线地图构建和运动预测中的效用，并在两项任务中都实现了性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 10,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Bench2ADVLM: A Closed-Loop Benchmark for Vision-language Models in Autonomous Driving",
        "summary": "Vision-Language Models (VLMs) have recently emerged as a promising paradigm\nin autonomous driving (AD). However, current performance evaluation protocols\nfor VLM-based AD systems (ADVLMs) are predominantly confined to open-loop\nsettings with static inputs, neglecting the more realistic and informative\nclosed-loop setting that captures interactive behavior, feedback resilience,\nand real-world safety. To address this, we introduce Bench2ADVLM, a unified\nhierarchical closed-loop evaluation framework for real-time, interactive\nassessment of ADVLMs across both simulation and physical platforms. Inspired by\ndual-process theories of cognition, we first adapt diverse ADVLMs to simulation\nenvironments via a dual-system adaptation architecture. In this design,\nheterogeneous high-level driving commands generated by target ADVLMs (fast\nsystem) are interpreted by a general-purpose VLM (slow system) into\nstandardized mid-level control actions suitable for execution in simulation. To\nbridge the gap between simulation and reality, we design a physical control\nabstraction layer that translates these mid-level actions into low-level\nactuation signals, enabling, for the first time, closed-loop testing of ADVLMs\non physical vehicles. To enable more comprehensive evaluation, Bench2ADVLM\nintroduces a self-reflective scenario generation module that automatically\nexplores model behavior and uncovers potential failure modes for\nsafety-critical scenario generation. Overall, Bench2ADVLM establishes a\nhierarchical evaluation pipeline that seamlessly integrates high-level abstract\nreasoning, mid-level simulation actions, and low-level real-world execution.\nExperiments on diverse scenarios across multiple state-of-the-art ADVLMs and\nphysical platforms validate the diagnostic strength of our framework, revealing\nthat existing ADVLMs still exhibit limited performance under closed-loop\nconditions.",
        "url": "http://arxiv.org/abs/2508.02028v1",
        "published_date": "2025-08-04T03:43:23+00:00",
        "updated_date": "2025-08-04T03:43:23+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianyuan Zhang",
            "Ting Jin",
            "Lu Wang",
            "Jiangfan Liu",
            "Siyuan Liang",
            "Mingchuan Zhang",
            "Aishan Liu",
            "Xianglong Liu"
        ],
        "tldr": "The paper introduces Bench2ADVLM, a closed-loop benchmark for evaluating Vision-Language Models in autonomous driving, addressing the limitations of existing open-loop evaluation methods by incorporating real-time interaction, feedback, and physical platform testing.",
        "tldr_zh": "该论文介绍了Bench2ADVLM，一个闭环基准，用于评估自动驾驶中的视觉-语言模型，通过结合实时交互、反馈和物理平台测试，解决了现有开环评估方法的局限性。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming",
        "summary": "Vision-Language Navigation (VLN) tasks often leverage panoramic RGB and depth\ninputs to provide rich spatial cues for action planning, but these sensors can\nbe costly or less accessible in real-world deployments. Recent approaches based\non Vision-Language Action (VLA) models achieve strong results with monocular\ninput, yet they still lag behind methods using panoramic RGB-D information. We\npresent MonoDream, a lightweight VLA framework that enables monocular agents to\nlearn a Unified Navigation Representation (UNR). This shared feature\nrepresentation jointly aligns navigation-relevant visual semantics (e.g.,\nglobal layout, depth, and future cues) and language-grounded action intent,\nenabling more reliable action prediction. MonoDream further introduces Latent\nPanoramic Dreaming (LPD) tasks to supervise the UNR, which train the model to\npredict latent features of panoramic RGB and depth observations at both current\nand future steps based on only monocular input. Experiments on multiple VLN\nbenchmarks show that MonoDream consistently improves monocular navigation\nperformance and significantly narrows the gap with panoramic-based agents.",
        "url": "http://arxiv.org/abs/2508.02549v1",
        "published_date": "2025-08-04T16:01:30+00:00",
        "updated_date": "2025-08-04T16:01:30+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Shuo Wang",
            "Yongcai Wang",
            "Wanting Li",
            "Yucheng Wang",
            "Maiyue Chen",
            "Kaihui Wang",
            "Zhizhong Su",
            "Xudong Cai",
            "Yeying Jin",
            "Deying Li",
            "Zhaoxin Fan"
        ],
        "tldr": "MonoDream is a monocular vision-language navigation framework that uses Latent Panoramic Dreaming to predict panoramic features, improving performance and closing the gap with panoramic RGB-D methods.",
        "tldr_zh": "MonoDream是一个单目视觉-语言导航框架，它使用潜在全景梦想来预测全景特征，从而提高性能并缩小与全景RGB-D方法的差距。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Uncertainty Estimation for Novel Views in Gaussian Splatting from Primitive-Based Representations of Error and Visibility",
        "summary": "In this work, we present a novel method for uncertainty estimation (UE) in\nGaussian Splatting. UE is crucial for using Gaussian Splatting in critical\napplications such as robotics and medicine. Previous methods typically estimate\nthe variance of Gaussian primitives and use the rendering process to obtain\npixel-wise uncertainties. Our method establishes primitive representations of\nerror and visibility of trainings views, which carries meaningful uncertainty\ninformation. This representation is obtained by projection of training error\nand visibility onto the primitives. Uncertainties of novel views are obtained\nby rendering the primitive representations of uncertainty for those novel\nviews, yielding uncertainty feature maps. To aggregate these uncertainty\nfeature maps of novel views, we perform a pixel-wise regression on holdout\ndata. In our experiments, we analyze the different components of our method,\ninvestigating various combinations of uncertainty feature maps and regression\nmodels. Furthermore, we considered the effect of separating splatting into\nforeground and background. Our UEs show high correlations to true errors,\noutperforming state-of-the-art methods, especially on foreground objects. The\ntrained regression models show generalization capabilities to new scenes,\nallowing uncertainty estimation without the need for holdout data.",
        "url": "http://arxiv.org/abs/2508.02443v1",
        "published_date": "2025-08-04T14:02:20+00:00",
        "updated_date": "2025-08-04T14:02:20+00:00",
        "categories": [
            "cs.GR",
            "cs.CV"
        ],
        "authors": [
            "Thomas Gottwald",
            "Edgar Heinert",
            "Matthias Rottmann"
        ],
        "tldr": "This paper presents a novel uncertainty estimation method for Gaussian Splatting by projecting training error and visibility onto primitives, achieving state-of-the-art performance, especially on foreground objects, and demonstrating generalization to new scenes.",
        "tldr_zh": "本文提出了一种新颖的 Gaussian Splatting 不确定性估计方法，通过将训练误差和可见性投影到基元上，实现了最先进的性能，尤其是在前景对象上，并展示了对新场景的泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Improving Generalization of Language-Conditioned Robot Manipulation",
        "summary": "The control of robots for manipulation tasks generally relies on visual\ninput. Recent advances in vision-language models (VLMs) enable the use of\nnatural language instructions to condition visual input and control robots in a\nwider range of environments. However, existing methods require a large amount\nof data to fine-tune VLMs for operating in unseen environments. In this paper,\nwe present a framework that learns object-arrangement tasks from just a few\ndemonstrations. We propose a two-stage framework that divides\nobject-arrangement tasks into a target localization stage, for picking the\nobject, and a region determination stage for placing the object. We present an\ninstance-level semantic fusion module that aligns the instance-level image\ncrops with the text embedding, enabling the model to identify the target\nobjects defined by the natural language instructions. We validate our method on\nboth simulation and real-world robotic environments. Our method, fine-tuned\nwith a few demonstrations, improves generalization capability and demonstrates\nzero-shot ability in real-robot manipulation scenarios.",
        "url": "http://arxiv.org/abs/2508.02405v1",
        "published_date": "2025-08-04T13:29:26+00:00",
        "updated_date": "2025-08-04T13:29:26+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Chenglin Cui",
            "Chaoran Zhu",
            "Changjae Oh",
            "Andrea Cavallaro"
        ],
        "tldr": "This paper introduces a two-stage framework with instance-level semantic fusion to improve the generalization of language-conditioned robot manipulation for object arrangement tasks, requiring only a few demonstrations.",
        "tldr_zh": "本文提出了一种两阶段框架，结合实例级语义融合，以提高语言条件下的机器人操作在物体排列任务中的泛化能力，且仅需少量演示。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "mmWave Radar-Based Non-Line-of-Sight Pedestrian Localization at T-Junctions Utilizing Road Layout Extraction via Camera",
        "summary": "Pedestrians Localization in Non-Line-of-Sight (NLoS) regions within urban\nenvironments poses a significant challenge for autonomous driving systems.\nWhile mmWave radar has demonstrated potential for detecting objects in such\nscenarios, the 2D radar point cloud (PCD) data is susceptible to distortions\ncaused by multipath reflections, making accurate spatial inference difficult.\nAdditionally, although camera images provide high-resolution visual\ninformation, they lack depth perception and cannot directly observe objects in\nNLoS regions. In this paper, we propose a novel framework that interprets radar\nPCD through road layout inferred from camera for localization of NLoS\npedestrians. The proposed method leverages visual information from the camera\nto interpret 2D radar PCD, enabling spatial scene reconstruction. The\neffectiveness of the proposed approach is validated through experiments\nconducted using a radar-camera system mounted on a real vehicle. The\nlocalization performance is evaluated using a dataset collected in outdoor NLoS\ndriving environments, demonstrating the practical applicability of the method.",
        "url": "http://arxiv.org/abs/2508.02348v1",
        "published_date": "2025-08-04T12:31:11+00:00",
        "updated_date": "2025-08-04T12:31:11+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Byeonggyu Park",
            "Hee-Yeun Kim",
            "Byonghyok Choi",
            "Hansang Cho",
            "Byungkwan Kim",
            "Soomok Lee",
            "Mingu Jeon",
            "Seong-Woo Kim"
        ],
        "tldr": "This paper presents a novel framework for localizing pedestrians in non-line-of-sight (NLoS) environments using mmWave radar and camera data, leveraging road layout information extracted from camera images to interpret radar point cloud data. It demonstrates practical applicability with experimental results from a real vehicle.",
        "tldr_zh": "本文提出了一种新的框架，利用毫米波雷达和相机数据在非视距（NLoS）环境中定位行人，利用从相机图像中提取的道路布局信息来解释雷达点云数据。 通过实际车辆的实验结果证明了其实际应用性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Unleashing the Temporal Potential of Stereo Event Cameras for Continuous-Time 3D Object Detection",
        "summary": "3D object detection is essential for autonomous systems, enabling precise\nlocalization and dimension estimation. While LiDAR and RGB cameras are widely\nused, their fixed frame rates create perception gaps in high-speed scenarios.\nEvent cameras, with their asynchronous nature and high temporal resolution,\noffer a solution by capturing motion continuously. The recent approach, which\nintegrates event cameras with conventional sensors for continuous-time\ndetection, struggles in fast-motion scenarios due to its dependency on\nsynchronized sensors. We propose a novel stereo 3D object detection framework\nthat relies solely on event cameras, eliminating the need for conventional 3D\nsensors. To compensate for the lack of semantic and geometric information in\nevent data, we introduce a dual filter mechanism that extracts both.\nAdditionally, we enhance regression by aligning bounding boxes with\nobject-centric information. Experiments show that our method outperforms prior\napproaches in dynamic environments, demonstrating the potential of event\ncameras for robust, continuous-time 3D perception. The code is available at\nhttps://github.com/mickeykang16/Ev-Stereo3D.",
        "url": "http://arxiv.org/abs/2508.02288v1",
        "published_date": "2025-08-04T10:57:03+00:00",
        "updated_date": "2025-08-04T10:57:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jae-Young Kang",
            "Hoonhee Cho",
            "Kuk-Jin Yoon"
        ],
        "tldr": "The paper presents a novel stereo 3D object detection framework using only event cameras, addressing limitations of synchronized sensor setups in fast-motion scenarios. It introduces a dual filter mechanism and object-centric bounding box regression, demonstrating improved performance in dynamic environments.",
        "tldr_zh": "该论文提出了一种仅使用事件相机的立体3D物体检测新框架，解决了快速运动场景中同步传感器设置的局限性。它引入了一种双重滤波机制和以物体为中心的边界框回归，证明了在动态环境中性能的提高。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "A Moment Matching-Based Method for Sparse and Noisy Point Cloud Registration",
        "summary": "Point cloud registration is a key step in robotic perception tasks, such as\nSimultaneous Localization and Mapping (SLAM). It is especially challenging in\nconditions with sparse points and heavy noise. Traditional registration\nmethods, such as Iterative Closest Point (ICP) and Normal Distributions\nTransform (NDT), often have difficulties in achieving a robust and accurate\nalignment under these conditions. In this paper, we propose a registration\nframework based on moment matching. In particular, the point clouds are\nregarded as i.i.d. samples drawn from the same distribution observed in the\nsource and target frames. We then match the generalized Gaussian Radial Basis\nmoments calculated from the point clouds to estimate the rigid transformation\nbetween two frames. Moreover, such method does not require explicit\npoint-to-point correspondences among the point clouds. We further show the\nconsistency of the proposed method. Experiments on synthetic and real-world\ndatasets show that our approach achieves higher accuracy and robustness than\nexisting methods. In addition, we integrate our framework into a 4D Radar SLAM\nsystem. The proposed method significantly improves the localization performance\nand achieves results comparable to LiDAR-based systems. These findings\ndemonstrate the potential of moment matching technique for robust point cloud\nregistration in sparse and noisy scenarios.",
        "url": "http://arxiv.org/abs/2508.02187v1",
        "published_date": "2025-08-04T08:31:53+00:00",
        "updated_date": "2025-08-04T08:31:53+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Xingyi Li",
            "Han Zhang",
            "Ziliang Wang",
            "Yukai Yang",
            "Weidong Chen"
        ],
        "tldr": "This paper introduces a novel point cloud registration method based on moment matching, which demonstrates robustness and accuracy in sparse and noisy conditions, particularly within a 4D Radar SLAM system.",
        "tldr_zh": "本文提出了一种基于矩匹配的新型点云配准方法，该方法在稀疏和噪声环境下表现出稳健性和准确性，尤其是在4D雷达SLAM系统中。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Test-Time Model Adaptation for Quantized Neural Networks",
        "summary": "Quantizing deep models prior to deployment is a widely adopted technique to\nspeed up inference for various real-time applications, such as autonomous\ndriving. However, quantized models often suffer from severe performance\ndegradation in dynamic environments with potential domain shifts and this\ndegradation is significantly more pronounced compared with their full-precision\ncounterparts, as shown by our theoretical and empirical illustrations. To\naddress the domain shift problem, test-time adaptation (TTA) has emerged as an\neffective solution by enabling models to learn adaptively from test data.\nUnfortunately, existing TTA methods are often impractical for quantized models\nas they typically rely on gradient backpropagation--an operation that is\nunsupported on quantized models due to vanishing gradients, as well as memory\nand latency constraints. In this paper, we focus on TTA for quantized models to\nimprove their robustness and generalization ability efficiently. We propose a\ncontinual zeroth-order adaptation (ZOA) framework that enables efficient model\nadaptation using only two forward passes, eliminating the computational burden\nof existing methods. Moreover, we propose a domain knowledge management scheme\nto store and reuse different domain knowledge with negligible memory\nconsumption, reducing the interference of different domain knowledge and\nfostering the knowledge accumulation during long-term adaptation. Experimental\nresults on three classical architectures, including quantized transformer-based\nand CNN-based models, demonstrate the superiority of our methods for quantized\nmodel adaptation. On the quantized W6A6 ViT-B model, our ZOA is able to achieve\na 5.0\\% improvement over the state-of-the-art FOA on ImageNet-C dataset. The\nsource code is available at https://github.com/DengZeshuai/ZOA.",
        "url": "http://arxiv.org/abs/2508.02180v1",
        "published_date": "2025-08-04T08:24:19+00:00",
        "updated_date": "2025-08-04T08:24:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zeshuai Deng",
            "Guohao Chen",
            "Shuaicheng Niu",
            "Hui Luo",
            "Shuhai Zhang",
            "Yifan Yang",
            "Renjie Chen",
            "Wei Luo",
            "Mingkui Tan"
        ],
        "tldr": "The paper introduces a novel continual zeroth-order adaptation (ZOA) framework for test-time adaptation of quantized neural networks, addressing performance degradation in dynamic environments with domain shifts. It achieves efficient adaptation using only two forward passes and a domain knowledge management scheme.",
        "tldr_zh": "该论文介绍了一种新颖的持续零阶自适应（ZOA）框架，用于量化神经网络的测试时自适应，解决了动态环境中领域偏移导致的性能下降问题。它仅使用两次前向传递和一个领域知识管理方案即可实现高效自适应。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GaussianCross: Cross-modal Self-supervised 3D Representation Learning via Gaussian Splatting",
        "summary": "The significance of informative and robust point representations has been\nwidely acknowledged for 3D scene understanding. Despite existing\nself-supervised pre-training counterparts demonstrating promising performance,\nthe model collapse and structural information deficiency remain prevalent due\nto insufficient point discrimination difficulty, yielding unreliable\nexpressions and suboptimal performance. In this paper, we present\nGaussianCross, a novel cross-modal self-supervised 3D representation learning\narchitecture integrating feed-forward 3D Gaussian Splatting (3DGS) techniques\nto address current challenges. GaussianCross seamlessly converts\nscale-inconsistent 3D point clouds into a unified cuboid-normalized Gaussian\nrepresentation without missing details, enabling stable and generalizable\npre-training. Subsequently, a tri-attribute adaptive distillation splatting\nmodule is incorporated to construct a 3D feature field, facilitating synergetic\nfeature capturing of appearance, geometry, and semantic cues to maintain\ncross-modal consistency. To validate GaussianCross, we perform extensive\nevaluations on various benchmarks, including ScanNet, ScanNet200, and S3DIS. In\nparticular, GaussianCross shows a prominent parameter and data efficiency,\nachieving superior performance through linear probing (<0.1% parameters) and\nlimited data training (1% of scenes) compared to state-of-the-art methods.\nFurthermore, GaussianCross demonstrates strong generalization capabilities,\nimproving the full fine-tuning accuracy by 9.3% mIoU and 6.1% AP$_{50}$ on\nScanNet200 semantic and instance segmentation tasks, respectively, supporting\nthe effectiveness of our approach. The code, weights, and visualizations are\npublicly available at\n\\href{https://rayyoh.github.io/GaussianCross/}{https://rayyoh.github.io/GaussianCross/}.",
        "url": "http://arxiv.org/abs/2508.02172v1",
        "published_date": "2025-08-04T08:12:44+00:00",
        "updated_date": "2025-08-04T08:12:44+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.MM"
        ],
        "authors": [
            "Lei Yao",
            "Yi Wang",
            "Yi Zhang",
            "Moyun Liu",
            "Lap-Pui Chau"
        ],
        "tldr": "The paper introduces GaussianCross, a cross-modal self-supervised 3D representation learning method using Gaussian Splatting, achieving strong performance and generalization with high parameter and data efficiency on 3D scene understanding benchmarks.",
        "tldr_zh": "本文介绍了一种名为GaussianCross的跨模态自监督3D表示学习方法，该方法利用高斯溅射技术，在3D场景理解基准测试中实现了强大的性能和泛化能力，同时具有很高的参数和数据效率。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Beyond RGB and Events: Enhancing Object Detection under Adverse Lighting with Monocular Normal Maps",
        "summary": "Accurate object detection under adverse lighting conditions is critical for\nreal-world applications such as autonomous driving. Although neuromorphic event\ncameras have been introduced to handle these scenarios, adverse lighting often\ninduces distracting reflections from tunnel walls or road surfaces, which\nfrequently lead to false obstacle detections. However, neither RGB nor event\ndata alone is robust enough to address these complexities, and mitigating these\nissues without additional sensors remains underexplored. To overcome these\nchallenges, we propose leveraging normal maps, directly predicted from\nmonocular RGB images, as robust geometric cues to suppress false positives and\nenhance detection accuracy. We introduce NRE-Net, a novel multi-modal detection\nframework that effectively fuses three complementary modalities: monocularly\npredicted surface normal maps, RGB images, and event streams. To optimize the\nfusion process, our framework incorporates two key modules: the Adaptive\nDual-stream Fusion Module (ADFM), which integrates RGB and normal map features,\nand the Event-modality Aware Fusion Module (EAFM), which adapts to the high\ndynamic range characteristics of event data. Extensive evaluations on the\nDSEC-Det-sub and PKU-DAVIS-SOD datasets demonstrate that NRE-Net significantly\noutperforms state-of-the-art methods. Our approach achieves mAP50 improvements\nof 7.9% and 6.1% over frame-based approaches (e.g., YOLOX), while surpassing\nthe fusion-based SFNet by 2.7% on the DSEC-Det-sub dataset and SODFormer by\n7.1% on the PKU-DAVIS-SOD dataset.",
        "url": "http://arxiv.org/abs/2508.02127v1",
        "published_date": "2025-08-04T07:19:20+00:00",
        "updated_date": "2025-08-04T07:19:20+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mingjie Liu",
            "Hanqing Liu",
            "Chuang Zhu"
        ],
        "tldr": "This paper introduces NRE-Net, a novel multi-modal object detection framework that fuses RGB images, event streams, and monocularly predicted surface normal maps to improve object detection accuracy under adverse lighting conditions. The method demonstrates significant improvements over state-of-the-art approaches on relevant datasets.",
        "tldr_zh": "本文介绍了一种新的多模态目标检测框架NRE-Net，它融合了RGB图像、事件流和单目预测的表面法线图，以提高在不利光照条件下的目标检测精度。该方法在相关数据集上表现出优于现有技术的显著改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Mapillary Vistas Validation for Fine-Grained Traffic Signs: A Benchmark Revealing Vision-Language Model Limitations",
        "summary": "Obtaining high-quality fine-grained annotations for traffic signs is critical\nfor accurate and safe decision-making in autonomous driving. Widely used\ndatasets, such as Mapillary, often provide only coarse-grained labels - without\ndistinguishing semantically important types such as stop signs or speed limit\nsigns. To this end, we present a new validation set for traffic signs derived\nfrom the Mapillary dataset called Mapillary Vistas Validation for Traffic Signs\n(MVV), where we decompose composite traffic signs into granular, semantically\nmeaningful categories. The dataset includes pixel-level instance masks and has\nbeen manually annotated by expert annotators to ensure label fidelity. Further,\nwe benchmark several state-of-the-art VLMs against the self-supervised DINOv2\nmodel on this dataset and show that DINOv2 consistently outperforms all VLM\nbaselines-not only on traffic sign recognition, but also on heavily represented\ncategories like vehicles and humans. Our analysis reveals significant\nlimitations in current vision-language models for fine-grained visual\nunderstanding and establishes DINOv2 as a strong baseline for dense semantic\nmatching in autonomous driving scenarios. This dataset and evaluation framework\npave the way for more reliable, interpretable, and scalable perception systems.\n  Code and data are available at: https://github.com/nec-labs-ma/relabeling",
        "url": "http://arxiv.org/abs/2508.02047v1",
        "published_date": "2025-08-04T04:29:06+00:00",
        "updated_date": "2025-08-04T04:29:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Sparsh Garg",
            "Abhishek Aich"
        ],
        "tldr": "The paper introduces a new, fine-grained traffic sign validation dataset (MVV) derived from Mapillary Vistas and benchmarks several vision-language models, finding DINOv2 to outperform them, revealing limitations in VLMs for fine-grained visual understanding in autonomous driving.",
        "tldr_zh": "该论文介绍了一个新的、精细化的交通标志验证数据集 (MVV)，该数据集源于 Mapillary Vistas。论文对几个视觉-语言模型进行了基准测试，发现 DINOv2 的性能优于它们，揭示了视觉-语言模型在自动驾驶中进行精细化视觉理解的局限性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "On-the-Fly Object-aware Representative Point Selection in Point Cloud",
        "summary": "Point clouds are essential for object modeling and play a critical role in\nassisting driving tasks for autonomous vehicles (AVs). However, the significant\nvolume of data generated by AVs creates challenges for storage, bandwidth, and\nprocessing cost. To tackle these challenges, we propose a representative point\nselection framework for point cloud downsampling, which preserves critical\nobject-related information while effectively filtering out irrelevant\nbackground points. Our method involves two steps: (1) Object Presence\nDetection, where we introduce an unsupervised density peak-based classifier and\na supervised Na\\\"ive Bayes classifier to handle diverse scenarios, and (2)\nSampling Budget Allocation, where we propose a strategy that selects\nobject-relevant points while maintaining a high retention rate of object\ninformation. Extensive experiments on the KITTI and nuScenes datasets\ndemonstrate that our method consistently outperforms state-of-the-art baselines\nin both efficiency and effectiveness across varying sampling rates. As a\nmodel-agnostic solution, our approach integrates seamlessly with diverse\ndownstream models, making it a valuable and scalable addition to the 3D point\ncloud downsampling toolkit for AV applications.",
        "url": "http://arxiv.org/abs/2508.01980v1",
        "published_date": "2025-08-04T01:39:09+00:00",
        "updated_date": "2025-08-04T01:39:09+00:00",
        "categories": [
            "cs.CV",
            "cs.MM"
        ],
        "authors": [
            "Xiaoyu Zhang",
            "Ziwei Wang",
            "Hai Dong",
            "Zhifeng Bao",
            "Jiajun Liu"
        ],
        "tldr": "This paper presents a novel point cloud downsampling framework for autonomous vehicles that prioritizes object-related information by combining unsupervised and supervised classification for object presence detection and strategic sampling budget allocation, demonstrating state-of-the-art performance on KITTI and nuScenes.",
        "tldr_zh": "本文提出了一种用于自动驾驶车辆的新型点云下采样框架，该框架通过结合无监督和监督分类进行对象存在检测以及战略采样预算分配，从而优先考虑与对象相关的信息，并在KITTI和nuScenes数据集上展示了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Photons to Physics: Autonomous Indoor Drones and the Future of Objective Property Assessment",
        "summary": "The convergence of autonomous indoor drones with physics-aware sensing\ntechnologies promises to transform property assessment from subjective visual\ninspection to objective, quantitative measurement. This comprehensive review\nexamines the technical foundations enabling this paradigm shift across four\ncritical domains: (1) platform architectures optimized for indoor navigation,\nwhere weight constraints drive innovations in heterogeneous computing,\ncollision-tolerant design, and hierarchical control systems; (2) advanced\nsensing modalities that extend perception beyond human vision, including\nhyperspectral imaging for material identification, polarimetric sensing for\nsurface characterization, and computational imaging with metaphotonics enabling\nradical miniaturization; (3) intelligent autonomy through active reconstruction\nalgorithms, where drones equipped with 3D Gaussian Splatting make strategic\ndecisions about viewpoint selection to maximize information gain within battery\nconstraints; and (4) integration pathways with existing property workflows,\nincluding Building Information Modeling (BIM) systems and industry standards\nlike Uniform Appraisal Dataset (UAD) 3.6.",
        "url": "http://arxiv.org/abs/2508.01965v1",
        "published_date": "2025-08-04T00:20:25+00:00",
        "updated_date": "2025-08-04T00:20:25+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "I.2.9; I.4.8; I.2.10; C.3; J.2"
        ],
        "authors": [
            "Petteri Teikari",
            "Mike Jarrell",
            "Irene Bandera Moreno",
            "Harri Pesola"
        ],
        "tldr": "This paper reviews the potential of autonomous indoor drones equipped with advanced sensing and 3D reconstruction techniques to revolutionize property assessment by providing objective, quantitative measurements integrated with BIM and industry standards.",
        "tldr_zh": "本文综述了配备先进传感和3D重建技术的自主室内无人机在革新资产评估方面的潜力，通过提供与BIM和行业标准集成的客观、定量测量来实现。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks",
        "summary": "Vision-language models (VLMs) have exhibited impressive capabilities across\ndiverse image understanding tasks, but still struggle in settings that require\nreasoning over extended sequences of camera frames from a video. This limits\ntheir utility in embodied settings, which require reasoning over long frame\nsequences from a continuous stream of visual input at each moment of a task\nattempt. To address this limitation, we propose ROVER (Reasoning Over VidEo\nRecursively), a framework that enables the model to recursively decompose\nlong-horizon video trajectories into segments corresponding to shorter subtasks\nwithin the trajectory. In doing so, ROVER facilitates more focused and accurate\nreasoning over temporally localized frame sequences without losing global\ncontext. We evaluate ROVER, implemented using an in-context learning approach,\non diverse OpenX Embodiment videos and on a new dataset derived from RoboCasa\nthat consists of 543 videos showing both expert and perturbed non-expert\ntrajectories across 27 robotic manipulation tasks. ROVER outperforms strong\nbaselines across three video reasoning tasks: task progress estimation,\nframe-level natural language reasoning, and video question answering. We\nobserve that, by reducing the number of frames the model reasons over at each\ntimestep, ROVER mitigates hallucinations, especially during unexpected or\nnon-optimal moments of a trajectory. In addition, by enabling the\nimplementation of a subtask-specific sliding context window, ROVER's time\ncomplexity scales linearly with video length, an asymptotic improvement over\nbaselines. Demos, code, and data available at: https://rover-vlm.github.io",
        "url": "http://arxiv.org/abs/2508.01943v1",
        "published_date": "2025-08-03T22:33:43+00:00",
        "updated_date": "2025-08-03T22:33:43+00:00",
        "categories": [
            "cs.CL",
            "cs.AI",
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Philip Schroeder",
            "Ondrej Biza",
            "Thomas Weng",
            "Hongyin Luo",
            "James Glass"
        ],
        "tldr": "The paper introduces ROVER, a framework that uses recursive decomposition of long-horizon video trajectories for improved reasoning in embodied tasks, showing improved performance and scalability compared to baselines.",
        "tldr_zh": "本文介绍了ROVER，该框架通过递归分解长时程视频轨迹来改进具身任务中的推理，与基线方法相比，显示出更好的性能和可扩展性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CVD-SfM: A Cross-View Deep Front-end Structure-from-Motion System for Sparse Localization in Multi-Altitude Scenes",
        "summary": "We present a novel multi-altitude camera pose estimation system, addressing\nthe challenges of robust and accurate localization across varied altitudes when\nonly considering sparse image input. The system effectively handles diverse\nenvironmental conditions and viewpoint variations by integrating the cross-view\ntransformer, deep features, and structure-from-motion into a unified framework.\nTo benchmark our method and foster further research, we introduce two newly\ncollected datasets specifically tailored for multi-altitude camera pose\nestimation; datasets of this nature remain rare in the current literature. The\nproposed framework has been validated through extensive comparative analyses on\nthese datasets, demonstrating that our system achieves superior performance in\nboth accuracy and robustness for multi-altitude sparse pose estimation tasks\ncompared to existing solutions, making it well suited for real-world robotic\napplications such as aerial navigation, search and rescue, and automated\ninspection.",
        "url": "http://arxiv.org/abs/2508.01936v1",
        "published_date": "2025-08-03T22:11:48+00:00",
        "updated_date": "2025-08-03T22:11:48+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Yaxuan Li",
            "Yewei Huang",
            "Bijay Gaudel",
            "Hamidreza Jafarnejadsani",
            "Brendan Englot"
        ],
        "tldr": "The paper introduces a novel Structure-from-Motion system (CVD-SfM) for robust multi-altitude camera pose estimation using cross-view transformers and deep features, validated on new datasets tailored for this task, demonstrating superior performance over existing methods.",
        "tldr_zh": "该论文提出了一种新的基于跨视角Transformer和深度特征的Structure-from-Motion系统(CVD-SfM)，用于鲁棒的多高度相机姿态估计。该方法在专门为此任务设计的新数据集上进行了验证，并展示了优于现有方法的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video Understanding",
        "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
        "url": "http://arxiv.org/abs/2508.01875v1",
        "published_date": "2025-08-03T18:15:42+00:00",
        "updated_date": "2025-08-03T18:15:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haolin Yang",
            "Feilong Tang",
            "Linxiao Zhao",
            "Xiang An",
            "Ming Hu",
            "Huifa Li",
            "Xinlin Zhuang",
            "Boqian Wang",
            "Yifan Lu",
            "Xiaofeng Zhang",
            "Abdalla Swikir",
            "Junjun He",
            "Zongyuan Ge",
            "Imran Razzak"
        ],
        "tldr": "StreamAgent introduces an anticipatory agent for real-time streaming video understanding, using a prompting mechanism and hierarchical KV-cache to improve responsiveness and efficiency in tasks like autonomous driving and surveillance.",
        "tldr_zh": "StreamAgent 提出了一种用于实时流视频理解的预测代理，它使用提示机制和分层 KV 缓存来提高在自动驾驶和监控等任务中的响应能力和效率。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots",
        "summary": "Panoramic cameras, capturing comprehensive 360-degree environmental data, are\nsuitable for quadruped robots in surrounding perception and interaction with\ncomplex environments. However, the scarcity of high-quality panoramic training\ndata-caused by inherent kinematic constraints and complex sensor calibration\nchallenges-fundamentally limits the development of robust perception systems\ntailored to these embodied platforms. To address this issue, we propose\nQuaDreamer-the first panoramic data generation engine specifically designed for\nquadruped robots. QuaDreamer focuses on mimicking the motion paradigm of\nquadruped robots to generate highly controllable, realistic panoramic videos,\nproviding a data source for downstream tasks. Specifically, to effectively\ncapture the unique vertical vibration characteristics exhibited during\nquadruped locomotion, we introduce Vertical Jitter Encoding (VJE). VJE extracts\ncontrollable vertical signals through frequency-domain feature filtering and\nprovides high-quality prompts. To facilitate high-quality panoramic video\ngeneration under jitter signal control, we propose a Scene-Object Controller\n(SOC) that effectively manages object motion and boosts background jitter\ncontrol through the attention mechanism. To address panoramic distortions in\nwide-FoV video generation, we propose the Panoramic Enhancer (PE)-a dual-stream\narchitecture that synergizes frequency-texture refinement for local detail\nenhancement with spatial-structure correction for global geometric consistency.\nWe further demonstrate that the generated video sequences can serve as training\ndata for the quadruped robot's panoramic visual perception model, enhancing the\nperformance of multi-object tracking in 360-degree scenes. The source code and\nmodel weights will be publicly available at\nhttps://github.com/losehu/QuaDreamer.",
        "url": "http://arxiv.org/abs/2508.02512v1",
        "published_date": "2025-08-04T15:18:01+00:00",
        "updated_date": "2025-08-04T15:18:01+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "eess.IV"
        ],
        "authors": [
            "Sheng Wu",
            "Fei Teng",
            "Hao Shi",
            "Qi Jiang",
            "Kai Luo",
            "Kaiwei Wang",
            "Kailun Yang"
        ],
        "tldr": "QuaDreamer is a novel panoramic data generation engine for quadruped robots, addressing the scarcity of training data by generating controllable, realistic panoramic videos with vertical jitter encoding, scene-object control, and panoramic enhancement.",
        "tldr_zh": "QuaDreamer是一个新颖的四足机器人全景数据生成引擎，通过生成可控的、逼真的全景视频，解决了训练数据稀缺的问题，其采用了垂直抖动编码、场景-对象控制和全景增强技术。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]