[
    {
        "title": "ScenePilot-Bench: A Large-Scale Dataset and Benchmark for Evaluation of Vision-Language Models in Autonomous Driving",
        "summary": "In this paper, we introduce ScenePilot-Bench, a large-scale first-person driving benchmark designed to evaluate vision-language models (VLMs) in autonomous driving scenarios. ScenePilot-Bench is built upon ScenePilot-4K, a diverse dataset comprising 3,847 hours of driving videos, annotated with multi-granularity information including scene descriptions, risk assessments, key participant identification, ego trajectories, and camera parameters. The benchmark features a four-axis evaluation suite that assesses VLM capabilities in scene understanding, spatial perception, motion planning, and GPT-Score, with safety-aware metrics and cross-region generalization settings. We benchmark representative VLMs on ScenePilot-Bench, providing empirical analyses that clarify current performance boundaries and identify gaps for driving-oriented reasoning. ScenePilot-Bench offers a comprehensive framework for evaluating and advancing VLMs in safety-critical autonomous driving contexts.",
        "url": "http://arxiv.org/abs/2601.19582v1",
        "published_date": "2026-01-27T13:17:50+00:00",
        "updated_date": "2026-01-27T13:17:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yujin Wang",
            "Yutong Zheng",
            "Wenxian Fan",
            "Tianyi Wang",
            "Hongqing Chu",
            "Daxin Tian",
            "Bingzhao Gao",
            "Jianqiang Wang",
            "Hong Chen"
        ],
        "tldr": "The paper introduces ScenePilot-Bench, a large-scale driving dataset and benchmark to evaluate vision-language models (VLMs) in autonomous driving across scene understanding, spatial perception, motion planning, and safety.",
        "tldr_zh": "本文介绍了ScenePilot-Bench，一个大规模驾驶数据集和基准，用于评估视觉语言模型（VLMs）在自动驾驶中跨场景理解、空间感知、运动规划和安全方面的能力。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "DeFM: Learning Foundation Representations from Depth for Robotics",
        "summary": "Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: https://de-fm.github.io/",
        "url": "http://arxiv.org/abs/2601.18923v1",
        "published_date": "2026-01-26T19:45:31+00:00",
        "updated_date": "2026-01-26T19:45:31+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Manthan Patel",
            "Jonas Frey",
            "Mayank Mittal",
            "Fan Yang",
            "Alexander Hansson",
            "Amir Bar",
            "Cesar Cadena",
            "Marco Hutter"
        ],
        "tldr": "The paper introduces DeFM, a self-supervised foundation model trained on a large depth image dataset for robotics, demonstrating state-of-the-art performance on various depth-based robotic tasks with strong sim-to-real generalization.",
        "tldr_zh": "该论文介绍了DeFM，一个在大型深度图像数据集上训练的自监督基础模型，用于机器人技术，在各种基于深度的机器人任务上表现出最先进的性能，并具有强大的模拟到真实泛化能力。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Dynamic Worlds, Dynamic Humans: Generating Virtual Human-Scene Interaction Motion in Dynamic Scenes",
        "summary": "Scenes are continuously undergoing dynamic changes in the real world. However, existing human-scene interaction generation methods typically treat the scene as static, which deviates from reality. Inspired by world models, we introduce Dyn-HSI, the first cognitive architecture for dynamic human-scene interaction, which endows virtual humans with three humanoid components. (1)Vision (human eyes): we equip the virtual human with a Dynamic Scene-Aware Navigation, which continuously perceives changes in the surrounding environment and adaptively predicts the next waypoint. (2)Memory (human brain): we equip the virtual human with a Hierarchical Experience Memory, which stores and updates experiential data accumulated during training. This allows the model to leverage prior knowledge during inference for context-aware motion priming, thereby enhancing both motion quality and generalization. (3) Control (human body): we equip the virtual human with Human-Scene Interaction Diffusion Model, which generates high-fidelity interaction motions conditioned on multimodal inputs. To evaluate performance in dynamic scenes, we extend the existing static human-scene interaction datasets to construct a dynamic benchmark, Dyn-Scenes. We conduct extensive qualitative and quantitative experiments to validate Dyn-HSI, showing that our method consistently outperforms existing approaches and generates high-quality human-scene interaction motions in both static and dynamic settings.",
        "url": "http://arxiv.org/abs/2601.19484v1",
        "published_date": "2026-01-27T11:16:42+00:00",
        "updated_date": "2026-01-27T11:16:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yin Wang",
            "Zhiying Leng",
            "Haitian Liu",
            "Frederick W. B. Li",
            "Mu Li",
            "Xiaohui Liang"
        ],
        "tldr": "This paper introduces Dyn-HSI, a novel cognitive architecture for generating virtual human-scene interaction motion in dynamic environments, using vision, memory, and control components, and validates it on a newly created dynamic benchmark dataset.",
        "tldr_zh": "该论文介绍了Dyn-HSI，一种用于在动态环境中生成虚拟人-场景交互运动的新型认知架构，它利用视觉、记忆和控制组件，并在新创建的动态基准数据集上进行了验证。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Perception-to-Pursuit: Track-Centric Temporal Reasoning for Open-World Drone Detection and Autonomous Chasing",
        "summary": "Autonomous drone pursuit requires not only detecting drones but also predicting their trajectories in a manner that enables kinematically feasible interception. Existing tracking methods optimize for prediction accuracy but ignore pursuit feasibility, resulting in trajectories that are physically impossible to intercept 99.9% of the time. We propose Perception-to-Pursuit (P2P), a track-centric temporal reasoning framework that bridges detection and actionable pursuit planning. Our method represents drone motion as compact 8-dimensional tokens capturing velocity, acceleration, scale, and smoothness, enabling a 12-frame causal transformer to reason about future behavior. We introduce the Intercept Success Rate (ISR) metric to measure pursuit feasibility under realistic interceptor constraints. Evaluated on the Anti-UAV-RGBT dataset with 226 real drone sequences, P2P achieves 28.12 pixel average displacement error and 0.597 ISR, representing a 77% improvement in trajectory prediction and 597x improvement in pursuit feasibility over tracking-only baselines, while maintaining perfect drone classification accuracy (100%). Our work demonstrates that temporal reasoning over motion patterns enables both accurate prediction and actionable pursuit planning.",
        "url": "http://arxiv.org/abs/2601.19318v1",
        "published_date": "2026-01-27T07:57:29+00:00",
        "updated_date": "2026-01-27T07:57:29+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Venkatakrishna Reddy Oruganti"
        ],
        "tldr": "The paper introduces Perception-to-Pursuit (P2P), a track-centric temporal reasoning framework for autonomous drone pursuit that optimizes for both accurate trajectory prediction and pursuit feasibility, achieving significant improvements over tracking-only methods.",
        "tldr_zh": "该论文介绍了感知到追踪 (P2P)，一种以轨迹为中心的时序推理框架，用于自主无人机追踪，优化了精确的轨迹预测和追踪可行性，比仅追踪方法有了显著改进。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Instance-Guided Radar Depth Estimation for 3D Object Detection",
        "summary": "Accurate depth estimation is fundamental to 3D perception in autonomous driving, supporting tasks such as detection, tracking, and motion planning. However, monocular camera-based 3D detection suffers from depth ambiguity and reduced robustness under challenging conditions. Radar provides complementary advantages such as resilience to poor lighting and adverse weather, but its sparsity and low resolution limit its direct use in detection frameworks. This motivates the need for effective Radar-camera fusion with improved preprocessing and depth estimation strategies. We propose an end-to-end framework that enhances monocular 3D object detection through two key components. First, we introduce InstaRadar, an instance segmentation-guided expansion method that leverages pre-trained segmentation masks to enhance Radar density and semantic alignment, producing a more structured representation. InstaRadar achieves state-of-the-art results in Radar-guided depth estimation, showing its effectiveness in generating high-quality depth features. Second, we integrate the pre-trained RCDPT into the BEVDepth framework as a replacement for its depth module. With InstaRadar-enhanced inputs, the RCDPT integration consistently improves 3D detection performance. Overall, these components yield steady gains over the baseline BEVDepth model, demonstrating the effectiveness of InstaRadar and the advantage of explicit depth supervision in 3D object detection. Although the framework lags behind Radar-camera fusion models that directly extract BEV features, since Radar serves only as guidance rather than an independent feature stream, this limitation highlights potential for improvement. Future work will extend InstaRadar to point cloud-like representations and integrate a dedicated Radar branch with temporal cues for enhanced BEV fusion.",
        "url": "http://arxiv.org/abs/2601.19314v1",
        "published_date": "2026-01-27T07:53:24+00:00",
        "updated_date": "2026-01-27T07:53:24+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Chen-Chou Lo",
            "Patrick Vandewalle"
        ],
        "tldr": "The paper proposes InstaRadar, an instance segmentation-guided radar expansion method, to enhance radar density and semantic alignment for improved 3D object detection by effectively fusing radar and camera data within the BEVDepth framework. It achieves state-of-the-art radar-guided depth estimation.",
        "tldr_zh": "该论文提出了InstaRadar，一种实例分割引导的雷达扩展方法，旨在增强雷达密度和语义对齐，从而通过在BEVDepth框架内有效融合雷达和相机数据，改进3D物体检测。它实现了最先进的雷达引导深度估计。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]