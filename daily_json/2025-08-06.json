[
    {
        "title": "La La LiDAR: Large-Scale Layout Generation from LiDAR Data",
        "summary": "Controllable generation of realistic LiDAR scenes is crucial for applications\nsuch as autonomous driving and robotics. While recent diffusion-based models\nachieve high-fidelity LiDAR generation, they lack explicit control over\nforeground objects and spatial relationships, limiting their usefulness for\nscenario simulation and safety validation. To address these limitations, we\npropose Large-scale Layout-guided LiDAR generation model (\"La La LiDAR\"), a\nnovel layout-guided generative framework that introduces semantic-enhanced\nscene graph diffusion with relation-aware contextual conditioning for\nstructured LiDAR layout generation, followed by foreground-aware control\ninjection for complete scene generation. This enables customizable control over\nobject placement while ensuring spatial and semantic consistency. To support\nour structured LiDAR generation, we introduce Waymo-SG and nuScenes-SG, two\nlarge-scale LiDAR scene graph datasets, along with new evaluation metrics for\nlayout synthesis. Extensive experiments demonstrate that La La LiDAR achieves\nstate-of-the-art performance in both LiDAR generation and downstream perception\ntasks, establishing a new benchmark for controllable 3D scene generation.",
        "url": "http://arxiv.org/abs/2508.03691v1",
        "published_date": "2025-08-05T17:59:55+00:00",
        "updated_date": "2025-08-05T17:59:55+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Youquan Liu",
            "Lingdong Kong",
            "Weidong Yang",
            "Xin Li",
            "Ao Liang",
            "Runnan Chen",
            "Ben Fei",
            "Tongliang Liu"
        ],
        "tldr": "The paper introduces La La LiDAR, a layout-guided LiDAR scene generation model using scene graph diffusion, and two new datasets, Waymo-SG and nuScenes-SG. It achieves state-of-the-art performance in LiDAR generation and downstream perception tasks.",
        "tldr_zh": "该论文介绍了 La La LiDAR，一个使用场景图扩散的布局引导式 LiDAR 场景生成模型，以及两个新的数据集 Waymo-SG 和 nuScenes-SG。它在 LiDAR 生成和下游感知任务中实现了最先进的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "OmniShape: Zero-Shot Multi-Hypothesis Shape and Pose Estimation in the Real World",
        "summary": "We would like to estimate the pose and full shape of an object from a single\nobservation, without assuming known 3D model or category. In this work, we\npropose OmniShape, the first method of its kind to enable probabilistic pose\nand shape estimation. OmniShape is based on the key insight that shape\ncompletion can be decoupled into two multi-modal distributions: one capturing\nhow measurements project into a normalized object reference frame defined by\nthe dataset and the other modelling a prior over object geometries represented\nas triplanar neural fields. By training separate conditional diffusion models\nfor these two distributions, we enable sampling multiple hypotheses from the\njoint pose and shape distribution. OmniShape demonstrates compelling\nperformance on challenging real world datasets. Project website:\nhttps://tri-ml.github.io/omnishape",
        "url": "http://arxiv.org/abs/2508.03669v1",
        "published_date": "2025-08-05T17:30:41+00:00",
        "updated_date": "2025-08-05T17:30:41+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Katherine Liu",
            "Sergey Zakharov",
            "Dian Chen",
            "Takuya Ikeda",
            "Greg Shakhnarovich",
            "Adrien Gaidon",
            "Rares Ambrus"
        ],
        "tldr": "OmniShape is a novel method for zero-shot, multi-hypothesis 3D shape and pose estimation from single images, using conditional diffusion models to decouple shape completion into pose-normalized measurements and object geometry priors.",
        "tldr_zh": "OmniShape是一种新颖的方法，可以通过单张图像进行零样本、多假设的3D形状和姿态估计，使用条件扩散模型将形状补全分解为姿态归一化的测量和对象几何先验。",
        "relevance_score": 9,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences",
        "summary": "Generative world models have become essential data engines for autonomous\ndriving, yet most existing efforts focus on videos or occupancy grids,\noverlooking the unique LiDAR properties. Extending LiDAR generation to dynamic\n4D world modeling presents challenges in controllability, temporal coherence,\nand evaluation standardization. To this end, we present LiDARCrafter, a unified\nframework for 4D LiDAR generation and editing. Given free-form natural language\ninputs, we parse instructions into ego-centric scene graphs, which condition a\ntri-branch diffusion network to generate object structures, motion\ntrajectories, and geometry. These structured conditions enable diverse and\nfine-grained scene editing. Additionally, an autoregressive module generates\ntemporally coherent 4D LiDAR sequences with smooth transitions. To support\nstandardized evaluation, we establish a comprehensive benchmark with diverse\nmetrics spanning scene-, object-, and sequence-level aspects. Experiments on\nthe nuScenes dataset using this benchmark demonstrate that LiDARCrafter\nachieves state-of-the-art performance in fidelity, controllability, and\ntemporal consistency across all levels, paving the way for data augmentation\nand simulation. The code and benchmark are released to the community.",
        "url": "http://arxiv.org/abs/2508.03692v1",
        "published_date": "2025-08-05T17:59:56+00:00",
        "updated_date": "2025-08-05T17:59:56+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Ao Liang",
            "Youquan Liu",
            "Yu Yang",
            "Dongyue Lu",
            "Linfeng Li",
            "Lingdong Kong",
            "Huaici Zhao",
            "Wei Tsang Ooi"
        ],
        "tldr": "LiDARCrafter is a framework for generating and editing dynamic 4D LiDAR sequences from natural language input, using a diffusion network conditioned on scene graphs and demonstrating state-of-the-art performance on the nuScenes dataset.",
        "tldr_zh": "LiDARCrafter是一个用于从自然语言输入生成和编辑动态4D激光雷达序列的框架，它使用基于场景图的扩散网络，并在nuScenes数据集上展示了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Veila: Panoramic LiDAR Generation from a Monocular RGB Image",
        "summary": "Realistic and controllable panoramic LiDAR data generation is critical for\nscalable 3D perception in autonomous driving and robotics. Existing methods\neither perform unconditional generation with poor controllability or adopt\ntext-guided synthesis, which lacks fine-grained spatial control. Leveraging a\nmonocular RGB image as a spatial control signal offers a scalable and low-cost\nalternative, which remains an open problem. However, it faces three core\nchallenges: (i) semantic and depth cues from RGB are vary spatially,\ncomplicating reliable conditioning generation; (ii) modality gaps between RGB\nappearance and LiDAR geometry amplify alignment errors under noisy diffusion;\nand (iii) maintaining structural coherence between monocular RGB and panoramic\nLiDAR is challenging, particularly in non-overlap regions between images and\nLiDAR. To address these challenges, we propose Veila, a novel conditional\ndiffusion framework that integrates: a Confidence-Aware Conditioning Mechanism\n(CACM) that strengthens RGB conditioning by adaptively balancing semantic and\ndepth cues according to their local reliability; a Geometric Cross-Modal\nAlignment (GCMA) for robust RGB-LiDAR alignment under noisy diffusion; and a\nPanoramic Feature Coherence (PFC) for enforcing global structural consistency\nacross monocular RGB and panoramic LiDAR. Additionally, we introduce two\nmetrics, Cross-Modal Semantic Consistency and Cross-Modal Depth Consistency, to\nevaluate alignment quality across modalities. Experiments on nuScenes,\nSemanticKITTI, and our proposed KITTI-Weather benchmark demonstrate that Veila\nachieves state-of-the-art generation fidelity and cross-modal consistency,\nwhile enabling generative data augmentation that improves downstream LiDAR\nsemantic segmentation.",
        "url": "http://arxiv.org/abs/2508.03690v1",
        "published_date": "2025-08-05T17:59:53+00:00",
        "updated_date": "2025-08-05T17:59:53+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Youquan Liu",
            "Lingdong Kong",
            "Weidong Yang",
            "Ao Liang",
            "Jianxiong Gao",
            "Yang Wu",
            "Xiang Xu",
            "Xin Li",
            "Linfeng Li",
            "Runnan Chen",
            "Ben Fei"
        ],
        "tldr": "The paper introduces Veila, a conditional diffusion framework that generates realistic panoramic LiDAR data from monocular RGB images by addressing challenges in cross-modal alignment and structural coherence, and demonstrates its effectiveness through experiments and improved downstream performance.",
        "tldr_zh": "该论文介绍了一种名为Veila的条件扩散框架，它通过解决跨模态对齐和结构连贯性方面的挑战，从单目RGB图像生成逼真的全景LiDAR数据。实验结果表明其有效性，并能提高下游任务的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DiWA: Diffusion Policy Adaptation with World Models",
        "summary": "Fine-tuning diffusion policies with reinforcement learning (RL) presents\nsignificant challenges. The long denoising sequence for each action prediction\nimpedes effective reward propagation. Moreover, standard RL methods require\nmillions of real-world interactions, posing a major bottleneck for practical\nfine-tuning. Although prior work frames the denoising process in diffusion\npolicies as a Markov Decision Process to enable RL-based updates, its strong\ndependence on environment interaction remains highly inefficient. To bridge\nthis gap, we introduce DiWA, a novel framework that leverages a world model for\nfine-tuning diffusion-based robotic skills entirely offline with reinforcement\nlearning. Unlike model-free approaches that require millions of environment\ninteractions to fine-tune a repertoire of robot skills, DiWA achieves effective\nadaptation using a world model trained once on a few hundred thousand offline\nplay interactions. This results in dramatically improved sample efficiency,\nmaking the approach significantly more practical and safer for real-world robot\nlearning. On the challenging CALVIN benchmark, DiWA improves performance across\neight tasks using only offline adaptation, while requiring orders of magnitude\nfewer physical interactions than model-free baselines. To our knowledge, this\nis the first demonstration of fine-tuning diffusion policies for real-world\nrobotic skills using an offline world model. We make the code publicly\navailable at https://diwa.cs.uni-freiburg.de.",
        "url": "http://arxiv.org/abs/2508.03645v1",
        "published_date": "2025-08-05T16:55:50+00:00",
        "updated_date": "2025-08-05T16:55:50+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Akshay L Chandra",
            "Iman Nematollahi",
            "Chenguang Huang",
            "Tim Welschehold",
            "Wolfram Burgard",
            "Abhinav Valada"
        ],
        "tldr": "DiWA introduces a novel framework for fine-tuning diffusion-based robot skills offline using a world model, achieving significant sample efficiency and performance improvements on the CALVIN benchmark.",
        "tldr_zh": "DiWA 提出了一种新颖的框架，利用世界模型离线微调基于扩散的机器人技能，在 CALVIN 基准测试中实现了显著的样本效率提升和性能改进。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Uni3R: Unified 3D Reconstruction and Semantic Understanding via Generalizable Gaussian Splatting from Unposed Multi-View Images",
        "summary": "Reconstructing and semantically interpreting 3D scenes from sparse 2D views\nremains a fundamental challenge in computer vision. Conventional methods often\ndecouple semantic understanding from reconstruction or necessitate costly\nper-scene optimization, thereby restricting their scalability and\ngeneralizability. In this paper, we introduce Uni3R, a novel feed-forward\nframework that jointly reconstructs a unified 3D scene representation enriched\nwith open-vocabulary semantics, directly from unposed multi-view images. Our\napproach leverages a Cross-View Transformer to robustly integrate information\nacross arbitrary multi-view inputs, which then regresses a set of 3D Gaussian\nprimitives endowed with semantic feature fields. This unified representation\nfacilitates high-fidelity novel view synthesis, open-vocabulary 3D semantic\nsegmentation, and depth prediction, all within a single, feed-forward pass.\nExtensive experiments demonstrate that Uni3R establishes a new state-of-the-art\nacross multiple benchmarks, including 25.07 PSNR on RE10K and 55.84 mIoU on\nScanNet. Our work signifies a novel paradigm towards generalizable, unified 3D\nscene reconstruction and understanding. The code is available at\nhttps://github.com/HorizonRobotics/Uni3R.",
        "url": "http://arxiv.org/abs/2508.03643v1",
        "published_date": "2025-08-05T16:54:55+00:00",
        "updated_date": "2025-08-05T16:54:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xiangyu Sun",
            "Haoyi jiang",
            "Liu Liu",
            "Seungtae Nam",
            "Gyeongjin Kang",
            "Xinjie wang",
            "Wei Sui",
            "Zhizhong Su",
            "Wenyu Liu",
            "Xinggang Wang",
            "Eunbyung Park"
        ],
        "tldr": "Uni3R is a novel feed-forward framework that jointly reconstructs and semantically interprets 3D scenes from unposed multi-view images using Gaussian primitives, achieving state-of-the-art performance on multiple benchmarks.",
        "tldr_zh": "Uni3R 是一种新的前馈框架，它使用高斯基元从无姿态的多视图图像中联合重建和语义解释 3D 场景，并在多个基准测试中实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DyCAF-Net: Dynamic Class-Aware Fusion Network",
        "summary": "Recent advancements in object detection rely on modular architectures with\nmulti-scale fusion and attention mechanisms. However, static fusion heuristics\nand class-agnostic attention limit performance in dynamic scenes with\nocclusions, clutter, and class imbalance. We introduce Dynamic Class-Aware\nFusion Network (DyCAF-Net) that addresses these challenges through three\ninnovations: (1) an input-conditioned equilibrium-based neck that iteratively\nrefines multi-scale features via implicit fixed-point modeling, (2) a dual\ndynamic attention mechanism that adaptively recalibrates channel and spatial\nresponses using input- and class-dependent cues, and (3) class-aware feature\nadaptation that modulates features to prioritize discriminative regions for\nrare classes. Through comprehensive ablation studies with YOLOv8 and related\narchitectures, alongside benchmarking against nine state-of-the-art baselines,\nDyCAF-Net achieves significant improvements in precision, mAP@50, and mAP@50-95\nacross 13 diverse benchmarks, including occlusion-heavy and long-tailed\ndatasets. The framework maintains computational efficiency ($\\sim$11.1M\nparameters) and competitive inference speeds, while its adaptability to scale\nvariance, semantic overlaps, and class imbalance positions it as a robust\nsolution for real-world detection tasks in medical imaging, surveillance, and\nautonomous systems.",
        "url": "http://arxiv.org/abs/2508.03598v1",
        "published_date": "2025-08-05T16:06:26+00:00",
        "updated_date": "2025-08-05T16:06:26+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Md Abrar Jahin",
            "Shahriar Soudeep",
            "M. F. Mridha",
            "Nafiz Fahad",
            "Md. Jakir Hossen"
        ],
        "tldr": "The paper introduces DyCAF-Net, a dynamic class-aware fusion network that improves object detection performance in challenging scenarios using novel feature refinement, attention, and adaptation mechanisms, demonstrating improvements across various benchmarks.",
        "tldr_zh": "该论文介绍了DyCAF-Net，一个动态的类别感知融合网络，通过新颖的特征细化、注意力和适应机制，提高了在具有挑战性的场景中的目标检测性能，并在各种基准测试中展示了改进。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Robust Single-Stage Fully Sparse 3D Object Detection via Detachable Latent Diffusion",
        "summary": "Denoising Diffusion Probabilistic Models (DDPMs) have shown success in robust\n3D object detection tasks. Existing methods often rely on the score matching\nfrom 3D boxes or pre-trained diffusion priors. However, they typically require\nmulti-step iterations in inference, which limits efficiency. To address this,\nwe propose a \\textbf{R}obust single-stage fully \\textbf{S}parse 3D object\n\\textbf{D}etection \\textbf{Net}work with a Detachable Latent Framework (DLF) of\nDDPMs, named RSDNet. Specifically, RSDNet learns the denoising process in\nlatent feature spaces through lightweight denoising networks like multi-level\ndenoising autoencoders (DAEs). This enables RSDNet to effectively understand\nscene distributions under multi-level perturbations, achieving robust and\nreliable detection. Meanwhile, we reformulate the noising and denoising\nmechanisms of DDPMs, enabling DLF to construct multi-type and multi-level noise\nsamples and targets, enhancing RSDNet robustness to multiple perturbations.\nFurthermore, a semantic-geometric conditional guidance is introduced to\nperceive the object boundaries and shapes, alleviating the center feature\nmissing problem in sparse representations, enabling RSDNet to perform in a\nfully sparse detection pipeline. Moreover, the detachable denoising network\ndesign of DLF enables RSDNet to perform single-step detection in inference,\nfurther enhancing detection efficiency. Extensive experiments on public\nbenchmarks show that RSDNet can outperform existing methods, achieving\nstate-of-the-art detection.",
        "url": "http://arxiv.org/abs/2508.03252v1",
        "published_date": "2025-08-05T09:30:39+00:00",
        "updated_date": "2025-08-05T09:30:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wentao Qu",
            "Guofeng Mei",
            "Jing Wang",
            "Yujiao Wu",
            "Xiaoshui Huang",
            "Liang Xiao"
        ],
        "tldr": "This paper introduces RSDNet, a robust and efficient single-stage 3D object detection network using a detachable latent diffusion framework, achieving state-of-the-art performance on public benchmarks.",
        "tldr_zh": "本文介绍了一种名为RSDNet的鲁棒高效的单阶段3D目标检测网络，该网络使用可分离的潜在扩散框架，并在公共基准测试中实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MVTOP: Multi-View Transformer-based Object Pose-Estimation",
        "summary": "We present MVTOP, a novel transformer-based method for multi-view rigid\nobject pose estimation. Through an early fusion of the view-specific features,\nour method can resolve pose ambiguities that would be impossible to solve with\na single view or with a post-processing of single-view poses. MVTOP models the\nmulti-view geometry via lines of sight that emanate from the respective camera\ncenters. While the method assumes the camera interior and relative orientations\nare known for a particular scene, they can vary for each inference. This makes\nthe method versatile. The use of the lines of sight enables MVTOP to correctly\npredict the correct pose with the merged multi-view information. To show the\nmodel's capabilities, we provide a synthetic data set that can only be solved\nwith such holistic multi-view approaches since the poses in the dataset cannot\nbe solved with just one view. Our method outperforms single-view and all\nexisting multi-view approaches on our dataset and achieves competitive results\non the YCB-V dataset. To the best of our knowledge, no holistic multi-view\nmethod exists that can resolve such pose ambiguities reliably. Our model is\nend-to-end trainable and does not require any additional data, e.g., depth.",
        "url": "http://arxiv.org/abs/2508.03243v1",
        "published_date": "2025-08-05T09:21:14+00:00",
        "updated_date": "2025-08-05T09:21:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Lukas Ranftl",
            "Felix Brendel",
            "Bertram Drost",
            "Carsten Steger"
        ],
        "tldr": "MVTOP is a novel transformer-based multi-view object pose estimation method that fuses view-specific features early to resolve pose ambiguities, outperforming single-view and existing multi-view methods on a synthetic dataset and achieving competitive results on YCB-V.",
        "tldr_zh": "MVTOP是一种新的基于Transformer的多视角物体姿态估计方法，它通过早期融合特定视角的特征来解决姿态模糊问题，在合成数据集上优于单视角和现有的多视角方法，并在YCB-V数据集上取得了有竞争力的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ActionSink: Toward Precise Robot Manipulation with Dynamic Integration of Action Flow",
        "summary": "Language-instructed robot manipulation has garnered significant interest due\nto the potential of learning from collected data. While the challenges in\nhigh-level perception and planning are continually addressed along the progress\nof general large pre-trained models, the low precision of low-level action\nestimation has emerged as the key limiting factor in manipulation performance.\nTo this end, this paper introduces a novel robot manipulation framework, i.e.,\nActionSink, to pave the way toward precise action estimations in the field of\nlearning-based robot manipulation. As the name suggests, ActionSink\nreformulates the actions of robots as action-caused optical flows from videos,\ncalled \"action flow\", in a self-supervised manner, which are then used to be\nretrieved and integrated to enhance the action estimation. Specifically,\nActionSink incorporates two primary modules. The first module is a\ncoarse-to-fine action flow matcher, which continuously refines the accuracy of\naction flow via iterative retrieval and denoising process. The second module is\na dynamic action flow integrator, which employs a working memory pool that\ndynamically and efficiently manages the historical action flows that should be\nused to integrate to enhance the current action estimation. In this module, a\nmulti-layer fusion module is proposed to integrate direct estimation and action\nflows from both the current and the working memory, achieving highly accurate\naction estimation through a series of estimation-integration processes. Our\nActionSink framework outperformed prior SOTA on the LIBERO benchmark by a 7.9\\%\nsuccess rate, and obtained nearly an 8\\% accuracy gain on the challenging\nlong-horizon visual task LIBERO-Long.",
        "url": "http://arxiv.org/abs/2508.03218v1",
        "published_date": "2025-08-05T08:46:17+00:00",
        "updated_date": "2025-08-05T08:46:17+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shanshan Guo",
            "Xiwen Liang",
            "Junfan Lin",
            "Yuzheng Zhuang",
            "Liang Lin",
            "Xiaodan Liang"
        ],
        "tldr": "The paper introduces ActionSink, a novel framework for precise robot manipulation that uses action-caused optical flows to refine action estimation through iterative retrieval, denoising, and dynamic integration of historical action flows, achieving SOTA results on LIBERO benchmarks.",
        "tldr_zh": "该论文介绍了一种名为ActionSink的新型机器人精准操作框架，该框架使用动作引起的光流，通过迭代检索、去噪和历史动作流的动态整合来改进动作估计，并在LIBERO基准测试上取得了SOTA结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MIDAR: Mimicking LiDAR Detection for Traffic Applications with a Lightweight Plug-and-Play Model",
        "summary": "As autonomous driving (AD) technology advances, increasing research has\nfocused on leveraging cooperative perception (CP) data collected from multiple\nAVs to enhance traffic applications. Due to the impracticality of large-scale\nreal-world AV deployments, simulation has become the primary approach in most\nstudies. While game-engine-based simulators like CARLA generate high-fidelity\nraw sensor data (e.g., LiDAR point clouds) which can be used to produce\nrealistic detection outputs, they face scalability challenges in multi-AV\nscenarios. In contrast, microscopic traffic simulators such as SUMO scale\nefficiently but lack perception modeling capabilities. To bridge this gap, we\npropose MIDAR, a LiDAR detection mimicking model that approximates realistic\nLiDAR detections using vehicle-level features readily available from\nmicroscopic traffic simulators. Specifically, MIDAR predicts true positives\n(TPs) and false negatives (FNs) from ideal LiDAR detection results based on the\nspatial layouts and dimensions of surrounding vehicles. A Refined Multi-hop\nLine-of-Sight (RM-LoS) graph is constructed to encode the occlusion\nrelationships among vehicles, upon which MIDAR employs a GRU-enhanced APPNP\narchitecture to propagate features from the ego AV and occluding vehicles to\nthe prediction target. MIDAR achieves an AUC of 0.909 in approximating the\ndetection results generated by CenterPoint, a mainstream 3D LiDAR detection\nmodel, on the nuScenes AD dataset. Two CP-based traffic applications further\nvalidate the necessity of such realistic detection modeling, particularly for\ntasks requiring accurate individual vehicle observations (e.g., position,\nspeed, lane index). As demonstrated in the applications, MIDAR can be\nseamlessly integrated into traffic simulators and trajectory datasets and will\nbe open-sourced upon publication.",
        "url": "http://arxiv.org/abs/2508.02858v1",
        "published_date": "2025-08-04T19:35:05+00:00",
        "updated_date": "2025-08-04T19:35:05+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianheng Zhu",
            "Yiheng Feng"
        ],
        "tldr": "MIDAR is a lightweight, plug-and-play model that mimics LiDAR detection using vehicle-level features from microscopic traffic simulators, bridging the gap between scalability and realistic perception modeling for traffic applications.",
        "tldr_zh": "MIDAR是一个轻量级的即插即用模型，它使用微观交通模拟器中的车辆级特征来模仿激光雷达检测，弥合了交通应用中可扩展性和真实感知建模之间的差距。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces",
        "summary": "Vision-and-Language Navigation (VLN) refers to the task of enabling\nautonomous robots to navigate unfamiliar environments by following natural\nlanguage instructions. While recent Large Vision-Language Models (LVLMs) have\nshown promise in this task, most current VLM systems rely on models\nspecifically designed and optimized for navigation, leaving the potential of\noff-the-shelf LVLMs underexplored. Furthermore, while older VLN approaches used\nlow-level action spaces with egocentric views and atomic actions (such as \"turn\nleft\" or \"move forward\"), newer models tend to favor panoramic action spaces\nwith discrete navigable viewpoints. This paper investigates (1) whether\noff-the-shelf LVLMs (fine-tuned without architectural modifications or\nsimulator-based training) can effectively support VLN tasks and (2) whether\nsuch models can support both low-level and panoramic action paradigms. To this\nend, we fine-tune the open-source model Qwen2.5-VL-3B-Instruct on the\nRoom-to-Room (R2R) dataset and evaluate its empirical performance across both\nlow-level and panoramic action spaces. The best resulting model achieves a 41%\nsuccess rate on the R2R test set, demonstrating that while off-the-shelf LVLMs\ncan learn to perform Vision-and-Language Navigation, they still lag behind\nmodels specifically designed for this task.",
        "url": "http://arxiv.org/abs/2508.02917v1",
        "published_date": "2025-08-04T21:45:21+00:00",
        "updated_date": "2025-08-04T21:45:21+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.CL",
            "cs.RO"
        ],
        "authors": [
            "Vebjørn Haug Kåsene",
            "Pierre Lison"
        ],
        "tldr": "This paper explores the effectiveness of fine-tuning off-the-shelf Large Vision-Language Models (LVLMs) for Vision-and-Language Navigation (VLN) using both low-level and panoramic action spaces, finding they lag behind specialized models but still achieve a 41% success rate on the R2R dataset.",
        "tldr_zh": "本文探讨了微调现成的视觉语言大模型（LVLMs）在视觉语言导航（VLN）中使用低级和全景动作空间的有效性，发现它们落后于专用模型，但在R2R数据集上仍然实现了41%的成功率。",
        "relevance_score": 8,
        "novelty_claim_score": 6,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]