[
    {
        "title": "UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving",
        "summary": "World models have demonstrated significant promise for data synthesis in autonomous driving. However, existing methods predominantly concentrate on single-modality generation, typically focusing on either multi-camera video or LiDAR sequence synthesis. In this paper, we propose UniDriveDreamer, a single-stage unified multimodal world model for autonomous driving, which directly generates multimodal future observations without relying on intermediate representations or cascaded modules. Our framework introduces a LiDAR-specific variational autoencoder (VAE) designed to encode input LiDAR sequences, alongside a video VAE for multi-camera images. To ensure cross-modal compatibility and training stability, we propose Unified Latent Anchoring (ULA), which explicitly aligns the latent distributions of the two modalities. The aligned features are fused and processed by a diffusion transformer that jointly models their geometric correspondence and temporal evolution. Additionally, structured scene layout information is projected per modality as a conditioning signal to guide the synthesis. Extensive experiments demonstrate that UniDriveDreamer outperforms previous state-of-the-art methods in both video and LiDAR generation, while also yielding measurable improvements in downstream",
        "url": "http://arxiv.org/abs/2602.02002v1",
        "published_date": "2026-02-02T12:02:27+00:00",
        "updated_date": "2026-02-02T12:02:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Guosheng Zhao",
            "Yaozeng Wang",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Tingdong Yu",
            "Guan Huang",
            "Yongchen Zai",
            "Ji Jiao",
            "Changliang Xue",
            "Xiaole Wang",
            "Zhen Yang",
            "Futang Zhu",
            "Xingang Wang"
        ],
        "tldr": "UniDriveDreamer is a single-stage multimodal world model for autonomous driving that generates future video and LiDAR observations jointly, achieving state-of-the-art performance and improving downstream tasks.",
        "tldr_zh": "UniDriveDreamer是一个用于自动驾驶的单阶段多模态世界模型，可以联合生成未来的视频和激光雷达观测，实现了最先进的性能并改进了下游任务。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "From Perception to Action: Spatial AI Agents and World Models",
        "summary": "While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence.",
        "url": "http://arxiv.org/abs/2602.01644v1",
        "published_date": "2026-02-02T05:00:55+00:00",
        "updated_date": "2026-02-02T05:00:55+00:00",
        "categories": [
            "cs.LG",
            "cs.AI",
            "cs.CV",
            "cs.MA",
            "cs.RO"
        ],
        "authors": [
            "Gloria Felicia",
            "Nolan Bryant",
            "Handi Putra",
            "Ayaan Gazali",
            "Eliel Lobo",
            "Esteban Rojas"
        ],
        "tldr": "This paper presents a unified taxonomy for spatial AI agents, bridging the gap between agentic capabilities and spatial tasks. It highlights the importance of hierarchical memory, GNN-LLM integration, and world models for building spatially-aware autonomous systems.",
        "tldr_zh": "本文提出了一个统一的空间人工智能代理分类法，弥合了代理能力和空间任务之间的差距。它强调了分层记忆系统、GNN-LLM集成和世界模型对于构建具有空间感知能力的自主系统的重要性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "LIEREx: Language-Image Embeddings for Robotic Exploration",
        "summary": "Semantic maps allow a robot to reason about its surroundings to fulfill tasks such as navigating known environments, finding specific objects, and exploring unmapped areas. Traditional mapping approaches provide accurate geometric representations but are often constrained by pre-designed symbolic vocabularies. The reliance on fixed object classes makes it impractical to handle out-of-distribution knowledge not defined at design time. Recent advances in Vision-Language Foundation Models, such as CLIP, enable open-set mapping, where objects are encoded as high-dimensional embeddings rather than fixed labels. In LIEREx, we integrate these VLFMs with established 3D Semantic Scene Graphs to enable target-directed exploration by an autonomous agent in partially unknown environments.",
        "url": "http://arxiv.org/abs/2602.01930v1",
        "published_date": "2026-02-02T10:30:50+00:00",
        "updated_date": "2026-02-02T10:30:50+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Felix Igelbrink",
            "Lennart Niecksch",
            "Marian Renz",
            "Martin Günther",
            "Martin Atzmueller"
        ],
        "tldr": "LIEREx uses Vision-Language Foundation Models (VLFMs) like CLIP with 3D Semantic Scene Graphs to enable target-directed robotic exploration in unknown environments, allowing for open-set mapping beyond fixed object classes.",
        "tldr_zh": "LIEREx 使用像 CLIP 这样的视觉-语言基础模型 (VLFMs) 与 3D 语义场景图结合，以实现自主机器人在未知环境中的目标导向探索，从而实现超出固定对象类别的开放集映射。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Multi-Task Learning for Robot Perception with Imbalanced Data",
        "summary": "Multi-task problem solving has been shown to improve the accuracy of the individual tasks, which is an important feature for robots, as they have a limited resource. However, when the number of labels for each task is not equal, namely imbalanced data exist, a problem may arise due to insufficient number of samples, and labeling is not very easy for mobile robots in every environment. We propose a method that can learn tasks even in the absence of the ground truth labels for some of the tasks. We also provide a detailed analysis of the proposed method. An interesting finding is related to the interaction of the tasks. We show a methodology to find out which tasks can improve the performance of other tasks. We investigate this by training the teacher network with the task outputs such as depth as inputs. We further provide empirical evidence when trained with a small amount of data. We use semantic segmentation and depth estimation tasks on different datasets, NYUDv2 and Cityscapes.",
        "url": "http://arxiv.org/abs/2602.01899v1",
        "published_date": "2026-02-02T10:05:59+00:00",
        "updated_date": "2026-02-02T10:05:59+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Ozgur Erkent"
        ],
        "tldr": "This paper proposes a multi-task learning method for robot perception that addresses the problem of imbalanced data by learning tasks even with missing labels. They analyze task interactions and provide empirical evidence on semantic segmentation and depth estimation tasks.",
        "tldr_zh": "本文提出了一种用于机器人感知的多任务学习方法，通过学习即使在缺少标签的情况下也能完成任务，从而解决了数据不平衡的问题。他们分析了任务之间的相互作用，并提供了在语义分割和深度估计任务上的实验证据。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Efficient Cross-Country Data Acquisition Strategy for ADAS via Street-View Imagery",
        "summary": "Deploying ADAS and ADS across countries remains challenging due to differences in legislation, traffic infrastructure, and visual conventions, which introduce domain shifts that degrade perception performance. Traditional cross-country data collection relies on extensive on-road driving, making it costly and inefficient to identify representative locations. To address this, we propose a street-view-guided data acquisition strategy that leverages publicly available imagery to identify places of interest (POI). Two POI scoring methods are introduced: a KNN-based feature distance approach using a vision foundation model, and a visual-attribution approach using a vision-language model. To enable repeatable evaluation, we adopt a collect-detect protocol and construct a co-located dataset by pairing the Zenseact Open Dataset with Mapillary street-view images. Experiments on traffic sign detection, a task particularly sensitive to cross-country variations in sign appearance, show that our approach achieves performance comparable to random sampling while using only half of the target-domain data. We further provide cost estimations for full-country analysis, demonstrating that large-scale street-view processing remains economically feasible. These results highlight the potential of street-view-guided data acquisition for efficient and cost-effective cross-country model adaptation.",
        "url": "http://arxiv.org/abs/2602.01836v1",
        "published_date": "2026-02-02T09:09:07+00:00",
        "updated_date": "2026-02-02T09:09:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yin Wu",
            "Daniel Slieter",
            "Carl Esselborn",
            "Ahmed Abouelazm",
            "Tsung Yuan Tseng",
            "J. Marius Zöllner"
        ],
        "tldr": "The paper introduces a street-view-guided data acquisition strategy for efficient cross-country ADAS deployment, using vision and vision-language models to identify representative locations and reduce the cost of on-road data collection.",
        "tldr_zh": "该论文介绍了一种基于街景的跨国ADAS高效数据采集策略，利用视觉和视觉-语言模型来识别代表性地点，从而降低了路面数据采集的成本。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DDP-WM: Disentangled Dynamics Prediction for Efficient World Models",
        "summary": "World models are essential for autonomous robotic planning. However, the substantial computational overhead of existing dense Transformerbased models significantly hinders real-time deployment. To address this efficiency-performance bottleneck, we introduce DDP-WM, a novel world model centered on the principle of Disentangled Dynamics Prediction (DDP). We hypothesize that latent state evolution in observed scenes is heterogeneous and can be decomposed into sparse primary dynamics driven by physical interactions and secondary context-driven background updates. DDP-WM realizes this decomposition through an architecture that integrates efficient historical processing with dynamic localization to isolate primary dynamics. By employing a crossattention mechanism for background updates, the framework optimizes resource allocation and provides a smooth optimization landscape for planners. Extensive experiments demonstrate that DDP-WM achieves significant efficiency and performance across diverse tasks, including navigation, precise tabletop manipulation, and complex deformable or multi-body interactions. Specifically, on the challenging Push-T task, DDP-WM achieves an approximately 9 times inference speedup and improves the MPC success rate from 90% to98% compared to state-of-the-art dense models. The results establish a promising path for developing efficient, high-fidelity world models. Codes will be available at https://github.com/HCPLabSYSU/DDP-WM.",
        "url": "http://arxiv.org/abs/2602.01780v1",
        "published_date": "2026-02-02T08:04:25+00:00",
        "updated_date": "2026-02-02T08:04:25+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Shicheng Yin",
            "Kaixuan Yin",
            "Weixing Chen",
            "Yang Liu",
            "Guanbin Li",
            "Liang Lin"
        ],
        "tldr": "The paper introduces DDP-WM, a novel world model based on disentangled dynamics prediction, achieving significant efficiency gains and performance improvements in robotic tasks by decomposing latent state evolution into sparse primary dynamics and secondary background updates.",
        "tldr_zh": "该论文介绍了DDP-WM，一种基于解耦动力学预测的新型世界模型，通过将潜在状态演化分解为稀疏的主要动力学和次要背景更新，在机器人任务中实现了显著的效率提升和性能改进。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 8
    },
    {
        "title": "UniDWM: Towards a Unified Driving World Model via Multifaceted Representation Learning",
        "summary": "Achieving reliable and efficient planning in complex driving environments requires a model that can reason over the scene's geometry, appearance, and dynamics. We present UniDWM, a unified driving world model that advances autonomous driving through multifaceted representation learning. UniDWM constructs a structure- and dynamic-aware latent world representation that serves as a physically grounded state space, enabling consistent reasoning across perception, prediction, and planning. Specifically, a joint reconstruction pathway learns to recover the scene's structure, including geometry and visual texture, while a collaborative generation framework leverages a conditional diffusion transformer to forecast future world evolution within the latent space. Furthermore, we show that our UniDWM can be deemed as a variation of VAE, which provides theoretical guidance for the multifaceted representation learning. Extensive experiments demonstrate the effectiveness of UniDWM in trajectory planning, 4D reconstruction and generation, highlighting the potential of multifaceted world representations as a foundation for unified driving intelligence. The code will be publicly available at https://github.com/Say2L/UniDWM.",
        "url": "http://arxiv.org/abs/2602.01536v1",
        "published_date": "2026-02-02T02:10:51+00:00",
        "updated_date": "2026-02-02T02:10:51+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Shuai Liu",
            "Siheng Ren",
            "Xiaoyao Zhu",
            "Quanmin Liang",
            "Zefeng Li",
            "Qiang Li",
            "Xin Hu",
            "Kai Huang"
        ],
        "tldr": "The paper introduces UniDWM, a unified driving world model using multifaceted representation learning for autonomous driving, enabling consistent reasoning across perception, prediction, and planning. It utilizes a joint reconstruction pathway and a conditional diffusion transformer within a VAE framework.",
        "tldr_zh": "该论文介绍了UniDWM，一种统一的驾驶世界模型，使用多方面的表征学习来实现自动驾驶，从而在感知、预测和规划中实现一致的推理。 它利用联合重建路径和条件扩散变换器在VAE框架内运行。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "TreeLoc: 6-DoF LiDAR Global Localization in Forests via Inter-Tree Geometric Matching",
        "summary": "Reliable localization is crucial for navigation in forests, where GPS is often degraded and LiDAR measurements are repetitive, occluded, and structurally complex. These conditions weaken the assumptions of traditional urban-centric localization methods, which assume that consistent features arise from unique structural patterns, necessitating forest-centric solutions to achieve robustness in these environments. To address these challenges, we propose TreeLoc, a LiDAR-based global localization framework for forests that handles place recognition and 6-DoF pose estimation. We represent scenes using tree stems and their Diameter at Breast Height (DBH), which are aligned to a common reference frame via their axes and summarized using the tree distribution histogram (TDH) for coarse matching, followed by fine matching with a 2D triangle descriptor. Finally, pose estimation is achieved through a two-step geometric verification. On diverse forest benchmarks, TreeLoc outperforms baselines, achieving precise localization. Ablation studies validate the contribution of each component. We also propose applications for long-term forest management using descriptors from a compact global tree database. TreeLoc is open-sourced for the robotics community at https://github.com/minwoo0611/TreeLoc.",
        "url": "http://arxiv.org/abs/2602.01501v1",
        "published_date": "2026-02-02T00:32:07+00:00",
        "updated_date": "2026-02-02T00:32:07+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Minwoo Jung",
            "Nived Chebrolu",
            "Lucas Carvalho de Lima",
            "Haedam Oh",
            "Maurice Fallon",
            "Ayoung Kim"
        ],
        "tldr": "The paper introduces TreeLoc, a LiDAR-based global localization framework specifically designed for forests using tree stem geometry, which outperforms existing methods on forest benchmarks and is open-sourced.",
        "tldr_zh": "该论文介绍了一种名为TreeLoc的基于激光雷达的全局定位框架，专门为森林环境设计，利用树干几何形状进行定位。在森林基准测试中，TreeLoc的性能优于现有方法，并且已开源。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]