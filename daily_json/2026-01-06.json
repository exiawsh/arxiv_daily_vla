[
    {
        "title": "AlignDrive: Aligned Lateral-Longitudinal Planning for End-to-End Autonomous Driving",
        "summary": "End-to-end autonomous driving has rapidly progressed, enabling joint perception and planning in complex environments. In the planning stage, state-of-the-art (SOTA) end-to-end autonomous driving models decouple planning into parallel lateral and longitudinal predictions. While effective, this parallel design can lead to i) coordination failures between the planned path and speed, and ii) underutilization of the drive path as a prior for longitudinal planning, thus redundantly encoding static information. To address this, we propose a novel cascaded framework that explicitly conditions longitudinal planning on the drive path, enabling coordinated and collision-aware lateral and longitudinal planning. Specifically, we introduce a path-conditioned formulation that explicitly incorporates the drive path into longitudinal planning. Building on this, the model predicts longitudinal displacements along the drive path rather than full 2D trajectory waypoints. This design simplifies longitudinal reasoning and more tightly couples it with lateral planning. Additionally, we introduce a planning-oriented data augmentation strategy that simulates rare safety-critical events, such as vehicle cut-ins, by adding agents and relabeling longitudinal targets to avoid collision. Evaluated on the challenging Bench2Drive benchmark, our method sets a new SOTA, achieving a driving score of 89.07 and a success rate of 73.18%, demonstrating significantly improved coordination and safety",
        "url": "http://arxiv.org/abs/2601.01762v1",
        "published_date": "2026-01-05T03:41:20+00:00",
        "updated_date": "2026-01-05T03:41:20+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Yanhao Wu",
            "Haoyang Zhang",
            "Fei He",
            "Rui Wu",
            "Congpei Qiu",
            "Liang Gao",
            "Wei Ke",
            "Tong Zhang"
        ],
        "tldr": "The paper introduces AlignDrive, a cascaded end-to-end autonomous driving framework that coordinates lateral and longitudinal planning by explicitly conditioning longitudinal planning on the drive path, achieving state-of-the-art performance on the Bench2Drive benchmark.",
        "tldr_zh": "该论文介绍了AlignDrive，一种级联的端到端自动驾驶框架，通过显式地将纵向规划置于驾驶路径的条件下，协调横向和纵向规划，在Bench2Drive基准测试中取得了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "LabelAny3D: Label Any Object 3D in the Wild",
        "summary": "Detecting objects in 3D space from monocular input is crucial for applications ranging from robotics to scene understanding. Despite advanced performance in the indoor and autonomous driving domains, existing monocular 3D detection models struggle with in-the-wild images due to the lack of 3D in-the-wild datasets and the challenges of 3D annotation. We introduce LabelAny3D, an \\emph{analysis-by-synthesis} framework that reconstructs holistic 3D scenes from 2D images to efficiently produce high-quality 3D bounding box annotations. Built on this pipeline, we present COCO3D, a new benchmark for open-vocabulary monocular 3D detection, derived from the MS-COCO dataset and covering a wide range of object categories absent from existing 3D datasets. Experiments show that annotations generated by LabelAny3D improve monocular 3D detection performance across multiple benchmarks, outperforming prior auto-labeling approaches in quality. These results demonstrate the promise of foundation-model-driven annotation for scaling up 3D recognition in realistic, open-world settings.",
        "url": "http://arxiv.org/abs/2601.01676v1",
        "published_date": "2026-01-04T22:03:45+00:00",
        "updated_date": "2026-01-04T22:03:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jin Yao",
            "Radowan Mahmud Redoy",
            "Sebastian Elbaum",
            "Matthew B. Dwyer",
            "Zezhou Cheng"
        ],
        "tldr": "The paper introduces LabelAny3D, an analysis-by-synthesis framework to generate 3D bounding box annotations from 2D images, and presents COCO3D, a new benchmark derived from MS-COCO for open-vocabulary monocular 3D detection, demonstrating improved performance in realistic settings.",
        "tldr_zh": "该论文介绍了一种名为LabelAny3D的分析-综合框架，用于从2D图像生成3D边界框注释，并提出了COCO3D，这是一个从MS-COCO派生的新基准，用于开放词汇单目3D检测，展示了在实际场景中性能的提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Learnability-Driven Submodular Optimization for Active Roadside 3D Detection",
        "summary": "Roadside perception datasets are typically constructed via cooperative labeling between synchronized vehicle and roadside frame pairs. However, real deployment often requires annotation of roadside-only data due to hardware and privacy constraints. Even human experts struggle to produce accurate labels without vehicle-side data (image, LIDAR), which not only increases annotation difficulty and cost, but also reveals a fundamental learnability problem: many roadside-only scenes contain distant, blurred, or occluded objects whose 3D properties are ambiguous from a single view and can only be reliably annotated by cross-checking paired vehicle--roadside frames. We refer to such cases as inherently ambiguous samples. To reduce wasted annotation effort on inherently ambiguous samples while still obtaining high-performing models, we turn to active learning. This work focuses on active learning for roadside monocular 3D object detection and proposes a learnability-driven framework that selects scenes which are both informative and reliably labelable, suppressing inherently ambiguous samples while ensuring coverage. Experiments demonstrate that our method, LH3D, achieves 86.06%, 67.32%, and 78.67% of full-performance for vehicles, pedestrians, and cyclists respectively, using only 25% of the annotation budget on DAIR-V2X-I, significantly outperforming uncertainty-based baselines. This confirms that learnability, not uncertainty, matters for roadside 3D perception.",
        "url": "http://arxiv.org/abs/2601.01695v1",
        "published_date": "2026-01-04T23:59:06+00:00",
        "updated_date": "2026-01-04T23:59:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ruiyu Mao",
            "Baoming Zhang",
            "Nicholas Ruozzi",
            "Yunhui Guo"
        ],
        "tldr": "The paper introduces an active learning framework (LH3D) for roadside monocular 3D object detection that selects scenes based on learnability rather than uncertainty, achieving strong performance with only 25% of the annotation budget.",
        "tldr_zh": "该论文介绍了一种用于路侧单目3D物体检测的主动学习框架（LH3D），它基于可学习性而非不确定性选择场景，仅用25%的标注预算就实现了强大的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]