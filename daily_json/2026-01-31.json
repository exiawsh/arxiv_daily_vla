[
    {
        "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
        "summary": "Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.",
        "url": "http://arxiv.org/abs/2601.22153v1",
        "published_date": "2026-01-29T18:59:51+00:00",
        "updated_date": "2026-01-29T18:59:51+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Haozhe Xie",
            "Beichen Wen",
            "Jiarui Zheng",
            "Zhaoxi Chen",
            "Fangzhou Hong",
            "Haiwen Diao",
            "Ziwei Liu"
        ],
        "tldr": "The paper introduces DynamicVLA, a novel vision-language-action model designed for dynamic object manipulation, featuring temporal reasoning, continuous inference, and latent-aware action streaming, along with a new benchmark dataset, DOM.",
        "tldr_zh": "该论文介绍了一种名为 DynamicVLA 的新型视觉-语言-动作模型，专为动态物体操作而设计。它具有时间推理、连续推理和潜在感知动作流等特性，并提出了一个新的基准数据集 DOM。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]