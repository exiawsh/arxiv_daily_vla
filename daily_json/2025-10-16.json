[
    {
        "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy",
        "summary": "We introduce InternVLA-M1, a unified framework for spatial grounding and\nrobot control that advances instruction-following robots toward scalable,\ngeneral-purpose intelligence. Its core idea is spatially guided\nvision-language-action training, where spatial grounding serves as the critical\nlink between instructions and robot actions. InternVLA-M1 employs a two-stage\npipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning\ndata to determine ``where to act'' by aligning instructions with visual,\nembodiment-agnostic positions, and (ii) spatially guided action post-training\nto decide ``how to act'' by generating embodiment-aware actions through\nplug-and-play spatial prompting. This spatially guided training recipe yields\nconsistent gains: InternVLA-M1 outperforms its variant without spatial guidance\nby +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO\nFranka, while demonstrating stronger spatial reasoning capability in box,\npoint, and trace prediction. To further scale instruction following, we built a\nsimulation engine to collect 244K generalizable pick-and-place episodes,\nenabling a 6.2% average improvement across 200 tasks and 3K+ objects. In\nreal-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with\nsynthetic co-training, achieved +20.6% on unseen objects and novel\nconfigurations. Moreover, in long-horizon reasoning-intensive scenarios, it\nsurpassed existing works by over 10%. These results highlight spatially guided\ntraining as a unifying principle for scalable and resilient generalist robots.\nCode and models are available at\nhttps://github.com/InternRobotics/InternVLA-M1.",
        "url": "http://arxiv.org/abs/2510.13778v1",
        "published_date": "2025-10-15T17:30:05+00:00",
        "updated_date": "2025-10-15T17:30:05+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Xinyi Chen",
            "Yilun Chen",
            "Yanwei Fu",
            "Ning Gao",
            "Jiaya Jia",
            "Weiyang Jin",
            "Hao Li",
            "Yao Mu",
            "Jiangmiao Pang",
            "Yu Qiao",
            "Yang Tian",
            "Bin Wang",
            "Bolun Wang",
            "Fangjing Wang",
            "Hanqing Wang",
            "Tai Wang",
            "Ziqin Wang",
            "Xueyuan Wei",
            "Chao Wu",
            "Shuai Yang",
            "Jinhui Ye",
            "Junqiu Yu",
            "Jia Zeng",
            "Jingjing Zhang",
            "Jinyu Zhang",
            "Shi Zhang",
            "Feng Zheng",
            "Bowen Zhou",
            "Yangkun Zhu"
        ],
        "tldr": "InternVLA-M1 is a vision-language-action framework for robot control using spatial grounding to link instructions and actions, demonstrating significant performance improvements in simulation and real-world scenarios.",
        "tldr_zh": "InternVLA-M1 是一个视觉-语言-动作框架，通过空间定位将指令与机器人动作联系起来，用于机器人控制，在模拟和现实场景中表现出显著的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning",
        "summary": "Vision-Language-Action (VLA) models have recently shown impressive\ngeneralization and language-guided manipulation capabilities. However, their\nperformance degrades on tasks requiring precise spatial reasoning due to\nlimited spatial reasoning inherited from Vision-Language Models (VLMs).\nExisting VLAs rely on extensive action-data pretraining to ground VLMs in 3D\nspace, which reduces training efficiency and is still insufficient for accurate\nspatial understanding. In this work, we present DepthVLA, a simple yet\neffective VLA architecture that explicitly incorporates spatial awareness\nthrough a pretrained depth prediction module. DepthVLA adopts a\nmixture-of-transformers design that unifies a VLM, a depth transformer, and an\naction expert with fully shared attentions, forming an end-to-end model with\nenhanced spatial reasoning. Extensive evaluations in both real-world and\nsimulated environments show that DepthVLA outperforms state-of-the-art\napproaches, achieving 78.5% vs. 65.0% progress in real-world tasks, 94.9% vs.\n93.6% in the LIBERO simulator, and 74.8% vs. 58.8% in the Simpler simulator.\nOur code will be made publicly available.",
        "url": "http://arxiv.org/abs/2510.13375v1",
        "published_date": "2025-10-15T10:09:00+00:00",
        "updated_date": "2025-10-15T10:09:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Tianyuan Yuan",
            "Yicheng Liu",
            "Chenhao Lu",
            "Zhuoguang Chen",
            "Tao Jiang",
            "Hang Zhao"
        ],
        "tldr": "The paper introduces DepthVLA, a Vision-Language-Action model that incorporates a pretrained depth prediction module to improve spatial reasoning for manipulation tasks, demonstrating superior performance in both real-world and simulated environments.",
        "tldr_zh": "该论文介绍了 DepthVLA，一种视觉-语言-动作模型，它结合了预训练的深度预测模块，以提高操作任务中的空间推理能力，并在真实世界和模拟环境中都表现出卓越的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "SimULi: Real-Time LiDAR and Camera Simulation with Unscented Transforms",
        "summary": "Rigorous testing of autonomous robots, such as self-driving vehicles, is\nessential to ensure their safety in real-world deployments. This requires\nbuilding high-fidelity simulators to test scenarios beyond those that can be\nsafely or exhaustively collected in the real-world. Existing neural rendering\nmethods based on NeRF and 3DGS hold promise but suffer from low rendering\nspeeds or can only render pinhole camera models, hindering their suitability to\napplications that commonly require high-distortion lenses and LiDAR data.\nMulti-sensor simulation poses additional challenges as existing methods handle\ncross-sensor inconsistencies by favoring the quality of one modality at the\nexpense of others. To overcome these limitations, we propose SimULi, the first\nmethod capable of rendering arbitrary camera models and LiDAR data in\nreal-time. Our method extends 3DGUT, which natively supports complex camera\nmodels, with LiDAR support, via an automated tiling strategy for arbitrary\nspinning LiDAR models and ray-based culling. To address cross-sensor\ninconsistencies, we design a factorized 3D Gaussian representation and\nanchoring strategy that reduces mean camera and depth error by up to 40%\ncompared to existing methods. SimULi renders 10-20x faster than ray tracing\napproaches and 1.5-10x faster than prior rasterization-based work (and handles\na wider range of camera models). When evaluated on two widely benchmarked\nautonomous driving datasets, SimULi matches or exceeds the fidelity of existing\nstate-of-the-art methods across numerous camera and LiDAR metrics.",
        "url": "http://arxiv.org/abs/2510.12901v1",
        "published_date": "2025-10-14T18:22:45+00:00",
        "updated_date": "2025-10-14T18:22:45+00:00",
        "categories": [
            "cs.CV",
            "cs.GR",
            "cs.LG",
            "cs.RO"
        ],
        "authors": [
            "Haithem Turki",
            "Qi Wu",
            "Xin Kang",
            "Janick Martinez Esturo",
            "Shengyu Huang",
            "Ruilong Li",
            "Zan Gojcic",
            "Riccardo de Lutio"
        ],
        "tldr": "The paper introduces SimULi, a real-time LiDAR and camera simulator that supports arbitrary camera models and addresses cross-sensor inconsistencies, achieving significant speedups and fidelity improvements compared to existing methods.",
        "tldr_zh": "该论文介绍了 SimULi，一种实时的激光雷达和相机模拟器，支持任意相机模型并解决跨传感器不一致性问题，与现有方法相比，实现了显著的速度提升和保真度改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Dedelayed: Deleting remote inference delay via on-device correction",
        "summary": "Remote inference allows lightweight devices to leverage powerful cloud\nmodels. However, communication network latency makes predictions stale and\nunsuitable for real-time tasks. To address this, we introduce Dedelayed, a\ndelay-corrective method that mitigates arbitrary remote inference delays,\nallowing the local device to produce low-latency outputs in real time. Our\nmethod employs a lightweight local model that processes the current frame and\nfuses in features that a heavyweight remote model computes from past frames. On\nvideo from the BDD100K driving dataset, Dedelayed improves semantic\nsegmentation accuracy over the stronger of the local-only and remote-only\nbaselines across all realistic communication network delays beyond 33 ms.\nWithout incurring additional delay, it improves accuracy by 6.4 mIoU compared\nto fully local inference and 9.8 mIoU compared to remote inference, for a\nround-trip delay of 100 ms. The advantage grows under longer delays and\nhigher-motion scenes, as delay-mitigated split inference sustains accuracy more\neffectively, providing clear advantages for real-time tasks that must remain\naligned with the current world state.",
        "url": "http://arxiv.org/abs/2510.13714v1",
        "published_date": "2025-10-15T16:13:44+00:00",
        "updated_date": "2025-10-15T16:13:44+00:00",
        "categories": [
            "eess.IV",
            "cs.AI",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Dan Jacobellis",
            "Mateen Ulhaq",
            "Fabien Racapé",
            "Hyomin Choi",
            "Neeraja J. Yadwadkar"
        ],
        "tldr": "The paper introduces Dedelayed, a method to mitigate remote inference delays in real-time tasks by fusing features from a remote model's past frames with a local model processing the current frame, demonstrating improved accuracy on semantic segmentation for driving datasets.",
        "tldr_zh": "本文介绍了一种名为Dedelayed的方法，通过融合远程模型过去帧的特征与本地模型处理的当前帧，来缓解实时任务中的远程推理延迟，并在驾驶数据集的语义分割方面展示了更高的准确性。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models",
        "summary": "Visual-Language-Action (VLA) models report impressive success rates on\nrobotic manipulation benchmarks, yet these results may mask fundamental\nweaknesses in robustness. We perform a systematic vulnerability analysis by\nintroducing controlled perturbations across seven dimensions: objects layout,\ncamera viewpoints, robot initial states, language instructions, light\nconditions, background textures and sensor noise. We comprehensively analyzed\nmultiple state-of-the-art models and revealed consistent brittleness beneath\napparent competence. Our analysis exposes critical weaknesses: models exhibit\nextreme sensitivity to perturbation factors, including camera viewpoints and\nrobot initial states, with performance dropping from 95% to below 30% under\nmodest perturbations. Surprisingly, models are largely insensitive to language\nvariations, with further experiments revealing that models tend to ignore\nlanguage instructions completely. Our findings challenge the assumption that\nhigh benchmark scores equate to true competency and highlight the need for\nevaluation practices that assess reliability under realistic variation.",
        "url": "http://arxiv.org/abs/2510.13626v1",
        "published_date": "2025-10-15T14:51:36+00:00",
        "updated_date": "2025-10-15T14:51:36+00:00",
        "categories": [
            "cs.RO",
            "cs.CL",
            "cs.CV"
        ],
        "authors": [
            "Senyu Fei",
            "Siyin Wang",
            "Junhao Shi",
            "Zihao Dai",
            "Jikun Cai",
            "Pengfang Qian",
            "Li Ji",
            "Xinzhe He",
            "Shiduo Zhang",
            "Zhaoye Fei",
            "Jinlan Fu",
            "Jingjing Gong",
            "Xipeng Qiu"
        ],
        "tldr": "This paper presents a comprehensive robustness analysis of VLA models, revealing significant performance drops under various perturbations and a surprising insensitivity to language instructions, challenging the validity of high benchmark scores.",
        "tldr_zh": "本文对视觉-语言-动作 (VLA) 模型的鲁棒性进行了全面分析，揭示了模型在各种扰动下性能显著下降，并且对语言指令不敏感，从而质疑了高基准分数的有效性。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "XD-RCDepth: Lightweight Radar-Camera Depth Estimation with Explainability-Aligned and Distribution-Aware Distillation",
        "summary": "Depth estimation remains central to autonomous driving, and radar-camera\nfusion offers robustness in adverse conditions by providing complementary\ngeometric cues. In this paper, we present XD-RCDepth, a lightweight\narchitecture that reduces the parameters by 29.7% relative to the\nstate-of-the-art lightweight baseline while maintaining comparable accuracy. To\npreserve performance under compression and enhance interpretability, we\nintroduce two knowledge-distillation strategies: an explainability-aligned\ndistillation that transfers the teacher's saliency structure to the student,\nand a depth-distribution distillation that recasts depth regression as soft\nclassification over discretized bins. Together, these components reduce the MAE\ncompared with direct training with 7.97% and deliver competitive accuracy with\nreal-time efficiency on nuScenes and ZJU-4DRadarCam datasets.",
        "url": "http://arxiv.org/abs/2510.13565v1",
        "published_date": "2025-10-15T14:05:33+00:00",
        "updated_date": "2025-10-15T14:05:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Huawei Sun",
            "Zixu Wang",
            "Xiangyuan Peng",
            "Julius Ott",
            "Georg Stettinger",
            "Lorenzo Servadei",
            "Robert Wille"
        ],
        "tldr": "XD-RCDepth is a lightweight radar-camera depth estimation architecture using explainability-aligned and distribution-aware knowledge distillation, achieving comparable accuracy with state-of-the-art while reducing parameters and improving interpretability.",
        "tldr_zh": "XD-RCDepth 是一种轻量级的雷达-相机深度估计架构，它使用与可解释性对齐和分布感知的知识蒸馏，在减少参数的同时，实现了与最先进技术相当的精度，并提高了可解释性。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Accelerated Feature Detectors for Visual SLAM: A Comparative Study of FPGA vs GPU",
        "summary": "Feature detection is a common yet time-consuming module in Simultaneous\nLocalization and Mapping (SLAM) implementations, which are increasingly\ndeployed on power-constrained platforms, such as drones. Graphics Processing\nUnits (GPUs) have been a popular accelerator for computer vision in general,\nand feature detection and SLAM in particular.\n  On the other hand, System-on-Chips (SoCs) with integrated Field Programmable\nGate Array (FPGA) are also widely available. This paper presents the first\nstudy of hardware-accelerated feature detectors considering a Visual SLAM\n(V-SLAM) pipeline. We offer new insights by comparing the best GPU-accelerated\nFAST, Harris, and SuperPoint implementations against the FPGA-accelerated\ncounterparts on modern SoCs (Nvidia Jetson Orin and AMD Versal).\n  The evaluation shows that when using a non-learning-based feature detector\nsuch as FAST and Harris, their GPU implementations, and the GPU-accelerated\nV-SLAM can achieve better run-time performance and energy efficiency than the\nFAST and Harris FPGA implementations as well as the FPGA-accelerated V-SLAM.\nHowever, when considering a learning-based detector such as SuperPoint, its\nFPGA implementation can achieve better run-time performance and energy\nefficiency (up to 3.1$\\times$ and 1.4$\\times$ improvements, respectively) than\nthe GPU implementation. The FPGA-accelerated V-SLAM can also achieve comparable\nrun-time performance compared to the GPU-accelerated V-SLAM, with better FPS in\n2 out of 5 dataset sequences. When considering the accuracy, the results show\nthat the GPU-accelerated V-SLAM is more accurate than the FPGA-accelerated\nV-SLAM in general. Last but not least, the use of hardware acceleration for\nfeature detection could further improve the performance of the V-SLAM pipeline\nby having the global bundle adjustment module invoked less frequently without\nsacrificing accuracy.",
        "url": "http://arxiv.org/abs/2510.13546v1",
        "published_date": "2025-10-15T13:40:55+00:00",
        "updated_date": "2025-10-15T13:40:55+00:00",
        "categories": [
            "cs.CV",
            "cs.ET",
            "cs.PF",
            "cs.RO",
            "C.3; C.4; I.4.6"
        ],
        "authors": [
            "Ruiqi Ye",
            "Mikel Luján"
        ],
        "tldr": "This paper compares GPU and FPGA acceleration for feature detection in visual SLAM, finding that FPGAs outperform GPUs for learning-based detectors like SuperPoint in terms of runtime and energy efficiency, while GPUs generally provide better accuracy.",
        "tldr_zh": "本文比较了GPU和FPGA在视觉SLAM中特征检测的加速效果，发现对于像SuperPoint这样的基于学习的检测器，FPGA在运行时间和能源效率方面优于GPU，而GPU通常提供更好的精度。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Through the Lens of Doubt: Robust and Efficient Uncertainty Estimation for Visual Place Recognition",
        "summary": "Visual Place Recognition (VPR) enables robots and autonomous vehicles to\nidentify previously visited locations by matching current observations against\na database of known places. However, VPR systems face significant challenges\nwhen deployed across varying visual environments, lighting conditions, seasonal\nchanges, and viewpoints changes. Failure-critical VPR applications, such as\nloop closure detection in simultaneous localization and mapping (SLAM)\npipelines, require robust estimation of place matching uncertainty. We propose\nthree training-free uncertainty metrics that estimate prediction confidence by\nanalyzing inherent statistical patterns in similarity scores from any existing\nVPR method. Similarity Distribution (SD) quantifies match distinctiveness by\nmeasuring score separation between candidates; Ratio Spread (RS) evaluates\ncompetitive ambiguity among top-scoring locations; and Statistical Uncertainty\n(SU) is a combination of SD and RS that provides a unified metric that\ngeneralizes across datasets and VPR methods without requiring validation data\nto select the optimal metric. All three metrics operate without additional\nmodel training, architectural modifications, or computationally expensive\ngeometric verification. Comprehensive evaluation across nine state-of-the-art\nVPR methods and six benchmark datasets confirms that our metrics excel at\ndiscriminating between correct and incorrect VPR matches, and consistently\noutperform existing approaches while maintaining negligible computational\noverhead, making it deployable for real-time robotic applications across varied\nenvironmental conditions with improved precision-recall performance.",
        "url": "http://arxiv.org/abs/2510.13464v1",
        "published_date": "2025-10-15T12:12:55+00:00",
        "updated_date": "2025-10-15T12:12:55+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Emily Miller",
            "Michael Milford",
            "Muhammad Burhan Hafez",
            "SD Ramchurn",
            "Shoaib Ehsan"
        ],
        "tldr": "This paper introduces three training-free uncertainty metrics for Visual Place Recognition (VPR) that improve the robustness and reliability of VPR systems in challenging environments without requiring additional training or significant computational overhead.",
        "tldr_zh": "本文介绍了一种用于视觉位置识别(VPR)的三种免训练的不确定性指标，提高了VPR系统在具有挑战性的环境中的鲁棒性和可靠性，且不需要额外的训练或显著的计算开销。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CoDS: Enhancing Collaborative Perception in Heterogeneous Scenarios via Domain Separation",
        "summary": "Collaborative perception has been proven to improve individual perception in\nautonomous driving through multi-agent interaction. Nevertheless, most methods\noften assume identical encoders for all agents, which does not hold true when\nthese models are deployed in real-world applications. To realize collaborative\nperception in actual heterogeneous scenarios, existing methods usually align\nneighbor features to those of the ego vehicle, which is vulnerable to noise\nfrom domain gaps and thus fails to address feature discrepancies effectively.\nMoreover, they adopt transformer-based modules for domain adaptation, which\ncauses the model inference inefficiency on mobile devices. To tackle these\nissues, we propose CoDS, a Collaborative perception method that leverages\nDomain Separation to address feature discrepancies in heterogeneous scenarios.\nThe CoDS employs two feature alignment modules, i.e., Lightweight\nSpatial-Channel Resizer (LSCR) and Distribution Alignment via Domain Separation\n(DADS). Besides, it utilizes the Domain Alignment Mutual Information (DAMI)\nloss to ensure effective feature alignment. Specifically, the LSCR aligns the\nneighbor feature across spatial and channel dimensions using a lightweight\nconvolutional layer. Subsequently, the DADS mitigates feature distribution\ndiscrepancy with encoder-specific and encoder-agnostic domain separation\nmodules. The former removes domain-dependent information and the latter\ncaptures task-related information. During training, the DAMI loss maximizes the\nmutual information between aligned heterogeneous features to enhance the domain\nseparation process. The CoDS employs a fully convolutional architecture, which\nensures high inference efficiency. Extensive experiments demonstrate that the\nCoDS effectively mitigates feature discrepancies in heterogeneous scenarios and\nachieves a trade-off between detection accuracy and inference efficiency.",
        "url": "http://arxiv.org/abs/2510.13432v1",
        "published_date": "2025-10-15T11:29:14+00:00",
        "updated_date": "2025-10-15T11:29:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yushan Han",
            "Hui Zhang",
            "Honglei Zhang",
            "Chuntao Ding",
            "Yuanzhouhan Cao",
            "Yidong Li"
        ],
        "tldr": "The paper introduces CoDS, a collaborative perception method for autonomous driving in heterogeneous scenarios, using domain separation to address feature discrepancies and improve inference efficiency with a fully convolutional architecture.",
        "tldr_zh": "该论文介绍了一种名为CoDS的协同感知方法，用于解决自动驾驶中异构场景下的特征差异问题。该方法通过域分离技术，并采用全卷积架构，提高了推理效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CymbaDiff: Structured Spatial Diffusion for Sketch-based 3D Semantic Urban Scene Generation",
        "summary": "Outdoor 3D semantic scene generation produces realistic and semantically rich\nenvironments for applications such as urban simulation and autonomous driving.\nHowever, advances in this direction are constrained by the absence of publicly\navailable, well-annotated datasets. We introduce SketchSem3D, the first\nlarge-scale benchmark for generating 3D outdoor semantic scenes from abstract\nfreehand sketches and pseudo-labeled annotations of satellite images.\nSketchSem3D includes two subsets, Sketch-based SemanticKITTI and Sketch-based\nKITTI-360 (containing LiDAR voxels along with their corresponding sketches and\nannotated satellite images), to enable standardized, rigorous, and diverse\nevaluations. We also propose Cylinder Mamba Diffusion (CymbaDiff) that\nsignificantly enhances spatial coherence in outdoor 3D scene generation.\nCymbaDiff imposes structured spatial ordering, explicitly captures cylindrical\ncontinuity and vertical hierarchy, and preserves both physical neighborhood\nrelationships and global context within the generated scenes. Extensive\nexperiments on SketchSem3D demonstrate that CymbaDiff achieves superior\nsemantic consistency, spatial realism, and cross-dataset generalization. The\ncode and dataset will be available at\nhttps://github.com/Lillian-research-hub/CymbaDiff",
        "url": "http://arxiv.org/abs/2510.13245v1",
        "published_date": "2025-10-15T07:47:00+00:00",
        "updated_date": "2025-10-15T07:47:00+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Li Liang",
            "Bo Miao",
            "Xinyu Wang",
            "Naveed Akhtar",
            "Jordan Vice",
            "Ajmal Mian"
        ],
        "tldr": "The paper introduces SketchSem3D, a large-scale benchmark dataset for 3D urban scene generation from sketches, and CymbaDiff, a diffusion model that enhances spatial coherence in generated scenes using cylindrical continuity and vertical hierarchy.",
        "tldr_zh": "该论文介绍了SketchSem3D，一个用于从草图生成3D城市场景的大规模基准数据集，以及CymbaDiff，一个利用圆柱连续性和垂直层级结构来增强生成场景中空间连贯性的扩散模型。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FlyAwareV2: A Multimodal Cross-Domain UAV Dataset for Urban Scene Understanding",
        "summary": "The development of computer vision algorithms for Unmanned Aerial Vehicle\n(UAV) applications in urban environments heavily relies on the availability of\nlarge-scale datasets with accurate annotations. However, collecting and\nannotating real-world UAV data is extremely challenging and costly. To address\nthis limitation, we present FlyAwareV2, a novel multimodal dataset encompassing\nboth real and synthetic UAV imagery tailored for urban scene understanding\ntasks. Building upon the recently introduced SynDrone and FlyAware datasets,\nFlyAwareV2 introduces several new key contributions: 1) Multimodal data (RGB,\ndepth, semantic labels) across diverse environmental conditions including\nvarying weather and daytime; 2) Depth maps for real samples computed via\nstate-of-the-art monocular depth estimation; 3) Benchmarks for RGB and\nmultimodal semantic segmentation on standard architectures; 4) Studies on\nsynthetic-to-real domain adaptation to assess the generalization capabilities\nof models trained on the synthetic data. With its rich set of annotations and\nenvironmental diversity, FlyAwareV2 provides a valuable resource for research\non UAV-based 3D urban scene understanding.",
        "url": "http://arxiv.org/abs/2510.13243v1",
        "published_date": "2025-10-15T07:44:31+00:00",
        "updated_date": "2025-10-15T07:44:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Francesco Barbato",
            "Matteo Caligiuri",
            "Pietro Zanuttigh"
        ],
        "tldr": "FlyAwareV2 is a new multimodal dataset comprising real and synthetic UAV imagery with depth and semantic labels for urban scene understanding, aimed at improving domain adaptation for UAV applications.",
        "tldr_zh": "FlyAwareV2是一个新的多模态数据集，包含真实和合成的无人机图像，带有深度和语义标签，用于城市场景理解，旨在提高无人机应用的领域自适应能力。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models",
        "summary": "Vision-Language-Action (VLA) models have achieved revolutionary progress in\nrobot learning, enabling robots to execute complex physical robot tasks from\nnatural language instructions. Despite this progress, their adversarial\nrobustness remains underexplored. In this work, we propose both adversarial\npatch attack and corresponding defense strategies for VLA models. We first\nintroduce the Embedding Disruption Patch Attack (EDPA), a model-agnostic\nadversarial attack that generates patches directly placeable within the\ncamera's view. In comparison to prior methods, EDPA can be readily applied to\ndifferent VLA models without requiring prior knowledge of the model\narchitecture, or the controlled robotic manipulator. EDPA constructs these\npatches by (i) disrupting the semantic alignment between visual and textual\nlatent representations, and (ii) maximizing the discrepancy of latent\nrepresentations between adversarial and corresponding clean visual inputs.\nThrough the optimization of these objectives, EDPA distorts the VLA's\ninterpretation of visual information, causing the model to repeatedly generate\nincorrect actions and ultimately result in failure to complete the given\nrobotic task. To counter this, we propose an adversarial fine-tuning scheme for\nthe visual encoder, in which the encoder is optimized to produce similar latent\nrepresentations for both clean and adversarially perturbed visual inputs.\nExtensive evaluations on the widely recognized LIBERO robotic simulation\nbenchmark demonstrate that EDPA substantially increases the task failure rate\nof cutting-edge VLA models, while our proposed defense effectively mitigates\nthis degradation. The codebase is accessible via the homepage at\nhttps://edpa-attack.github.io/.",
        "url": "http://arxiv.org/abs/2510.13237v1",
        "published_date": "2025-10-15T07:42:44+00:00",
        "updated_date": "2025-10-15T07:42:44+00:00",
        "categories": [
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Haochuan Xu",
            "Yun Sing Koh",
            "Shuhuai Huang",
            "Zirun Zhou",
            "Di Wang",
            "Jun Sakuma",
            "Jingfeng Zhang"
        ],
        "tldr": "This paper proposes a model-agnostic adversarial patch attack (EDPA) for Vision-Language-Action (VLA) models and a corresponding adversarial fine-tuning defense strategy, demonstrating its effectiveness on the LIBERO benchmark.",
        "tldr_zh": "该论文提出了一种针对视觉-语言-动作 (VLA) 模型的模型无关的对抗补丁攻击 (EDPA) 以及相应的对抗微调防御策略，并在 LIBERO 基准测试上展示了其有效性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Complementary Information Guided Occupancy Prediction via Multi-Level Representation Fusion",
        "summary": "Camera-based occupancy prediction is a mainstream approach for 3D perception\nin autonomous driving, aiming to infer complete 3D scene geometry and semantics\nfrom 2D images. Almost existing methods focus on improving performance through\nstructural modifications, such as lightweight backbones and complex cascaded\nframeworks, with good yet limited performance. Few studies explore from the\nperspective of representation fusion, leaving the rich diversity of features in\n2D images underutilized. Motivated by this, we propose \\textbf{CIGOcc, a\ntwo-stage occupancy prediction framework based on multi-level representation\nfusion. \\textbf{CIGOcc extracts segmentation, graphics, and depth features from\nan input image and introduces a deformable multi-level fusion mechanism to fuse\nthese three multi-level features. Additionally, CIGOcc incorporates knowledge\ndistilled from SAM to further enhance prediction accuracy. Without increasing\ntraining costs, CIGOcc achieves state-of-the-art performance on the\nSemanticKITTI benchmark. The code is provided in the supplementary material and\nwill be released https://github.com/VitaLemonTea1/CIGOcc",
        "url": "http://arxiv.org/abs/2510.13198v1",
        "published_date": "2025-10-15T06:37:33+00:00",
        "updated_date": "2025-10-15T06:37:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Rongtao Xu",
            "Jinzhou Lin",
            "Jialei Zhou",
            "Jiahua Dong",
            "Changwei Wang",
            "Ruisheng Wang",
            "Li Guo",
            "Shibiao Xu",
            "Xiaodan Liang"
        ],
        "tldr": "This paper introduces CIGOcc, a two-stage occupancy prediction framework that fuses multi-level segmentation, graphics, and depth features, enhanced by knowledge distillation from SAM, to achieve state-of-the-art performance on the SemanticKITTI benchmark.",
        "tldr_zh": "本文介绍了一种名为CIGOcc的两阶段占用预测框架，该框架融合了多层次的分割、图形和深度特征，并通过SAM的知识蒸馏进行增强，从而在SemanticKITTI基准测试中实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DriveCritic: Towards Context-Aware, Human-Aligned Evaluation for Autonomous Driving with Vision-Language Models",
        "summary": "Benchmarking autonomous driving planners to align with human judgment remains\na critical challenge, as state-of-the-art metrics like the Extended Predictive\nDriver Model Score (EPDMS) lack context awareness in nuanced scenarios. To\naddress this, we introduce DriveCritic, a novel framework featuring two key\ncontributions: the DriveCritic dataset, a curated collection of challenging\nscenarios where context is critical for correct judgment and annotated with\npairwise human preferences, and the DriveCritic model, a Vision-Language Model\n(VLM) based evaluator. Fine-tuned using a two-stage supervised and\nreinforcement learning pipeline, the DriveCritic model learns to adjudicate\nbetween trajectory pairs by integrating visual and symbolic context.\nExperiments show DriveCritic significantly outperforms existing metrics and\nbaselines in matching human preferences and demonstrates strong context\nawareness. Overall, our work provides a more reliable, human-aligned foundation\nto evaluating autonomous driving systems.",
        "url": "http://arxiv.org/abs/2510.13108v1",
        "published_date": "2025-10-15T03:00:38+00:00",
        "updated_date": "2025-10-15T03:00:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Jingyu Song",
            "Zhenxin Li",
            "Shiyi Lan",
            "Xinglong Sun",
            "Nadine Chang",
            "Maying Shen",
            "Joshua Chen",
            "Katherine A. Skinner",
            "Jose M. Alvarez"
        ],
        "tldr": "This paper introduces DriveCritic, a new framework for evaluating autonomous driving planners using a Vision-Language Model and a new dataset of challenging scenarios annotated with human preferences, demonstrating improved alignment with human judgment compared to existing metrics.",
        "tldr_zh": "该论文介绍了DriveCritic，一个使用视觉-语言模型评估自动驾驶规划器的新框架，以及一个包含人类偏好标注的挑战性场景的新数据集，展示了相比现有指标，与人类判断的更好对齐。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UNCAP: Uncertainty-Guided Planning Using Natural Language Communication for Cooperative Autonomous Vehicles",
        "summary": "Safe large-scale coordination of multiple cooperative connected autonomous\nvehicles (CAVs) hinges on communication that is both efficient and\ninterpretable. Existing approaches either rely on transmitting high-bandwidth\nraw sensor data streams or neglect perception and planning uncertainties\ninherent in shared data, resulting in systems that are neither scalable nor\nsafe. To address these limitations, we propose Uncertainty-Guided Natural\nLanguage Cooperative Autonomous Planning (UNCAP), a vision-language model-based\nplanning approach that enables CAVs to communicate via lightweight natural\nlanguage messages while explicitly accounting for perception uncertainty in\ndecision-making. UNCAP features a two-stage communication protocol: (i) an ego\nCAV first identifies the subset of vehicles most relevant for information\nexchange, and (ii) the selected CAVs then transmit messages that quantitatively\nexpress their perception uncertainty. By selectively fusing messages that\nmaximize mutual information, this strategy allows the ego vehicle to integrate\nonly the most relevant signals into its decision-making, improving both the\nscalability and reliability of cooperative planning. Experiments across diverse\ndriving scenarios show a 63% reduction in communication bandwidth with a 31%\nincrease in driving safety score, a 61% reduction in decision uncertainty, and\na four-fold increase in collision distance margin during near-miss events.\nProject website: https://uncap-project.github.io/",
        "url": "http://arxiv.org/abs/2510.12992v1",
        "published_date": "2025-10-14T21:09:09+00:00",
        "updated_date": "2025-10-14T21:09:09+00:00",
        "categories": [
            "cs.RO",
            "cs.CL",
            "cs.CV",
            "cs.MA"
        ],
        "authors": [
            "Neel P. Bhatt",
            "Po-han Li",
            "Kushagra Gupta",
            "Rohan Siva",
            "Daniel Milan",
            "Alexander T. Hogue",
            "Sandeep P. Chinchali",
            "David Fridovich-Keil",
            "Zhangyang Wang",
            "Ufuk Topcu"
        ],
        "tldr": "UNCAP uses vision-language models for cooperative autonomous vehicle planning, enabling communication via natural language messages that incorporate perception uncertainty. This approach reduces bandwidth and improves safety.",
        "tldr_zh": "UNCAP使用视觉-语言模型进行协同自动驾驶车辆的规划，通过包含感知不确定性的自然语言消息进行通信。这种方法降低了带宽并提高了安全性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]