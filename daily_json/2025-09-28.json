[
    {
        "title": "WorldSplat: Gaussian-Centric Feed-Forward 4D Scene Generation for Autonomous Driving",
        "summary": "Recent advances in driving-scene generation and reconstruction have\ndemonstrated significant potential for enhancing autonomous driving systems by\nproducing scalable and controllable training data. Existing generation methods\nprimarily focus on synthesizing diverse and high-fidelity driving videos;\nhowever, due to limited 3D consistency and sparse viewpoint coverage, they\nstruggle to support convenient and high-quality novel-view synthesis (NVS).\nConversely, recent 3D/4D reconstruction approaches have significantly improved\nNVS for real-world driving scenes, yet inherently lack generative capabilities.\nTo overcome this dilemma between scene generation and reconstruction, we\npropose \\textbf{WorldSplat}, a novel feed-forward framework for 4D\ndriving-scene generation. Our approach effectively generates consistent\nmulti-track videos through two key steps: ((i)) We introduce a 4D-aware latent\ndiffusion model integrating multi-modal information to produce pixel-aligned 4D\nGaussians in a feed-forward manner. ((ii)) Subsequently, we refine the novel\nview videos rendered from these Gaussians using a enhanced video diffusion\nmodel. Extensive experiments conducted on benchmark datasets demonstrate that\n\\textbf{WorldSplat} effectively generates high-fidelity, temporally and\nspatially consistent multi-track novel view driving videos.",
        "url": "http://arxiv.org/abs/2509.23402v1",
        "published_date": "2025-09-27T16:47:44+00:00",
        "updated_date": "2025-09-27T16:47:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ziyue Zhu",
            "Zhanqian Wu",
            "Zhenxin Zhu",
            "Lijun Zhou",
            "Haiyang Sun",
            "Bing Wan",
            "Kun Ma",
            "Guang Chen",
            "Hangjun Ye",
            "Jin Xie",
            "jian Yang"
        ]
    },
    {
        "title": "Increasing the Diversity in RGB-to-Thermal Image Translation for Automotive Applications",
        "summary": "Thermal imaging in Advanced Driver Assistance Systems (ADAS) improves road\nsafety with superior perception in low-light and harsh weather conditions\ncompared to traditional RGB cameras. However, research in this area faces\nchallenges due to limited dataset availability and poor representation in\ndriving simulators. RGB-to-thermal image translation offers a potential\nsolution, but existing methods focus on one-to-one mappings. We propose a\none-to-many mapping using a multi-modal translation framework enhanced with our\nComponent-aware Adaptive Instance Normalization (CoAdaIN). Unlike the original\nAdaIN, which applies styles globally, CoAdaIN adapts styles to different image\ncomponents individually. The result, as we show, is more realistic and diverse\nthermal image translations. This is the accepted author manuscript of the paper\npublished in IEEE Sensors Conference 2024. The final published version is\navailable at 10.1109/SENSORS60989.2024.10785056.",
        "url": "http://arxiv.org/abs/2509.23243v1",
        "published_date": "2025-09-27T10:49:56+00:00",
        "updated_date": "2025-09-27T10:49:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Kaili Wang",
            "Leonardo Ravaglia",
            "Roberto Longo",
            "Lore Goetschalckx",
            "David Van Hamme",
            "Julie Moeyersoms",
            "Ben Stoffelen",
            "Tom De Schepper"
        ]
    },
    {
        "title": "Leave No Observation Behind: Real-time Correction for VLA Action Chunks",
        "summary": "To improve efficiency and temporal coherence, Vision-Language-Action (VLA)\nmodels often predict action chunks; however, this action chunking harms\nreactivity under inference delay and long horizons. We introduce Asynchronous\nAction Chunk Correction (A2C2), which is a lightweight real-time chunk\ncorrection head that runs every control step and adds a time-aware correction\nto any off-the-shelf VLA's action chunk. The module combines the latest\nobservation, the predicted action from VLA (base action), a positional feature\nthat encodes the index of the base action within the chunk, and some features\nfrom the base policy, then outputs a per-step correction. This preserves the\nbase model's competence while restoring closed-loop responsiveness. The\napproach requires no retraining of the base policy and is orthogonal to\nasynchronous execution schemes such as Real Time Chunking (RTC). On the dynamic\nKinetix task suite (12 tasks) and LIBERO Spatial, our method yields consistent\nsuccess rate improvements across increasing delays and execution horizons (+23%\npoint and +7% point respectively, compared to RTC), and also improves\nrobustness for long horizons even with zero injected delay. Since the\ncorrection head is small and fast, there is minimal overhead compared to the\ninference of large VLA models. These results indicate that A2C2 is an\neffective, plug-in mechanism for deploying high-capacity chunking policies in\nreal-time control.",
        "url": "http://arxiv.org/abs/2509.23224v1",
        "published_date": "2025-09-27T10:07:49+00:00",
        "updated_date": "2025-09-27T10:07:49+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.SY",
            "eess.SY"
        ],
        "authors": [
            "Kohei Sendai",
            "Maxime Alvarez",
            "Tatsuya Matsushima",
            "Yutaka Matsuo",
            "Yusuke Iwasawa"
        ]
    },
    {
        "title": "Unsupervised Online 3D Instance Segmentation with Synthetic Sequences and Dynamic Loss",
        "summary": "Unsupervised online 3D instance segmentation is a fundamental yet challenging\ntask, as it requires maintaining consistent object identities across LiDAR\nscans without relying on annotated training data. Existing methods, such as\nUNIT, have made progress in this direction but remain constrained by limited\ntraining diversity, rigid temporal sampling, and heavy dependence on noisy\npseudo-labels. We propose a new framework that enriches the training\ndistribution through synthetic point cloud sequence generation, enabling\ngreater diversity without relying on manual labels or simulation engines. To\nbetter capture temporal dynamics, our method incorporates a flexible sampling\nstrategy that leverages both adjacent and non-adjacent frames, allowing the\nmodel to learn from long-range dependencies as well as short-term variations.\nIn addition, a dynamic-weighting loss emphasizes confident and informative\nsamples, guiding the network toward more robust representations. Through\nextensive experiments on SemanticKITTI, nuScenes, and PandaSet, our method\nconsistently outperforms UNIT and other unsupervised baselines, achieving\nhigher segmentation accuracy and more robust temporal associations. The code\nwill be publicly available at github.com/Eaphan/SFT3D.",
        "url": "http://arxiv.org/abs/2509.23194v1",
        "published_date": "2025-09-27T08:53:27+00:00",
        "updated_date": "2025-09-27T08:53:27+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yifan Zhang",
            "Wei Zhang",
            "Chuangxin He",
            "Zhonghua Miao",
            "Junhui Hou"
        ]
    },
    {
        "title": "GeLoc3r: Enhancing Relative Camera Pose Regression with Geometric Consistency Regularization",
        "summary": "Prior ReLoc3R achieves breakthrough performance with fast 25ms inference and\nstate-of-the-art regression accuracy, yet our analysis reveals subtle geometric\ninconsistencies in its internal representations that prevent reaching the\nprecision ceiling of correspondence-based methods like MASt3R (which require\n300ms per pair). In this work, we present GeLoc3r, a novel approach to relative\ncamera pose estimation that enhances pose regression methods through Geometric\nConsistency Regularization (GCR). GeLoc3r overcomes the speed-accuracy dilemma\nby training regression networks to produce geometrically consistent poses\nwithout inference-time geometric computation. During training, GeLoc3r\nleverages ground-truth depth to generate dense 3D-2D correspondences, weights\nthem using a FusionTransformer that learns correspondence importance, and\ncomputes geometrically-consistent poses via weighted RANSAC. This creates a\nconsistency loss that transfers geometric knowledge into the regression\nnetwork. Unlike FAR method which requires both regression and geometric solving\nat inference, GeLoc3r only uses the enhanced regression head at test time,\nmaintaining ReLoc3R's fast speed and approaching MASt3R's high accuracy. On\nchallenging benchmarks, GeLoc3r consistently outperforms ReLoc3R, achieving\nsignificant improvements including 40.45% vs. 34.85% AUC@5{\\deg} on the CO3Dv2\ndataset (16% relative improvement), 68.66% vs. 66.70% AUC@5{\\deg} on\nRealEstate10K, and 50.45% vs. 49.60% on MegaDepth1500. By teaching geometric\nconsistency during training rather than enforcing it at inference, GeLoc3r\nrepresents a paradigm shift in how neural networks learn camera geometry,\nachieving both the speed of regression and the geometric understanding of\ncorrespondence methods.",
        "url": "http://arxiv.org/abs/2509.23038v1",
        "published_date": "2025-09-27T01:21:38+00:00",
        "updated_date": "2025-09-27T01:21:38+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jingxing Li",
            "Yongjae Lee",
            "Deliang Fan"
        ]
    },
    {
        "title": "Planning with Unified Multimodal Models",
        "summary": "With the powerful reasoning capabilities of large language models (LLMs) and\nvision-language models (VLMs), many recent works have explored using them for\ndecision-making. However, most of these approaches rely solely on\nlanguage-based reasoning, which limits their ability to reason and make\ninformed decisions. Recently, a promising new direction has emerged with\nunified multimodal models (UMMs), which support both multimodal inputs and\noutputs. We believe such models have greater potential for decision-making by\nenabling reasoning through generated visual content. To this end, we propose\nUni-Plan, a planning framework built on UMMs. Within this framework, a single\nmodel simultaneously serves as the policy, dynamics model, and value function.\nIn addition, to avoid hallucinations in dynamics predictions, we present a\nnovel approach self-discriminated filtering, where the generative model serves\nas a self-discriminator to filter out invalid dynamics predictions. Experiments\non long-horizon planning tasks show that Uni-Plan substantially improves\nsuccess rates compared to VLM-based methods, while also showing strong data\nscalability, requiring no expert demonstrations and achieving better\nperformance under the same training-data size. This work lays a foundation for\nfuture research in reasoning and decision-making with UMMs.",
        "url": "http://arxiv.org/abs/2509.23014v1",
        "published_date": "2025-09-27T00:13:13+00:00",
        "updated_date": "2025-09-27T00:13:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yihao Sun",
            "Zhilong Zhang",
            "Yang Yu",
            "Pierre-Luc Bacon"
        ]
    }
]