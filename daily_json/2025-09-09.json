[
    {
        "title": "F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions",
        "summary": "Executing language-conditioned tasks in dynamic visual environments remains a\ncentral challenge in embodied AI. Existing Vision-Language-Action (VLA) models\npredominantly adopt reactive state-to-action mappings, often leading to\nshort-sighted behaviors and poor robustness in dynamic scenes. In this paper,\nwe introduce F1, a pretrained VLA framework which integrates the visual\nforesight generation into decision-making pipeline. F1 adopts a\nMixture-of-Transformer architecture with dedicated modules for perception,\nforesight generation, and control, thereby bridging understanding, generation,\nand actions. At its core, F1 employs a next-scale prediction mechanism to\nsynthesize goal-conditioned visual foresight as explicit planning targets. By\nforecasting plausible future visual states, F1 reformulates action generation\nas a foresight-guided inverse dynamics problem, enabling actions that\nimplicitly achieve visual goals. To endow F1 with robust and generalizable\ncapabilities, we propose a three-stage training recipe on an extensive dataset\ncomprising over 330k trajectories across 136 diverse tasks. This training\nscheme enhances modular reasoning and equips the model with transferable visual\nforesight, which is critical for complex and dynamic environments. Extensive\nevaluations on real-world tasks and simulation benchmarks demonstrate F1\nconsistently outperforms existing approaches, achieving substantial gains in\nboth task success rate and generalization ability.",
        "url": "http://arxiv.org/abs/2509.06951v1",
        "published_date": "2025-09-08T17:58:30+00:00",
        "updated_date": "2025-09-08T17:58:30+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Qi Lv",
            "Weijie Kong",
            "Hao Li",
            "Jia Zeng",
            "Zherui Qiu",
            "Delin Qu",
            "Haoming Song",
            "Qizhi Chen",
            "Xiang Deng",
            "Jiangmiao Pang"
        ],
        "tldr": "The paper introduces F1, a Vision-Language-Action model that integrates visual foresight into decision-making by predicting future visual states and using them to guide action generation, achieving superior performance and generalization in dynamic environments.",
        "tldr_zh": "该论文介绍了F1，一种视觉-语言-动作模型，它通过预测未来视觉状态并利用这些状态来指导动作生成，从而将视觉预见整合到决策过程中，在动态环境中实现了卓越的性能和泛化能力。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "LLaDA-VLA: Vision Language Diffusion Action Models",
        "summary": "The rapid progress of auto-regressive vision-language models (VLMs) has\ninspired growing interest in vision-language-action models (VLA) for robotic\nmanipulation. Recently, masked diffusion models, a paradigm distinct from\nautoregressive models, have begun to demonstrate competitive performance in\ntext generation and multimodal applications, leading to the development of a\nseries of diffusion-based VLMs (d-VLMs). However, leveraging such models for\nrobot policy learning remains largely unexplored. In this work, we present\nLLaDA-VLA, the first Vision-Language-Diffusion-Action model built upon\npretrained d-VLMs for robotic manipulation. To effectively adapt d-VLMs to\nrobotic domain, we introduce two key designs: (1) a localized special-token\nclassification strategy that replaces full-vocabulary classification with\nspecial action token classification, reducing adaptation difficulty; (2) a\nhierarchical action-structured decoding strategy that decodes action sequences\nhierarchically considering the dependencies within and across actions.\nExtensive experiments demonstrate that LLaDA-VLA significantly outperforms\nstate-of-the-art VLAs on both simulation and real-world robots.",
        "url": "http://arxiv.org/abs/2509.06932v1",
        "published_date": "2025-09-08T17:45:40+00:00",
        "updated_date": "2025-09-08T17:45:40+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Yuqing Wen",
            "Hebei Li",
            "Kefan Gu",
            "Yucheng Zhao",
            "Tiancai Wang",
            "Xiaoyan Sun"
        ],
        "tldr": "This paper introduces LLaDA-VLA, a novel Vision-Language-Diffusion-Action model for robotic manipulation, which utilizes a localized special-token classification and a hierarchical action-structured decoding strategy, outperforming existing VLAs in simulated and real-world environments.",
        "tldr_zh": "该论文介绍了LLaDA-VLA，一种用于机器人操作的新型视觉-语言-扩散-动作模型。该模型利用局部化特殊token分类和分层动作结构解码策略，在模拟和真实环境中均优于现有的VLA模型。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "SynthDrive: Scalable Real2Sim2Real Sensor Simulation Pipeline for High-Fidelity Asset Generation and Driving Data Synthesis",
        "summary": "In the field of autonomous driving, sensor simulation is essential for\ngenerating rare and diverse scenarios that are difficult to capture in\nreal-world environments. Current solutions fall into two categories: 1)\nCG-based methods, such as CARLA, which lack diversity and struggle to scale to\nthe vast array of rare cases required for robust perception training; and 2)\nlearning-based approaches, such as NeuSim, which are limited to specific object\ncategories (vehicles) and require extensive multi-sensor data, hindering their\napplicability to generic objects. To address these limitations, we propose a\nscalable real2sim2real system that leverages 3D generation to automate asset\nmining, generation, and rare-case data synthesis.",
        "url": "http://arxiv.org/abs/2509.06798v1",
        "published_date": "2025-09-08T15:29:49+00:00",
        "updated_date": "2025-09-08T15:29:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Zhengqing Chen",
            "Ruohong Mei",
            "Xiaoyang Guo",
            "Qingjie Wang",
            "Yubin Hu",
            "Wei Yin",
            "Weiqiang Ren",
            "Qian Zhang"
        ],
        "tldr": "SynthDrive introduces a scalable real2sim2real pipeline for generating diverse driving scenarios and high-fidelity assets, addressing the limitations of existing CG and learning-based simulation methods.",
        "tldr_zh": "SynthDrive 提出了一种可扩展的 real2sim2real 流水线，用于生成多样化的驾驶场景和高保真资产，解决了现有基于 CG 和学习的仿真方法的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "UrbanTwin: High-Fidelity Synthetic Replicas of Roadside Lidar Datasets",
        "summary": "This article presents UrbanTwin datasets - high-fidelity, realistic replicas\nof three public roadside lidar datasets: LUMPI, V2X-Real-IC, and TUMTraf-I.\nEach UrbanTwin dataset contains 10K annotated frames corresponding to one of\nthe public datasets. Annotations include 3D bounding boxes, instance\nsegmentation labels, and tracking IDs for six object classes, along with\nsemantic segmentation labels for nine classes. These datasets are synthesized\nusing emulated lidar sensors within realistic digital twins, modeled based on\nsurrounding geometry, road alignment at lane level, and the lane topology and\nvehicle movement patterns at intersections of the actual locations\ncorresponding to each real dataset. Due to the precise digital twin modeling,\nthe synthetic datasets are well aligned with their real counterparts, offering\nstrong standalone and augmentative value for training deep learning models on\ntasks such as 3D object detection, tracking, and semantic and instance\nsegmentation. We evaluate the alignment of the synthetic replicas through\nstatistical and structural similarity analysis with real data, and further\ndemonstrate their utility by training 3D object detection models solely on\nsynthetic data and testing them on real, unseen data. The high similarity\nscores and improved detection performance, compared to the models trained on\nreal data, indicate that the UrbanTwin datasets effectively enhance existing\nbenchmark datasets by increasing sample size and scene diversity. In addition,\nthe digital twins can be adapted to test custom scenarios by modifying the\ndesign and dynamics of the simulations. To our knowledge, these are the first\ndigitally synthesized datasets that can replace in-domain real-world datasets\nfor lidar perception tasks. UrbanTwin datasets are publicly available at\nhttps://dataverse.harvard.edu/dataverse/ucf-ut.",
        "url": "http://arxiv.org/abs/2509.06781v1",
        "published_date": "2025-09-08T15:06:02+00:00",
        "updated_date": "2025-09-08T15:06:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Muhammad Shahbaz",
            "Shaurya Agarwal"
        ],
        "tldr": "The paper introduces UrbanTwin, a collection of high-fidelity synthetic LiDAR datasets mirroring three existing public datasets. It demonstrates the utility of these synthetic datasets for training 3D object detection models, achieving comparable or better performance than models trained on real data, suggesting their potential as a replacement for in-domain real-world datasets.",
        "tldr_zh": "本文介绍了 UrbanTwin，这是一组高保真合成 LiDAR 数据集，镜像了三个现有的公共数据集。 它展示了这些合成数据集在训练 3D 对象检测模型中的效用，实现了与在真实数据上训练的模型相当或更好的性能，表明它们有潜力替代领域内的真实世界数据集。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Cortex-Synth: Differentiable Topology-Aware 3D Skeleton Synthesis with Hierarchical Graph Attention",
        "summary": "We present Cortex Synth, a novel end-to-end differentiable framework for\njoint 3D skeleton geometry and topology synthesis from single 2D images. Our\narchitecture introduces three key innovations: (1) A hierarchical graph\nattention mechanism with multi-scale skeletal refinement, (2) Differentiable\nspectral topology optimization via Laplacian eigen decomposition, and (3)\nAdversarial geometric consistency training for pose structure alignment. The\nframework integrates four synergistic modules: a pseudo 3D point cloud\ngenerator, an enhanced PointNet encoder, a skeleton coordinate decoder, and a\nnovel Differentiable Graph Construction Network (DGCN). Our experiments\ndemonstrate state-of-the-art results with 18.7 percent improvement in MPJPE and\n27.3 percent in Graph Edit Distance on ShapeNet, while reducing topological\nerrors by 42 percent compared to previous approaches. The model's end-to-end\ndifferentiability enables applications in robotic manipulation, medical\nimaging, and automated character rigging.",
        "url": "http://arxiv.org/abs/2509.06705v1",
        "published_date": "2025-09-08T14:03:13+00:00",
        "updated_date": "2025-09-08T14:03:13+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mohamed Zayaan S"
        ],
        "tldr": "The paper introduces Cortex-Synth, an end-to-end differentiable framework for 3D skeleton synthesis from 2D images, achieving state-of-the-art results in accuracy and topological correctness. It leverages hierarchical graph attention and differentiable spectral topology optimization.",
        "tldr_zh": "该论文介绍了Cortex-Synth，一个端到端可微框架，用于从2D图像合成3D骨架，并在准确性和拓扑正确性方面取得了最先进的结果。它利用了分层图注意力机制和可微谱拓扑优化。",
        "relevance_score": 7,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes",
        "summary": "Understanding 3D spatial relationships remains a major limitation of current\nVision-Language Models (VLMs). Prior work has addressed this issue by creating\nspatial question-answering (QA) datasets based on single images or indoor\nvideos. However, real-world embodied AI agents such as robots and self-driving\ncars typically rely on ego-centric, multi-view observations. To this end, we\nintroduce Ego3D-Bench, a new benchmark designed to evaluate the spatial\nreasoning abilities of VLMs using ego-centric, multi-view outdoor data.\nEgo3D-Bench comprises over 8,600 QA pairs, created with significant involvement\nfrom human annotators to ensure quality and diversity. We benchmark 16 SOTA\nVLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our results\nreveal a notable performance gap between human level scores and VLM\nperformance, highlighting that current VLMs still fall short of human level\nspatial understanding. To bridge this gap, we propose Ego3D-VLM, a\npost-training framework that enhances 3D spatial reasoning of VLMs. Ego3D-VLM\ngenerates cognitive map based on estimated global 3D coordinates, resulting in\n12% average improvement on multi-choice QA and 56% average improvement on\nabsolute distance estimation. Ego3D-VLM is modular and can be integrated with\nany existing VLM. Together, Ego3D-Bench and Ego3D-VLM offer valuable tools for\nadvancing toward human level spatial understanding in real-world, multi-view\nenvironments.",
        "url": "http://arxiv.org/abs/2509.06266v1",
        "published_date": "2025-09-08T01:08:41+00:00",
        "updated_date": "2025-09-08T01:08:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Mohsen Gholami",
            "Ahmad Rezaei",
            "Zhou Weimin",
            "Yong Zhang",
            "Mohammad Akbari"
        ],
        "tldr": "The paper introduces Ego3D-Bench, a new benchmark for evaluating spatial reasoning in VLMs using ego-centric multi-view data, and proposes Ego3D-VLM, a post-training framework that enhances VLMs' 3D spatial reasoning abilities, showing promising improvements.",
        "tldr_zh": "该论文介绍了Ego3D-Bench，这是一个新的基准，用于评估视觉语言模型（VLMs）在以自我为中心的、多视点数据中的空间推理能力。同时，提出了Ego3D-VLM，一种后训练框架，能够增强VLMs的3D空间推理能力，并显示出有希望的改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "O$^3$Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation",
        "summary": "Grounding object affordance is fundamental to robotic manipulation as it\nestablishes the critical link between perception and action among interacting\nobjects. However, prior works predominantly focus on predicting single-object\naffordance, overlooking the fact that most real-world interactions involve\nrelationships between pairs of objects. In this work, we address the challenge\nof object-to-object affordance grounding under limited data contraints.\nInspired by recent advances in few-shot learning with 2D vision foundation\nmodels, we propose a novel one-shot 3D object-to-object affordance learning\napproach for robotic manipulation. Semantic features from vision foundation\nmodels combined with point cloud representation for geometric understanding\nenable our one-shot learning pipeline to generalize effectively to novel\nobjects and categories. We further integrate our 3D affordance representation\nwith large language models (LLMs) for robotics manipulation, significantly\nenhancing LLMs' capability to comprehend and reason about object interactions\nwhen generating task-specific constraint functions. Our experiments on 3D\nobject-to-object affordance grounding and robotic manipulation demonstrate that\nour O$^3$Afford significantly outperforms existing baselines in terms of both\naccuracy and generalization capability.",
        "url": "http://arxiv.org/abs/2509.06233v1",
        "published_date": "2025-09-07T22:45:06+00:00",
        "updated_date": "2025-09-07T22:45:06+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Tongxuan Tian",
            "Xuhui Kang",
            "Yen-Ling Kuo"
        ],
        "tldr": "The paper introduces O$^3$Afford, a one-shot 3D object-to-object affordance learning approach using vision foundation models and LLMs to improve robotic manipulation, demonstrating enhanced accuracy and generalization.",
        "tldr_zh": "该论文介绍了O$^3$Afford，一种单样本3D对象到对象可供性学习方法，它利用视觉基础模型和LLM来改进机器人操作，并展示了更高的准确性和泛化能力。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Learning in ImaginationLand: Omnidirectional Policies through 3D Generative Models (OP-Gen)",
        "summary": "Recent 3D generative models, which are capable of generating full object\nshapes from just a few images, now open up new opportunities in robotics. In\nthis work, we show that 3D generative models can be used to augment a dataset\nfrom a single real-world demonstration, after which an omnidirectional policy\ncan be learned within this imagined dataset. We found that this enables a robot\nto perform a task when initialised from states very far from those observed\nduring the demonstration, including starting from the opposite side of the\nobject relative to the real-world demonstration, significantly reducing the\nnumber of demonstrations required for policy learning. Through several\nreal-world experiments across tasks such as grasping objects, opening a drawer,\nand placing trash into a bin, we study these omnidirectional policies by\ninvestigating the effect of various design choices on policy behaviour, and we\nshow superior performance to recent baselines which use alternative methods for\ndata augmentation.",
        "url": "http://arxiv.org/abs/2509.06191v1",
        "published_date": "2025-09-07T20:00:59+00:00",
        "updated_date": "2025-09-07T20:00:59+00:00",
        "categories": [
            "cs.RO",
            "cs.CV",
            "cs.LG"
        ],
        "authors": [
            "Yifei Ren",
            "Edward Johns"
        ],
        "tldr": "This paper presents a method, OP-Gen, which uses 3D generative models to augment real-world demonstrations, enabling the learning of omnidirectional robotic policies that perform tasks from a wider range of initial states than traditional methods.",
        "tldr_zh": "本文提出了一种名为OP-Gen的方法，该方法使用3D生成模型来扩充真实世界的演示数据，从而能够学习全向机器人策略，这些策略可以从比传统方法更广泛的初始状态执行任务。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]