[
    {
        "title": "RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping",
        "summary": "General robotic grasping systems require accurate object affordance\nperception in diverse open-world scenarios following human instructions.\nHowever, current studies suffer from the problem of lacking reasoning-based\nlarge-scale affordance prediction data, leading to considerable concern about\nopen-world effectiveness. To address this limitation, we build a large-scale\ngrasping-oriented affordance segmentation benchmark with human-like\ninstructions, named RAGNet. It contains 273k images, 180 categories, and 26k\nreasoning instructions. The images cover diverse embodied data domains, such as\nwild, robot, ego-centric, and even simulation data. They are carefully\nannotated with an affordance map, while the difficulty of language instructions\nis largely increased by removing their category name and only providing\nfunctional descriptions. Furthermore, we propose a comprehensive\naffordance-based grasping framework, named AffordanceNet, which consists of a\nVLM pre-trained on our massive affordance data and a grasping network that\nconditions an affordance map to grasp the target. Extensive experiments on\naffordance segmentation benchmarks and real-robot manipulation tasks show that\nour model has a powerful open-world generalization ability. Our data and code\nis available at https://github.com/wudongming97/AffordanceNet.",
        "url": "http://arxiv.org/abs/2507.23734v1",
        "published_date": "2025-07-31T17:17:05+00:00",
        "updated_date": "2025-07-31T17:17:05+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Dongming Wu",
            "Yanping Fu",
            "Saike Huang",
            "Yingfei Liu",
            "Fan Jia",
            "Nian Liu",
            "Feng Dai",
            "Tiancai Wang",
            "Rao Muhammad Anwer",
            "Fahad Shahbaz Khan",
            "Jianbing Shen"
        ],
        "tldr": "The paper introduces RAGNet, a large-scale dataset for reasoning-based affordance segmentation to improve robotic grasping in open-world scenarios, and proposes AffordanceNet, a grasping framework pre-trained on this dataset.",
        "tldr_zh": "该论文介绍了RAGNet，一个大规模的基于推理的示能分割数据集，旨在提高机器人抓取在开放世界场景中的能力，并提出了AffordanceNet，一个基于该数据集预训练的抓取框架。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "A Unified Perception-Language-Action Framework for Adaptive Autonomous Driving",
        "summary": "Autonomous driving systems face significant challenges in achieving\nhuman-like adaptability, robustness, and interpretability in complex,\nopen-world environments. These challenges stem from fragmented architectures,\nlimited generalization to novel scenarios, and insufficient semantic extraction\nfrom perception. To address these limitations, we propose a unified\nPerception-Language-Action (PLA) framework that integrates multi-sensor fusion\n(cameras, LiDAR, radar) with a large language model (LLM)-augmented\nVision-Language-Action (VLA) architecture, specifically a GPT-4.1-powered\nreasoning core. This framework unifies low-level sensory processing with\nhigh-level contextual reasoning, tightly coupling perception with natural\nlanguage-based semantic understanding and decision-making to enable\ncontext-aware, explainable, and safety-bounded autonomous driving. Evaluations\non an urban intersection scenario with a construction zone demonstrate superior\nperformance in trajectory tracking, speed prediction, and adaptive planning.\nThe results highlight the potential of language-augmented cognitive frameworks\nfor advancing the safety, interpretability, and scalability of autonomous\ndriving systems.",
        "url": "http://arxiv.org/abs/2507.23540v1",
        "published_date": "2025-07-31T13:30:47+00:00",
        "updated_date": "2025-07-31T13:30:47+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV"
        ],
        "authors": [
            "Yi Zhang",
            "Erik Leo Haß",
            "Kuo-Yi Chao",
            "Nenad Petrovic",
            "Yinglei Song",
            "Chengdong Wu",
            "Alois Knoll"
        ],
        "tldr": "This paper introduces a unified Perception-Language-Action (PLA) framework for autonomous driving that uses multi-sensor fusion and a GPT-4.1-powered Vision-Language-Action (VLA) architecture to improve adaptability, robustness, and interpretability.",
        "tldr_zh": "本文介绍了一个统一的感知-语言-动作 (PLA) 框架，用于自动驾驶。该框架利用多传感器融合和一个基于 GPT-4.1 的视觉-语言-动作 (VLA) 架构，以提高适应性、鲁棒性和可解释性。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning",
        "summary": "Vision-Language-Action (VLA) models have demonstrated significant potential\nin complex scene understanding and action reasoning, leading to their\nincreasing adoption in end-to-end autonomous driving systems. However, the long\nvisual tokens of VLA models greatly increase computational costs. Current\nvisual token pruning methods in Vision-Language Models (VLM) rely on either\nvisual token similarity or visual-text attention, but both have shown poor\nperformance in autonomous driving scenarios. Given that human drivers\nconcentrate on relevant foreground areas while driving, we assert that\nretaining visual tokens containing this foreground information is essential for\neffective decision-making. Inspired by this, we propose FastDriveVLA, a novel\nreconstruction-based vision token pruning framework designed specifically for\nautonomous driving. FastDriveVLA includes a plug-and-play visual token pruner\ncalled ReconPruner, which prioritizes foreground information through MAE-style\npixel reconstruction. A novel adversarial foreground-background reconstruction\nstrategy is designed to train ReconPruner for the visual encoder of VLA models.\nOnce trained, ReconPruner can be seamlessly applied to different VLA models\nwith the same visual encoder without retraining. To train ReconPruner, we also\nintroduce a large-scale dataset called nuScenes-FG, consisting of 241K\nimage-mask pairs with annotated foreground regions. Our approach achieves\nstate-of-the-art results on the nuScenes closed-loop planning benchmark across\ndifferent pruning ratios.",
        "url": "http://arxiv.org/abs/2507.23318v1",
        "published_date": "2025-07-31T07:55:56+00:00",
        "updated_date": "2025-07-31T07:55:56+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jiajun Cao",
            "Qizhe Zhang",
            "Peidong Jia",
            "Xuhui Zhao",
            "Bo Lan",
            "Xiaoan Zhang",
            "Xiaobao Wei",
            "Sixiang Chen",
            "Zhuo Li",
            "Yang Wang",
            "Liyun Li",
            "Xianming Liu",
            "Ming Lu",
            "Shanghang Zhang"
        ],
        "tldr": "The paper introduces FastDriveVLA, a reconstruction-based token pruning method for VLA models in autonomous driving, using a novel adversarial foreground-background reconstruction strategy and a new dataset nuScenes-FG to achieve state-of-the-art results on the nuScenes benchmark.",
        "tldr_zh": "该论文介绍了 FastDriveVLA，一种基于重建的 VLA 模型token剪枝方法，用于自动驾驶。该方法使用了一种新的对抗性前景-背景重建策略和一个名为 nuScenes-FG 的新数据集，在 nuScenes 基准测试中取得了最先进的结果。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "Vision-Language Fusion for Real-Time Autonomous Driving: Goal-Centered Cross-Attention of Camera, HD-Map, & Waypoints",
        "summary": "Autonomous cars need geometric accuracy and semantic understanding to\nnavigate complex environments, yet most stacks handle them separately. We\npresent XYZ-Drive, a single vision-language model that reads a front-camera\nframe, a 25m $\\times$ 25m overhead map, and the next waypoint, then outputs\nsteering and speed. A lightweight goal-centered cross-attention layer lets\nwaypoint tokens highlight relevant image and map patches, supporting both\naction and textual explanations, before the fused tokens enter a partially\nfine-tuned LLaMA-3.2 11B model.\n  On the MD-NEX Outdoor-Driving benchmark XYZ-Drive attains 95% success and\n0.80 Success weighted by Path Length (SPL), surpassing PhysNav-DG by 15%. and\nhalving collisions, all while significantly improving efficiency by using only\na single branch. Sixteen ablations explain the gains. Removing any modality\n(vision, waypoint, map) drops success by up to 11%, confirming their\ncomplementary roles and rich connections. Replacing goal-centered attention\nwith simple concatenation cuts 3% in performance, showing query-based fusion\ninjects map knowledge more effectively. Keeping the transformer frozen loses\n5%, showing the importance of fine-tuning when applying VLMs for specific tasks\nsuch as autonomous driving. Coarsening map resolution from 10 cm to 40 cm blurs\nlane edges and raises crash rate.\n  Overall, these results demonstrate that early, token-level fusion of intent\nand map layout enables accurate, transparent, real-time driving.",
        "url": "http://arxiv.org/abs/2507.23064v1",
        "published_date": "2025-07-30T19:51:23+00:00",
        "updated_date": "2025-07-30T19:51:23+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.RO",
            "I.4.8; I.2.10; I.2.6; C.3.3; I.4.9"
        ],
        "authors": [
            "Santosh Patapati",
            "Trisanth Srinivasan",
            "Murari Ambati"
        ],
        "tldr": "XYZ-Drive, a vision-language model, achieves state-of-the-art autonomous driving performance by fusing camera images, HD maps, and waypoints with a goal-centered cross-attention mechanism, demonstrating the importance of early token-level fusion.",
        "tldr_zh": "XYZ-Drive是一个视觉-语言模型，通过融合摄像头图像、高清地图和航点，并采用以目标为中心的交叉注意力机制，实现了最先进的自动驾驶性能，证明了早期token级别融合的重要性。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Early Goal-Guided Multi-Scale Fusion for Real-Time Vision-Language Driving",
        "summary": "Autonomous vehicles must react in milliseconds while reasoning about road\ngeometry and traffic intent to navigate complex situations. We introduce\nNovaDrive, a single-branch vision-language architecture that processes\nfront-camera images, HD-map tiles, LiDAR depth, and textual waypoints in a\nsingle branch. A lightweight, two-stage cross-attention block first aligns\nwaypoint tokens with the HD map, then refines attention over fine-grained image\nand depth patches. Coupled with a novel smoothness loss that discourages abrupt\nsteering and speed changes, this design eliminates the need for recurrent\nmemory. We fine-tune the top 15 layers of an 11B LLaMA-3.2 vision-language\nbackbone, enabling real-time inference. On the nuScenes / Waymo subset of the\nMD-NEX Outdoor benchmark, NovaDrive raises success rate to 84% (+4%), boosts\npath-efficiency (SPL) to 0.66 (+0.11), and reduces collision frequency from\n2.6% to 1.2% (-1.4%) relative to the previous state-of-the-art. Our ablations\nconfirm that waypoint tokens, partial VLM fine-tuning, and the cross-attention\nfusion each contribute the most to these gains. Beyond safety, NovaDrive's\nshorter routes (resulting from the novel smoothness loss) translate to lower\nfuel or battery usage, pointing toward leaner, more easily updated driving\nstacks. NovaDrive can be extended to other embodied-AI domains as well.",
        "url": "http://arxiv.org/abs/2507.23042v1",
        "published_date": "2025-07-30T19:12:42+00:00",
        "updated_date": "2025-07-30T19:12:42+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.LG",
            "cs.MM",
            "cs.RO",
            "I.2.6; I.2.9; I.2.10; C.3.3"
        ],
        "authors": [
            "Santosh Patapati",
            "Trisanth Srinivasan"
        ],
        "tldr": "NovaDrive is a novel vision-language architecture for autonomous driving that achieves state-of-the-art performance on MD-NEX Outdoor benchmark by using early goal-guided multi-scale fusion and a smoothness loss.",
        "tldr_zh": "NovaDrive是一种新颖的用于自动驾驶的视觉语言架构，通过使用早期目标引导的多尺度融合和平滑损失，在MD-NEX Outdoor基准测试中实现了最先进的性能。",
        "relevance_score": 10,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "SeqAffordSplat: Scene-level Sequential Affordance Reasoning on 3D Gaussian Splatting",
        "summary": "3D affordance reasoning, the task of associating human instructions with the\nfunctional regions of 3D objects, is a critical capability for embodied agents.\nCurrent methods based on 3D Gaussian Splatting (3DGS) are fundamentally limited\nto single-object, single-step interactions, a paradigm that falls short of\naddressing the long-horizon, multi-object tasks required for complex real-world\napplications. To bridge this gap, we introduce the novel task of Sequential 3D\nGaussian Affordance Reasoning and establish SeqAffordSplat, a large-scale\nbenchmark featuring 1800+ scenes to support research on long-horizon affordance\nunderstanding in complex 3DGS environments. We then propose SeqSplatNet, an\nend-to-end framework that directly maps an instruction to a sequence of 3D\naffordance masks. SeqSplatNet employs a large language model that\nautoregressively generates text interleaved with special segmentation tokens,\nguiding a conditional decoder to produce the corresponding 3D mask. To handle\ncomplex scene geometry, we introduce a pre-training strategy, Conditional\nGeometric Reconstruction, where the model learns to reconstruct complete\naffordance region masks from known geometric observations, thereby building a\nrobust geometric prior. Furthermore, to resolve semantic ambiguities, we design\na feature injection mechanism that lifts rich semantic features from 2D Vision\nFoundation Models (VFM) and fuses them into the 3D decoder at multiple scales.\nExtensive experiments demonstrate that our method sets a new state-of-the-art\non our challenging benchmark, effectively advancing affordance reasoning from\nsingle-step interactions to complex, sequential tasks at the scene level.",
        "url": "http://arxiv.org/abs/2507.23772v1",
        "published_date": "2025-07-31T17:56:55+00:00",
        "updated_date": "2025-07-31T17:56:55+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Di Li",
            "Jie Feng",
            "Jiahao Chen",
            "Weisheng Dong",
            "Guanbin Li",
            "Yuhui Zheng",
            "Mingtao Feng",
            "Guangming Shi"
        ],
        "tldr": "The paper introduces SeqAffordSplat, a benchmark and method (SeqSplatNet) for scene-level sequential affordance reasoning on 3D Gaussian Splatting, addressing long-horizon, multi-object interaction tasks. They use a LLM and VFM-enhanced 3D decoder with a novel pre-training strategy.",
        "tldr_zh": "该论文介绍了SeqAffordSplat，一个用于3D高斯溅射场景级连续可供性推理的基准和方法（SeqSplatNet），解决了长程、多对象交互任务。 他们使用LLM和VFM增强的3D解码器，并采用了新颖的预训练策略。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "I2V-GS: Infrastructure-to-Vehicle View Transformation with Gaussian Splatting for Autonomous Driving Data Generation",
        "summary": "Vast and high-quality data are essential for end-to-end autonomous driving\nsystems. However, current driving data is mainly collected by vehicles, which\nis expensive and inefficient. A potential solution lies in synthesizing data\nfrom real-world images. Recent advancements in 3D reconstruction demonstrate\nphotorealistic novel view synthesis, highlighting the potential of generating\ndriving data from images captured on the road. This paper introduces a novel\nmethod, I2V-GS, to transfer the Infrastructure view To the Vehicle view with\nGaussian Splatting. Reconstruction from sparse infrastructure viewpoints and\nrendering under large view transformations is a challenging problem. We adopt\nthe adaptive depth warp to generate dense training views. To further expand the\nrange of views, we employ a cascade strategy to inpaint warped images, which\nalso ensures inpainting content is consistent across views. To further ensure\nthe reliability of the diffusion model, we utilize the cross-view information\nto perform a confidenceguided optimization. Moreover, we introduce RoadSight, a\nmulti-modality, multi-view dataset from real scenarios in infrastructure views.\nTo our knowledge, I2V-GS is the first framework to generate autonomous driving\ndatasets with infrastructure-vehicle view transformation. Experimental results\ndemonstrate that I2V-GS significantly improves synthesis quality under vehicle\nview, outperforming StreetGaussian in NTA-Iou, NTL-Iou, and FID by 45.7%,\n34.2%, and 14.9%, respectively.",
        "url": "http://arxiv.org/abs/2507.23683v1",
        "published_date": "2025-07-31T15:59:16+00:00",
        "updated_date": "2025-07-31T15:59:16+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jialei Chen",
            "Wuhao Xu",
            "Sipeng He",
            "Baoru Huang",
            "Dongchun Ren"
        ],
        "tldr": "The paper introduces I2V-GS, a novel method using Gaussian Splatting to synthesize autonomous driving data by transforming infrastructure views to vehicle views, along with the RoadSight dataset, demonstrating significant improvements over existing methods.",
        "tldr_zh": "该论文介绍了一种名为I2V-GS的新方法，该方法使用高斯溅射（Gaussian Splatting）通过将基础设施视角转换为车辆视角来合成自动驾驶数据，同时还推出了RoadSight数据集，实验结果表明该方法显著优于现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DA-Occ: Efficient 3D Voxel Occupancy Prediction via Directional 2D for Geometric Structure Preservation",
        "summary": "Efficient and high-accuracy 3D occupancy prediction is crucial for ensuring\nthe performance of autonomous driving (AD) systems. However, many current\nmethods focus on high accuracy at the expense of real-time processing needs. To\naddress this challenge of balancing accuracy and inference speed, we propose a\ndirectional pure 2D approach. Our method involves slicing 3D voxel features to\npreserve complete vertical geometric information. This strategy compensates for\nthe loss of height cues in Bird's-Eye View (BEV) representations, thereby\nmaintaining the integrity of the 3D geometric structure. By employing a\ndirectional attention mechanism, we efficiently extract geometric features from\ndifferent orientations, striking a balance between accuracy and computational\nefficiency. Experimental results highlight the significant advantages of our\napproach for autonomous driving. On the Occ3D-nuScenes, the proposed method\nachieves an mIoU of 39.3% and an inference speed of 27.7 FPS, effectively\nbalancing accuracy and efficiency. In simulations on edge devices, the\ninference speed reaches 14.8 FPS, further demonstrating the method's\napplicability for real-time deployment in resource-constrained environments.",
        "url": "http://arxiv.org/abs/2507.23599v1",
        "published_date": "2025-07-31T14:39:31+00:00",
        "updated_date": "2025-07-31T14:39:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yuchen Zhou",
            "Yan Luo",
            "Xiangang Wang",
            "Xingjian Gu",
            "Mingzhou Lu"
        ],
        "tldr": "This paper introduces a directional 2D approach (DA-Occ) for efficient 3D voxel occupancy prediction, aiming to balance accuracy and speed in autonomous driving, achieving real-time performance even on edge devices.",
        "tldr_zh": "该论文提出了一种定向2D方法（DA-Occ），用于高效的3D体素占用预测，旨在平衡自动驾驶中的准确性和速度，即使在边缘设备上也能实现实时性能。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MamV2XCalib: V2X-based Target-less Infrastructure Camera Calibration with State Space Model",
        "summary": "As cooperative systems that leverage roadside cameras to assist autonomous\nvehicle perception become increasingly widespread, large-scale precise\ncalibration of infrastructure cameras has become a critical issue. Traditional\nmanual calibration methods are often time-consuming, labor-intensive, and may\nrequire road closures. This paper proposes MamV2XCalib, the first V2X-based\ninfrastructure camera calibration method with the assistance of vehicle-side\nLiDAR. MamV2XCalib only requires autonomous vehicles equipped with LiDAR to\ndrive near the cameras to be calibrated in the infrastructure, without the need\nfor specific reference objects or manual intervention. We also introduce a new\ntargetless LiDAR-camera calibration method, which combines multi-scale features\nand a 4D correlation volume to estimate the correlation between vehicle-side\npoint clouds and roadside images. We model the temporal information and\nestimate the rotation angles with Mamba, effectively addressing calibration\nfailures in V2X scenarios caused by defects in the vehicle-side data (such as\nocclusions) and large differences in viewpoint. We evaluate MamV2XCalib on the\nV2X-Seq and TUMTraf-V2X real-world datasets, demonstrating the effectiveness\nand robustness of our V2X-based automatic calibration approach. Compared to\nprevious LiDAR-camera methods designed for calibration on one car, our approach\nachieves better and more stable calibration performance in V2X scenarios with\nfewer parameters. The code is available at\nhttps://github.com/zhuyaoye/MamV2XCalib.",
        "url": "http://arxiv.org/abs/2507.23595v1",
        "published_date": "2025-07-31T14:33:45+00:00",
        "updated_date": "2025-07-31T14:33:45+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yaoye Zhu",
            "Zhe Wang",
            "Yan Wang"
        ],
        "tldr": "MamV2XCalib introduces a novel V2X-based targetless infrastructure camera calibration method using vehicle-side LiDAR and Mamba-based temporal modeling, achieving improved accuracy and robustness in real-world V2X scenarios.",
        "tldr_zh": "MamV2XCalib提出了一种新的基于V2X的无目标基础设施相机标定方法，该方法利用车辆侧的激光雷达和基于Mamba的时序建模，在真实的V2X场景中实现了更高的精度和鲁棒性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "3D-MOOD: Lifting 2D to 3D for Monocular Open-Set Object Detection",
        "summary": "Monocular 3D object detection is valuable for various applications such as\nrobotics and AR/VR. Existing methods are confined to closed-set settings, where\nthe training and testing sets consist of the same scenes and/or object\ncategories. However, real-world applications often introduce new environments\nand novel object categories, posing a challenge to these methods. In this\npaper, we address monocular 3D object detection in an open-set setting and\nintroduce the first end-to-end 3D Monocular Open-set Object Detector (3D-MOOD).\nWe propose to lift the open-set 2D detection into 3D space through our designed\n3D bounding box head, enabling end-to-end joint training for both 2D and 3D\ntasks to yield better overall performance. We condition the object queries with\ngeometry prior and overcome the generalization for 3D estimation across diverse\nscenes. To further improve performance, we design the canonical image space for\nmore efficient cross-dataset training. We evaluate 3D-MOOD on both closed-set\nsettings (Omni3D) and open-set settings (Omni3D to Argoverse 2, ScanNet), and\nachieve new state-of-the-art results. Code and models are available at\nroyyang0714.github.io/3D-MOOD.",
        "url": "http://arxiv.org/abs/2507.23567v1",
        "published_date": "2025-07-31T13:56:41+00:00",
        "updated_date": "2025-07-31T13:56:41+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yung-Hsu Yang",
            "Luigi Piccinelli",
            "Mattia Segu",
            "Siyuan Li",
            "Rui Huang",
            "Yuqian Fu",
            "Marc Pollefeys",
            "Hermann Blum",
            "Zuria Bauer"
        ],
        "tldr": "The paper introduces 3D-MOOD, the first end-to-end monocular 3D object detector for open-set scenarios, addressing the challenge of novel environments and object categories. It achieves state-of-the-art performance by lifting 2D open-set detections into 3D, using geometry priors, and a canonical image space for cross-dataset training.",
        "tldr_zh": "该论文介绍了3D-MOOD，这是第一个用于开放场景的端到端单目3D物体检测器，旨在解决新环境和物体类别带来的挑战。通过将2D开放集检测提升到3D空间，利用几何先验和用于跨数据集训练的规范图像空间，实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "FastPoint: Accelerating 3D Point Cloud Model Inference via Sample Point Distance Prediction",
        "summary": "Deep neural networks have revolutionized 3D point cloud processing, yet\nefficiently handling large and irregular point clouds remains challenging. To\ntackle this problem, we introduce FastPoint, a novel software-based\nacceleration technique that leverages the predictable distance trend between\nsampled points during farthest point sampling. By predicting the distance\ncurve, we can efficiently identify subsequent sample points without\nexhaustively computing all pairwise distances. Our proposal substantially\naccelerates farthest point sampling and neighbor search operations while\npreserving sampling quality and model performance. By integrating FastPoint\ninto state-of-the-art 3D point cloud models, we achieve 2.55x end-to-end\nspeedup on NVIDIA RTX 3090 GPU without sacrificing accuracy.",
        "url": "http://arxiv.org/abs/2507.23480v1",
        "published_date": "2025-07-31T12:02:40+00:00",
        "updated_date": "2025-07-31T12:02:40+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Donghyun Lee",
            "Dawoon Jeong",
            "Jae W. Lee",
            "Hongil Yoon"
        ],
        "tldr": "FastPoint accelerates 3D point cloud model inference by predicting distance trends between sampled points during farthest point sampling, achieving significant speedup without sacrificing accuracy.",
        "tldr_zh": "FastPoint通过预测最远点采样期间采样点之间的距离趋势来加速3D点云模型推理，在不牺牲准确性的前提下实现了显著的加速。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding",
        "summary": "Large vision-language models (VLMs) have made significant strides in 2D\nvisual understanding tasks, sparking interest in extending these capabilities\nto 3D scene understanding. However, current 3D VLMs often struggle with robust\nreasoning and generalization due to limitations in high-quality spatial data\nand the static nature of viewpoint assumptions. To address these challenges, we\npropose 3D-R1, a foundation model that enhances the reasoning capabilities of\n3D VLMs. Specifically, we first construct a high-quality synthetic dataset with\nCoT, named Scene-30K, leveraging existing 3D-VL datasets and a data engine\nbased on Gemini 2.5 Pro. It serves as cold-start initialization data for 3D-R1.\nMoreover, we leverage RLHF policy such as GRPO in the reinforcement learning\ntraining process to enhance reasoning capabilities and introduce three reward\nfunctions: a perception reward, a semantic similarity reward and a format\nreward to maintain detection accuracy and answer semantic precision.\nFurthermore, we introduce a dynamic view selection strategy that adaptively\nchooses the most informative perspectives for 3D scene understanding. Extensive\nexperiments demonstrate that 3D-R1 delivers an average improvement of 10%\nacross various 3D scene benchmarks, highlighting its effectiveness in enhancing\nreasoning and generalization in 3D scene understanding. Code:\nhttps://github.com/AIGeeksGroup/3D-R1. Website:\nhttps://aigeeksgroup.github.io/3D-R1.",
        "url": "http://arxiv.org/abs/2507.23478v1",
        "published_date": "2025-07-31T11:59:06+00:00",
        "updated_date": "2025-07-31T11:59:06+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Ting Huang",
            "Zeyu Zhang",
            "Hao Tang"
        ],
        "tldr": "The paper introduces 3D-R1, a foundation model that enhances reasoning in 3D VLMs by using a high-quality synthetic dataset with CoT and RLHF with tailored reward functions and a dynamic view selection strategy, achieving a 10% improvement across various 3D scene benchmarks.",
        "tldr_zh": "该论文介绍了3D-R1，一个通过使用高质量的CoT合成数据集和具有定制奖励函数的RLHF，以及动态视角选择策略来增强3D VLM推理的基础模型，在多个3D场景基准测试中实现了10%的改进。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MagicRoad: Semantic-Aware 3D Road Surface Reconstruction via Obstacle Inpainting",
        "summary": "Road surface reconstruction is essential for autonomous driving, supporting\ncentimeter-accurate lane perception and high-definition mapping in complex\nurban environments.While recent methods based on mesh rendering or 3D Gaussian\nsplatting (3DGS) achieve promising results under clean and static conditions,\nthey remain vulnerable to occlusions from dynamic agents, visual clutter from\nstatic obstacles, and appearance degradation caused by lighting and weather\nchanges. We present a robust reconstruction framework that integrates\nocclusion-aware 2D Gaussian surfels with semantic-guided color enhancement to\nrecover clean, consistent road surfaces. Our method leverages a planar-adapted\nGaussian representation for efficient large-scale modeling, employs\nsegmentation-guided video inpainting to remove both dynamic and static\nforeground objects, and enhances color coherence via semantic-aware correction\nin HSV space. Extensive experiments on urban-scale datasets demonstrate that\nour framework produces visually coherent and geometrically faithful\nreconstructions, significantly outperforming prior methods under real-world\nconditions.",
        "url": "http://arxiv.org/abs/2507.23340v1",
        "published_date": "2025-07-31T08:38:36+00:00",
        "updated_date": "2025-07-31T08:38:36+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xingyue Peng",
            "Yuandong Lyu",
            "Lang Zhang",
            "Jian Zhu",
            "Songtao Wang",
            "Jiaxin Deng",
            "Songxin Lu",
            "Weiliang Ma",
            "Dangen She",
            "Peng Jia",
            "XianPeng Lang"
        ],
        "tldr": "The paper introduces MagicRoad, a method for robust 3D road surface reconstruction that utilizes occlusion-aware Gaussian surfels and semantic-guided color enhancement to overcome limitations of existing approaches in dynamic and cluttered urban environments.",
        "tldr_zh": "本文介绍了MagicRoad，一种鲁棒的3D道路表面重建方法，它利用具有遮挡感知能力的高斯曲面元素和语义引导的颜色增强来克服现有方法在动态和杂乱的城市环境中的局限性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Contrastive Learning-Driven Traffic Sign Perception: Multi-Modal Fusion of Text and Vision",
        "summary": "Traffic sign recognition, as a core component of autonomous driving\nperception systems, directly influences vehicle environmental awareness and\ndriving safety. Current technologies face two significant challenges: first,\nthe traffic sign dataset exhibits a pronounced long-tail distribution,\nresulting in a substantial decline in recognition performance of traditional\nconvolutional networks when processing low-frequency and out-of-distribution\nclasses; second, traffic signs in real-world scenarios are predominantly small\ntargets with significant scale variations, making it difficult to extract\nmulti-scale features.To overcome these issues, we propose a novel two-stage\nframework combining open-vocabulary detection and cross-modal learning. For\ntraffic sign detection, our NanoVerse YOLO model integrates a reparameterizable\nvision-language path aggregation network (RepVL-PAN) and an SPD-Conv module to\nspecifically enhance feature extraction for small, multi-scale targets. For\ntraffic sign classification, we designed a Traffic Sign Recognition Multimodal\nContrastive Learning model (TSR-MCL). By contrasting visual features from a\nVision Transformer with semantic features from a rule-based BERT, TSR-MCL\nlearns robust, frequency-independent representations, effectively mitigating\nclass confusion caused by data imbalance. On the TT100K dataset, our method\nachieves a state-of-the-art 78.4% mAP in the long-tail detection task for\nall-class recognition. The model also obtains 91.8% accuracy and 88.9% recall,\nsignificantly outperforming mainstream algorithms and demonstrating superior\naccuracy and generalization in complex, open-world scenarios.",
        "url": "http://arxiv.org/abs/2507.23331v1",
        "published_date": "2025-07-31T08:23:30+00:00",
        "updated_date": "2025-07-31T08:23:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qiang Lu",
            "Waikit Xiu",
            "Xiying Li",
            "Shenyu Hu",
            "Shengbo Sun"
        ],
        "tldr": "This paper introduces a two-stage framework for traffic sign recognition that addresses long-tail distribution and scale variation challenges using a novel NanoVerse YOLO model for detection and a cross-modal contrastive learning model (TSR-MCL) for classification, achieving state-of-the-art results on the TT100K dataset.",
        "tldr_zh": "该论文介绍了一个用于交通标志识别的两阶段框架，该框架使用 NanoVerse YOLO 模型进行检测，并使用跨模态对比学习模型 (TSR-MCL) 进行分类，从而解决了长尾分布和尺度变化挑战，并在 TT100K 数据集上实现了最先进的结果。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "FASTopoWM: Fast-Slow Lane Segment Topology Reasoning with Latent World Models",
        "summary": "Lane segment topology reasoning provides comprehensive bird's-eye view (BEV)\nroad scene understanding, which can serve as a key perception module in\nplanning-oriented end-to-end autonomous driving systems. Existing lane topology\nreasoning methods often fall short in effectively leveraging temporal\ninformation to enhance detection and reasoning performance. Recently,\nstream-based temporal propagation method has demonstrated promising results by\nincorporating temporal cues at both the query and BEV levels. However, it\nremains limited by over-reliance on historical queries, vulnerability to pose\nestimation failures, and insufficient temporal propagation. To overcome these\nlimitations, we propose FASTopoWM, a novel fast-slow lane segment topology\nreasoning framework augmented with latent world models. To reduce the impact of\npose estimation failures, this unified framework enables parallel supervision\nof both historical and newly initialized queries, facilitating mutual\nreinforcement between the fast and slow systems. Furthermore, we introduce\nlatent query and BEV world models conditioned on the action latent to propagate\nthe state representations from past observations to the current timestep. This\ndesign substantially improves the performance of temporal perception within the\nslow pipeline. Extensive experiments on the OpenLane-V2 benchmark demonstrate\nthat FASTopoWM outperforms state-of-the-art methods in both lane segment\ndetection (37.4% v.s. 33.6% on mAP) and centerline perception (46.3% v.s. 41.5%\non OLS).",
        "url": "http://arxiv.org/abs/2507.23325v1",
        "published_date": "2025-07-31T08:12:56+00:00",
        "updated_date": "2025-07-31T08:12:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yiming Yang",
            "Hongbin Lin",
            "Yueru Luo",
            "Suzhong Fu",
            "Chao Zheng",
            "Xinrui Yan",
            "Shuqi Mei",
            "Kun Tang",
            "Shuguang Cui",
            "Zhen Li"
        ],
        "tldr": "FASTopoWM is proposed to improve lane segment topology reasoning in autonomous driving by using a fast-slow system with latent world models to overcome the limitations of existing temporal perception methods, demonstrating state-of-the-art performance on OpenLane-V2.",
        "tldr_zh": "FASTopoWM被提出，通过使用具有潜在世界模型的快慢系统，改进自动驾驶中的车道线拓扑推理，克服了现有时间感知方法的局限性，并在OpenLane-V2上展示了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PriorFusion: Unified Integration of Priors for Robust Road Perception in Autonomous Driving",
        "summary": "With the growing interest in autonomous driving, there is an increasing\ndemand for accurate and reliable road perception technologies. In complex\nenvironments without high-definition map support, autonomous vehicles must\nindependently interpret their surroundings to ensure safe and robust\ndecision-making. However, these scenarios pose significant challenges due to\nthe large number, complex geometries, and frequent occlusions of road elements.\nA key limitation of existing approaches lies in their insufficient exploitation\nof the structured priors inherently present in road elements, resulting in\nirregular, inaccurate predictions. To address this, we propose PriorFusion, a\nunified framework that effectively integrates semantic, geometric, and\ngenerative priors to enhance road element perception. We introduce an\ninstance-aware attention mechanism guided by shape-prior features, then\nconstruct a data-driven shape template space that encodes low-dimensional\nrepresentations of road elements, enabling clustering to generate anchor points\nas reference priors. We design a diffusion-based framework that leverages these\nprior anchors to generate accurate and complete predictions. Experiments on\nlarge-scale autonomous driving datasets demonstrate that our method\nsignificantly improves perception accuracy, particularly under challenging\nconditions. Visualization results further confirm that our approach produces\nmore accurate, regular, and coherent predictions of road elements.",
        "url": "http://arxiv.org/abs/2507.23309v1",
        "published_date": "2025-07-31T07:43:19+00:00",
        "updated_date": "2025-07-31T07:43:19+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xuewei Tang",
            "Mengmeng Yang",
            "Tuopu Wen",
            "Peijin Jia",
            "Le Cui",
            "Mingshang Luo",
            "Kehua Sheng",
            "Bo Zhang",
            "Diange Yang",
            "Kun Jiang"
        ],
        "tldr": "The paper introduces PriorFusion, a novel framework integrating semantic, geometric, and generative priors for robust road element perception in autonomous driving, demonstrating improved accuracy and coherence in challenging conditions.",
        "tldr_zh": "该论文介绍了一种名为PriorFusion的新框架，它整合了语义、几何和生成先验知识，用于在自动驾驶中实现鲁棒的道路元素感知，并在具有挑战性的条件下展示了更高的准确性和一致性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "GSFusion:Globally Optimized LiDAR-Inertial-Visual Mapping for Gaussian Splatting",
        "summary": "While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic mapping,\nconventional approaches based on camera sensor, even RGB-D, suffer from\nfundamental limitations such as high computational load, failure in\nenvironments with poor texture or illumination, and short operational ranges.\nLiDAR emerges as a robust alternative, but its integration with 3DGS introduces\nnew challenges, such as the need for exceptional global alignment for\nphotorealistic quality and prolonged optimization times caused by sparse data.\nTo address these challenges, we propose GSFusion, an online\nLiDAR-Inertial-Visual mapping system that ensures high-precision map\nconsistency through a surfel-to-surfel constraint in the global pose-graph\noptimization. To handle sparse data, our system employs a pixel-aware Gaussian\ninitialization strategy for efficient representation and a bounded sigmoid\nconstraint to prevent uncontrolled Gaussian growth. Experiments on public and\nour datasets demonstrate our system outperforms existing 3DGS SLAM systems in\nterms of rendering quality and map-building efficiency.",
        "url": "http://arxiv.org/abs/2507.23273v1",
        "published_date": "2025-07-31T06:15:51+00:00",
        "updated_date": "2025-07-31T06:15:51+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Jaeseok Park",
            "Chanoh Park",
            "Minsu Kim",
            "Soohwan Kim"
        ],
        "tldr": "GSFusion introduces a novel LiDAR-Inertial-Visual mapping system for 3D Gaussian Splatting that addresses limitations of existing camera-based approaches by using surfel-to-surfel constraints and pixel-aware Gaussian initialization for improved rendering quality and map-building efficiency.",
        "tldr_zh": "GSFusion 提出了一种新的 LiDAR-惯性-视觉映射系统，用于三维高斯溅射，通过使用surfel-to-surfel约束和像素感知高斯初始化，解决了现有基于相机的方法的局限性，从而提高了渲染质量和地图构建效率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Reference-Guided Diffusion Inpainting For Multimodal Counterfactual Generation",
        "summary": "Safety-critical applications, such as autonomous driving and medical image\nanalysis, require extensive multimodal data for rigorous testing. Synthetic\ndata methods are gaining prominence due to the cost and complexity of gathering\nreal-world data, but they demand a high degree of realism and controllability\nto be useful. This work introduces two novel methods for synthetic data\ngeneration in autonomous driving and medical image analysis, namely MObI and\nAnydoorMed, respectively. MObI is a first-of-its-kind framework for Multimodal\nObject Inpainting that leverages a diffusion model to produce realistic and\ncontrollable object inpaintings across perceptual modalities, demonstrated\nsimultaneously for camera and lidar. Given a single reference RGB image, MObI\nenables seamless object insertion into existing multimodal scenes at a\nspecified 3D location, guided by a bounding box, while maintaining semantic\nconsistency and multimodal coherence. Unlike traditional inpainting methods\nthat rely solely on edit masks, this approach uses 3D bounding box conditioning\nto ensure accurate spatial positioning and realistic scaling. AnydoorMed\nextends this paradigm to the medical imaging domain, focusing on\nreference-guided inpainting for mammography scans. It leverages a\ndiffusion-based model to inpaint anomalies with impressive detail preservation,\nmaintaining the reference anomaly's structural integrity while semantically\nblending it with the surrounding tissue. Together, these methods demonstrate\nthat foundation models for reference-guided inpainting in natural images can be\nreadily adapted to diverse perceptual modalities, paving the way for the next\ngeneration of systems capable of constructing highly realistic, controllable\nand multimodal counterfactual scenarios.",
        "url": "http://arxiv.org/abs/2507.23058v1",
        "published_date": "2025-07-30T19:43:47+00:00",
        "updated_date": "2025-07-30T19:43:47+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Alexandru Buburuzan"
        ],
        "tldr": "The paper introduces two novel reference-guided diffusion inpainting methods (MObI for autonomous driving with multimodal data and AnydoorMed for medical imaging) for generating realistic and controllable counterfactual scenarios.",
        "tldr_zh": "该论文介绍了两种新的参考引导扩散修复方法（MObI用于自动驾驶，使用多模态数据；AnydoorMed用于医学影像），用于生成逼真且可控的反事实场景。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Modeling Human Gaze Behavior with Diffusion Models for Unified Scanpath Prediction",
        "summary": "Predicting human gaze scanpaths is crucial for understanding visual\nattention, with applications in human-computer interaction, autonomous systems,\nand cognitive robotics. While deep learning models have advanced scanpath\nprediction, most existing approaches generate averaged behaviors, failing to\ncapture the variability of human visual exploration. In this work, we present\nScanDiff, a novel architecture that combines diffusion models with Vision\nTransformers to generate diverse and realistic scanpaths. Our method explicitly\nmodels scanpath variability by leveraging the stochastic nature of diffusion\nmodels, producing a wide range of plausible gaze trajectories. Additionally, we\nintroduce textual conditioning to enable task-driven scanpath generation,\nallowing the model to adapt to different visual search objectives. Experiments\non benchmark datasets show that ScanDiff surpasses state-of-the-art methods in\nboth free-viewing and task-driven scenarios, producing more diverse and\naccurate scanpaths. These results highlight its ability to better capture the\ncomplexity of human visual behavior, pushing forward gaze prediction research.\nSource code and models are publicly available at\nhttps://aimagelab.github.io/ScanDiff.",
        "url": "http://arxiv.org/abs/2507.23021v1",
        "published_date": "2025-07-30T18:36:09+00:00",
        "updated_date": "2025-07-30T18:36:09+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Giuseppe Cartella",
            "Vittorio Cuculo",
            "Alessandro D'Amelio",
            "Marcella Cornia",
            "Giuseppe Boccignone",
            "Rita Cucchiara"
        ],
        "tldr": "The paper introduces ScanDiff, a novel architecture combining diffusion models and Vision Transformers for generating diverse and realistic human gaze scanpaths, outperforming existing methods in both free-viewing and task-driven scenarios.",
        "tldr_zh": "该论文介绍了一种名为ScanDiff的新架构，它结合了扩散模型和视觉Transformer，用于生成多样且逼真的人类注视扫描路径，在自由观察和任务驱动的场景中均优于现有方法。",
        "relevance_score": 7,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 7
    }
]