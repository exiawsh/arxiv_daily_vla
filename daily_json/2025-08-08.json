[
    {
        "title": "Physical Adversarial Camouflage through Gradient Calibration and Regularization",
        "summary": "The advancement of deep object detectors has greatly affected safety-critical\nfields like autonomous driving. However, physical adversarial camouflage poses\na significant security risk by altering object textures to deceive detectors.\nExisting techniques struggle with variable physical environments, facing two\nmain challenges: 1) inconsistent sampling point densities across distances\nhinder the gradient optimization from ensuring local continuity, and 2)\nupdating texture gradients from multiple angles causes conflicts, reducing\noptimization stability and attack effectiveness. To address these issues, we\npropose a novel adversarial camouflage framework based on gradient\noptimization. First, we introduce a gradient calibration strategy, which\nensures consistent gradient updates across distances by propagating gradients\nfrom sparsely to unsampled texture points. Additionally, we develop a gradient\ndecorrelation method, which prioritizes and orthogonalizes gradients based on\nloss values, enhancing stability and effectiveness in multi-angle optimization\nby eliminating redundant or conflicting updates. Extensive experimental results\non various detection models, angles and distances show that our method\nsignificantly exceeds the state of the art, with an average increase in attack\nsuccess rate (ASR) of 13.46% across distances and 11.03% across angles.\nFurthermore, empirical evaluation in real-world scenarios highlights the need\nfor more robust system design.",
        "url": "http://arxiv.org/abs/2508.05414v1",
        "published_date": "2025-08-07T14:07:49+00:00",
        "updated_date": "2025-08-07T14:07:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jiawei Liang",
            "Siyuan Liang",
            "Jianjie Huang",
            "Chenxi Si",
            "Ming Zhang",
            "Xiaochun Cao"
        ],
        "tldr": "This paper introduces a novel adversarial camouflage technique for deceiving object detectors in physical environments by addressing inconsistencies in gradient optimization across distances and angles, demonstrating significant improvements in attack success rates.",
        "tldr_zh": "本文提出了一种新的对抗性伪装技术，通过解决不同距离和角度下梯度优化中的不一致性来欺骗物理环境中的目标检测器，并展示了攻击成功率的显著提高。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "B4DL: A Benchmark for 4D LiDAR LLM in Spatio-Temporal Understanding",
        "summary": "Understanding dynamic outdoor environments requires capturing complex object\ninteractions and their evolution over time. LiDAR-based 4D point clouds provide\nprecise spatial geometry and rich temporal cues, making them ideal for\nrepresenting real-world scenes. However, despite their potential, 4D LiDAR\nremains underexplored in the context of Multimodal Large Language Models\n(MLLMs) due to the absence of high-quality, modality-specific annotations and\nthe lack of MLLM architectures capable of processing its high-dimensional\ncomposition. To address these challenges, we introduce B4DL, a new benchmark\nspecifically designed for training and evaluating MLLMs on 4D LiDAR\nunderstanding. In addition, we propose a scalable data generation pipeline and\nan MLLM model that, for the first time, directly processes raw 4D LiDAR by\nbridging it with language understanding. Combined with our dataset and\nbenchmark, our model offers a unified solution for spatio-temporal reasoning in\ndynamic outdoor environments. We provide rendered 4D LiDAR videos, generated\ndataset, and inference outputs on diverse scenarios at:\nhttps://mmb4dl.github.io/mmb4dl/",
        "url": "http://arxiv.org/abs/2508.05269v1",
        "published_date": "2025-08-07T11:11:56+00:00",
        "updated_date": "2025-08-07T11:11:56+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Changho Choi",
            "Youngwoo Shin",
            "Gyojin Han",
            "Dong-Jae Lee",
            "Junmo Kim"
        ],
        "tldr": "The paper introduces B4DL, a new benchmark dataset and MLLM model for 4D LiDAR understanding, addressing the lack of suitable resources for spatio-temporal reasoning in dynamic outdoor environments.",
        "tldr_zh": "本文介绍了一个新的基准数据集B4DL和一个MLLM模型，用于4D LiDAR理解，旨在解决动态户外环境中缺乏合适的时空推理资源的问题。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "PhysPatch: A Physically Realizable and Transferable Adversarial Patch Attack for Multimodal Large Language Models-based Autonomous Driving Systems",
        "summary": "Multimodal Large Language Models (MLLMs) are becoming integral to autonomous\ndriving (AD) systems due to their strong vision-language reasoning\ncapabilities. However, MLLMs are vulnerable to adversarial attacks,\nparticularly adversarial patch attacks, which can pose serious threats in\nreal-world scenarios. Existing patch-based attack methods are primarily\ndesigned for object detection models and perform poorly when transferred to\nMLLM-based systems due to the latter's complex architectures and reasoning\nabilities. To address these limitations, we propose PhysPatch, a physically\nrealizable and transferable adversarial patch framework tailored for MLLM-based\nAD systems. PhysPatch jointly optimizes patch location, shape, and content to\nenhance attack effectiveness and real-world applicability. It introduces a\nsemantic-based mask initialization strategy for realistic placement, an\nSVD-based local alignment loss with patch-guided crop-resize to improve\ntransferability, and a potential field-based mask refinement method. Extensive\nexperiments across open-source, commercial, and reasoning-capable MLLMs\ndemonstrate that PhysPatch significantly outperforms prior methods in steering\nMLLM-based AD systems toward target-aligned perception and planning outputs.\nMoreover, PhysPatch consistently places adversarial patches in physically\nfeasible regions of AD scenes, ensuring strong real-world applicability and\ndeployability.",
        "url": "http://arxiv.org/abs/2508.05167v1",
        "published_date": "2025-08-07T08:54:54+00:00",
        "updated_date": "2025-08-07T08:54:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Qi Guo",
            "Xiaojun Jia",
            "Shanmin Pang",
            "Simeng Qin",
            "Lin Wang",
            "Ju Jia",
            "Yang Liu",
            "Qing Guo"
        ],
        "tldr": "The paper introduces PhysPatch, a physically realizable adversarial patch framework designed to effectively attack multimodal large language models in autonomous driving systems, demonstrating improved transferability and real-world applicability compared to existing methods.",
        "tldr_zh": "该论文介绍了PhysPatch，一个物理上可实现的对抗补丁框架，旨在有效攻击自动驾驶系统中基于多模态大型语言模型的系统。相比现有方法，PhysPatch展示了更好的可迁移性和现实应用性。",
        "relevance_score": 10,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation",
        "summary": "We introduce Genie Envisioner (GE), a unified world foundation platform for\nrobotic manipulation that integrates policy learning, evaluation, and\nsimulation within a single video-generative framework. At its core, GE-Base is\na large-scale, instruction-conditioned video diffusion model that captures the\nspatial, temporal, and semantic dynamics of real-world robotic interactions in\na structured latent space. Built upon this foundation, GE-Act maps latent\nrepresentations to executable action trajectories through a lightweight,\nflow-matching decoder, enabling precise and generalizable policy inference\nacross diverse embodiments with minimal supervision. To support scalable\nevaluation and training, GE-Sim serves as an action-conditioned neural\nsimulator, producing high-fidelity rollouts for closed-loop policy development.\nThe platform is further equipped with EWMBench, a standardized benchmark suite\nmeasuring visual fidelity, physical consistency, and instruction-action\nalignment. Together, these components establish Genie Envisioner as a scalable\nand practical foundation for instruction-driven, general-purpose embodied\nintelligence. All code, models, and benchmarks will be released publicly.",
        "url": "http://arxiv.org/abs/2508.05635v1",
        "published_date": "2025-08-07T17:59:44+00:00",
        "updated_date": "2025-08-07T17:59:44+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Yue Liao",
            "Pengfei Zhou",
            "Siyuan Huang",
            "Donglin Yang",
            "Shengcong Chen",
            "Yuxin Jiang",
            "Yue Hu",
            "Jingbin Cai",
            "Si Liu",
            "Jianlan Luo",
            "Liliang Chen",
            "Shuicheng Yan",
            "Maoqing Yao",
            "Guanghui Ren"
        ],
        "tldr": "The paper introduces Genie Envisioner (GE), a unified platform for robotic manipulation using a video-generative framework that integrates policy learning, evaluation, and simulation, offering a scalable solution for instruction-driven embodied intelligence. It will also be released publicly.",
        "tldr_zh": "该论文介绍了Genie Envisioner (GE)，一个统一的机器人操作平台，它使用视频生成框架集成了策略学习、评估和模拟，为指令驱动的具身智能提供了一个可扩展的解决方案。所有代码，模型和benchmark都会开源。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling",
        "summary": "Mobile robots navigating in crowds trained using reinforcement learning are\nknown to suffer performance degradation when faced with out-of-distribution\nscenarios. We propose that by properly accounting for the uncertainties of\npedestrians, a robot can learn safe navigation policies that are robust to\ndistribution shifts. Our method augments agent observations with prediction\nuncertainty estimates generated by adaptive conformal inference, and it uses\nthese estimates to guide the agent's behavior through constrained reinforcement\nlearning. The system helps regulate the agent's actions and enables it to adapt\nto distribution shifts. In the in-distribution setting, our approach achieves a\n96.93% success rate, which is over 8.80% higher than the previous\nstate-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times\nfewer intrusions into ground-truth human future trajectories. In three\nout-of-distribution scenarios, our method shows much stronger robustness when\nfacing distribution shifts in velocity variations, policy changes, and\ntransitions from individual to group dynamics. We deploy our method on a real\nrobot, and experiments show that the robot makes safe and robust decisions when\ninteracting with both sparse and dense crowds. Our code and videos are\navailable on https://gen-safe-nav.github.io/.",
        "url": "http://arxiv.org/abs/2508.05634v1",
        "published_date": "2025-08-07T17:59:43+00:00",
        "updated_date": "2025-08-07T17:59:43+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.LG",
            "cs.SY",
            "eess.SY"
        ],
        "authors": [
            "Jianpeng Yao",
            "Xiaopan Zhang",
            "Yu Xia",
            "Zejin Wang",
            "Amit K. Roy-Chowdhury",
            "Jiachen Li"
        ],
        "tldr": "This paper presents a reinforcement learning-based crowd navigation method that uses adaptive conformal inference to estimate pedestrian uncertainty and guide the robot's behavior, demonstrating improved safety and robustness in both simulated and real-world environments.",
        "tldr_zh": "本文提出了一种基于强化学习的人群导航方法，该方法使用自适应共形推断来估计行人的不确定性并指导机器人的行为，在模拟和现实环境中都表现出更高的安全性和鲁棒性。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DistillDrive: End-to-End Multi-Mode Autonomous Driving Distillation by Isomorphic Hetero-Source Planning Model",
        "summary": "End-to-end autonomous driving has been recently seen rapid development,\nexerting a profound influence on both industry and academia. However, the\nexisting work places excessive focus on ego-vehicle status as their sole\nlearning objectives and lacks of planning-oriented understanding, which limits\nthe robustness of the overall decision-making prcocess. In this work, we\nintroduce DistillDrive, an end-to-end knowledge distillation-based autonomous\ndriving model that leverages diversified instance imitation to enhance\nmulti-mode motion feature learning. Specifically, we employ a planning model\nbased on structured scene representations as the teacher model, leveraging its\ndiversified planning instances as multi-objective learning targets for the\nend-to-end model. Moreover, we incorporate reinforcement learning to enhance\nthe optimization of state-to-decision mappings, while utilizing generative\nmodeling to construct planning-oriented instances, fostering intricate\ninteractions within the latent space. We validate our model on the nuScenes and\nNAVSIM datasets, achieving a 50\\% reduction in collision rate and a 3-point\nimprovement in closed-loop performance compared to the baseline model. Code and\nmodel are publicly available at https://github.com/YuruiAI/DistillDrive",
        "url": "http://arxiv.org/abs/2508.05402v1",
        "published_date": "2025-08-07T13:54:35+00:00",
        "updated_date": "2025-08-07T13:54:35+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Rui Yu",
            "Xianghang Zhang",
            "Runkai Zhao",
            "Huaicheng Yan",
            "Meng Wang"
        ],
        "tldr": "The paper introduces DistillDrive, an end-to-end autonomous driving model using knowledge distillation with a planning model as a teacher, reinforcement learning, and generative modeling to improve performance, showing a 50% collision rate reduction and 3-point closed-loop improvement.",
        "tldr_zh": "该论文介绍了 DistillDrive，一种端到端自动驾驶模型，它使用知识蒸馏，以规划模型作为教师，并结合强化学习和生成模型来提高性能，实现了 50% 的碰撞率降低和 3 个点的闭环性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Cross-View Localization via Redundant Sliced Observations and A-Contrario Validation",
        "summary": "Cross-view localization (CVL) matches ground-level images with aerial\nreferences to determine the geo-position of a camera, enabling smart vehicles\nto self-localize offline in GNSS-denied environments. However, most CVL methods\noutput only a single observation, the camera pose, and lack the redundant\nobservations required by surveying principles, making it challenging to assess\nlocalization reliability through the mutual validation of observational data.\nTo tackle this, we introduce Slice-Loc, a two-stage method featuring an\na-contrario reliability validation for CVL. Instead of using the query image as\na single input, Slice-Loc divides it into sub-images and estimates the 3-DoF\npose for each slice, creating redundant and independent observations. Then, a\ngeometric rigidity formula is proposed to filter out the erroneous 3-DoF poses,\nand the inliers are merged to generate the final camera pose. Furthermore, we\npropose a model that quantifies the meaningfulness of localization by\nestimating the number of false alarms (NFA), according to the distribution of\nthe locations of the sliced images. By eliminating gross errors, Slice-Loc\nboosts localization accuracy and effectively detects failures. After filtering\nout mislocalizations, Slice-Loc reduces the proportion of errors exceeding 10 m\nto under 3\\%. In cross-city tests on the DReSS dataset, Slice-Loc cuts the mean\nlocalization error from 4.47 m to 1.86 m and the mean orientation error from\n$\\mathbf{3.42^{\\circ}}$ to $\\mathbf{1.24^{\\circ}}$, outperforming\nstate-of-the-art methods. Code and dataset will be available at:\nhttps://github.com/bnothing/Slice-Loc.",
        "url": "http://arxiv.org/abs/2508.05369v1",
        "published_date": "2025-08-07T13:15:49+00:00",
        "updated_date": "2025-08-07T13:15:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yongjun Zhang",
            "Mingtao Xiong",
            "Yi Wan",
            "Gui-Song Xia"
        ],
        "tldr": "The paper introduces Slice-Loc, a novel cross-view localization method that improves accuracy and reliability by dividing query images into slices, estimating poses for each slice, and using a-contrario validation to filter out erroneous poses. Results show significant improvement in localization accuracy and failure detection compared to state-of-the-art methods.",
        "tldr_zh": "该论文介绍了一种名为 Slice-Loc 的新型跨视角定位方法，通过将查询图像分割成小块，为每个小块估计姿态，并使用 a-contrario 验证来过滤掉错误的姿态，从而提高准确性和可靠性。结果表明，与最先进的方法相比，定位精度和故障检测能力都有显著提高。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ArbiViewGen: Controllable Arbitrary Viewpoint Camera Data Generation for Autonomous Driving via Stable Diffusion Models",
        "summary": "Arbitrary viewpoint image generation holds significant potential for\nautonomous driving, yet remains a challenging task due to the lack of\nground-truth data for extrapolated views, which hampers the training of\nhigh-fidelity generative models. In this work, we propose Arbiviewgen, a novel\ndiffusion-based framework for the generation of controllable camera images from\narbitrary points of view. To address the absence of ground-truth data in unseen\nviews, we introduce two key components: Feature-Aware Adaptive View Stitching\n(FAVS) and Cross-View Consistency Self-Supervised Learning (CVC-SSL). FAVS\nemploys a hierarchical matching strategy that first establishes coarse\ngeometric correspondences using camera poses, then performs fine-grained\nalignment through improved feature matching algorithms, and identifies\nhigh-confidence matching regions via clustering analysis. Building upon this,\nCVC-SSL adopts a self-supervised training paradigm where the model reconstructs\nthe original camera views from the synthesized stitched images using a\ndiffusion model, enforcing cross-view consistency without requiring supervision\nfrom extrapolated data. Our framework requires only multi-camera images and\ntheir associated poses for training, eliminating the need for additional\nsensors or depth maps. To our knowledge, Arbiviewgen is the first method\ncapable of controllable arbitrary view camera image generation in multiple\nvehicle configurations.",
        "url": "http://arxiv.org/abs/2508.05236v1",
        "published_date": "2025-08-07T10:24:47+00:00",
        "updated_date": "2025-08-07T10:24:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yatong Lan",
            "Jingfeng Chen",
            "Yiru Wang",
            "Lei He"
        ],
        "tldr": "ArbiViewGen is a novel diffusion-based framework for generating controllable arbitrary viewpoint camera images for autonomous driving, using feature-aware view stitching and cross-view consistency self-supervised learning without requiring ground-truth data for extrapolated views.",
        "tldr_zh": "ArbiViewGen是一个新颖的基于扩散模型的框架，用于生成可控的任意视点相机图像，以用于自动驾驶。它采用特征感知的视图拼接和交叉视图一致性自监督学习，无需外推视图的ground-truth数据。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Learning to See and Act: Task-Aware View Planning for Robotic Manipulation",
        "summary": "Recent vision-language-action (VLA) models for multi-task robotic\nmanipulation commonly rely on static viewpoints and shared visual encoders,\nwhich limit 3D perception and cause task interference, hindering robustness and\ngeneralization. In this work, we propose Task-Aware View Planning (TAVP), a\nframework designed to overcome these challenges by integrating active view\nplanning with task-specific representation learning. TAVP employs an efficient\nexploration policy, accelerated by a novel pseudo-environment, to actively\nacquire informative views. Furthermore, we introduce a Mixture-of-Experts (MoE)\nvisual encoder to disentangle features across different tasks, boosting both\nrepresentation fidelity and task generalization. By learning to see the world\nin a task-aware way, TAVP generates more complete and discriminative visual\nrepresentations, demonstrating significantly enhanced action prediction across\na wide array of manipulation challenges. Extensive experiments on RLBench tasks\nshow that our proposed TAVP model achieves superior performance over\nstate-of-the-art fixed-view approaches. Visual results and code are provided\nat: https://hcplab-sysu.github.io/TAVP.",
        "url": "http://arxiv.org/abs/2508.05186v1",
        "published_date": "2025-08-07T09:21:20+00:00",
        "updated_date": "2025-08-07T09:21:20+00:00",
        "categories": [
            "cs.RO",
            "cs.CV"
        ],
        "authors": [
            "Yongjie Bai",
            "Zhouxia Wang",
            "Yang Liu",
            "Weixing Chen",
            "Ziliang Chen",
            "Mingtong Dai",
            "Yongsen Zheng",
            "Lingbo Liu",
            "Guanbin Li",
            "Liang Lin"
        ],
        "tldr": "This paper introduces Task-Aware View Planning (TAVP) to improve robotic manipulation by actively selecting informative viewpoints and disentangling task-specific visual features using a Mixture-of-Experts encoder, achieving superior performance on RLBench tasks.",
        "tldr_zh": "本文介绍了任务感知视图规划（TAVP），通过主动选择信息丰富的视点并使用混合专家编码器解耦特定于任务的视觉特征来改进机器人操作，从而在RLBench任务上实现了卓越的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Propagating Sparse Depth via Depth Foundation Model for Out-of-Distribution Depth Completion",
        "summary": "Depth completion is a pivotal challenge in computer vision, aiming at\nreconstructing the dense depth map from a sparse one, typically with a paired\nRGB image. Existing learning based models rely on carefully prepared but\nlimited data, leading to significant performance degradation in\nout-of-distribution (OOD) scenarios. Recent foundation models have demonstrated\nexceptional robustness in monocular depth estimation through large-scale\ntraining, and using such models to enhance the robustness of depth completion\nmodels is a promising solution. In this work, we propose a novel depth\ncompletion framework that leverages depth foundation models to attain\nremarkable robustness without large-scale training. Specifically, we leverage a\ndepth foundation model to extract environmental cues, including structural and\nsemantic context, from RGB images to guide the propagation of sparse depth\ninformation into missing regions. We further design a dual-space propagation\napproach, without any learnable parameters, to effectively propagates sparse\ndepth in both 3D and 2D spaces to maintain geometric structure and local\nconsistency. To refine the intricate structure, we introduce a learnable\ncorrection module to progressively adjust the depth prediction towards the real\ndepth. We train our model on the NYUv2 and KITTI datasets as in-distribution\ndatasets and extensively evaluate the framework on 16 other datasets. Our\nframework performs remarkably well in the OOD scenarios and outperforms\nexisting state-of-the-art depth completion methods. Our models are released in\nhttps://github.com/shenglunch/PSD.",
        "url": "http://arxiv.org/abs/2508.04984v1",
        "published_date": "2025-08-07T02:38:24+00:00",
        "updated_date": "2025-08-07T02:38:24+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shenglun Chen",
            "Xinzhu Ma",
            "Hong Zhang",
            "Haojie Li",
            "Zhihui Wang"
        ],
        "tldr": "This paper introduces a depth completion framework leveraging depth foundation models for improved out-of-distribution robustness. It propagates sparse depth using environmental cues, a dual-space propagation approach, and a learnable correction module, achieving state-of-the-art performance on various datasets.",
        "tldr_zh": "本文提出了一种深度补全框架，该框架利用深度基础模型来提高分布外 (OOD) 的鲁棒性。 它利用环境线索、双空间传播方法和可学习的校正模块来传播稀疏深度，在各种数据集上实现了最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "CSRAP: Enhanced Canvas Attention Scheduling for Real-Time Mission Critical Perception",
        "summary": "Real-time perception on edge platforms faces a core challenge: executing\nhigh-resolution object detection under stringent latency constraints on limited\ncomputing resources. Canvas-based attention scheduling was proposed in earlier\nwork as a mechanism to reduce the resource demands of perception subsystems. It\nconsolidates areas of interest in an input data frame onto a smaller area,\ncalled a canvas frame, that can be processed at the requisite frame rate. This\npaper extends prior canvas-based attention scheduling literature by (i)\nallowing for variable-size canvas frames and (ii) employing selectable canvas\nframe rates that may depart from the original data frame rate. We evaluate our\nsolution by running YOLOv11, as the perception module, on an NVIDIA Jetson Orin\nNano to inspect video frames from the Waymo Open Dataset. Our results show that\nthe additional degrees of freedom improve the attainable quality/cost\ntrade-offs, thereby allowing for a consistently higher mean average precision\n(mAP) and recall with respect to the state of the art.",
        "url": "http://arxiv.org/abs/2508.04976v1",
        "published_date": "2025-08-07T02:03:50+00:00",
        "updated_date": "2025-08-07T02:03:50+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Md Iftekharul Islam Sakib",
            "Yigong Hu",
            "Tarek Abdelzaher"
        ],
        "tldr": "The paper introduces an enhanced canvas attention scheduling method (CSRAP) for real-time object detection on edge platforms, improving quality/cost trade-offs by using variable-size canvas frames and selectable frame rates. It is evaluated using YOLOv11 on a Jetson Orin Nano with the Waymo Open Dataset.",
        "tldr_zh": "该论文介绍了一种增强的画布注意力调度方法 (CSRAP)，用于边缘平台上的实时目标检测，通过使用可变大小的画布帧和可选择的帧速率，提高了质量/成本的权衡。它使用 YOLOv11 在 Jetson Orin Nano 上使用 Waymo 开放数据集进行了评估。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "Open-world Point Cloud Semantic Segmentation: A Human-in-the-loop Framework",
        "summary": "Open-world point cloud semantic segmentation (OW-Seg) aims to predict point\nlabels of both base and novel classes in real-world scenarios. However,\nexisting methods rely on resource-intensive offline incremental learning or\ndensely annotated support data, limiting their practicality. To address these\nlimitations, we propose HOW-Seg, the first human-in-the-loop framework for\nOW-Seg. Specifically, we construct class prototypes, the fundamental\nsegmentation units, directly on the query data, avoiding the prototype bias\ncaused by intra-class distribution shifts between the support and query data.\nBy leveraging sparse human annotations as guidance, HOW-Seg enables\nprototype-based segmentation for both base and novel classes. Considering the\nlack of granularity of initial prototypes, we introduce a hierarchical\nprototype disambiguation mechanism to refine ambiguous prototypes, which\ncorrespond to annotations of different classes. To further enrich contextual\nawareness, we employ a dense conditional random field (CRF) upon the refined\nprototypes to optimize their label assignments. Through iterative human\nfeedback, HOW-Seg dynamically improves its predictions, achieving high-quality\nsegmentation for both base and novel classes. Experiments demonstrate that with\nsparse annotations (e.g., one-novel-class-one-click), HOW-Seg matches or\nsurpasses the state-of-the-art generalized few-shot segmentation (GFS-Seg)\nmethod under the 5-shot setting. When using advanced backbones (e.g.,\nStratified Transformer) and denser annotations (e.g., 10 clicks per sub-scene),\nHOW-Seg achieves 85.27% mIoU on S3DIS and 66.37% mIoU on ScanNetv2,\nsignificantly outperforming alternatives.",
        "url": "http://arxiv.org/abs/2508.04962v1",
        "published_date": "2025-08-07T01:20:41+00:00",
        "updated_date": "2025-08-07T01:20:41+00:00",
        "categories": [
            "cs.CV",
            "cs.GR"
        ],
        "authors": [
            "Peng Zhang",
            "Songru Yang",
            "Jinsheng Sun",
            "Weiqing Li",
            "Zhiyong Su"
        ],
        "tldr": "The paper introduces HOW-Seg, a human-in-the-loop framework for open-world point cloud semantic segmentation, utilizing sparse human annotations to improve prototype-based segmentation of base and novel classes, achieving state-of-the-art results with minimal annotation effort.",
        "tldr_zh": "该论文介绍了一种人机协作的开放世界点云语义分割框架HOW-Seg，该框架利用稀疏的人工标注来改进基于原型的分割，适用于基类和新类，并以最小的标注代价实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]