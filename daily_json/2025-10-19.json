[
    {
        "title": "Enhancing Rotated Object Detection via Anisotropic Gaussian Bounding Box and Bhattacharyya Distance",
        "summary": "Detecting rotated objects accurately and efficiently is a significant\nchallenge in computer vision, particularly in applications such as aerial\nimagery, remote sensing, and autonomous driving. Although traditional object\ndetection frameworks are effective for axis-aligned objects, they often\nunderperform in scenarios involving rotated objects due to their limitations in\ncapturing orientation variations. This paper introduces an improved loss\nfunction aimed at enhancing detection accuracy and robustness by leveraging the\nGaussian bounding box representation and Bhattacharyya distance. In addition,\nwe advocate for the use of an anisotropic Gaussian representation to address\nthe issues associated with isotropic variance in square-like objects. Our\nproposed method addresses these challenges by incorporating a\nrotation-invariant loss function that effectively captures the geometric\nproperties of rotated objects. We integrate this proposed loss function into\nstate-of-the-art deep learning-based rotated object detection detectors, and\nextensive experiments demonstrated significant improvements in mean Average\nPrecision metrics compared to existing methods. The results highlight the\npotential of our approach to establish new benchmark in rotated object\ndetection, with implications for a wide range of applications requiring precise\nand reliable object localization irrespective of orientation.",
        "url": "http://arxiv.org/abs/2510.16445v1",
        "published_date": "2025-10-18T10:42:30+00:00",
        "updated_date": "2025-10-18T10:42:30+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Chien Thai",
            "Mai Xuan Trang",
            "Huong Ninh",
            "Hoang Hiep Ly",
            "Anh Son Le"
        ],
        "tldr": "This paper introduces a novel loss function using anisotropic Gaussian bounding box representation and Bhattacharyya distance to improve rotated object detection accuracy, demonstrating significant improvements in mAP.",
        "tldr_zh": "本文提出了一种新的损失函数，使用各向异性高斯边界框表示和Bhattacharyya距离来提高旋转目标检测的准确性，并在mAP上取得了显著提升。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting",
        "summary": "Bridging the gap between complex human instructions and precise 3D object\ngrounding remains a significant challenge in vision and robotics. Existing 3D\nsegmentation methods often struggle to interpret ambiguous, reasoning-based\ninstructions, while 2D vision-language models that excel at such reasoning lack\nintrinsic 3D spatial understanding. In this paper, we introduce REALM, an\ninnovative MLLM-agent framework that enables open-world reasoning-based\nsegmentation without requiring extensive 3D-specific post-training. We perform\nsegmentation directly on 3D Gaussian Splatting representations, capitalizing on\ntheir ability to render photorealistic novel views that are highly suitable for\nMLLM comprehension. As directly feeding one or more rendered views to the MLLM\ncan lead to high sensitivity to viewpoint selection, we propose a novel\nGlobal-to-Local Spatial Grounding strategy. Specifically, multiple global views\nare first fed into the MLLM agent in parallel for coarse-level localization,\naggregating responses to robustly identify the target object. Then, several\nclose-up novel views of the object are synthesized to perform fine-grained\nlocal segmentation, yielding accurate and consistent 3D masks. Extensive\nexperiments show that REALM achieves remarkable performance in interpreting\nboth explicit and implicit instructions across LERF, 3D-OVS, and our newly\nintroduced REALM3D benchmarks. Furthermore, our agent framework seamlessly\nsupports a range of 3D interaction tasks, including object removal,\nreplacement, and style transfer, demonstrating its practical utility and\nversatility. Project page: https://ChangyueShi.github.io/REALM.",
        "url": "http://arxiv.org/abs/2510.16410v1",
        "published_date": "2025-10-18T08:53:08+00:00",
        "updated_date": "2025-10-18T08:53:08+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Changyue Shi",
            "Minghao Chen",
            "Yiping Mao",
            "Chuxiao Yang",
            "Xinyuan Hu",
            "Jiajun Ding",
            "Zhou Yu"
        ],
        "tldr": "REALM is an MLLM-agent framework that performs open-world reasoning-based 3D segmentation and editing on Gaussian Splatting without 3D-specific post-training, using a novel global-to-local spatial grounding strategy.",
        "tldr_zh": "REALM是一个MLLM-agent框架，它可以在高斯溅射上执行基于开放世界推理的3D分割和编辑，无需3D特定的后期训练，并采用了一种新颖的全局到局部空间定位策略。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "ObjectTransforms for Uncertainty Quantification and Reduction in Vision-Based Perception for Autonomous Vehicles",
        "summary": "Reliable perception is fundamental for safety critical decision making in\nautonomous driving. Yet, vision based object detector neural networks remain\nvulnerable to uncertainty arising from issues such as data bias and\ndistributional shifts. In this paper, we introduce ObjectTransforms, a\ntechnique for quantifying and reducing uncertainty in vision based object\ndetection through object specific transformations at both training and\ninference times. At training time, ObjectTransforms perform color space\nperturbations on individual objects, improving robustness to lighting and color\nvariations. ObjectTransforms also uses diffusion models to generate realistic,\ndiverse pedestrian instances. At inference time, object perturbations are\napplied to detected objects and the variance of detection scores are used to\nquantify predictive uncertainty in real time. This uncertainty signal is then\nused to filter out false positives and also recover false negatives, improving\nthe overall precision recall curve. Experiments with YOLOv8 on the NuImages 10K\ndataset demonstrate that our method yields notable accuracy improvements and\nuncertainty reduction across all object classes during training, while\npredicting desirably higher uncertainty values for false positives as compared\nto true positives during inference. Our results highlight the potential of\nObjectTransforms as a lightweight yet effective mechanism for reducing and\nquantifying uncertainty in vision-based perception during training and\ninference respectively.",
        "url": "http://arxiv.org/abs/2510.16118v1",
        "published_date": "2025-10-17T18:04:31+00:00",
        "updated_date": "2025-10-17T18:04:31+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Nishad Sahu",
            "Shounak Sural",
            "Aditya Satish Patil",
            "Ragunathan",
            "Rajkumar"
        ],
        "tldr": "The paper introduces ObjectTransforms, a method to quantify and reduce uncertainty in vision-based object detection for autonomous vehicles by applying object-specific transformations during training (using color space perturbations and diffusion models) and inference (using variance of detection scores). It shows improved accuracy and uncertainty reduction on the NuImages dataset using YOLOv8.",
        "tldr_zh": "该论文介绍了一种名为ObjectTransforms的方法，通过在训练时（使用颜色空间扰动和扩散模型）和推理时（使用检测分数的方差）应用特定于对象的变换，来量化和减少自动驾驶车辆中基于视觉的目标检测的不确定性。 在NuImages数据集上使用YOLOv8的实验表明，该方法提高了准确性并减少了不确定性。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]