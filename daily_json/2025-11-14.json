[
    {
        "title": "VISTA: A Vision and Intent-Aware Social Attention Framework for Multi-Agent Trajectory Prediction",
        "summary": "Multi-agent trajectory prediction is crucial for autonomous systems operating in dense, interactive environments. Existing methods often fail to jointly capture agents' long-term goals and their fine-grained social interactions, which leads to unrealistic multi-agent futures. We propose VISTA, a recursive goal-conditioned transformer for multi-agent trajectory forecasting. VISTA combines (i) a cross-attention fusion module that integrates long-horizon intent with past motion, (ii) a social-token attention mechanism for flexible interaction modeling across agents, and (iii) pairwise attention maps that make social influence patterns interpretable at inference time. Our model turns single-agent goal-conditioned prediction into a coherent multi-agent forecasting framework. Beyond standard displacement metrics, we evaluate trajectory collision rates as a measure of joint realism. On the high-density MADRAS benchmark and on SDD, VISTA achieves state-of-the-art accuracy and substantially fewer collisions. On MADRAS, it reduces the average collision rate of strong baselines from 2.14 to 0.03 percent, and on SDD it attains zero collisions while improving ADE, FDE, and minFDE. These results show that VISTA generates socially compliant, goal-aware, and interpretable trajectories, making it promising for safety-critical autonomous systems.",
        "url": "http://arxiv.org/abs/2511.10203v1",
        "published_date": "2025-11-13T11:17:01+00:00",
        "updated_date": "2025-11-14T01:38:40+00:00",
        "categories": [
            "cs.CV",
            "cs.AI",
            "cs.RO"
        ],
        "authors": [
            "Stephane Da Silva Martins",
            "Emanuel Aldea",
            "Sylvie Le Hégarat-Mascle"
        ],
        "tldr": "The paper introduces VISTA, a novel goal-conditioned transformer framework for multi-agent trajectory prediction that achieves state-of-the-art accuracy and significantly reduces collision rates compared to existing methods. It uses attention mechanisms to model social interactions and long-term goals.",
        "tldr_zh": "该论文介绍了一种名为VISTA的新型目标条件Transformer框架，用于多智能体轨迹预测，与现有方法相比，它实现了最先进的精度并显著降低了碰撞率。它使用注意力机制来建模社会交互和长期目标。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "HCC-3D: Hierarchical Compensatory Compression for 98% 3D Token Reduction in Vision-Language Models",
        "summary": "3D understanding has drawn significant attention recently, leveraging Vision-Language Models (VLMs) to enable multi-modal reasoning between point cloud and text data. Current 3D-VLMs directly embed the 3D point clouds into 3D tokens, following large 2D-VLMs with powerful reasoning capabilities. However, this framework has a great computational cost limiting its application, where we identify that the bottleneck lies in processing all 3D tokens in the Large Language Model (LLM) part. This raises the question: how can we reduce the computational overhead introduced by 3D tokens while preserving the integrity of their essential information? To address this question, we introduce Hierarchical Compensatory Compression (HCC-3D) to efficiently compress 3D tokens while maintaining critical detail retention. Specifically, we first propose a global structure compression (GSC), in which we design global queries to compress all 3D tokens into a few key tokens while keeping overall structural information. Then, to compensate for the information loss in GSC, we further propose an adaptive detail mining (ADM) module that selectively recompresses salient but under-attended features through complementary scoring. Extensive experiments demonstrate that HCC-3D not only achieves extreme compression ratios (approximately 98%) compared to previous 3D-VLMs, but also achieves new state-of-the-art performance, showing the great improvements on both efficiency and performance.",
        "url": "http://arxiv.org/abs/2511.09883v1",
        "published_date": "2025-11-13T02:28:10+00:00",
        "updated_date": "2025-11-14T01:15:39+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Liheng Zhang",
            "Jin Wang",
            "Hui Li",
            "Bingfeng Zhang",
            "Weifeng Liu"
        ],
        "tldr": "The paper introduces Hierarchical Compensatory Compression (HCC-3D) to significantly reduce the computational cost of 3D-VLMs by compressing 3D tokens while preserving essential information, achieving 98% token reduction and state-of-the-art performance.",
        "tldr_zh": "该论文介绍了分层补偿压缩 (HCC-3D)，通过压缩 3D tokens 同时保留基本信息，显著降低 3D-VLM 的计算成本，实现了 98% 的 token 减少和最先进的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "OmniVGGT: Omni-Modality Driven Visual Geometry Grounded",
        "summary": "General 3D foundation models have started to lead the trend of unifying diverse vision tasks, yet most assume RGB-only inputs and ignore readily available geometric cues (e.g., camera intrinsics, poses, and depth maps). To address this issue, we introduce OmniVGGT, a novel framework that can effectively benefit from an arbitrary number of auxiliary geometric modalities during both training and inference. In our framework, a GeoAdapter is proposed to encode depth and camera intrinsics/extrinsics into a spatial foundation model. It employs zero-initialized convolutions to progressively inject geometric information without disrupting the foundation model's representation space. This design ensures stable optimization with negligible overhead, maintaining inference speed comparable to VGGT even with multiple additional inputs. Additionally, a stochastic multimodal fusion regimen is proposed, which randomly samples modality subsets per instance during training. This enables an arbitrary number of modality inputs during testing and promotes learning robust spatial representations instead of overfitting to auxiliary cues. Comprehensive experiments on monocular/multi-view depth estimation, multi-view stereo, and camera pose estimation demonstrate that OmniVGGT outperforms prior methods with auxiliary inputs and achieves state-of-the-art results even with RGB-only input. To further highlight its practical utility, we integrated OmniVGGT into vision-language-action (VLA) models. The enhanced VLA model by OmniVGGT not only outperforms the vanilla point-cloud-based baseline on mainstream benchmarks, but also effectively leverages accessible auxiliary inputs to achieve consistent gains on robotic tasks.",
        "url": "http://arxiv.org/abs/2511.10560v1",
        "published_date": "2025-11-13T17:59:01+00:00",
        "updated_date": "2025-11-14T01:57:42+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Haosong Peng",
            "Hao Li",
            "Yalun Dai",
            "Yushi Lan",
            "Yihang Luo",
            "Tianyu Qi",
            "Zhengshen Zhang",
            "Yufeng Zhan",
            "Junfei Zhang",
            "Wenchao Xu",
            "Ziwei Liu"
        ],
        "tldr": "OmniVGGT is a novel framework that leverages geometric cues (depth, intrinsics/extrinsics) to enhance 3D foundation models for various vision tasks, achieving SOTA results and improving VLA model performance on robotic tasks.",
        "tldr_zh": "OmniVGGT是一个新框架，利用几何线索（深度、内外参数）来增强3D基础模型，从而用于各种视觉任务，实现了SOTA结果，并提高了VLA模型在机器人任务中的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "LiNeXt: Revisiting LiDAR Completion with Efficient Non-Diffusion Architectures",
        "summary": "3D LiDAR scene completion from point clouds is a fundamental component of perception systems in autonomous vehicles. Previous methods have predominantly employed diffusion models for high-fidelity reconstruction. However, their multi-step iterative sampling incurs significant computational overhead, limiting its real-time applicability. To address this, we propose LiNeXt-a lightweight, non-diffusion network optimized for rapid and accurate point cloud completion. Specifically, LiNeXt first applies the Noise-to-Coarse (N2C) Module to denoise the input noisy point cloud in a single pass, thereby obviating the multi-step iterative sampling of diffusion-based methods. The Refine Module then takes the coarse point cloud and its intermediate features from the N2C Module to perform more precise refinement, further enhancing structural completeness. Furthermore, we observe that LiDAR point clouds exhibit a distance-dependent spatial distribution, being densely sampled at proximal ranges and sparsely sampled at distal ranges. Accordingly, we propose the Distance-aware Selected Repeat strategy to generate a more uniformly distributed noisy point cloud. On the SemanticKITTI dataset, LiNeXt achieves a 199.8x speedup in inference, reduces Chamfer Distance by 50.7%, and uses only 6.1% of the parameters compared with LiDiff. These results demonstrate the superior efficiency and effectiveness of LiNeXt for real-time scene completion.",
        "url": "http://arxiv.org/abs/2511.10209v1",
        "published_date": "2025-11-13T11:28:18+00:00",
        "updated_date": "2025-11-14T01:39:07+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenzhe He",
            "Xiaojun Chen",
            "Ruiqi Wang",
            "Ruihui Li",
            "Huilong Pi",
            "Jiapeng Zhang",
            "Zhuo Tang",
            "Kenli Li"
        ],
        "tldr": "LiNeXt presents a novel, efficient non-diffusion network for LiDAR scene completion that achieves significant speedups and accuracy improvements over diffusion-based methods, making it suitable for real-time autonomous driving applications. This is achieved using a Noise-to-Coarse module and a Refine module, combined with a distance-aware sampling strategy.",
        "tldr_zh": "LiNeXt 提出了一种新颖、高效的非扩散网络，用于 LiDAR 场景补全，与基于扩散的方法相比，实现了显著的速度提升和精度改进，适用于实时自动驾驶应用。该方法使用噪声到粗糙模块和细化模块，并结合距离感知的采样策略来实现。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation",
        "summary": "Vision-Language-Action (VLA) models have advanced in robotic manipulation, yet practical deployment remains hindered by two key limitations: 1) perceptual redundancy, where irrelevant visual inputs are processed inefficiently, and 2) superficial instruction-vision alignment, which hampers semantic grounding of actions. In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation. Specifically: 1) To sparsify redundant perception while preserving semantic alignment, Semantic-guided Dual Visual Pruner (SD-Pruner) performs: Instruction-driven Pruner (ID-Pruner) extracts global action cues and local semantic anchors in SigLIP; Spatial-aggregation Pruner (SA-Pruner) compacts geometry-rich features into task-adaptive tokens in DINOv2. 2) To exploit sparsified features and integrate semantics with spatial geometry, Semantic-complementary Hierarchical Fuser (SH-Fuser) fuses dense patches and sparse tokens across SigLIP and DINOv2 for coherent representation. 3) To enhance the transformation from perception to action, Semantic-conditioned Action Coupler (SA-Coupler) replaces the conventional observation-to-DoF approach, yielding more efficient and interpretable behavior modeling for manipulation tasks. Extensive experiments on simulation and real-world tasks show that SemanticVLA sets a new SOTA in both performance and efficiency. SemanticVLA surpasses OpenVLA on LIBERO benchmark by 21.1% in success rate, while reducing training cost and inference latency by 3.0-fold and 2.7-fold.SemanticVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/SemanticVLA",
        "url": "http://arxiv.org/abs/2511.10518v1",
        "published_date": "2025-11-13T17:24:37+00:00",
        "updated_date": "2025-11-14T01:55:58+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Wei Li",
            "Renshan Zhang",
            "Rui Shao",
            "Zhijian Fang",
            "Kaiwen Zhou",
            "Zhuotao Tian",
            "Liqiang Nie"
        ],
        "tldr": "SemanticVLA introduces a novel Vision-Language-Action framework that uses semantic-aligned sparsification and enhancement techniques to improve the efficiency and performance of robotic manipulation, achieving state-of-the-art results with reduced computational costs.",
        "tldr_zh": "SemanticVLA 提出了一种新的视觉-语言-动作框架，该框架采用语义对齐的稀疏化和增强技术来提高机器人操作的效率和性能，实现了最先进的结果，并降低了计算成本。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation",
        "summary": "Embodied navigation is a fundamental capability for robotic agents operating. Real-world deployment requires open vocabulary generalization and low training overhead, motivating zero-shot methods rather than task-specific RL training. However, existing zero-shot methods that build explicit 3D scene graphs often compress rich visual observations into text-only relations, leading to high construction cost, irreversible loss of visual evidence, and constrained vocabularies. To address these limitations, we introduce the Multi-modal 3D Scene Graph (M3DSG), which preserves visual cues by replacing textual relational edges with dynamically assigned images. Built on M3DSG, we propose MSGNav, a zero-shot navigation system that includes a Key Subgraph Selection module for efficient reasoning, an Adaptive Vocabulary Update module for open vocabulary support, and a Closed-Loop Reasoning module for accurate exploration reasoning. Additionally, we further identify the last-mile problem in zero-shot navigation - determining the feasible target location with a suitable final viewpoint, and propose a Visibility-based Viewpoint Decision module to explicitly resolve it. Comprehensive experimental results demonstrate that MSGNav achieves state-of-the-art performance on GOAT-Bench and HM3D-OVON datasets. The open-source code will be publicly available.",
        "url": "http://arxiv.org/abs/2511.10376v1",
        "published_date": "2025-11-13T14:51:21+00:00",
        "updated_date": "2025-11-14T01:47:53+00:00",
        "categories": [
            "cs.CV",
            "cs.RO"
        ],
        "authors": [
            "Xun Huang",
            "Shijia Zhao",
            "Yunxiang Wang",
            "Xin Lu",
            "Wanfa Zhang",
            "Rongsheng Qu",
            "Weixin Li",
            "Yunhong Wang",
            "Chenglu Wen"
        ],
        "tldr": "The paper introduces MSGNav, a zero-shot navigation system using a Multi-modal 3D Scene Graph (M3DSG) that incorporates visual cues for improved performance and open vocabulary generalization, achieving state-of-the-art results on standard benchmarks.",
        "tldr_zh": "该论文介绍了MSGNav，一个零样本导航系统，它使用多模态3D场景图（M3DSG），该图结合了视觉线索，以提高性能和开放词汇泛化能力，并在标准基准上实现了最先进的结果。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "HeatV2X: Scalable Heterogeneous Collaborative Perception via Efficient Alignment and Interaction",
        "summary": "Vehicle-to-Everything (V2X) collaborative perception extends sensing beyond single vehicle limits through transmission. However, as more agents participate, existing frameworks face two key challenges: (1) the participating agents are inherently multi-modal and heterogeneous, and (2) the collaborative framework must be scalable to accommodate new agents. The former requires effective cross-agent feature alignment to mitigate heterogeneity loss, while the latter renders full-parameter training impractical, highlighting the importance of scalable adaptation. To address these issues, we propose Heterogeneous Adaptation (HeatV2X), a scalable collaborative framework. We first train a high-performance agent based on heterogeneous graph attention as the foundation for collaborative learning. Then, we design Local Heterogeneous Fine-Tuning and Global Collaborative Fine-Tuning to achieve effective alignment and interaction among heterogeneous agents. The former efficiently extracts modality-specific differences using Hetero-Aware Adapters, while the latter employs the Multi-Cognitive Adapter to enhance cross-agent collaboration and fully exploit the fusion potential. These designs enable substantial performance improvement of the collaborative framework with minimal training cost. We evaluate our approach on the OPV2V-H and DAIR-V2X datasets. Experimental results demonstrate that our method achieves superior perception performance with significantly reduced training overhead, outperforming existing state-of-the-art approaches. Our implementation will be released soon.",
        "url": "http://arxiv.org/abs/2511.10211v1",
        "published_date": "2025-11-13T11:33:22+00:00",
        "updated_date": "2025-11-14T01:39:33+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yueran Zhao",
            "Zhang Zhang",
            "Chao Sun",
            "Tianze Wang",
            "Chao Yue",
            "Nuoran Li"
        ],
        "tldr": "The paper introduces HeatV2X, a scalable collaborative perception framework for autonomous driving that addresses heterogeneity among agents by using Hetero-Aware Adapters and a Multi-Cognitive Adapter for efficient alignment and interaction, demonstrating superior performance with reduced training overhead.",
        "tldr_zh": "本文介绍了一种名为HeatV2X的可扩展协作感知框架，用于自动驾驶。该框架通过使用异构感知适配器和多认知适配器来解决agent之间的异构性，从而实现高效的对齐和交互，并在降低训练开销的同时展示出卓越的性能。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "IFG: Internet-Scale Guidance for Functional Grasping Generation",
        "summary": "Large Vision Models trained on internet-scale data have demonstrated strong capabilities in segmenting and semantically understanding object parts, even in cluttered, crowded scenes. However, while these models can direct a robot toward the general region of an object, they lack the geometric understanding required to precisely control dexterous robotic hands for 3D grasping. To overcome this, our key insight is to leverage simulation with a force-closure grasping generation pipeline that understands local geometries of the hand and object in the scene. Because this pipeline is slow and requires ground-truth observations, the resulting data is distilled into a diffusion model that operates in real-time on camera point clouds. By combining the global semantic understanding of internet-scale models with the geometric precision of a simulation-based locally-aware force-closure, \\our achieves high-performance semantic grasping without any manually collected training data. For visualizations of this please visit our website at https://ifgrasping.github.io/",
        "url": "http://arxiv.org/abs/2511.09558v1",
        "published_date": "2025-11-12T18:59:49+00:00",
        "updated_date": "2025-11-13T02:02:10+00:00",
        "categories": [
            "cs.RO",
            "cs.AI",
            "cs.CV",
            "cs.GR",
            "cs.LG"
        ],
        "authors": [
            "Ray Muxin Liu",
            "Mingxuan Li",
            "Kenneth Shaw",
            "Deepak Pathak"
        ],
        "tldr": "This paper proposes a method combining internet-scale vision models and simulation-based force-closure grasping to achieve high-performance semantic grasping in robotics, without manual training data.",
        "tldr_zh": "该论文提出了一种结合互联网规模视觉模型和基于仿真的力闭合抓取方法，以实现高性能的机器人语义抓取，无需手动训练数据。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Trapped by Their Own Light: Deployable and Stealth Retroreflective Patch Attacks on Traffic Sign Recognition Systems",
        "summary": "Traffic sign recognition plays a critical role in ensuring safe and efficient transportation of autonomous vehicles but remain vulnerable to adversarial attacks using stickers or laser projections. While existing attack vectors demonstrate security concerns, they suffer from visual detectability or implementation constraints, suggesting unexplored vulnerability surfaces in TSR systems. We introduce the Adversarial Retroreflective Patch (ARP), a novel attack vector that combines the high deployability of patch attacks with the stealthiness of laser projections by utilizing retroreflective materials activated only under victim headlight illumination. We develop a retroreflection simulation method and employ black-box optimization to maximize attack effectiveness. ARP achieves $\\geq$93.4\\% success rate in dynamic scenarios at 35 meters and $\\geq$60\\% success rate against commercial TSR systems in real-world conditions. Our user study demonstrates that ARP attacks maintain near-identical stealthiness to benign signs while achieving $\\geq$1.9\\% higher stealthiness scores than previous patch attacks. We propose the DPR Shield defense, employing strategically placed polarized filters, which achieves $\\geq$75\\% defense success rates for stop signs and speed limit signs against micro-prism patches.",
        "url": "http://arxiv.org/abs/2511.10050v1",
        "published_date": "2025-11-13T07:48:30+00:00",
        "updated_date": "2025-11-14T01:28:46+00:00",
        "categories": [
            "cs.CR",
            "cs.CV"
        ],
        "authors": [
            "Go Tsuruoka",
            "Takami Sato",
            "Qi Alfred Chen",
            "Kazuki Nomoto",
            "Ryunosuke Kobayashi",
            "Yuna Tanaka",
            "Tatsuya Mori"
        ],
        "tldr": "This paper introduces a novel adversarial attack on traffic sign recognition systems using retroreflective patches, which are stealthy under normal conditions but become visible under headlight illumination, achieving high success rates in both simulated and real-world environments. A defense mechanism using polarized filters is also proposed.",
        "tldr_zh": "本文提出了一种新的针对交通标志识别系统的对抗性攻击，该攻击使用反光贴片，在正常条件下具有隐蔽性，但在前照灯照射下变得可见，在模拟和真实环境中都取得了很高的成功率。同时，也提出了一种使用偏振滤光片的防御机制。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models",
        "summary": "Effective human-agent collaboration in physical environments requires understanding not only what to act upon, but also where the actionable elements are and how to interact with them. Existing approaches often operate at the object level or disjointedly handle fine-grained affordance reasoning, lacking coherent, instruction-driven grounding and reasoning. In this work, we introduce a new task: Fine-grained 3D Embodied Reasoning, which requires an agent to predict, for each referenced affordance element in a 3D scene, a structured triplet comprising its spatial location, motion type, and motion axis, based on a task instruction. To solve this task, we propose AffordBot, a novel framework that integrates Multimodal Large Language Models (MLLMs) with a tailored chain-of-thought (CoT) reasoning paradigm. To bridge the gap between 3D input and 2D-compatible MLLMs, we render surround-view images of the scene and project 3D element candidates into these views, forming a rich visual representation aligned with the scene geometry. Our CoT pipeline begins with an active perception stage, prompting the MLLM to select the most informative viewpoint based on the instruction, before proceeding with step-by-step reasoning to localize affordance elements and infer plausible interaction motions. Evaluated on the SceneFun3D dataset, AffordBot achieves state-of-the-art performance, demonstrating strong generalization and physically grounded reasoning with only 3D point cloud input and MLLMs.",
        "url": "http://arxiv.org/abs/2511.10017v1",
        "published_date": "2025-11-13T06:43:00+00:00",
        "updated_date": "2025-11-14T01:26:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xinyi Wang",
            "Xun Yang",
            "Yanlong Xu",
            "Yuchen Wu",
            "Zhen Li",
            "Na Zhao"
        ],
        "tldr": "The paper introduces AffordBot, a framework using MLLMs and a chain-of-thought approach for fine-grained 3D embodied reasoning, enabling agents to predict the spatial location, motion type, and motion axis of actionable elements in a 3D scene based on instructions.",
        "tldr_zh": "该论文介绍了AffordBot，一个利用多模态大型语言模型（MLLM）和链式思考方法（CoT）的框架，用于细粒度的3D具身推理，使智能体能够根据指令预测3D场景中可操作元素的空间位置、运动类型和运动轴。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "MOBA: A Material-Oriented Backdoor Attack against LiDAR-based 3D Object Detection Systems",
        "summary": "LiDAR-based 3D object detection is widely used in safety-critical systems. However, these systems remain vulnerable to backdoor attacks that embed hidden malicious behaviors during training. A key limitation of existing backdoor attacks is their lack of physical realizability, primarily due to the digital-to-physical domain gap. Digital triggers often fail in real-world settings because they overlook material-dependent LiDAR reflection properties. On the other hand, physically constructed triggers are often unoptimized, leading to low effectiveness or easy detectability.This paper introduces Material-Oriented Backdoor Attack (MOBA), a novel framework that bridges the digital-physical gap by explicitly modeling the material properties of real-world triggers. MOBA tackles two key challenges in physical backdoor design: 1) robustness of the trigger material under diverse environmental conditions, 2) alignment between the physical trigger's behavior and its digital simulation. First, we propose a systematic approach to selecting robust trigger materials, identifying titanium dioxide (TiO_2) for its high diffuse reflectivity and environmental resilience. Second, to ensure the digital trigger accurately mimics the physical behavior of the material-based trigger, we develop a novel simulation pipeline that features: (1) an angle-independent approximation of the Oren-Nayar BRDF model to generate realistic LiDAR intensities, and (2) a distance-aware scaling mechanism to maintain spatial consistency across varying depths. We conduct extensive experiments on state-of-the-art LiDAR-based and Camera-LiDAR fusion models, showing that MOBA achieves a 93.50% attack success rate, outperforming prior methods by over 41%. Our work reveals a new class of physically realizable threats and underscores the urgent need for defenses that account for material-level properties in real-world environments.",
        "url": "http://arxiv.org/abs/2511.09999v1",
        "published_date": "2025-11-13T06:10:21+00:00",
        "updated_date": "2025-11-14T01:25:02+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Saket S. Chaturvedi",
            "Gaurav Bagwe",
            "Lan Zhang",
            "Pan He",
            "Xiaoyong Yuan"
        ],
        "tldr": "This paper introduces MOBA, a novel physically realizable backdoor attack against LiDAR-based 3D object detection by explicitly modeling material properties, achieving a high attack success rate.",
        "tldr_zh": "本文介绍了一种名为MOBA的新型物理可实现的后门攻击，该攻击通过显式建模材料属性来对抗基于激光雷达的 3D 目标检测，并实现了很高的攻击成功率。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "IPCD: Intrinsic Point-Cloud Decomposition",
        "summary": "Point clouds are widely used in various fields, including augmented reality (AR) and robotics, where relighting and texture editing are crucial for realistic visualization. Achieving these tasks requires accurately separating albedo from shade. However, performing this separation on point clouds presents two key challenges: (1) the non-grid structure of point clouds makes conventional image-based decomposition models ineffective, and (2) point-cloud models designed for other tasks do not explicitly consider global-light direction, resulting in inaccurate shade. In this paper, we introduce \\textbf{Intrinsic Point-Cloud Decomposition (IPCD)}, which extends image decomposition to the direct decomposition of colored point clouds into albedo and shade. To overcome challenge (1), we propose \\textbf{IPCD-Net} that extends image-based model with point-wise feature aggregation for non-grid data processing. For challenge (2), we introduce \\textbf{Projection-based Luminance Distribution (PLD)} with a hierarchical feature refinement, capturing global-light ques via multi-view projection. For comprehensive evaluation, we create a synthetic outdoor-scene dataset. Experimental results demonstrate that IPCD-Net reduces cast shadows in albedo and enhances color accuracy in shade. Furthermore, we showcase its applications in texture editing, relighting, and point-cloud registration under varying illumination. Finally, we verify the real-world applicability of IPCD-Net.",
        "url": "http://arxiv.org/abs/2511.09866v1",
        "published_date": "2025-11-13T01:57:14+00:00",
        "updated_date": "2025-11-14T01:14:14+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Shogo Sato",
            "Takuhiro Kaneko",
            "Shoichiro Takeda",
            "Tomoyasu Shimada",
            "Kazuhiko Murasaki",
            "Taiga Yoshida",
            "Ryuichi Tanida",
            "Akisato Kimura"
        ],
        "tldr": "The paper introduces IPCD, a novel method for decomposing colored point clouds into albedo and shade, addressing challenges related to non-grid structure and global-light direction, with applications in AR, robotics, texture editing, relighting and point-cloud registration.",
        "tldr_zh": "该论文介绍了IPCD，一种将彩色点云分解为反照率和阴影的新方法，解决了与非网格结构和全局光方向相关的挑战，并应用于增强现实、机器人、纹理编辑、重新照明和点云配准。",
        "relevance_score": 8,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "STORM: Segment, Track, and Object Re-Localization from a Single 3D Model",
        "summary": "Accurate 6D pose estimation and tracking are fundamental capabilities for physical AI systems such as robots. However, existing approaches typically rely on a manually annotated segmentation mask of the target in the first frame, which is labor-intensive and leads to reduced performance when faced with occlusions or rapid movement. To address these limi- tations, we propose STORM (Segment, Track, and Object Re-localization from a single 3D Model), an open-source robust real-time 6D pose estimation system that requires no manual annotation. STORM employs a novel three-stage pipeline combining vision-language understanding with self-supervised feature matching: contextual object descriptions guide localization, self-cross-attention mechanisms identify candidate regions, and a segmentation model produces precise masks for accurate pose estimation. Another key innovation is our automatic re-registration mechanism that detects tracking failures through feature similarity monitoring and recovers from severe occlusions or rapid motion. STORM achieves state-of-the-art accuracy on challenging industrial datasets featuring multi-object occlusions, high-speed motion, and varying illumination, while operating at real-time speeds without additional training. This annotation-free approach significantly reduces deployment overhead, providing a practical solution for modern applications, such as flexible manufacturing and intelligent quality control.",
        "url": "http://arxiv.org/abs/2511.09771v1",
        "published_date": "2025-11-12T22:06:51+00:00",
        "updated_date": "2025-11-14T01:08:49+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Yu Deng",
            "Teng Cao",
            "Hikaru Shindo",
            "Jiahong Xue",
            "Quentin Delfosse",
            "Kristian Kersting"
        ],
        "tldr": "The paper presents STORM, a novel real-time 6D pose estimation system that requires no manual annotation by combining vision-language understanding and self-supervised feature matching, achieving state-of-the-art accuracy on challenging industrial datasets.",
        "tldr_zh": "该论文提出了STORM，一种新颖的实时6D姿态估计系统，该系统结合了视觉语言理解和自监督特征匹配，无需手动注释，并在具有挑战性的工业数据集上实现了最先进的精度。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection",
        "summary": "As a critical task in autonomous driving perception systems, 3D object detection is used to identify and track key objects, such as vehicles and pedestrians. However, detecting distant, small, or occluded objects (hard instances) remains a challenge, which directly compromises the safety of autonomous driving systems. We observe that existing multi-modal 3D object detection methods often follow a single-guided paradigm, failing to account for the differences in information density of hard instances between modalities. In this work, we propose DGFusion, based on the Dual-guided paradigm, which fully inherits the advantages of the Point-guide-Image paradigm and integrates the Image-guide-Point paradigm to address the limitations of the single paradigms. The core of DGFusion, the Difficulty-aware Instance Pair Matcher (DIPM), performs instance-level feature matching based on difficulty to generate easy and hard instance pairs, while the Dual-guided Modules exploit the advantages of both pair types to enable effective multi-modal feature fusion. Experimental results demonstrate that our DGFusion outperforms the baseline methods, with respective improvements of +1.0\\% mAP, +0.8\\% NDS, and +1.3\\% average recall on nuScenes. Extensive experiments demonstrate consistent robustness gains for hard instance detection across ego-distance, size, visibility, and small-scale training scenarios.",
        "url": "http://arxiv.org/abs/2511.10035v1",
        "published_date": "2025-11-13T07:18:58+00:00",
        "updated_date": "2025-11-14T01:27:26+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Feiyang Jia",
            "Caiyan Jia",
            "Ailin Liu",
            "Shaoqing Xu",
            "Qiming Xia",
            "Lin Liu",
            "Lei Yang",
            "Yan Gong",
            "Ziying Song"
        ],
        "tldr": "This paper introduces DGFusion, a dual-guided multi-modal 3D object detection method that addresses the challenge of detecting hard instances by utilizing a difficulty-aware instance pair matcher and dual-guided modules for effective feature fusion, achieving improved performance on the nuScenes dataset.",
        "tldr_zh": "该论文介绍了DGFusion，一种双重引导的多模态3D物体检测方法，旨在通过利用难度感知实例对匹配器和双重引导模块进行有效的特征融合，解决检测困难实例的挑战，并在nuScenes数据集上取得了性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "Robust Object Detection with Pseudo Labels from VLMs using Per-Object Co-teaching",
        "summary": "Foundation models, especially vision-language models (VLMs), offer compelling zero-shot object detection for applications like autonomous driving, a domain where manual labelling is prohibitively expensive. However, their detection latency and tendency to hallucinate predictions render them unsuitable for direct deployment. This work introduces a novel pipeline that addresses this challenge by leveraging VLMs to automatically generate pseudo-labels for training efficient, real-time object detectors. Our key innovation is a per-object co-teaching-based training strategy that mitigates the inherent noise in VLM-generated labels. The proposed per-object coteaching approach filters noisy bounding boxes from training instead of filtering the entire image. Specifically, two YOLO models learn collaboratively, filtering out unreliable boxes from each mini-batch based on their peers' per-object loss values. Overall, our pipeline provides an efficient, robust, and scalable approach to train high-performance object detectors for autonomous driving, significantly reducing reliance on costly human annotation. Experimental results on the KITTI dataset demonstrate that our method outperforms a baseline YOLOv5m model, achieving a significant mAP@0.5 boost ($31.12\\%$ to $46.61\\%$) while maintaining real-time detection latency. Furthermore, we show that supplementing our pseudo-labelled data with a small fraction of ground truth labels ($10\\%$) leads to further performance gains, reaching $57.97\\%$ mAP@0.5 on the KITTI dataset. We observe similar performance improvements for the ACDC and BDD100k datasets.",
        "url": "http://arxiv.org/abs/2511.09955v1",
        "published_date": "2025-11-13T04:37:35+00:00",
        "updated_date": "2025-11-14T01:21:28+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Uday Bhaskar",
            "Rishabh Bhattacharya",
            "Avinash Patel",
            "Sarthak Khoche",
            "Praveen Anil Kulkarni",
            "Naresh Manwani"
        ],
        "tldr": "This paper introduces a novel per-object co-teaching approach using VLM-generated pseudo-labels to train robust and efficient object detectors for autonomous driving, achieving significant mAP improvements on KITTI, ACDC, and BDD100k datasets.",
        "tldr_zh": "本文提出了一种新的基于对象协同教学的方法，利用VLM生成的伪标签来训练用于自动驾驶的鲁棒高效的目标检测器，并在KITTI、ACDC和BDD100k数据集上取得了显著的mAP提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "From Street to Orbit: Training-Free Cross-View Retrieval via Location Semantics and LLM Guidance",
        "summary": "Cross-view image retrieval, particularly street-to-satellite matching, is a critical task for applications such as autonomous navigation, urban planning, and localization in GPS-denied environments. However, existing approaches often require supervised training on curated datasets and rely on panoramic or UAV-based images, which limits real-world deployment. In this paper, we present a simple yet effective cross-view image retrieval framework that leverages a pretrained vision encoder and a large language model (LLM), requiring no additional training. Given a monocular street-view image, our method extracts geographic cues through web-based image search and LLM-based location inference, generates a satellite query via geocoding API, and retrieves matching tiles using a pretrained vision encoder (e.g., DINOv2) with PCA-based whitening feature refinement. Despite using no ground-truth supervision or finetuning, our proposed method outperforms prior learning-based approaches on the benchmark dataset under zero-shot settings. Moreover, our pipeline enables automatic construction of semantically aligned street-to-satellite datasets, which is offering a scalable and cost-efficient alternative to manual annotation. All source codes will be made publicly available at https://jeonghomin.github.io/street2orbit.github.io/.",
        "url": "http://arxiv.org/abs/2511.09820v1",
        "published_date": "2025-11-12T23:51:46+00:00",
        "updated_date": "2025-11-14T01:11:19+00:00",
        "categories": [
            "cs.CV",
            "cs.AI"
        ],
        "authors": [
            "Jeongho Min",
            "Dongyoung Kim",
            "Jaehyup Lee"
        ],
        "tldr": "This paper introduces a training-free cross-view image retrieval method for street-to-satellite matching, leveraging LLMs and pretrained vision encoders for zero-shot performance exceeding existing supervised methods and automatic dataset creation.",
        "tldr_zh": "本文介绍了一种无需训练的街景到卫星图像跨视角检索方法，利用大型语言模型和预训练视觉编码器实现零样本性能，超越了现有的监督方法，并能自动创建数据集。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "RWKV-PCSSC: Exploring RWKV Model for Point Cloud Semantic Scene Completion",
        "summary": "Semantic Scene Completion (SSC) aims to generate a complete semantic scene from an incomplete input. Existing approaches often employ dense network architectures with a high parameter count, leading to increased model complexity and resource demands. To address these limitations, we propose RWKV-PCSSC, a lightweight point cloud semantic scene completion network inspired by the Receptance Weighted Key Value (RWKV) mechanism. Specifically, we introduce a RWKV Seed Generator (RWKV-SG) module that can aggregate features from a partial point cloud to produce a coarse point cloud with coarse features. Subsequently, the point-wise feature of the point cloud is progressively restored through multiple stages of the RWKV Point Deconvolution (RWKV-PD) modules. By leveraging a compact and efficient design, our method achieves a lightweight model representation. Experimental results demonstrate that RWKV-PCSSC reduces the parameter count by 4.18$\\times$ and improves memory efficiency by 1.37$\\times$ compared to state-of-the-art methods PointSSC. Furthermore, our network achieves state-of-the-art performance on established indoor (SSC-PC, NYUCAD-PC) and outdoor (PointSSC) scene dataset, as well as on our proposed datasets (NYUCAD-PC-V2, 3D-FRONT-PC).",
        "url": "http://arxiv.org/abs/2511.09878v1",
        "published_date": "2025-11-13T02:22:40+00:00",
        "updated_date": "2025-11-14T01:15:11+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Wenzhe He",
            "Xiaojun Chen",
            "Wentang Chen",
            "Hongyu Wang",
            "Ying Liu",
            "Ruihui Li"
        ],
        "tldr": "This paper introduces RWKV-PCSSC, a lightweight point cloud semantic scene completion network using RWKV mechanisms, achieving state-of-the-art performance with fewer parameters and improved memory efficiency.",
        "tldr_zh": "本文介绍了RWKV-PCSSC，一个轻量级的点云语义场景补全网络，它使用RWKV机制，以更少的参数和更高的内存效率实现了最先进的性能。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    }
]