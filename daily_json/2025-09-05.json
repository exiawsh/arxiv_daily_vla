[
    {
        "title": "QuantV2X: A Fully Quantized Multi-Agent System for Cooperative Perception",
        "summary": "Cooperative perception through Vehicle-to-Everything (V2X) communication\noffers significant potential for enhancing vehicle perception by mitigating\nocclusions and expanding the field of view. However, past research has\npredominantly focused on improving accuracy metrics without addressing the\ncrucial system-level considerations of efficiency, latency, and real-world\ndeployability. Noticeably, most existing systems rely on full-precision models,\nwhich incur high computational and transmission costs, making them impractical\nfor real-time operation in resource-constrained environments. In this paper, we\nintroduce \\textbf{QuantV2X}, the first fully quantized multi-agent system\ndesigned specifically for efficient and scalable deployment of multi-modal,\nmulti-agent V2X cooperative perception. QuantV2X introduces a unified\nend-to-end quantization strategy across both neural network models and\ntransmitted message representations that simultaneously reduces computational\nload and transmission bandwidth. Remarkably, despite operating under low-bit\nconstraints, QuantV2X achieves accuracy comparable to full-precision systems.\nMore importantly, when evaluated under deployment-oriented metrics, QuantV2X\nreduces system-level latency by 3.2$\\times$ and achieves a +9.5 improvement in\nmAP30 over full-precision baselines. Furthermore, QuantV2X scales more\neffectively, enabling larger and more capable models to fit within strict\nmemory budgets. These results highlight the viability of a fully quantized\nmulti-agent intermediate fusion system for real-world deployment. The system\nwill be publicly released to promote research in this field:\nhttps://github.com/ucla-mobility/QuantV2X.",
        "url": "http://arxiv.org/abs/2509.03704v1",
        "published_date": "2025-09-03T20:39:03+00:00",
        "updated_date": "2025-09-03T20:39:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Seth Z. Zhao",
            "Huizhi Zhang",
            "Zhaowei Li",
            "Juntong Peng",
            "Anthony Chui",
            "Zewei Zhou",
            "Zonglin Meng",
            "Hao Xiang",
            "Zhiyu Huang",
            "Fujia Wang",
            "Ran Tian",
            "Chenfeng Xu",
            "Bolei Zhou",
            "Jiaqi Ma"
        ],
        "tldr": "The paper introduces QuantV2X, a fully quantized multi-agent system for cooperative perception in autonomous driving, demonstrating significant improvements in efficiency, latency, and scalability compared to full-precision systems. It achieves comparable accuracy with low-bit constraints and offers a publicly available system.",
        "tldr_zh": "该论文介绍了 QuantV2X，一个用于自动驾驶中协同感知的全量化多智能体系统，与全精度系统相比，在效率、延迟和可扩展性方面表现出显著的改进。它在低比特约束下实现了可比的精度，并提供了一个公开可用的系统。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 9,
        "overall_priority_score": 9
    },
    {
        "title": "Reg3D: Reconstructive Geometry Instruction Tuning for 3D Scene Understanding",
        "summary": "The rapid development of Large Multimodal Models (LMMs) has led to remarkable\nprogress in 2D visual understanding; however, extending these capabilities to\n3D scene understanding remains a significant challenge. Existing approaches\npredominantly rely on text-only supervision, which fails to provide the\ngeometric constraints required for learning robust 3D spatial representations.\nIn this paper, we introduce Reg3D, a novel Reconstructive Geometry Instruction\nTuning framework that addresses this limitation by incorporating geometry-aware\nsupervision directly into the training process. Our key insight is that\neffective 3D understanding necessitates reconstructing underlying geometric\nstructures rather than merely describing them. Unlike existing methods that\ninject 3D information solely at the input level, Reg3D adopts a\ndual-supervision paradigm that leverages 3D geometric information both as input\nand as explicit learning targets. Specifically, we design complementary\nobject-level and frame-level reconstruction tasks within a dual-encoder\narchitecture, enforcing geometric consistency to encourage the development of\nspatial reasoning capabilities. Extensive experiments on ScanQA, Scan2Cap,\nScanRefer, and SQA3D demonstrate that Reg3D delivers substantial performance\nimprovements, establishing a new training paradigm for spatially aware\nmultimodal models.",
        "url": "http://arxiv.org/abs/2509.03635v1",
        "published_date": "2025-09-03T18:36:44+00:00",
        "updated_date": "2025-09-03T18:36:44+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Hongpei Zheng",
            "Lintao Xiang",
            "Qijun Yang",
            "Qian Lin",
            "Hujun Yin"
        ],
        "tldr": "The paper introduces Reg3D, a novel framework for 3D scene understanding that incorporates geometry-aware supervision directly into the training process of large multimodal models using reconstructive geometry instruction tuning, achieving significant performance improvements on several benchmarks.",
        "tldr_zh": "本文介绍了一种名为 Reg3D 的新型框架，用于 3D 场景理解。该框架通过重建几何指令调整，将几何感知监督直接融入大型多模态模型的训练过程中，并在多个基准测试中取得了显著的性能提升。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 9
    },
    {
        "title": "One Flight Over the Gap: A Survey from Perspective to Panoramic Vision",
        "summary": "Driven by the demand for spatial intelligence and holistic scene perception,\nomnidirectional images (ODIs), which provide a complete 360\\textdegree{} field\nof view, are receiving growing attention across diverse applications such as\nvirtual reality, autonomous driving, and embodied robotics. Despite their\nunique characteristics, ODIs exhibit remarkable differences from perspective\nimages in geometric projection, spatial distribution, and boundary continuity,\nmaking it challenging for direct domain adaption from perspective methods. This\nsurvey reviews recent panoramic vision techniques with a particular emphasis on\nthe perspective-to-panorama adaptation. We first revisit the panoramic imaging\npipeline and projection methods to build the prior knowledge required for\nanalyzing the structural disparities. Then, we summarize three challenges of\ndomain adaptation: severe geometric distortions near the poles, non-uniform\nsampling in Equirectangular Projection (ERP), and periodic boundary continuity.\nBuilding on this, we cover 20+ representative tasks drawn from more than 300\nresearch papers in two dimensions. On one hand, we present a cross-method\nanalysis of representative strategies for addressing panoramic specific\nchallenges across different tasks. On the other hand, we conduct a cross-task\ncomparison and classify panoramic vision into four major categories: visual\nquality enhancement and assessment, visual understanding, multimodal\nunderstanding, and visual generation. In addition, we discuss open challenges\nand future directions in data, models, and applications that will drive the\nadvancement of panoramic vision research. We hope that our work can provide new\ninsight and forward looking perspectives to advance the development of\npanoramic vision technologies. Our project page is\nhttps://insta360-research-team.github.io/Survey-of-Panorama",
        "url": "http://arxiv.org/abs/2509.04444v1",
        "published_date": "2025-09-04T17:59:10+00:00",
        "updated_date": "2025-09-04T17:59:10+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Xin Lin",
            "Xian Ge",
            "Dizhe Zhang",
            "Zhaoliang Wan",
            "Xianshun Wang",
            "Xiangtai Li",
            "Wenjie Jiang",
            "Bo Du",
            "Dacheng Tao",
            "Ming-Hsuan Yang",
            "Lu Qi"
        ],
        "tldr": "This survey paper comprehensively reviews recent panoramic vision techniques, focusing on adapting perspective-based methods to handle the unique challenges of omnidirectional images across various applications and tasks.",
        "tldr_zh": "这篇综述论文全面回顾了最新的全景视觉技术，重点关注如何调整基于透视的方法来处理全方位图像在各种应用和任务中的独特挑战。",
        "relevance_score": 8,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "PAOLI: Pose-free Articulated Object Learning from Sparse-view Images",
        "summary": "We present a novel self-supervised framework for learning articulated object\nrepresentations from sparse-view, unposed images. Unlike prior methods that\nrequire dense multi-view observations and ground-truth camera poses, our\napproach operates with as few as four views per articulation and no camera\nsupervision. To address the inherent challenges, we first reconstruct each\narticulation independently using recent advances in sparse-view 3D\nreconstruction, then learn a deformation field that establishes dense\ncorrespondences across poses. A progressive disentanglement strategy further\nseparates static from moving parts, enabling robust separation of camera and\nobject motion. Finally, we jointly optimize geometry, appearance, and\nkinematics with a self-supervised loss that enforces cross-view and cross-pose\nconsistency. Experiments on the standard benchmark and real-world examples\ndemonstrate that our method produces accurate and detailed articulated object\nrepresentations under significantly weaker input assumptions than existing\napproaches.",
        "url": "http://arxiv.org/abs/2509.04276v1",
        "published_date": "2025-09-04T14:51:03+00:00",
        "updated_date": "2025-09-04T14:51:03+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Jianning Deng",
            "Kartic Subr",
            "Hakan Bilen"
        ],
        "tldr": "This paper introduces a self-supervised framework (PAOLI) for learning articulated object representations from sparse-view images without ground-truth camera poses, achieving accurate and detailed representations under weaker input assumptions than existing methods.",
        "tldr_zh": "本文介绍了一种自监督框架（PAOLI），用于从稀疏视图图像中学习铰接对象表示，无需相机位姿真值。该方法在比现有方法更弱的输入假设下，实现了准确且详细的表示。",
        "relevance_score": 8,
        "novelty_claim_score": 9,
        "clarity_score": 8,
        "potential_impact_score": 7,
        "overall_priority_score": 8
    },
    {
        "title": "TriLiteNet: Lightweight Model for Multi-Task Visual Perception",
        "summary": "Efficient perception models are essential for Advanced Driver Assistance\nSystems (ADAS), as these applications require rapid processing and response to\nensure safety and effectiveness in real-world environments. To address the\nreal-time execution needs of such perception models, this study introduces the\nTriLiteNet model. This model can simultaneously manage multiple tasks related\nto panoramic driving perception. TriLiteNet is designed to optimize performance\nwhile maintaining low computational costs. Experimental results on the BDD100k\ndataset demonstrate that the model achieves competitive performance across\nthree key tasks: vehicle detection, drivable area segmentation, and lane line\nsegmentation. Specifically, the TriLiteNet_{base} demonstrated a recall of\n85.6% for vehicle detection, a mean Intersection over Union (mIoU) of 92.4% for\ndrivable area segmentation, and an Acc of 82.3% for lane line segmentation with\nonly 2.35M parameters and a computational cost of 7.72 GFLOPs. Our proposed\nmodel includes a tiny configuration with just 0.14M parameters, which provides\na multi-task solution with minimal computational demand. Evaluated for latency\nand power consumption on embedded devices, TriLiteNet in both configurations\nshows low latency and reasonable power during inference. By balancing\nperformance, computational efficiency, and scalability, TriLiteNet offers a\npractical and deployable solution for real-world autonomous driving\napplications. Code is available at https://github.com/chequanghuy/TriLiteNet.",
        "url": "http://arxiv.org/abs/2509.04092v1",
        "published_date": "2025-09-04T10:48:25+00:00",
        "updated_date": "2025-09-04T10:48:25+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Quang-Huy Che",
            "Duc-Khai Lam"
        ],
        "tldr": "TriLiteNet is a lightweight multi-task model for autonomous driving perception, achieving competitive performance on vehicle detection, drivable area segmentation, and lane line segmentation with low computational cost, suitable for deployment on embedded devices.",
        "tldr_zh": "TriLiteNet是一个轻量级的多任务模型，用于自动驾驶感知，在车辆检测、可驾驶区域分割和车道线分割方面实现了具有竞争力的性能，且计算成本低，适合部署在嵌入式设备上。",
        "relevance_score": 9,
        "novelty_claim_score": 7,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "SliceSemOcc: Vertical Slice Based Multimodal 3D Semantic Occupancy Representation",
        "summary": "Driven by autonomous driving's demands for precise 3D perception, 3D semantic\noccupancy prediction has become a pivotal research topic. Unlike\nbird's-eye-view (BEV) methods, which restrict scene representation to a 2D\nplane, occupancy prediction leverages a complete 3D voxel grid to model spatial\nstructures in all dimensions, thereby capturing semantic variations along the\nvertical axis. However, most existing approaches overlook height-axis\ninformation when processing voxel features. And conventional SENet-style\nchannel attention assigns uniform weight across all height layers, limiting\ntheir ability to emphasize features at different heights. To address these\nlimitations, we propose SliceSemOcc, a novel vertical slice based multimodal\nframework for 3D semantic occupancy representation. Specifically, we extract\nvoxel features along the height-axis using both global and local vertical\nslices. Then, a global local fusion module adaptively reconciles fine-grained\nspatial details with holistic contextual information. Furthermore, we propose\nthe SEAttention3D module, which preserves height-wise resolution through\naverage pooling and assigns dynamic channel attention weights to each height\nlayer. Extensive experiments on nuScenes-SurroundOcc and nuScenes-OpenOccupancy\ndatasets verify that our method significantly enhances mean IoU, achieving\nespecially pronounced gains on most small-object categories. Detailed ablation\nstudies further validate the effectiveness of the proposed SliceSemOcc\nframework.",
        "url": "http://arxiv.org/abs/2509.03999v1",
        "published_date": "2025-09-04T08:27:54+00:00",
        "updated_date": "2025-09-04T08:27:54+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Han Huang",
            "Han Sun",
            "Ningzhong Liu",
            "Huiyu Zhou",
            "Jiaquan Shen"
        ],
        "tldr": "The paper introduces SliceSemOcc, a novel multimodal 3D semantic occupancy representation framework that uses vertical slices and a new attention mechanism to improve 3D semantic occupancy prediction, particularly for small objects, outperforming existing methods on nuScenes datasets.",
        "tldr_zh": "该论文介绍了SliceSemOcc，一种新的多模态3D语义占据表示框架，它使用垂直切片和一个新的注意力机制来改进3D语义占据预测，尤其是在小物体方面，优于在nuScenes数据集上的现有方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 9,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    },
    {
        "title": "OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction",
        "summary": "In this paper, we propose OccTENS, a generative occupancy world model that\nenables controllable, high-fidelity long-term occupancy generation while\nmaintaining computational efficiency. Different from visual generation, the\noccupancy world model must capture the fine-grained 3D geometry and dynamic\nevolution of the 3D scenes, posing great challenges for the generative models.\nRecent approaches based on autoregression (AR) have demonstrated the potential\nto predict vehicle movement and future occupancy scenes simultaneously from\nhistorical observations, but they typically suffer from \\textbf{inefficiency},\n\\textbf{temporal degradation} in long-term generation and \\textbf{lack of\ncontrollability}. To holistically address these issues, we reformulate the\noccupancy world model as a temporal next-scale prediction (TENS) task, which\ndecomposes the temporal sequence modeling problem into the modeling of spatial\nscale-by-scale generation and temporal scene-by-scene prediction. With a\n\\textbf{TensFormer}, OccTENS can effectively manage the temporal causality and\nspatial relationships of occupancy sequences in a flexible and scalable way. To\nenhance the pose controllability, we further propose a holistic pose\naggregation strategy, which features a unified sequence modeling for occupancy\nand ego-motion. Experiments show that OccTENS outperforms the state-of-the-art\nmethod with both higher occupancy quality and faster inference time.",
        "url": "http://arxiv.org/abs/2509.03887v1",
        "published_date": "2025-09-04T05:06:47+00:00",
        "updated_date": "2025-09-04T05:06:47+00:00",
        "categories": [
            "cs.CV"
        ],
        "authors": [
            "Bu Jin",
            "Songen Gu",
            "Xiaotao Hu",
            "Yupeng Zheng",
            "Xiaoyang Guo",
            "Qian Zhang",
            "Xiaoxiao Long",
            "Wei Yin"
        ],
        "tldr": "The paper introduces OccTENS, a novel generative occupancy world model based on temporal next-scale prediction (TENS) to achieve controllable, high-fidelity, and efficient long-term occupancy generation for 3D scenes, outperforming existing autoregressive methods.",
        "tldr_zh": "该论文介绍了 OccTENS，一种基于时间下一尺度预测 (TENS) 的新型生成式占用世界模型，旨在实现可控、高保真和高效的 3D 场景长期占用生成，优于现有的自回归方法。",
        "relevance_score": 9,
        "novelty_claim_score": 8,
        "clarity_score": 8,
        "potential_impact_score": 8,
        "overall_priority_score": 8
    }
]